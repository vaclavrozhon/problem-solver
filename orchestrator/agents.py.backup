"""
Agent interaction functions for the orchestrator system.

This module contains functions for calling various AI agents (prover, verifier,
summarizer, paper suggester, paper fixer) with appropriate prompts and schemas.
"""

import os
import json
import time
from pathlib import Path
from typing import Optional, Tuple, Any, Dict
from openai import OpenAI

from .models import (
    ProverOutput, VerifierOutput, VerifierCombinedOutput,
    SummarizerOutput, PaperSuggesterOutput, PaperFixerOutput,
    NotesUpdate, OutputUpdate
)
from .utils import (
    load_prompt, write_status, dump_io, normalize_schema_strict,
    extract_json_from_response, compile_tex_string
)
from .papers import read_problem_context


# Initialize OpenAI client
client = OpenAI()

# Get model configuration from environment
MODEL_PROVER = os.environ.get("OPENAI_MODEL_PROVER", "gpt-5")
MODEL_VERIFIER = os.environ.get("OPENAI_MODEL_VERIFIER", "gpt-5")
MODEL_SUMMARIZER = os.environ.get("OPENAI_MODEL_SUMMARIZER", "gpt-5-mini")
MODEL_PAPER_SUGGESTER = os.environ.get("OPENAI_MODEL_PAPER_SUGGESTER", MODEL_PROVER)
MODEL_PAPER_FIXER = os.environ.get("OPENAI_MODEL_PAPER_FIXER", MODEL_PROVER)
TEMPERATURE_PROVER = float(os.environ.get("AR_PROVER_TEMPERATURE", "0.8"))

# New reasoning configuration
REASONING_EFFORT = os.environ.get("AR_REASONING_EFFORT", "high")  # low|medium|high
REASONING_SUMMARY = os.environ.get("AR_REASONING_SUMMARY", "auto")  # auto|detailed|none
MAX_OUTPUT_TOKENS = int(os.environ.get("AR_MAX_OUTPUT_TOKENS", "8192"))


def is_o_model(name: str) -> bool:
    """Check if model is an O-series model."""
    return name.startswith("o1") or name.startswith("o3")


def is_reasoning_model(name: str) -> bool:
    """Check if model supports reasoning controls via Responses API."""
    return any(name.startswith(p) for p in ["o3", "o4", "gpt-5"])


def save_response_id(problem_dir: Path, round_idx: int, agent: str, response_id: str):
    """Save response ID for reasoning state preservation."""
    if not response_id:
        return
    
    response_ids_file = problem_dir / "runs" / f"round-{round_idx:04d}" / "response_ids.json"
    response_ids = {}
    
    if response_ids_file.exists():
        import json
        response_ids = json.loads(response_ids_file.read_text(encoding="utf-8"))
    
    response_ids[agent] = response_id
    response_ids_file.write_text(json.dumps(response_ids, indent=2), encoding="utf-8")


def load_previous_response_id(problem_dir: Path, round_idx: int, agent: str) -> str:
    """Load previous response ID for reasoning state preservation."""
    if round_idx <= 1:
        return None
    
    prev_round_idx = round_idx - 1
    response_ids_file = problem_dir / "runs" / f"round-{prev_round_idx:04d}" / "response_ids.json"
    
    if not response_ids_file.exists():
        return None
    
    try:
        import json
        response_ids = json.loads(response_ids_file.read_text(encoding="utf-8"))
        return response_ids.get(agent)
    except:
        return None


def load_prover_focus_prompts() -> dict:
    """Load prover focus mini-prompts from JSON file."""
    focus_file = Path(__file__).parent.parent / "prompts" / "prover_focus.json"
    if not focus_file.exists():
        return {"default": {"name": "No special instructions", "prompt": ""}}
    
    try:
        return json.loads(focus_file.read_text(encoding="utf-8"))
    except (json.JSONDecodeError, FileNotFoundError):
        return {"default": {"name": "No special instructions", "prompt": ""}}


def complete_text(model: str, system_prompt: str, user_message: str,
                 response_format: Any = None, temperature: float = 0.2, 
                 previous_response_id: str = None, tools: list = None) -> Tuple[str, float, str]:
    """Complete text using OpenAI API with appropriate settings for the model.
    
    Returns: (response_text, duration, response_id) where response_id can be used
    for reasoning state preservation in subsequent calls.
    """
    start_time = time.time()
    
    if is_reasoning_model(model):
        # Use Responses API for reasoning models (GPT-5, O3, O4)
        input_msgs = []
        if system_prompt:
            # For reasoning models, use developer role for system prompts
            input_msgs.append({"role": "developer", "content": system_prompt})
        input_msgs.append({"role": "user", "content": user_message})

        kwargs = {
            "model": model,
            "input": input_msgs,
            "reasoning": {"effort": REASONING_EFFORT, "summary": REASONING_SUMMARY},
            "max_output_tokens": MAX_OUTPUT_TOKENS,
        }
        
        if response_format:
            kwargs["response_format"] = response_format
            
        if previous_response_id:
            kwargs["previous_response_id"] = previous_response_id
            
        if tools:
            kwargs["tools"] = tools

        completion = client.responses.create(**kwargs)

        # Extract text from Responses API format
        text = getattr(completion, "output_text", None)
        if text is None:
            # Fallback: stitch text pieces for older SDK versions
            text = ""
            for item in completion.output:
                if hasattr(item, "content"):
                    for piece in item.content:
                        if hasattr(piece, "text"):
                            text += piece.text
        
        duration = time.time() - start_time
        response_id = getattr(completion, "id", None)
        return text, duration, response_id
        
    elif is_o_model(model):
        # O-series models don't support system prompts or temperature
        combined_message = f"{system_prompt}\n\n{user_message}" if system_prompt else user_message
        messages = [{"role": "user", "content": combined_message}]
        
        completion = client.chat.completions.create(
            model=model,
            messages=messages,
            response_format=response_format
        )
    else:
        # Standard models (GPT-4, etc.)
        messages = []
        if system_prompt:
            messages.append({"role": "system", "content": system_prompt})
        messages.append({"role": "user", "content": user_message})
        
        kwargs = {
            "model": model,
            "messages": messages,
        }
        
        # Only add temperature for non-GPT-5 models
        if not (model.startswith("gpt-5") or model.startswith("gpt-5-")):
            kwargs["temperature"] = temperature
        
        if response_format:
            kwargs["response_format"] = response_format
        
        completion = client.chat.completions.create(**kwargs)
    
    duration = time.time() - start_time
    return completion.choices[0].message.content, duration, None



def apply_notes_update(problem_dir: Path, update: NotesUpdate, round_idx: int = 0) -> None:
    """Call a single prover agent."""
    round_dir = problem_dir / "runs" / f"round-{round_idx:04d}"
    round_dir.mkdir(parents=True, exist_ok=True)
    
    agent = f"prover-{prover_idx:02d}"
    print(f"  [{agent}] Running (model: {MODEL_PROVER})...")
    
    # Load prompts and context
    system_prompt = load_prompt("prover")
    
    # Apply per-prover configuration
    prover_config = prover_config or {}
    has_calculator = prover_config.get("calculator", False)
    focus_type = prover_config.get("focus", "default")
    
    # Add focus instructions to system prompt
    focus_prompts = load_prover_focus_prompts()
    if focus_type in focus_prompts and focus_prompts[focus_type]["prompt"]:
        system_prompt += focus_prompts[focus_type]["prompt"]
    
    # Prepare tools if calculator access is enabled
    tools = None
    if has_calculator:
        tools = ["code_interpreter"]
        
    problem_context = read_problem_context(problem_dir)
    
    # Build user message
    user_parts = [problem_context]
    
    # Add previous rounds' summaries
    for prev_idx in range(1, round_idx):
        summary_file = problem_dir / "runs" / f"round-{prev_idx:04d}" / "summarizer.summary.md"
        if summary_file.exists():
            user_parts.append(f"\n\n=== Round {prev_idx} Summary ===\n")
            user_parts.append(summary_file.read_text(encoding="utf-8"))
    
    # Add verifier feedback from previous round
    if round_idx > 1:
        prev_feedback = problem_dir / "runs" / f"round-{round_idx-1:04d}" / "verifier.feedback.md"
        if prev_feedback.exists():
            user_parts.append(f"\n\n=== Previous Round Feedback ===\n")
            user_parts.append(prev_feedback.read_text(encoding="utf-8"))
    
    user_message = "\n".join(user_parts)
    
    # Prepare response format
    response_format = {
        "type": "json_schema",
        "json_schema": {
            "name": "prover_output",
            "strict": True,
            "schema": normalize_schema_strict(ProverOutput.model_json_schema())
        }
    }
    
    # Load previous response ID for reasoning state preservation
    agent = f"prover-{prover_idx:02d}"
    previous_response_id = load_previous_response_id(problem_dir, round_idx, agent)
    
    # Call the model
    response_text, duration, response_id = complete_text(
        MODEL_PROVER, system_prompt, user_message,
        response_format=response_format,
        temperature=TEMPERATURE_PROVER,
        previous_response_id=previous_response_id,
        tools=tools
    )
    
    # Save response ID for future rounds
    save_response_id(problem_dir, round_idx, agent, response_id)
    
    # Parse response
    response_obj = ProverOutput.model_validate_json(response_text)
    
    # Save outputs
    dump_io(round_dir, agent, system_prompt, user_message,
            response_text, response_obj, duration, MODEL_PROVER)
    
    # Save progress as text
    (round_dir / f"{agent}.text.txt").write_text(
        response_obj.progress_md, encoding="utf-8"
    )
    
    print(f"  [{agent}] Complete ({duration:.1f}s)")
    return response_obj


def call_verifier_combined(problem_dir: Path, round_idx: int, num_provers: int) -> VerifierCombinedOutput:
    """Call verifier for multiple provers."""
    round_dir = problem_dir / "runs" / f"round-{round_idx:04d}"
    
    write_status(problem_dir, "verifier", round_idx)
    print(f"  [verifier] Running (model: {MODEL_VERIFIER})...")
    
    # Load prompts and context
    system_prompt = load_prompt("verifier")
    problem_context = read_problem_context(problem_dir)
    
    # Build user message
    user_parts = [problem_context]
    
    # Add all provers' outputs
    for prover_idx in range(1, num_provers + 1):
        prover_file = round_dir / f"prover-{prover_idx:02d}.text.txt"
        if prover_file.exists():
            user_parts.append(f"\n\n=== Prover {prover_idx:02d} Output ===\n")
            user_parts.append(prover_file.read_text(encoding="utf-8"))
    
    user_message = "\n".join(user_parts)
    
    # Prepare response format
    response_format = {
        "type": "json_schema",
        "json_schema": {
            "name": "verifier_output",
            "strict": True,
            "schema": normalize_schema_strict(VerifierCombinedOutput.model_json_schema())
        }
    }
    
    # Load previous response ID for reasoning state preservation
    agent = "verifier"
    previous_response_id = load_previous_response_id(problem_dir, round_idx, agent)
    
    # Call the model
    response_text, duration, response_id = complete_text(
        MODEL_VERIFIER, system_prompt, user_message,
        response_format=response_format,
        temperature=0.2,
        previous_response_id=previous_response_id
    )
    
    # Save response ID for future rounds
    save_response_id(problem_dir, round_idx, agent, response_id)
    
    # Parse response
    response_obj = VerifierCombinedOutput.model_validate_json(response_text)
    
    # Save outputs
    dump_io(round_dir, "verifier", system_prompt, user_message,
            response_text, response_obj, duration, MODEL_VERIFIER)
    
    # Save feedback and summary as text files
    (round_dir / "verifier.feedback.md").write_text(
        response_obj.feedback_md, encoding="utf-8"
    )
    (round_dir / "verifier.summary.md").write_text(
        response_obj.summary_md, encoding="utf-8"
    )
    
    # Apply notes/output updates if provided
    if response_obj.notes_update:
        apply_notes_update(problem_dir, response_obj.notes_update, round_idx)
    if response_obj.output_update:
        apply_output_update(problem_dir, response_obj.output_update, round_idx)
    
    print(f"  [verifier] Complete ({duration:.1f}s) - Verdict: {response_obj.verdict}")
    return response_obj


def call_summarizer(problem_dir: Path, round_idx: int) -> SummarizerOutput:
    """Call the summarizer agent."""
    round_dir = problem_dir / "runs" / f"round-{round_idx:04d}"
    
    write_status(problem_dir, "summarizer", round_idx)
    print(f"  [summarizer] Running (model: {MODEL_SUMMARIZER})...")
    
    # Load prompts
    system_prompt = load_prompt("summarizer")
    
    # Build user message with round outputs
    user_parts = []
    
    # Add prover outputs
    for prover_file in sorted(round_dir.glob("prover-*.text.txt")):
        prover_name = prover_file.stem.replace('.text', '')
        user_parts.append(f"=== {prover_name} ===\n")
        user_parts.append(prover_file.read_text(encoding="utf-8"))
        user_parts.append("\n")
    
    # Add verifier feedback
    verifier_feedback = round_dir / "verifier.feedback.md"
    if verifier_feedback.exists():
        user_parts.append("=== Verifier Feedback ===\n")
        user_parts.append(verifier_feedback.read_text(encoding="utf-8"))
    
    user_message = "\n".join(user_parts)
    
    # Prepare response format
    response_format = {
        "type": "json_schema",
        "json_schema": {
            "name": "summarizer_output",
            "strict": True,
            "schema": normalize_schema_strict(SummarizerOutput.model_json_schema())
        }
    }
    
    # Load previous response ID for reasoning state preservation
    agent = "summarizer"
    previous_response_id = load_previous_response_id(problem_dir, round_idx, agent)
    
    # Call the model
    response_text, duration, response_id = complete_text(
        MODEL_SUMMARIZER, system_prompt, user_message,
        response_format=response_format,
        temperature=0.3,
        previous_response_id=previous_response_id
    )
    
    # Save response ID for future rounds
    save_response_id(problem_dir, round_idx, agent, response_id)
    
    # Parse response
    response_obj = SummarizerOutput.model_validate_json(response_text)
    
    # Save outputs
    dump_io(round_dir, "summarizer", system_prompt, user_message,
            response_text, response_obj, duration, MODEL_SUMMARIZER)
    
    # Save summary as text
    (round_dir / "summarizer.summary.md").write_text(
        response_obj.summary_md, encoding="utf-8"
    )
    
    print(f"  [summarizer] Complete ({duration:.1f}s)")
    return response_obj


def call_paper_suggester(problem_dir: Path, round_idx: int) -> PaperSuggesterOutput:
    """Call the paper suggester agent."""
    round_dir = problem_dir / "runs" / f"round-{round_idx:04d}"
    round_dir.mkdir(parents=True, exist_ok=True)
    
    write_status(problem_dir, "paper_suggester", round_idx)
    print(f"  [paper_suggester] Running (model: {MODEL_PAPER_SUGGESTER})...")
    
    # Load prompts and context
    system_prompt = load_prompt("paper_suggester")
    problem_context = read_problem_context(problem_dir)
    
    # Build user message
    user_parts = [problem_context]
    
    # Add current draft
    draft_dir = problem_dir / "drafts"
    if draft_dir.exists():
        for draft_file in sorted(draft_dir.glob("*.tex")):
            user_parts.append(f"\n\n=== Current Draft: {draft_file.name} ===\n")
            user_parts.append(draft_file.read_text(encoding="utf-8"))
            break  # Use first/latest draft
    
    # Add final output if exists
    final_tex = problem_dir / "final_output.tex"
    if final_tex.exists():
        user_parts.append("\n\n=== Current Final Output ===\n")
        user_parts.append(final_tex.read_text(encoding="utf-8"))
    
    user_message = "\n".join(user_parts)
    
    # Prepare response format
    response_format = {
        "type": "json_schema",
        "json_schema": {
            "name": "paper_suggester_output",
            "strict": True,
            "schema": normalize_schema_strict(PaperSuggesterOutput.model_json_schema())
        }
    }
    
    # Load previous response ID for reasoning state preservation
    agent = "paper_suggester"
    previous_response_id = load_previous_response_id(problem_dir, round_idx, agent)
    
    # Call the model
    response_text, duration, response_id = complete_text(
        MODEL_PAPER_SUGGESTER, system_prompt, user_message,
        response_format=response_format,
        temperature=0.3,
        previous_response_id=previous_response_id
    )
    
    # Save response ID for future rounds
    save_response_id(problem_dir, round_idx, agent, response_id)
    
    # Parse response
    response_obj = PaperSuggesterOutput.model_validate_json(response_text)
    
    # Save outputs
    dump_io(round_dir, "paper_suggester", system_prompt, user_message,
            response_text, response_obj, duration, MODEL_PAPER_SUGGESTER)
    
    print(f"  [paper_suggester] Complete ({duration:.1f}s)")
    return response_obj


def call_paper_fixer(problem_dir: Path, round_idx: int, 
                     suggester: Optional[PaperSuggesterOutput]) -> Tuple[PaperFixerOutput, bool, str]:
    """Call the paper fixer/writer agent."""
    round_dir = problem_dir / "runs" / f"round-{round_idx:04d}"
    
    write_status(problem_dir, "paper_fixer", round_idx)
    print(f"  [paper_fixer] Running (model: {MODEL_PAPER_FIXER})...")
    
    # Load prompts and context
    system_prompt = load_prompt("paper_fixer")
    problem_context = read_problem_context(problem_dir)
    
    # Build user message
    user_parts = [problem_context]
    
    # Add suggester advice if available
    if suggester:
        user_parts.append("\n\n=== Paper Suggester Advice ===\n")
        user_parts.append(suggester.advice_md)
        if suggester.priority_items:
            user_parts.append("\n\nPriority Items:")
            for item in suggester.priority_items:
                user_parts.append(f"- {item}")
    
    # Add current draft
    draft_dir = problem_dir / "drafts"
    if draft_dir.exists():
        for draft_file in sorted(draft_dir.glob("*.tex")):
            user_parts.append(f"\n\n=== Current Draft: {draft_file.name} ===\n")
            user_parts.append(draft_file.read_text(encoding="utf-8"))
            break
    
    # Add final output if exists
    final_tex = problem_dir / "final_output.tex"
    if final_tex.exists():
        user_parts.append("\n\n=== Current Final Output ===\n")
        user_parts.append(final_tex.read_text(encoding="utf-8"))
    
    user_message = "\n".join(user_parts)
    
    # Prepare response format
    response_format = {
        "type": "json_schema",
        "json_schema": {
            "name": "paper_fixer_output",
            "strict": True,
            "schema": normalize_schema_strict(PaperFixerOutput.model_json_schema())
        }
    }
    
    # Load previous response ID for reasoning state preservation
    agent = "paper_fixer"
    previous_response_id = load_previous_response_id(problem_dir, round_idx, agent)
    
    # Call the model
    response_text, duration, response_id = complete_text(
        MODEL_PAPER_FIXER, system_prompt, user_message,
        response_format=response_format,
        temperature=0.2,
        previous_response_id=previous_response_id
    )
    
    # Save response ID for future rounds
    save_response_id(problem_dir, round_idx, agent, response_id)
    
    # Parse response
    response_obj = PaperFixerOutput.model_validate_json(response_text)
    
    # Save outputs
    dump_io(round_dir, "paper_fixer", system_prompt, user_message,
            response_text, response_obj, duration, MODEL_PAPER_FIXER)
    
    # Try to compile the LaTeX
    compile_success = False
    compile_log = ""
    
    if response_obj.new_tex:
        write_status(problem_dir, "paper_compile", round_idx)
        print(f"  [paper_compile] Attempting to compile LaTeX...")
        
        compile_success, compile_log, pdf_path = compile_tex_string(
            problem_dir, round_idx, response_obj.new_tex
        )
        
        # Save compile result
        compile_result = {"ok": compile_success}
        (round_dir / "paper.compile.result.json").write_text(
            json.dumps(compile_result, indent=2), encoding="utf-8"
        )
        
        if compile_success:
            print(f"  [paper_compile] Success! PDF generated")
        else:
            print(f"  [paper_compile] Failed - see compile.log")
    
    print(f"  [paper_fixer] Complete ({duration:.1f}s)")
    return response_obj, compile_success, compile_log


def apply_notes_update(problem_dir: Path, update: NotesUpdate, round_idx: int = 0) -> None:
    """Apply an update to the notes.md file and save versioned copy."""
    notes_file = problem_dir / "notes.md"
    
    # Save current version before updating (if it exists)
    if notes_file.exists() and round_idx > 0:
        versions_dir = problem_dir / "versions"
        versions_dir.mkdir(exist_ok=True)
        version_file = versions_dir / f"notes-round-{round_idx:04d}.md"
        current_content = notes_file.read_text(encoding="utf-8")
        version_file.write_text(current_content, encoding="utf-8")
    
    if update.mode == "append":
        current = notes_file.read_text(encoding="utf-8") if notes_file.exists() else ""
        notes_file.write_text(current + "\n\n" + update.content_md, encoding="utf-8")
    elif update.mode == "replace":
        notes_file.write_text(update.content_md, encoding="utf-8")


def apply_output_update(problem_dir: Path, update: OutputUpdate, round_idx: int = 0) -> None:
    """Apply an update to the output.md file and save versioned copy."""
    output_file = problem_dir / "output.md"
    
    # Save current version before updating (if it exists)
    if output_file.exists() and round_idx > 0:
        versions_dir = problem_dir / "versions"
        versions_dir.mkdir(exist_ok=True)
        version_file = versions_dir / f"output-round-{round_idx:04d}.md"
        current_content = output_file.read_text(encoding="utf-8")
        version_file.write_text(current_content, encoding="utf-8")
    
    if update.mode == "append":
        current = output_file.read_text(encoding="utf-8") if output_file.exists() else ""
        output_file.write_text(current + "\n\n" + update.content_md, encoding="utf-8")
    elif update.mode == "replace":
        output_file.write_text(update.content_md, encoding="utf-8")