--- SYSTEM ---
You are a research **summarizer**. Summarize this *last round only* for a human log.

Include:
- 3–7 bullet **Highlights** (facts learned, reductions, counterexamples, partial bounds).
- 1–3 **Open questions / TODOs** for the next round.
- 1–3 **Citations to files** you relied on (e.g. round files or papers by name).

**Return strictly JSON**:
{
  "summary_md": "Readable summary of the round (≤200 words)",
  "highlights": [],
  "next_questions": []
}

--- USER ---
=== prover-01.out.json (progress_md) ===
## Round 0001 — 2025-08-27T10:32:01.431305Z

Mini-plan
- Specialize the feasible-function framework of the paper to radius-1, β-normalized LCLs on globally oriented paths, and isolate a finite-state summary (types) that is exponential but not doubly exponential in β.
- Show we can deterministically compute all reachable types and a successor function δ(Type, bit) in time 2^{poly(β)} by dynamic programming on boundary-extendibility tables.
- Define, for each type t, a macro relation R_t ⊆ Σ_out × Σ_out that summarizes “what boundary output labels can be bridged across a block of type t”.
- Give a NEXPTIME (indeed EXPTIME) decision procedure for the ω(log* n)—o(n) gap by nondeterministically guessing a small table F on contexts of the form (leftType, 2-bit S, rightType) and verifying extendibility using the R_t relations.
- Give a NEXPTIME decision procedure for the ω(1)—o(log* n) gap by nondeterministically guessing boundary quadruples per type (first/second/second-last/last outputs) and verifying the wrap-around and bridging conditions via R_t and local checks.
- Validate with toy examples and note failure modes; identify concrete next steps to formalize the type-update computation and the verification routines.

Context and notation (radius 1, β-normalized)
- Inputs: Σ_in = {0,1}. Outputs: Σ_out with |Σ_out| = β. Legality: (in, out) ∈ C_in–out and (out, out′) ∈ C_out–out on each directed edge.
- Let E ⊆ Σ_out × Σ_out denote the directed edge relation from C_out–out. Let A_0, A_1 ⊆ Σ_out denote allowed outputs per input bit (rows of C_in–out).
- For a (long) input block P (string over {0,1}), write its type Type(P) as in the paper: r = 1 ⇒ D1 are the two endpoints; D2 are their immediate neighbors; extendibility is captured by which assignments of output labels to the four boundary nodes D1 ∪ D2 can be extended to a full legal labeling on the interior D3.

Claim 1 (Type-count bound specialized to r = 1)
Statement. The number of distinct types (equivalence classes ⋆∼) is at most ℓ_pump ≤ 16 · 2^{β^4}.
Why useful here. This bounds the state space of our finite abstraction by a single-exponential function of β, enabling 2^{poly(β)} algorithms.
Sketch. For r = 1, D1 ∪ D2 contains 4 boundary nodes. There are 2^4 choices of boundary input bits. For each, the extendibility predicate is a Boolean function over assignments of 4 boundary output labels, i.e., a subset of Σ_out^4, hence at most 2^{β^4} possibilities. Multiplying gives ≤ 16 · 2^{β^4}.
How it can fail. If radius r > 1 were needed, the exponent would grow as β^{4r}, but still single-exponential for constant r. No failure for r=1.
Quick test. With β=2, bound is 16·2^{16} ≈ 1M types (a rough upper bound); fine for theoretical complexity, impractical to enumerate in practice but enough for 2^{poly(β)} complexity.

Claim 2 (Deterministically computing all reachable types and δ in 2^{poly(β)} time)
Statement. We can compute the set T of all types reachable from length-≤4 words under appending bits, together with a deterministic successor function δ: T × {0,1} → T, in deterministic time 2^{poly(β)}.
Why useful here. Avoids enumerating 2^{ℓ_pump} strings. Once T and δ are known, all further checks quantify only over T (singly exponential), not over words (doubly exponential).
Construction and verification.
- Represent a type t ∈ T by (Lbits, Rbits, Ext_t), where Lbits, Rbits ∈ {0,1}^2 are the 2-bit prefix/suffix of any representative, and Ext_t ⊆ Σ_out^4 is the set of boundary 4-tuples (o_L1, o_L2, o_R2, o_R1) that are extendible to a full legal labeling inside.
- Initialization. Enumerate all input strings P of length k ∈ {1,2,3,4} (≤ 16 of them). For each, compute Ext_{Type(P)} exactly by dynamic programming on the tiny interior: enumerate all 4-tuples of boundary outputs consistent with C_in–out on boundary nodes and E on boundary edges; check extendibility of the (empty or short) interior by brute force (cost β^{O(4)} per P). Insert the resulting canonical type into a dictionary keyed by (Lbits, Rbits, Ext_t).
- Update (δ). Given t = (Lbits, Rbits, Ext_t) and a ∈ {0,1}, we can compute t′ = δ(t, a) as follows: t′ has Lbits′ = first two bits of (Lbits concatenated if current length<2 else unchanged), Rbits′ = updated last-two-bits after appending a, and Ext_{t′} is obtained by a constant-size “boundary-augmentation” DP: a boundary 4-tuple (o_L1, o_L2, o_R2′, o_R1′) is in Ext_{t′} iff there exist o_R2, o_R1 and an output label o_new for the appended node such that
  (i) (o_L1, o_L2, o_R2, o_R1) ∈ Ext_t,
  (ii) membership constraints at the new last node: o_new ∈ A_a,
  (iii) E-edges: (o_R1, o_new) ∈ E, and then set o_R2′=o_new (the new second-last), o_R1′ = ?
  Careful: after appending one node, the new last two boundary outputs are (old last, new), so interpret indexes accordingly. Full derivation requires casework for short representatives (k < 4), but all cases are constant and handled locally.
- BFS enumeration. Starting from the set of types derived for k ≤ 4, add δ(t,0) and δ(t,1) for each discovered t until closure. Each δ-computation runs in β^{O(4)} time (constant-factor DP over Σ_out^4). The number of types discovered is ≤ ℓ_pump. Thus total time ≤ ℓ_pump · poly(β) = 2^{poly(β)}.
How it can fail. The update must be consistent for small-k degeneracies (k < 4) where D1∪D2 overlaps; if mishandled, one might compute inconsistent Ext sets. We mitigate by bootstrapping directly from exact computation for k ≤ 4 and carefully distinguishing cases in the update.
Quick test. For a trivial LCL where E is complete and A_0=A_1=Σ_out, Ext_t=Σ_out^4 for all t. Then δ creates only 16 distinct types (Lbits,Rbits, full Ext). BFS closes after O(1) expansions—consistent and fast.

Claim 3 (Macro bridging relation R_t)
Statement. For each type t, we can compute a relation R_t ⊆ Σ_out × Σ_out such that (x, y) ∈ R_t iff there exists a legal labeling of some block P of type t that bridges the outer boundary labels x → y, in the sense: there exist boundary outputs (o_L1,o_L2,o_R2,o_R1) ∈ Ext_t with E-edges (x, o_L1) and (o_R1, y).
Why useful here. R_t summarizes the net effect of an unlabeled block of type t on the neighboring outside outputs. It collapses an entire block to a β×β boolean matrix for later composition checks.
Computation. For each t and each (x,y) ∈ Σ_out^2, test whether ∃ (o_L1,o_L2,o_R2,o_R1) ∈ Ext_t such that (x, o_L1) ∈ E and (o_R1, y) ∈ E. This costs O(β^4) per (x,y), hence O(β^6) per t.
How it can fail. If we misinterpret boundary directions (left/right), the composition in later checks breaks. We fix orientation globally: x is the output immediately before the block on the left, y is the output immediately after the block on the right.
Quick test. If E has a self-loop at c and c ∈ A_0∩A_1 at all boundary nodes, then (c,c) ∈ R_t for all t, since boundary tuple (c,c,c,c) is valid: sanity check.

Claim 4 (NEXPTIME decision for ω(log* n)—o(n): existence of a context-local f on 2-node windows)
Statement. There is a nondeterministic algorithm running in time 2^{poly(β)} that decides whether a feasible function f (in the sense of Section 4.2 of the paper) exists; equivalently, whether the LCL has deterministic LOCAL complexity o(n) instead of Ω(n).
Why useful here. This separates Ω(n) from o(n) in NEXPTIME, as requested.
Definition (specialized). For r=1, f takes as input a triple (t_L, S, t_R) where t_L, t_R ∈ T are types of the left/right contexts, and S ∈ {0,1}^2 is the 2-bit middle window; f outputs L_S ∈ Σ_out^2 for S, with (i) (S[1], L_S[1]) ∈ C_in–out, (S[2], L_S[2]) ∈ C_in–out, and (ii) (L_S[1], L_S[2]) ∈ E.
Verification condition (extendibility). For all quadruples (t_a, t_b, t_c, t_d) ∈ T^4, there exist orientation choices for S1,S2 (i.e., optionally reverse them; this toggles which side of L_S we use as boundary) such that, writing s1_last as the output of the last node of S1 under f and s2_first as the output of the first node of S2 under f, we have
  ∃ m ∈ Σ_out: (s1_last, m) ∈ R_{t_b} and (m, s2_first) ∈ R_{t_c}.
Algorithm.
1) Deterministically compute T, δ, and all R_t (Claim 2 and 3) in 2^{poly(β)} time.
2) Nondeterministically guess a table F: T × {0,1}^2 × T → Σ_out^2 assigning outputs to the 2-node window for every context. Enforce local per-node legality and (L_S[1],L_S[2]) ∈ E.
3) Deterministically verify the extendibility condition above for all |T|^4 quadruples (trying the 4 orientation choices for S1,S2 and scanning m ∈ Σ_out).
Complexity. |T| ≤ ℓ_pump ≤ 16·2^{β^4}. Step 3 costs O(|T|^4 · β), hence 2^{poly(β)}. The guess size |F| is |T|^2·4 entries, each an ordered pair in Σ_out^2: total certificate size 2^{poly(β)}.
Correctness sketch. This is the paper’s feasible-function notion specialized to r=1; replace arbitrary words by their types (Lemma 10/11), and label only the 2-node windows. The R_t composition exactly captures the existence of fillings for the unlabeled blocks w_b and w_c between S1 and S2. If such F exists, the o(n) construction of Section 4.2 yields an O(log* n)-round algorithm; if not, the problem has complexity Ω(n).
How it can fail. Two pitfalls: (i) Orientation/reversal: when we choose to label S1 or SR_1 we must use the appropriate context (swap t_a and t_b, and reverse the 2-bit S; our check enumerates both options). (ii) Using types: one must ensure the type-invariance lemma (Lemma 10/11) applies; here it does, since we only rely on boundary extendibility.
Quick test. Consider an LCL where A_0 ∩ A_1 = ∅ and E disallows any edge between outputs allowed by 0 and those by 1. Then R_t is empty whenever the block contains both 0 and 1 near its boundary, and no F can satisfy the bridging, so we correctly conclude Ω(n).

Claim 5 (NEXPTIME decision for ω(1)—o(log* n): existence of wrap-around boundary quadruples per type)
Statement. There is a nondeterministic algorithm running in time 2^{poly(β)} that decides whether the LCL has complexity O(1) vs Ω(log* n), assuming it is o(n) (i.e., Claim 4’s test passed).
Why useful here. This separates the two sublinear regimes in NEXPTIME, completing the trichotomy decision.
Witness to guess.
- For each type t ∈ T, guess a boundary quadruple Q_t = (L1_t, L2_t, R2_t, R1_t) ∈ Σ_out^4 intended to be the outputs on the first two and last two nodes of any block labeled by the constant-time scheme (cf. Section 4.4 of the paper).
- Reuse the table F from Claim 4 (or guess a fresh f for 2-node windows; both lead to 2^{poly(β)} guesses).
Checks.
(i) Local and wrap-around validity per type. For each t with boundary input bits (ℓ1,ℓ2) at the left and (r2,r1) at the right, enforce
  - L1_t ∈ A_{ℓ1}, L2_t ∈ A_{ℓ2}, R2_t ∈ A_{r2}, R1_t ∈ A_{r1};
  - (L1_t, L2_t) ∈ E, (R2_t, R1_t) ∈ E;
  - (L1_t, L2_t, R2_t, R1_t) ∈ Ext_t (so the interior is fillable);
  - wrap-around across repetitions: (R1_t, L1_t) ∈ E.
(ii) Bridging via 2-node windows. For all pairs (t_b, t_c) ∈ T^2 and for all S ∈ {00,01,10,11}, there exist 2-node outputs L_S (these can be taken from F(t_b, S, t_c) if we reuse F) satisfying per-node legality and (L_S[1], L_S[2]) ∈ E, such that
  ∃ m ∈ Σ_out: (R1_{t_b}, m) ∈ R_{t_b} and (m, L1_{t_c}) ∈ R_{t_c}, with the boundaries of S chosen appropriately (last of S feeds into t_b, first of S feeds from t_c). Equivalently, there exists a labeling of S whose last/first outputs connect to the block boundary outputs via R_{t_b} and R_{t_c}.
Complexity. The number of checks is O(|T| + |T|^2·4) = 2^{poly(β)}. Each check costs poly(β). Guess size is O(|T|·log β^4) plus the 2-node window table as in Claim 4.
Correctness sketch. This mirrors Section 4.4–4.5: the quadruple Q_t guarantees we can tile long periodic parts by repeating the chosen boundary scheme (wrap-around holds), and the 2-node windows with F stitch the remaining short/irregular parts. The R_t matrices certify fillability of the unlabeled blocks adjacent to the 2-node windows. If such witnesses exist, the O(1)-round construction follows; otherwise Ω(log* n).
How it can fail. Subtlety: for very short representatives (length < 4), Ext_t must be computed accurately so that (L1_t, L2_t, R2_t, R1_t) ∈ Ext_t truly guarantees interior extendibility. This is covered by the initialization in Claim 2.
Quick test. Suppose E has a self-loop at c and c ∈ A_0 ∩ A_1. Pick Q_t = (c,c,c,c) for all t and L_S = (c,c) for all S. Then all constraints pass, classifying the problem as O(1), which is correct (output c everywhere).

Toy examples
- Example A (No constant-time solution, Ω(n)). Let β=2, Σ_out={a,b}, A_0={a}, A_1={b}, and E = {(a,b), (b,a)} (alternating). Then any legal labeling forces alternation matching the input’s parity; stitching arbitrary 2-node windows across arbitrary blocks fails because R_t is empty for many contexts. Claim 4 rejects F, giving Ω(n).
- Example B (Constant-time trivial). Let A_0 = A_1 = Σ_out and E complete. Then O(1) holds with Q_t=(a,a,a,a) and L_S=(a,a) (for any fixed a). Claims 4 and 5 accept.

Why the naive algorithm is doubly exponential and how we avoid it
- Naively, checking feasibility would range over all words w with |w| ≤ ℓ_pump; since ℓ_pump itself is exponential in β, the total number of words is doubly exponential in β. The paper’s decidability argument (“feasible function exists”) is based on types but, if implemented by enumerating domain strings of f, becomes 2^{2^{poly(β)}}.
- Our approach collapses all universal quantification over words to universal quantification over types, whose number is ℓ_pump ≤ 2^{poly(β)}. All witnesses (F, boundary quadruples) are tables indexed by types (and the 4 possible 2-bit S), of size 2^{poly(β)}. All checks boil down to iterating over |T|^{O(1)} combinations and scanning β-sized alphabets—total time 2^{poly(β)}.
- Moreover, computing T and δ deterministically by DP on boundary-extendibility tables avoids guessing exponentially many representatives.

Complexity summary (in terms of input size N = poly(β))
- Compute T, δ, Ext_t, and R_t: time 2^{poly(β)} = 2^{poly(N)}.
- Stage 1 (Ω(n) vs o(n)): nondeterministically guess F of size 2^{poly(N)} and verify in time 2^{poly(N)}.
- Stage 2 (O(1) vs Ω(log* n)): nondeterministically guess Q_t (and optionally reuse F) of size 2^{poly(N)} and verify in time 2^{poly(N)}.
Hence: a NEXPTIME (indeed EXPTIME) trichotomy decision procedure for β-normalized, radius-1 LCLs on globally oriented paths.

Limits and open technical points (to be finalized)
- Type update (δ) for short representatives. We outlined casework; formalizing the exact DP for k ∈ {1,2,3} is needed to ensure correctness at the base.
- Reversal handling. When allowing reversed 2-node windows, we must consistently use the reversed type t^R (swap L/R bits and reverse boundary order). Precomputing t ↦ t^R is straightforward from Ext_t.
- Proof detail that the chosen Q_t and F suffice to reconstruct the O(1) algorithm exactly as in the paper’s Section 4.5. Our checks mirror their feasible-function definition; we will write the full reduction argument for completeness.

Next concrete steps
- Formalize the dynamic programming that computes Ext_{t′} from Ext_t and a new bit a (cleanly handling all small-k cases), and write a proof of its correctness (specialized Lemma 12 made algorithmic).
- Implement the computation of T, δ and R_t (pseudo-code sketch), and quantify exact polynomial exponents in β for time/memory.
- Write the precise verification routines for Stage 1 and Stage 2, including the reversal operator on types and on the 2-node window table F.
- Exercise the routines on a few canonical LCLs (e.g., 2-coloring over oriented paths, trivial constant labeling, parity constraints) to sanity-check the checks.


=== prover-02.out.json (progress_md) ===
## Round 0001 — 2025-08-27T10:32:01.431496Z

Mini-plan
- Formalize an exponential-size, finite state abstraction ("types") tailored to radius-1, β-normalized LCLs on globally oriented paths; give an explicit constructive algorithm to enumerate types in time 2^{poly(N)}.
- Recast the paper’s “feasible function” definitions purely in terms of these types, so that the witness size is 2^{poly(N)} and feasibility checking is deterministic 2^{poly(N)}.
- Design a nondeterministic exponential-time (NEXPTIME) procedure to distinguish Ω(n) vs o(n) by guessing a type-indexed feasible function f and verifying its universal constraints by dynamic-programming/composition over types.
- Sketch an NEXPTIME procedure to distinguish O(1) vs Ω(log* n) by guessing a succinct witness of constant-time solvability and checking it via the same type machinery. Identify and minimize what must be guessed to keep the certificate within 2^{poly(N)}.
- Stress-test key steps on toy instances (tiny β) and list potential failure modes (e.g., size blow-ups, missing symmetry cases) with quick checks.

Context recap and normalization
- Input: β-normalized radius-1 LCL on globally oriented paths, encoded by two Boolean tables: C_in–out ⊆ {0,1} × Σ_out and C_out–out ⊆ Σ_out × Σ_out. Encoding size N = Θ(β^2) (dominant term β×β out–out table plus 2×β in–out rows). Goal: decide local complexity class ∈ {O(1), Θ(log* n), Θ(n)}.
- Known (Balliu et al.): on labeled paths/cycles, deterministic complexity is in {O(1), Θ(log* n), Θ(n)}; decidability shown via “types”/pumping and feasible functions, but naive runtime can be doubly exponential in β by ranging over strings of length ℓ_pump (itself exponential in β).
- Our aim: reorganize everything at the level of types (finite-state summaries), never at the level of strings of length ℓ_pump; then guessing the witness (a mapping on types) takes 2^{poly(N)} bits/time; verifying universally quantified constraints over all contexts reduces to 2^{poly(N)} checks, each doable in poly(β)·|T| time. This yields NEXPTIME overall.

Core technical object: type summaries for radius 1
- Radius r = 1. For any input-labeled directed path P, define D1 = {first, last node} and D2 = {second, second-last}. Define a summary Summ(P) consisting of:
  - Boundary inputs b_in(P) = (i1, i2, i3, i4) ∈ {0,1}^4 for the nodes in D1 ∪ D2, in order (leftmost to rightmost).
  - A relation R_P ⊆ Σ_out^4 over boundary outputs (o1,o2,o3,o4), which holds iff “there exists a complete output labeling on the interior (D3) s.t. all nodes in D2∪D3 are locally consistent, and all edges internal to P touching D2 are locally consistent.” This is exactly the paper’s extendibility notion with boundary fixed on D1∪D2.
- Two paths have the same type iff their (b_in, R) are identical. The number of possible types is ≤ 2^{|Σ_out|^{4}} · |Σ_in|^{4} = 2^{β^4} · 16. This matches Lemma 13 (with r=1). Importantly, types form a finite set T of size τ ≤ 16·2^{β^4} = 2^{poly(N)}.
- Composition primitives (needed later):
  1) Appending one input bit α ∈ {0,1}. If Summ(P) = (i1,i2,i3,i4, R) and P’ = P·α, then Summ(P’) = (i1,i2,i4,α, R’) where for any boundary outputs (o1’,o2’,o3’,o4’), we have (o1’,o2’,o3’,o4’) ∈ R’ iff ∃y ∈ Σ_out s.t. (o1’,o2’,o3’,y) ∈ R, (y,o4’) ∈ C_out–out, and (α,o4’) ∈ C_in–out. This is computable by iterating over β^5 possibilities, hence poly(β) time per boundary quadruple; R’ has size β^4 entries.
  2) Concatenation B = P·Q. If Summ(P)=(i1,i2,i3,i4,R_P), Summ(Q)=(j1,j2,j3,j4,R_Q), and the actual concatenation has those boundary inputs (which hold by construction when P and Q are actual representatives we will compute), then Summ(B) carries boundary inputs (i1,i2,j3,j4) and relation R_B consists of all (o1,o2,o3’,o4’) for which ∃(x3,x4,x1’,x2’) with (o1,o2,x3,x4)∈R_P, (x1’,x2’,o3’,o4’)∈R_Q, and (x4,x1’) ∈ C_out–out (check only the single joint edge; all other local conditions are already encoded in R_P and R_Q). This can be computed in time roughly O(β^8) naively, still poly(β) for fixed β.

Algorithmic building of types in exp time (deterministic)
Claim A (constructibility): We can enumerate all reachable type summaries T and their transitions (append 0/1) in deterministic time 2^{poly(N)}.
- Construction: Initialize S with the 16 types of length-4 strings w ∈ {0,1}^4 by direct evaluation of Summ(w). For each s ∈ S and α ∈ {0,1}, compute s’ = δ(s,α) using Appending-1-bit primitive; if s’ new, add to S; iterate to fixpoint. This is a standard DFA closure over inputs {0,1} in state space of size ≤ τ.
- Costs: There are ≤ τ states, each yields ≤ 2 successors; each δ computation requires O(β^5)·β^4 = poly(β)·β^4 boolean assignments processing to fill R’ (β^4 table). Overall time and memory ≤ τ·poly(β)·β^4 = 2^{poly(N)}. Hence EXPTIME (deterministic). We will use this construction as a preprocessing step; it avoids handling strings of length ℓ_pump explicitly.
- Why useful here: Everything we must check later depends only on types and their relations; this preprocessing gives us a finite, explicit universe T with computable composition.
- How it can fail: Subtlety around paths of length < 4. We avoided it by seeding S with all length-4 strings and then only appending (growing) strings; definitions of Summ for length 4 are exact (D3 empty). Quick test: For β=2, compute all 16 initial summaries and one δ step; check counts stay ≤ τ bound and relations are computed as specified.

Deciding Ω(n) vs o(n): NEXPTIME via type-indexed feasible function f
Background: Paper’s Lemma 17–18: o(n) iff there exists a feasible function f that locally pre-labels certain 2-node separators S (here |S|=2 since r=1) so that the remaining parts can always be extended.
Our type-level specialization (globally oriented paths: no reversal needed):
- Domain of f: triples (tL, s_in, tR) where tL, tR ∈ T and s_in ∈ {0,1}^2 is the input pattern on the 2-node separator S. Range: a 2-tuple of outputs in Σ_out^2 prescribing the outputs on the two nodes of S.
- Size of domain: |T|^2 · 4 = 2^{poly(N)}. So f can be stored in 2^{poly(N)} bits.
- Universal feasibility condition to verify for a guessed f:
  For all t_a, t_b, t_c, t_d ∈ T and s1, s2 ∈ {0,1}^2, consider the input-labeled path
    P = rep(t_a) · S1 · rep(t_b) · rep(t_c) · S2 · rep(t_d),
  where rep(t) is any fixed representative string realizing type t (we pick those during preprocessing when we first discover t). Let the outputs on S1 and S2 be f(t_a, s1, t_b) and f(t_c, s2, t_d) respectively. Then we require that there exists an output assignment for the middle block B = rep(t_b) · rep(t_c) such that:
  (i) both S1 nodes are locally consistent (C_in–out and the single crossing edge to B),
  (ii) all nodes of B are locally consistent,
  (iii) both S2 nodes are locally consistent (C_in–out and the crossing edge from B).
- How to check exist(B): compute Summ(B) by concatenating Summ(rep(t_b)) and Summ(rep(t_c)) via the concatenation primitive. Let αL be the rightmost output of S1 (the second component of f(t_a,s1,t_b)), and αR the leftmost output of S2 (the first component of f(t_c,s2,t_d)). Then exist(B) holds iff there exist outputs (o1,o2,o3,o4) on B’s D1∪D2 such that (o1,o2,o3,o4)∈R_B, (αL,o1)∈C_out–out, (o4,αR)∈C_out–out, and (input-consistency of the 2 nodes of S1 and S2) holds. This is a finite search over β^4 tuples (o1..o4), i.e., poly(β).
- NEXPTIME procedure (stage 1):
  1) Deterministically build T and, for each t∈T, a representative rep(t), plus Summ(rep(t)).
  2) Nondeterministically guess f: T×{0,1}^2×T → Σ_out^2. Guessing costs |T|^2·4·2logβ bits.
  3) Deterministically verify the universal feasibility condition across all |T|^4·16 cases (composition per case in time poly(β) and looking up precomputed summaries). If all pass, conclude o(n); else Ω(n).
- Complexity: Step (1) runs in 2^{poly(N)} time deterministically. Step (2) guessing uses ≤ 2^{poly(N)} bits/time. Step (3) runs in |T|^4·poly(β) ≤ 2^{poly(N)} time. Hence nondeterministic 2^{poly(N)} overall. This achieves NEXPTIME for separating Ω(n) vs o(n).
- How it can fail: We used a representative rep(t_b)·rep(t_c) for B, but the feasible-function property quantifies over all strings of those types. This is safe by Lemma 10/11: extendibility on D2∪D3 is preserved when replacing a subpath with another of the same type, for any boundary assignment on D1∪D2. Our check existentially quantifies over B’s boundary outputs (and only constrains the crossing edges to S1/S2), exactly the setting where type replacement is sound.
- Quick toy check: β=2, choose trivial constraints that allow any (in,out) and any (out,out). Then T collapses to a tiny set, and any f passes; algorithm outputs o(n) (indeed the problem is solvable in 0 rounds). If we force a “global bit copy to all nodes” constraint (as in the PSPACE-hard encoding, but with tiny B), the check should fail for any f, yielding Ω(n).

Deciding O(1) vs Ω(log* n): toward a type-succinct constant-time witness
We need a 2^{poly(N)}-size witness for O(1) (avoid listing a full periodic output string for every length-≤ℓ_pump w).
Key observation from the O(1) framework (Section 4.4–4.5 of the paper):
- The O(1) algorithm only uses boundary behavior of the repetitive patterns: it pre-labels 2-node separators inside “repetitive” blocks and uses type pumping to argue about extendibility. Concretely, when verifying that the pattern w can be tiled, what matters is the existence of consistent boundary outputs for copies of rep(t), not the entire output string.
- We can therefore witness O(1) by giving, for each type t∈T, a small boundary interface consisting of four outputs:
  • pref2(t) ∈ Σ_out^2 for the first two nodes of rep(t), and
  • suff2(t) ∈ Σ_out^2 for the last two nodes of rep(t),
  such that (a) the boundary quadruple (pref2(t), suff2(t)) is in R_{rep(t)}, and (b) the “join” edge (suff2(t)[2], pref2(t)[1]) lies in C_out–out so two adjacent copies can be glued. This captures the periodic tiling constraint without spelling the whole interior.
- This is sufficient for the two obligations in the paper’s feasible function for O(1):
  (i) Tiling consistency: For each t, existence of (pref2,suff2) in R_{rep(t)} with (suff2[2],pref2[1]) allowed ensures that arbitrarily many copies of rep(t) can be locally labeled so that all nodes in the interior copies are locally consistent. We do not need to fix the interior outputs; Summ(rep(t)) witnesses their existence.
  (ii) Bridging through any S: For t1,t2 and any middle S (represented by some tS), we look only at the last 2 outputs of the block to the left and first 2 outputs of the block to the right. Those are suff2(t1) and pref2(t2). The existence of a consistent labeling of S is exactly: pick Summ(rep(tS)) and check whether ∃(o1,o2,o3,o4)∈R_{rep(tS)} with (suff2(t1)[2], o1)∈C_out–out and (o4,pref2(t2)[1])∈C_out–out. This is the same pattern as in the o(n) verifier.
- NEXPTIME procedure (stage 2):
  1) Reuse T and Summ(rep(t)).
  2) Nondeterministically guess, for each t ∈ T, two 2-tuples pref2(t), suff2(t) ∈ Σ_out^2. Certificate size: |T|·4·logβ = 2^{poly(N)} bits.
  3) Deterministically verify:
     (a) For all t: (pref2(t)[1], pref2(t)[2], suff2(t)[1], suff2(t)[2]) ∈ R_{rep(t)} and (suff2(t)[2], pref2(t)[1]) ∈ C_out–out.
     (b) For all t1,t2 and for all tS ∈ T ∪ {empty} (we handle S=∅ by a direct edge check): compute R_{rep(tS)} and check ∃(o1,o2,o3,o4)∈R_{rep(tS)} with (suff2(t1)[2], o1) and (o4, pref2(t2)[1]) in C_out–out. (As in the paper, only representatives of each type need to be tested.)
  4) If all pass, output O(1); else Ω(log* n).
- Correctness intuition: This compresses the paper’s function g (which labeled entire w) to only its boundary interface. Existence of interior labelings is furnished by the type summaries R and concatenation checking; the O(1) construction in Lemma 27 never needs to commit to specific interior outputs ahead of time, only to consistent boundary interfaces. The MIS-free decomposition of cycles into repetitive segments is emulated here by universal checks over types tS capturing all patterns of S.
- Complexity: Step (2) guessing takes 2^{poly(N)} bits/time. Step (3) checks |T| + |T|^2·|T| = |T|^3 = 2^{poly(N)} cases, each a poly(β) lookup, so 2^{poly(N)} time. Thus the O(1) vs Ω(log* n) decision is in NEXPTIME.
- How it can fail: We rely on the boundary-only witness (pref2,suff2) being sufficient for tiling; if C_out–out had longer-range dependencies, this would fail, but our LCL is radius-1, so only immediate neighbors matter. Quick test: with C_out–out equal to equality of outputs (proper 1D “matching”), pref2/suff2 must match at the join, and R_{rep(t)} enforces internal equalities; the checker’s conditions reduce to expected constraints and accept exactly the O(1) cases.

Putting it all together: full NEXPTIME classification
- Preprocessing (deterministic EXPTIME): enumerate T, representatives rep(t), and Summ(rep(t)).
- Stage 2 (NEXPTIME): Guess boundary interfaces (pref2,suff2) per type and verify O(1) via the conditions above. If pass, answer O(1).
- Stage 1 (NEXPTIME): Otherwise, guess feasible f on type triples and verify; if pass, answer Θ(log* n); else answer Θ(n).
- Overall: nondeterministic 2^{poly(N)} time and space. Witness sizes are capped at 2^{poly(N)}.

Sanity checks and small examples
- Example 1 (trivial always-accepting LCL): C_in–out ≡ Σ_out for both inputs; C_out–out ≡ Σ_out×Σ_out. Then the precomputed R_{rep(t)} are full Σ_out^4 for all t. Guess pref2,suff2 arbitrarily, all checks pass; we output O(1). 
- Example 2 (force global agreement on a secret): C_in–out allows any output, C_out–out forces the whole path to output the same color c (i.e., adjacency allowed only when equal). Our checker fails O(1) because bridging through an S with mismatched c cannot be met uniformly; then it fails log* via the feasible-f check since the middle block cannot enforce equality across adversarially chosen separators without long-range coordination; we output Θ(n).
- Example 3 (proper 2-coloring independent of inputs): C_in–out allows both colors irrespective of input; C_out–out forbids equal adjacent outputs. Our checker will fail O(1) (no boundary pair can be glued periodically because (last,first) must alternate), but succeed for log*: a feasible f exists assigning alternating labels on separators; output Θ(log* n).

Why the improvement over the naive “doubly exponential” approach
- The naive approach ranges over strings w of length ℓ_pump (which is ≈ |T| ≈ 2^{poly(N)}), causing double-exponential enumeration (2^{ℓ_pump}) to define/verify feasible functions.
- Our approach works purely at the type level. The number of types is τ ≤ 2^{poly(N)}, and all witnesses (pref2/suff2 per type; or f on type triples) are of size polynomial in τ, thus 2^{poly(N)}. All universal verifications iterate over O(τ^k) combinations and use polynomial-time (in β) composition/DP, yielding nondeterministic 2^{poly(N)} time overall.

Dependencies on path vs cycle and reversals
- We explicitly target globally oriented paths; no reversal options are needed in the feasible-f definition (simplifies verification). The same machinery supports cycles by adding the reversal choice as in the paper, at a constant multiplicative factor.

Potential refinements and open items
- Tighten composition: We can speed up concatenation by precomputing 3D tensors to reduce O(β^8) to O(β^6) per composition via semiring convolution; not essential for complexity class.
- Proof polishing for O(1) stage: We sketched sufficiency of boundary interfaces; to make it watertight, formalize that every use of repeated blocks in Lemma 27 only needs (i) existence of internal completions consistent with fixed 2-node boundary, and (ii) a valid join edge between adjacent copies. Both are exactly our checks.
- Memory model: The algorithm stores τ many β^4-bit tables (R relations). This is 2^{poly(N)} space; acceptable for complexity theory, but huge in practice. Possible compression: store only sets of allowed pairs for each marginal (e.g., precompute allowed (o2,o3) pairs) to exploit sparsity in C_out–out.



=== Verifier (summary_md) ===
- The finite-state type abstraction and the single-exponential bound in β look correct and useful; this keeps all search tables and verification at size 2^{poly(β)}.
- Major soundness gap: composing macro relations R_t via an intermediate color m is wrong; you must form concatenated summaries (Ext) and derive R for the concatenation before testing bridging. A small 3-color counterexample breaks the m-composition.
- The append-one-bit transition δ is the linchpin: prove it is well-defined on types (congruence) and implement the correct DP that maps Ext_t to Ext_{t′} with precise boundary indexing; current formulas are incorrect/underspecified.
- Stage 1 (Ω(n) vs o(n)) is promising if rephrased to use concatenation of summaries; Prover 02’s version is close to correct once δ is fixed.
- Stage 2 (O(1) vs Ω(log* n)) via boundary interfaces is plausible but needs a clear equivalence proof to the paper’s feasible-function framework; otherwise, be ready to augment the witness.
- Complexity: NEXPTIME membership is well supported. Avoid claiming EXPTIME without an explicit deterministic solver for the existential witnesses.
- Next: formalize δ and concatenation lemmas, implement the summary closure, and test on counterexamples that distinguish R-composition from summary-concatenation; then write the Stage 1/2 verifiers against the corrected primitives.
=== Verifier (verdict) ===
promising

=== Writer (status) ===
success

