{
  "content": "# Goal\nProve that Python\u2019s TimSort (with the corrected merge-collapse condition) uses at most n H + O(n) comparisons, where H = \u2211 (\u2113_i/n) log2(n/\u2113_i) for the run-lengths \u2113_1,\u2026,\u2113_r of the input.\n\nReferences: Auger\u2013Jug\u00e9\u2013Nicaud\u2013Pivoteau (A\u2013J\u2013N\u2013P) [paper1], Munro\u2013Wild [paper2]. We follow A\u2013J\u2013N\u2013P\u2019s amortized analysis, isolating the key lemmas needed for an auditable, entropy-adaptive bound.\n\n\nSetup and model\n- TimSort variant: Python\u2019s corrected version (with the extra blue condition in merge_collapse in paper1). Java\u2019s legacy variant requires additional technicalities; the conclusion still holds, cf. paper1.\n- Cost model: Merging runs of sizes a and b costs at most a + b \u2212 1 comparisons; we upper bound by the merge cost a + b. Scanning for runs uses O(n) comparisons. Hence number of comparisons \u2264 total merge cost + O(n).\n- Objective: Prove total merge cost \u2264 n H + O(n). Then comparisons \u2264 n H + O(n).\n\n\nKey invariant and growth\nClaim 1 (Stack invariant; why useful: enforces exponential growth in run lengths on the stack). During the main loop, after each call to merge_collapse, the run sizes (top = index 1) satisfy for all i \u2265 1: r_{i+1} > r_i and for all i \u2265 1 with i + 2 \u2264 h: r_{i+2} > r_{i+1} + r_i.\nSketch. Direct case analysis on the five cases (#1 push, #2\u2013#5 merges) shows the inequalities are preserved after each update (A\u2013J\u2013N\u2013P, Lemma 1).\n\nCorollary 2 (Exponential growth along the stack; why useful: converts length relations to logarithmic height bounds). If a run is about to be pushed, then for all i \u2264 j \u2264 h we have r_i \u2264 2^{(i+1\u2212j)/2} r_j.\nReason. From r_{i+2} \u2265 2 r_i, repeatedly.\n\n\nEncoding updates and two phases\nWe encode the updates by symbols #1,\u2026,#5 (exactly as in A\u2013J\u2013N\u2013P). Splitting each iteration at the first occurrence of #3,#4, or #5 yields:\n- Starting sequence: a #1 followed by as many #2 as possible.\n- Ending sequence: starts with #3, #4, or #5 (may be empty), then no #1.\nWe bound costs of starting sequences by O(n); ending sequences by O(n + n H).\n\n\nStarting sequences cost O(n)\nLemma 3 (why useful: peels off the easy part). The total cost of all merges performed in starting sequences is O(n). More precisely, if a run R of length r starts a starting-sequence of k \u2265 1 steps (k\u22121 merges #2), its total cost \u2264 \u03b3 r with \u03b3 = 2\u2211_{j\u22651} j\u00b72^{\u2212j/2} < \u221e.\nProof sketch. After pushing R of size r, the sequence of k\u22121 merges #2 merges (R_1,R_2), (R_2,R_3), \u2026, (R_{k\u22121},R_k). Using Corollary 2 on the pre-push stack, r \u2265 r_k \u2265 2^{(k\u22121\u2212i)/2} r_i for 1 \u2264 i \u2264 k, so the cost C \u2264 \u2211_{i=1}^k (k+1\u2212i) r_i \u2264 r \u00b7 2\u2211_{j=1}^k j 2^{\u2212j/2} < \u03b3 r. Summing over runs gives O(n).\n\n\nToken accounting for ending sequences\nWe amortize ending-sequence merges by per-element tokens credited when height decreases during an ending sequence:\n- Credits: Each time an element\u2019s height decreases due to an ending-sequence merge, we credit it 2 c-tokens and 1 s-token.\n- Spending: For a merge during ending sequences:\n  \u2022 Case #2: every element of R_1 and R_2 spends 1 c-token (pays for cost r_2 + r_3 \u2264 r_1 + r_2).\n  \u2022 Case #3: every element of R_1 spends 2 c-tokens (cost \u2264 2 r_1).\n  \u2022 Case #4 or #5: every element of R_1 spends 1 c-token and every element of R_2 spends 1 s-token (cost r_1 + r_2).\n\nLemma 4 (token balances nonnegative; why useful: validates the amortization). No element ever owes tokens.\nReason. In #4/#5, elements of R_2 spend one s-token, but the next operation is necessarily another merge in the same ending sequence, reducing their height and re-crediting 1 s-token before they could spend it again. c-tokens are credited on every height decrease; the spends are covered by these credits (A\u2013J\u2013N\u2013P, Lemma 2).\n\nHeight bound after starting sequence\nLemma 5 (height bound; why useful: per-element credits scale like log(n/|R|)). Let R be a run of length r; when its starting sequence finishes, the stack height h satisfies h \u2264 4 + 2 log2(n/r).\nProof sketch. Runs R_3,\u2026,R_h did not change during the starting sequence. By Corollary 2 applied to the pre-push stack, r_3 \u2264 2^{2\u2212h/2} \u00b7 r_h \u2264 2^{2\u2212h/2} n. Since (by end of starting sequence) r = r_1 \u2264 r_3, we get r \u2264 2^{2\u2212h/2} n, i.e., h \u2264 4 + 2 log2(n/r).\n\nBounding ending-sequence cost by entropy\nObservation. Each element accrues tokens only when its height decreases in an ending sequence. By Lemma 5, the number of height levels encountered by an element belonging to a run of length r is O(1 + log(n/r)). In A\u2013J\u2013N\u2013P this is made explicit: per-element credits \u2264 (2h c-tokens + h s-tokens) = O(1 + log(n/r)).\n\nLemma 6 (why useful: adds up to entropy). Summing token credits over all elements yields O(\u2211_i \u2113_i (1 + log(n/\u2113_i))) = O(n + n H). Since the total tokens spent equal the total cost of ending-sequence merges (by Lemma 4), the ending sequences incur cost O(n + n H).\n\nPutting it together\n- Run detection: O(n) comparisons.\n- Starting sequences: O(n) merge cost (Lemma 3).\n- Ending sequences: O(n + n H) merge cost (Lemma 6).\n- Final collapse: either handled within the same framework by appending a dummy run of length > n (A\u2013J\u2013N\u2013P Remark), or charged O(n).\n- Comparisons: Each merge uses at most (size sum \u2212 1) comparisons, so comparisons \u2264 merge cost + O(#merges) \u2264 merge cost + O(n).\n\nConclusion (Main Theorem). TimSort performs at most n H + O(n) comparisons on any input whose runs have lengths \u2113_1,\u2026,\u2113_r. (More precisely, the merge cost is O(n + n H), hence comparisons are \u2264 n H + O(n).)\n\nOptimality (lower bound; why useful: shows bound is tight up to linear terms). For distinct elements, the number of permutations compatible with given runs is n!/\u220f_i (\u2113_i!). Hence any comparison sort needs \u2265 log2(n!) \u2212 \u2211 log2(\u2113_i!) = n H \u2212 O(n) comparisons (Stirling/concavity; cf. [paper2, \u00a7Lower bound] or A\u2013J\u2013N\u2013P Prop. 2). Thus n H + O(n) is optimal up to linear terms.\n\n\nExamples and sanity checks\n- All runs length 1: H = log2 n; bound gives O(n log n).\n- One run length n: H = 0; the algorithm detects sorted input and merges nothing: O(n) comparisons.\n- Two runs (a,b): H = (a/n)log(n/a) + (b/n)log(n/b) \u2264 1, bound gives O(n).\n\n\nObstacles\n- The invariant proofs are a routine but tedious case analysis over #2\u2013#5; we rely on A\u2013J\u2013N\u2013P\u2019s correctness (paper1) for the Python-corrected variant.\n- The Java legacy variant lacks the blue condition; analogous results hold but require heavier technical work (paper1, \u00a7Java).\n\n\nNext steps\n- Refinement to 1.5 n H + O(n) (merge cost) follows from a potential-function analysis with \u03a6(r) = 1.5 r log r (paper1, \u00a74); this yields the sharp constant 3/2 (tight by Buss\u2013Knop\u2019s lower family).\n- Implementation audit: ensure the analyzed merge rules match the production code (the extra blue condition).\n"
}