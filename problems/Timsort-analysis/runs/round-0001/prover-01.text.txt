# Ideas
- Clarify the exact claim that is true for TimSort: Python’s TimSort (with the de Gouw–et al. fix) makes at most O(n + nH) comparisons on inputs whose natural run decomposition has lengths ℓ1,…,ℓr, where H = Σ (ℓi/n) log2(n/ℓi) is the binary Shannon entropy. The stronger claim “≤ nH + O(n)” with leading constant 1 is false for TimSort; its worst-case merge cost is (3/2) nH + O(n).
- Give a fully self-contained, rigorous proof of the O(n + nH) bound (i.e., an entropy-adaptive guarantee), based only on a simple model of TimSort’s core stack discipline and run merges. Then sketch the refined (3/2) nH + O(n) worst-case upper bound and point to a family of inputs forcing a ≥ (3/2 − o(1))n log2 n cost.

# Model and notation
- Input: array of n elements, decomposed greedily into r monotone runs of lengths ℓ1,…,ℓr (with ℓi ≥ 2 for i < r and ℓr ≥ 1). Let H = Σi (ℓi/n) log2(n/ℓi).
- We analyze Python TimSort’s core mechanism (with the extra “r2 + r3 ≥ r4” guard). It maintains a stack of run lengths (top to bottom) r1,…,rh. After pushing a new run, while one of the following holds, it merges accordingly:
  (i) h ≥ 3 and r1 > r3: merge (R2,R3);
  (ii) h ≥ 2 and r1 ≥ r2: merge (R1,R2);
  (iii) h ≥ 3 and r1 + r2 ≥ r3: merge (R1,R2);
  (iv) h ≥ 4 and r2 + r3 ≥ r4: merge (R1,R2).
  At the end, force-collapse merges until one run remains.
- Cost model: Define the merge cost of merging runs of sizes a and b as a + b. Counting comparisons: each merge uses at most a + b − 1 comparisons, and detecting runs and loop overhead costs O(n). Hence total comparisons ≤ (total merge cost) + O(n).

# A key stack-growth invariant
Lemma 1 (Fibonacci-type growth). At any time just before a push of a new run, the stack satisfies r1 < r2, r1 + r2 < r3, r2 + r3 < r4, and for all i ≥ 3 we have ri + ri+1 < ri+2.
Proof. Immediate by case analysis: after each merge the property is preserved because indices shift down; after a push, the while-loop ensures none of the merging conditions holds, implying the first three inequalities; the last follows by induction. ∎
Corollary 2 (exponential growth). For i ≤ j ≤ h we have ri ≤ 2^{(i+1−j)/2} rj. In particular, the stack height h satisfies h ≤ 4 + 2 log2(n/r1) at the end of the “starting sequence” for the run of length r1.
Proof. From ri+2 ≥ 2 ri, inductively ri ≤ 2^{−k} ri+2k ≤ 2^{−k} ri+2k+1. The height bound follows since the bottom run is at most n. ∎

# Decomposition into starting and ending sequences
- For each pushed run R of length r, the merges until the first time a case among (ii)-(iv) fires form the starting sequence; thereafter, all merges until the next push form the ending sequence of R. We bound starting sequences by O(n) total and ending sequences by O(n + nH) total.

Lemma 3 (starting sequences cost O(n)). The total merge cost of all starting sequences is O(n).
Proof. Consider a starting sequence for run R of length r that merges k ≥ 2 pre-existing top runs R1,…,Rk. Its cost is C = Σ_{i=1}^k (k+1−i) ri. Since the last merge of case (i) requires r > rk and by Corollary 2 we have rk ≥ 2^{(k−1−i)/2} ri, we get C/r ≤ Σ_{j=1}^k j·2^{−j/2} < γ for a universal constant γ. Summing over all pushed runs (Σ r = n) gives O(n). ∎

# Token accounting for ending sequences
We use two token types per element: c-tokens and s-tokens.
- Credits: when a run is pushed, every element in it gets 2 c-tokens and 1 s-token. Whenever an ending merge reduces an element’s height, it receives again 2 c-tokens and 1 s-token.
- Expenses per merge:
  (i) r1 > r3, merge (R2,R3): every element in R1 and R2 pays 1 c-token; that covers r2 + r1 ≥ r2 + r3.
  (ii) r1 ≥ r2, merge (R1,R2): every element in R1 pays 2 c-tokens; cost ≤ 2 r1.
  (iii) r1 + r2 ≥ r3, merge (R1,R2): every element in R1 pays 1 c-token, every element in R2 pays 1 s-token; total equals r1 + r2.
  (iv) r2 + r3 ≥ r4, merge (R1,R2): same spending as (iii).

Lemma 4 (nonnegative balances). No element ever runs out of c- or s-tokens.
Proof. In (i)–(iv) a merge that lowers an element’s height also immediately credits fresh tokens. For s-tokens: in (iii) and (iv), elements of R2 spend 1 s-token; the resulting top run must be merged again (by the very condition that fired), so their height drops and they regain an s-token before any demand for s-tokens can recur. For c-tokens, credits on height drops cover expenditures in the same or next merges. Formal induction on the sequence of merges yields nonnegativity. ∎

Lemma 5 (height bound per element). Fix an element belonging to a run of length r when pushed. Let h be the stack height at the end of its starting sequence. Then h ≤ 4 + 2 log2(n/r).
Proof. Corollary 2 with r1 = r. ∎

Corollary 6 (ending sequences cost O(n + nH)). Each element spends O(h) c-tokens and O(h) s-tokens during its ending sequence, with h bounded as above. Hence total ending-sequence cost is O(Σ r (1 + log2(n/r))) = O(n + nH).
Proof. By Lemma 4, every unit of merge cost is paid by a token; an element is credited at most O(h) times; Lemma 5 plugs in h; sum over all elements. Using Σ r log2(n/r) = nH finishes. ∎

Combining Lemma 3 and Corollary 6 with “comparisons ≤ merge cost + O(n)” yields:

Theorem A (entropy adaptivity). Python TimSort performs at most O(n + nH) comparisons on inputs with run lengths ℓ1,…,ℓr.

# Lower bound (optimality of the entropy term)
Proposition B (information-theoretic lower bound). Any comparison sort needs at least nH − O(n) comparisons on inputs consisting of r known runs of lengths ℓ1,…,ℓr with distinct keys.
Sketch. There are n!/(ℓ1!⋯ℓr!) permutations compatible with the run partition. Hence any decision tree must have height ≥ log2(n!) − Σ log2(ℓi!) = nH − O(n) (by Stirling bounds). ∎

Thus Theorem A is optimal up to constant factors and lower-order terms.

# Stronger, precise bound (sketch)
Define potential Φ(r) = (3/2) r log2 r for a run of length r. Let ΔΦ be the potential change over a merge. One checks:
- Any “balanced” merge (sizes within a factor φ^2, where φ is the golden ratio) satisfies merge cost ≤ ΔΦ.
- Unbalanced merges of type (i) can be paired with their successor to be jointly dominated by potential gain; the first merge of an ending sequence may incur an extra additive +r slack, which telescopes to +n over all sequences.
- Invariants strengthening Lemma 1 (e.g., r2,r3 < r4, and Σ_{k=2}^{i−1} rk < φ·ri) ensure the balancing tests. Grouping merges inside each ending sequence and summing ΔΦ shows that total merge cost ≤ Φ(n) − Σ Φ(ℓi) + O(n) = (3/2) nH + O(n).
Finally, comparisons ≤ merge cost + O(n), so

Theorem C (tight worst-case). TimSort uses at most (3/2) nH + O(n) comparisons in the worst case, and this is tight: there is a family of inputs forcing ≥ (3/2 − o(1)) n log2 n comparisons (hence ≥ (3/2) nH − O(n)).

# Obstacle (why “≤ nH + O(n)” is false for TimSort)
Buss–Knop exhibited explicit run-length sequences R(n) for which TimSort’s merge cost satisfies a recurrence yielding c(R(n)) ≥ (3/2 − o(1)) n log2 n. Since H ≈ log2 n for such families, the coefficient 1 is unachievable for TimSort.

# Next steps
- If the target is truly “≤ nH + O(n)”, use Huffman-merge or nearly-optimal stable mergesorts (e.g., peeksort/powersort) which provably achieve comparisons ≤ nH + O(n).
- For TimSort: the entropy-adaptive O(n + nH) bound (Theorem A) is best possible up to a constant factor, and (3/2) is the exact tight leading constant.

# Summary of proven claims
- Lemma 1 and Corollary 2: exponential growth invariant and stack-height bound.
- Lemma 3: O(n) total cost for starting sequences.
- Lemmas 4–6: token accounting for ending sequences gives O(n + nH) ending cost.
- Theorem A: TimSort makes O(n + nH) comparisons.
- Proposition B: lower bound nH − O(n) comparisons is necessary.
- Theorem C (sketch): TimSort’s exact worst-case is (3/2) nH + O(n), and this is tight.
