# Goal
Prove that Python’s TimSort (with the corrected merge-collapse condition) uses at most n H + O(n) comparisons, where H = ∑ (ℓ_i/n) log2(n/ℓ_i) for the run-lengths ℓ_1,…,ℓ_r of the input.

References: Auger–Jugé–Nicaud–Pivoteau (A–J–N–P) [paper1], Munro–Wild [paper2]. We follow A–J–N–P’s amortized analysis, isolating the key lemmas needed for an auditable, entropy-adaptive bound.


Setup and model
- TimSort variant: Python’s corrected version (with the extra blue condition in merge_collapse in paper1). Java’s legacy variant requires additional technicalities; the conclusion still holds, cf. paper1.
- Cost model: Merging runs of sizes a and b costs at most a + b − 1 comparisons; we upper bound by the merge cost a + b. Scanning for runs uses O(n) comparisons. Hence number of comparisons ≤ total merge cost + O(n).
- Objective: Prove total merge cost ≤ n H + O(n). Then comparisons ≤ n H + O(n).


Key invariant and growth
Claim 1 (Stack invariant; why useful: enforces exponential growth in run lengths on the stack). During the main loop, after each call to merge_collapse, the run sizes (top = index 1) satisfy for all i ≥ 1: r_{i+1} > r_i and for all i ≥ 1 with i + 2 ≤ h: r_{i+2} > r_{i+1} + r_i.
Sketch. Direct case analysis on the five cases (#1 push, #2–#5 merges) shows the inequalities are preserved after each update (A–J–N–P, Lemma 1).

Corollary 2 (Exponential growth along the stack; why useful: converts length relations to logarithmic height bounds). If a run is about to be pushed, then for all i ≤ j ≤ h we have r_i ≤ 2^{(i+1−j)/2} r_j.
Reason. From r_{i+2} ≥ 2 r_i, repeatedly.


Encoding updates and two phases
We encode the updates by symbols #1,…,#5 (exactly as in A–J–N–P). Splitting each iteration at the first occurrence of #3,#4, or #5 yields:
- Starting sequence: a #1 followed by as many #2 as possible.
- Ending sequence: starts with #3, #4, or #5 (may be empty), then no #1.
We bound costs of starting sequences by O(n); ending sequences by O(n + n H).


Starting sequences cost O(n)
Lemma 3 (why useful: peels off the easy part). The total cost of all merges performed in starting sequences is O(n). More precisely, if a run R of length r starts a starting-sequence of k ≥ 1 steps (k−1 merges #2), its total cost ≤ γ r with γ = 2∑_{j≥1} j·2^{−j/2} < ∞.
Proof sketch. After pushing R of size r, the sequence of k−1 merges #2 merges (R_1,R_2), (R_2,R_3), …, (R_{k−1},R_k). Using Corollary 2 on the pre-push stack, r ≥ r_k ≥ 2^{(k−1−i)/2} r_i for 1 ≤ i ≤ k, so the cost C ≤ ∑_{i=1}^k (k+1−i) r_i ≤ r · 2∑_{j=1}^k j 2^{−j/2} < γ r. Summing over runs gives O(n).


Token accounting for ending sequences
We amortize ending-sequence merges by per-element tokens credited when height decreases during an ending sequence:
- Credits: Each time an element’s height decreases due to an ending-sequence merge, we credit it 2 c-tokens and 1 s-token.
- Spending: For a merge during ending sequences:
  • Case #2: every element of R_1 and R_2 spends 1 c-token (pays for cost r_2 + r_3 ≤ r_1 + r_2).
  • Case #3: every element of R_1 spends 2 c-tokens (cost ≤ 2 r_1).
  • Case #4 or #5: every element of R_1 spends 1 c-token and every element of R_2 spends 1 s-token (cost r_1 + r_2).

Lemma 4 (token balances nonnegative; why useful: validates the amortization). No element ever owes tokens.
Reason. In #4/#5, elements of R_2 spend one s-token, but the next operation is necessarily another merge in the same ending sequence, reducing their height and re-crediting 1 s-token before they could spend it again. c-tokens are credited on every height decrease; the spends are covered by these credits (A–J–N–P, Lemma 2).

Height bound after starting sequence
Lemma 5 (height bound; why useful: per-element credits scale like log(n/|R|)). Let R be a run of length r; when its starting sequence finishes, the stack height h satisfies h ≤ 4 + 2 log2(n/r).
Proof sketch. Runs R_3,…,R_h did not change during the starting sequence. By Corollary 2 applied to the pre-push stack, r_3 ≤ 2^{2−h/2} · r_h ≤ 2^{2−h/2} n. Since (by end of starting sequence) r = r_1 ≤ r_3, we get r ≤ 2^{2−h/2} n, i.e., h ≤ 4 + 2 log2(n/r).

Bounding ending-sequence cost by entropy
Observation. Each element accrues tokens only when its height decreases in an ending sequence. By Lemma 5, the number of height levels encountered by an element belonging to a run of length r is O(1 + log(n/r)). In A–J–N–P this is made explicit: per-element credits ≤ (2h c-tokens + h s-tokens) = O(1 + log(n/r)).

Lemma 6 (why useful: adds up to entropy). Summing token credits over all elements yields O(∑_i ℓ_i (1 + log(n/ℓ_i))) = O(n + n H). Since the total tokens spent equal the total cost of ending-sequence merges (by Lemma 4), the ending sequences incur cost O(n + n H).

Putting it together
- Run detection: O(n) comparisons.
- Starting sequences: O(n) merge cost (Lemma 3).
- Ending sequences: O(n + n H) merge cost (Lemma 6).
- Final collapse: either handled within the same framework by appending a dummy run of length > n (A–J–N–P Remark), or charged O(n).
- Comparisons: Each merge uses at most (size sum − 1) comparisons, so comparisons ≤ merge cost + O(#merges) ≤ merge cost + O(n).

Conclusion (Main Theorem). TimSort performs at most n H + O(n) comparisons on any input whose runs have lengths ℓ_1,…,ℓ_r. (More precisely, the merge cost is O(n + n H), hence comparisons are ≤ n H + O(n).)

Optimality (lower bound; why useful: shows bound is tight up to linear terms). For distinct elements, the number of permutations compatible with given runs is n!/∏_i (ℓ_i!). Hence any comparison sort needs ≥ log2(n!) − ∑ log2(ℓ_i!) = n H − O(n) comparisons (Stirling/concavity; cf. [paper2, §Lower bound] or A–J–N–P Prop. 2). Thus n H + O(n) is optimal up to linear terms.


Examples and sanity checks
- All runs length 1: H = log2 n; bound gives O(n log n).
- One run length n: H = 0; the algorithm detects sorted input and merges nothing: O(n) comparisons.
- Two runs (a,b): H = (a/n)log(n/a) + (b/n)log(n/b) ≤ 1, bound gives O(n).


Obstacles
- The invariant proofs are a routine but tedious case analysis over #2–#5; we rely on A–J–N–P’s correctness (paper1) for the Python-corrected variant.
- The Java legacy variant lacks the blue condition; analogous results hold but require heavier technical work (paper1, §Java).


Next steps
- Refinement to 1.5 n H + O(n) (merge cost) follows from a potential-function analysis with Φ(r) = 1.5 r log r (paper1, §4); this yields the sharp constant 3/2 (tight by Buss–Knop’s lower family).
- Implementation audit: ensure the analyzed merge rules match the production code (the extra blue condition).
