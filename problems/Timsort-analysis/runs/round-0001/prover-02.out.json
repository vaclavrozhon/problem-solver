{
  "content": "# Ideas\n\n- Correct target statement: For Python\u2019s TimSort (with the repaired invariant), the total number of comparisons on an input of length n whose run decomposition has lengths (\u21131,\u2026,\u2113r) is O(n + n H), where H = \u2211i (\u2113i/n) log2(n/\u2113i). This is the right \u201centropy-adaptive\u201d bound.\n- The stronger claim \u201ccomparisons \u2264 n H + O(n)\u201d is false for TimSort: there are inputs on which TimSort needs at least (3/2) n log2 n \u2212 O(n) comparisons while H = log2 n (up to lower-order terms), so the leading constant cannot be 1. We point this out and then prove the correct O(n + n H) upper bound with a self-contained argument.\n- Proof strategy: (i) analyze only TimSort\u2019s core merging scheme (stack-based merges) in a simple translation capturing Python\u2019s fix; (ii) parse each iteration into a \u201cstarting sequence\u201d (some number of \u201c#2\u201d merges collapsing below a newly pushed run) and an \u201cending sequence\u201d (the merges that re-establish the invariant); (iii) prove an exponential growth invariant for runs on the stack; (iv) bound the merge cost of all starting sequences by O(n); (v) pay for ending sequences with a token scheme credited to elements, and bound per-element tokens by O(1 + log2(n/r)) via a stack-height bound; (vi) translate merge-cost bounds into comparison bounds and add O(n) for scanning runs.\n\n# Setup and model\n\n- We view TimSort as a natural mergesort: the input array is greedily decomposed into maximal monotone runs (increasing or decreasing, with decreasing runs reversed stably). Let these maximal runs have lengths \u21131,\u2026,\u2113r, \u2211\u2113i = n. Define H = \u2211i (\u2113i/n) log2(n/\u2113i).\n- Cost model: in merging two runs of lengths a and b, at most a + b \u2212 1 comparisons are executed; we upper bound by a + b whenever convenient. Counting the run-decomposition scan adds at most n \u2212 1 comparisons. Thus it suffices to bound the total \u201cmerge cost\u201d (sum over merges of a + b) by O(n + n H).\n- We use the standard \u201ctranslated\u201d TimSort core (Python \u2265 3.6 with the De Gouw et al. repair): maintain a stack of runs R1,\u2026,Rh (top to bottom) with lengths r1,\u2026,rh. After pushing a new run, repeatedly apply the first applicable rule:\n  1) if h \u2265 3 and r1 > r3, merge R2 and R3 (tag #2)\n  2) else if h \u2265 2 and r1 \u2265 r2, merge R1 and R2 (tag #3)\n  3) else if h \u2265 3 and r1 + r2 \u2265 r3, merge R1 and R2 (tag #4)\n  4) else if h \u2265 4 and r2 + r3 \u2265 r4, merge R1 and R2 (tag #5)\n  5) else break; after all runs pushed, merge top pairs until one run remains.\n\nWe call one pass of pushing a run and doing the ensuing while-loop one iteration; its merges decompose into: a starting sequence (#2 repeated as long as applicable, possibly empty) and the ending sequence (beginning with #3/4/5 and containing no #1).\n\n# Lemmas and proofs\n\nWe write S = (R1,\u2026,Rh) for the current stack (top to bottom) and ri = |Ri|.\n\nLemma 1 (Fibonacci-type growth). At any moment when no rule applies (i.e., right before the next push), the lengths satisfy for all i: r1 < r2, r1 + r2 < r3, r2 + r3 < r4, and for all 3 \u2264 i \u2264 h \u2212 2, ri + ri+1 < ri+2. In particular, ri \u2264 2\u2212k ri+2k and ri \u2264 2\u2212k ri+2k+1 for all k \u2265 0.\n\nProof. Straightforward induction on updates: any merge keeps the tail (R3,\u2026,Rh) shifting up and preserves ri + ri+1 < ri+2 for i \u2265 3; when a push occurs and no further merges apply, the three displayed inequalities hold by the structure of rules; chaining yields the binary-exponential decay. \u220e\n\nCorollary 2 (Stack height bound after starting sequence). Let a run R of length r be pushed and let h be the stack height after its starting sequence finishes (i.e., just before the ending sequence for this push). Then h \u2264 4 + 2 log2(n/r).\n\nProof. At that moment no rule applies, hence r = r1 \u2264 r3. From Lemma 1 applied to the pre-push stack tail, r3 \u2264 22\u2212h/2 rh \u2264 22\u2212h/2 n. Thus r \u2264 22\u2212h/2 n, which rearranges to the claim. \u220e\n\nLemma 3 (Starting sequences cost O(n)). Let a starting sequence begin with pushing a run R of length r and consist of k \u2212 1 applications of #2, so R2,\u2026,Rk are merged (into R2,\u2026). Its merge cost is at most \u03b3 r for a universal constant \u03b3 = 2 \u2211j\u22651 j 2\u2212j/2. Summing over all starting sequences yields total merge cost \u2264 \u03b3 n.\n\nProof. The cost equals C = \u2211i=1..k (k + 1 \u2212 i) ri. As no rule applies after the final #2, we have r > rk \u2265 2(k\u22121\u2212i)/2 ri for all i. Thus C/r \u2264 \u2211i (k + 1 \u2212 i) 2(i+1\u2212k)/2 = 2 \u2211j\u22651 j 2\u2212j/2 = \u03b3. Each run is pushed once, \u2211r = n. \u220e\n\nWe now treat ending sequences. We introduce two token types (both redeemable for comparisons): c-tokens and s-tokens.\n\nToken scheme (credits and charges).\n- Credits: When a run R is pushed, each element of R is credited 2 c-tokens and 1 s-token. Furthermore, whenever an ending-sequence merge decreases an element\u2019s height on the stack, that element is credited again with 2 c-tokens; and elements of the second-from-top run in a #4/#5 merge get an extra 1 s-token (they will be at the new top and will soon trigger the next ending merge).\n- Charges: For rule #2, every element of R1 and R2 pays 1 c-token (covering the cost of merging R2 and R3 since r1 > r3 \u21d2 r1 + r2 \u2265 r2 + r3). For rule #3, every element of R1 pays 2 c-tokens (since r1 \u2265 r2 \u21d2 a + b \u2264 2 r1). For rules #4/#5, every element of R1 pays 1 c-token and every element of R2 pays 1 s-token; that equals r1 + r2.\n\nLemma 4 (Non-negativity). Throughout, no element\u2019s c-token or s-token balance becomes negative.\n\nProof. In each charge, merging reduces the heights of elements of the top one or two runs. Our credit rule gives back at least as many c-tokens as were just spent when height decreases; for s-tokens, a #4/#5 charge for R2\u2019s elements is followed immediately by another ending-sequence merge (since after #4/#5 the top run violates #3 or #4 again), which reduces their height and refunds their s-token; s-tokens are never required of the top run. Induction over merges establishes non-negativity. \u220e\n\nLemma 5 (Per-element budget). Fix a pushed run R of length r. Let h be the stack height at the end of its starting sequence. During the whole execution, any element of R is credited at most 2 h c-tokens and h s-tokens.\n\nProof. Each time an ending merge reduces the element\u2019s height, it gets 2 c-tokens; height can decrease at most h times. Similarly, s-tokens (one per #4/#5) are refunded at the subsequent merge and thus each element spends at most one s-token per height drop, hence \u2264 h in total. \u220e\n\nCombining Corollary 2 and Lemma 5 shows that the total number of tokens available to the elements of R is O(r (1 + log2(n/r))). Since tokens cover all comparisons in ending sequences (Lemma 4), we get the entropy-adaptation bound.\n\nProposition 6 (Ending sequences cost O(n + n H)). Summing over all runs, the total number of comparisons in ending sequences is \u2264 C1 n + C2 \u2211i \u2113i log2(n/\u2113i) for universal constants C1,C2.\n\nProof. From Lemma 5 and Corollary 2, elements of the run of length r contribute O(r (1 + log2(n/r))) tokens, each redeemable for one comparison in an ending sequence. Summing over all runs gives O(\u2211r + \u2211 r log(n/r)) = O(n + n H). \u220e\n\nWe now add up all contributions.\n\nTheorem 7 (Main upper bound). Let an array of length n have maximal-run decomposition with lengths \u21131,\u2026,\u2113r and H = \u2211i (\u2113i/n) log2(n/\u2113i). Python\u2019s TimSort (with the repaired invariant) uses at most C n + C\u2032 n H comparisons for universal constants C,C\u2032. In particular, comparisons = O(n + n H).\n\nProof. Comparisons = (run scan) + (starting sequences) + (ending sequences) + (final collapses, which can be folded into one last \u201citeration\u201d by appending a fictitious run of length > n). The run scan costs at most n \u2212 1 comparisons. Starting sequences cost at most \u03b3 n by Lemma 3. Ending sequences cost O(n + n H) by Proposition 6. Final collapses are absorbed by appending a fictitious run, not changing the bounds. Altogether comparisons \u2264 C n + C\u2032 n H. \u220e\n\nRemark 8 (Tightness and the false \u201c\u2264 n H + O(n)\u201d). Buss\u2013Knop exhibited run-length sequences for which TimSort\u2019s merge cost (hence comparisons) is \u2265 (3/2) n log2 n \u2212 O(n). Since for such inputs H = log2 n \u00b1 O(1), TimSort cannot satisfy a bound of the form comparisons \u2264 n H + O(n). Our Theorem 7 is optimal up to a constant factor; the sharp upper bound is comparisons \u2264 (3/2) n H + O(n) (provable by a refined potential analysis).\n\n# Obstacles and how we addressed them\n\n- A direct per-merge \u201cbalanced vs. unbalanced\u201d analysis yields only coarse constants; the exponential-growth invariant and the split into starting/ending sequences isolate the hard merges and allow amortization.\n- The token scheme needs careful design to avoid deficits on #4/#5: the \u201crefunded s-token\u201d device is the key and needs the repaired Python rule #5 to ensure an immediate subsequent merge.\n\n# Examples/ sanity checks\n\n- One big run \u21131 = n (H = 0): no merges; scan O(n). Bound gives O(n).\n- \u03c1 equal runs \u2113i = n/\u03c1 (H = log2 \u03c1): our bound gives O(n + n log \u03c1). In worst case \u03c1 = \u0398(n), this is O(n log n); matches classical upper bounds.\n\n# Next steps\n\n- If desired, we can supply the refined potential-function proof that TimSort\u2019s merge cost is \u2264 (3/2) n H + O(n), matching the known lower bound and giving the optimal leading constant.\n- We can also extract a fully formal pseudocode version (the translated rules #2\u2013#5) and isolate the exact constants C,C\u2032 from the above analysis.\n"
}