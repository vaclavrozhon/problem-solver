{
  "content": "# Goal\nProve that Python\u2019s TimSort (with the corrected invariant) adapts to the entropy of run lengths: on an input of length n whose greedy run decomposition has lengths \u2113\u2081,\u2026,\u2113\u1d63 (sum \u2113\u1d62=n), the number of comparisons is at most n\u00b7H + O(n), where H = \u2211\u1d62 (\u2113\u1d62/n) log\u2082(n/\u2113\u1d62).\n\n\nIdeas\n- Measure work via merge cost: merging runs of sizes a,b has cost a+b, which upper-bounds comparisons (standard merge uses \u2264 a+b\u22121 comparisons). Thus bounding total merge cost by n\u00b7H+O(n) implies the same (up to +O(n)) for comparisons (add O(n) to scan runs).\n- Use the (Python) TimSort stack invariant (with the De Gouw et al. fix): if runs (top to bottom) have lengths r\u2081,\u2026,r_h, then at all times r_{i+2} > r_{i+1}+r_i and r_{i+1} > r_i for i \u2265 1. This implies Fibonacci growth and logarithmic stack height.\n- Partition the main loop into \u201cstarting sequences\u201d (a push followed by zero or more case-#2 merges) and \u201cending sequences\u201d (the subsequent merges (#3\u2013#5) until the invariant is restored). Show: starting sequences cost O(n) in total; ending sequences cost \u2211\u1d62 \u2113\u1d62\u00b7O(1+log(n/\u2113\u1d62)), which sums to n\u00b7H+O(n).\n- For ending sequences, use an amortized token scheme: charge merges to runs by height decreases; bound per-element tokens by O(1+log(n/\u2113\u1d62)).\n\nModel and preliminaries\n- We analyze Python\u2019s TimSort core (Algorithm in the paper), with the additional condition (case #5) enforcing the stack invariant. Galloping only reduces comparisons; we ignore it (upper bounds remain valid).\n- Merge cost = size of result; number of comparisons \u2264 merge cost (\u2212#merges), hence comparisons \u2264 merge cost + O(n).\n- Let S = (R\u2081,\u2026,R_h) be the stack (top to bottom), with lengths r\u2081,\u2026,r_h.\n\nLemma 1 (Stack growth invariant)\nDuring the main loop, the invariant r_{i+2} > r_{i+1}+r_i and r_{i+1}>r_i holds for 1 \u2264 i \u2264 h\u22122.\nSketch. Induction over stack updates. Each update is one of: push (#1) then repeatedly merge (#2\u2013#5). The conditions that trigger merges prevent violations. Straightforward case analysis as in the cited paper verifies preservation. A corollary: before a push finishes, r_j \u2264 2^{(j+1\u2212i)/2}\u00b7r_i (Fibonacci-like growth), so the stack height is logarithmic in the ratio of bottom to top run sizes.\n\nLemma 2 (Height bound after a starting sequence)\nLet a run R of length r be pushed; when its starting sequence finishes, the stack height h satisfies h \u2264 4 + 2 log\u2082(n/r).\nProof sketch. At the end of the starting sequence, r\u2081 \u2264 r\u2083, and none of R\u2083,\u2026 is merged. By Lemma 1 and its corollary applied to the pre-push stack, r\u2083 \u2264 2^{2\u2212h/2}\u00b7n. Hence r = r\u2081 \u2264 r\u2083 \u2264 2^{2\u2212h/2}\u00b7n, yielding the claim.\n\nLemma 3 (Starting sequences cost O(n))\nConsider a push of a run R of length r, followed by k\u22121 (k\u22651) #2-merges that eliminate the next k\u22121 stack runs R\u2081,\u2026,R_k. The cost C \u2264 \u03b3\u00b7r, where \u03b3 = 2\u2211_{j\u22651} j\u00b72^{\u2212j/2} is a constant; summing over all pushes gives O(n).\nProof. The total cost C of the k\u22121 merges is \u2264 \u2211_{i=1}^k (k+1\u2212i) r_i. From the final #2-condition r > r_k and Lemma 1\u2019s corollary applied to the pre-push stack, r \u2265 r_k \u2265 2^{(k\u22121\u2212i)/2} r_i for i=1,\u2026,k, giving C/r \u2264 \u2211 (k+1\u2212i)\u00b72^{(i+1\u2212k)/2} = 2\u2211_{j=1}^k j\u00b72^{\u2212j/2} < \u03b3.\n\nToken scheme for ending sequences\n- Crediting rule: an element gets 2 \u201cc-tokens\u201d and 1 \u201cs-token\u201d (a) when its run is pushed, and (b) whenever its height decreases due to a merge in an ending sequence. Intuition: height decreases are the \u201cbudget\u201d events; s-tokens shift budget from one merge to the next within an ending sequence.\n- Spending rule per merge (R\u2081 on top, R\u2082 next, R\u2083 third):\n  \u2022 Case #2 (merge R\u2082,R\u2083): each element of R\u2081 and R\u2082 pays 1 c-token (\u2265 r\u2081+r\u2082 \u2265 r\u2082+r\u2083 cost since r\u2081>r\u2083).\n  \u2022 Case #3 (merge R\u2081,R\u2082 with r\u2081\u2265r\u2082): each element of R\u2081 pays 2 c-tokens (cost \u2264 2r\u2081).\n  \u2022 Cases #4/#5 (merge R\u2081,R\u2082 with r\u2081+r\u2082\u2265r\u2083 or r\u2082+r\u2083\u2265r\u2084): each element of R\u2081 pays 1 c-token and each element of R\u2082 pays 1 s-token (exactly r\u2081+r\u2082 tokens).\n- Balance lemma: s-tokens never go negative. After a #4/#5 merge of R\u2081 and R\u2082, the resulting top run R\u0304\u2081 must be merged again immediately (still in the ending sequence); when that next merge happens, the height of former R\u2082 drops, and those elements are re-credited one s-token before any s-token is spent by the top run. Thus s-tokens are self-replenishing across successive merges of one ending sequence; c-tokens are supplied on height decreases and withdrawals never exceed deposits in any case.\n\nLemma 4 (Per-element token bound)\nFix a run R of length r. Between its push and the end of its starting sequence, its elements can be credited at most O(h) times where h is the stack height then; by Lemma 2, h \u2264 4+2log\u2082(n/r). Thus each element of R receives O(1+log(n/r)) c-tokens and s-tokens in total. Since c/s-spending per element across all ending sequences is bounded by these credits and spending exactly equals merge cost in ending sequences, the total ending-sequence cost charged to R is O(r(1+log(n/r))).\n\nPutting it together\n- Starting sequences: total merge cost O(n) by Lemma 3.\n- Ending sequences: by Lemma 4, total merge cost \u2264 C\u00b7\u2211\u1d62 \u2113\u1d62(1+log(n/\u2113\u1d62)) = C\u00b7(n + \u2211\u1d62 \u2113\u1d62 log(n/\u2113\u1d62)) = C\u00b7(n + n\u00b7H), for a universal constant C.\n- Final force-collapse merges: can be accounted for within the token/cost framework by appending a fictitious last run of length > n (they are absorbed in the above accounting), or bounded by O(n) separately; in any case, they do not change the asymptotic bound.\n- Comparisons: Each merge of sizes a,b uses \u2264 a+b\u22121 comparisons; summing over all merges replaces total merge cost by at most \u201cmerge cost \u2212 (#merges)\u201d and thus differs by O(n). Run detection costs O(n) comparisons. Therefore total comparisons \u2264 (merge cost) + O(n) \u2264 n\u00b7H + O(n) (absorbing constants in the O(n) term).\n\nTheorem (Entropy adaptivity of TimSort)\nLet \u2113\u2081,\u2026,\u2113\u1d63 be the greedy run lengths of an input of length n, and H = \u2211\u1d62 (\u2113\u1d62/n) log\u2082(n/\u2113\u1d62). Python\u2019s TimSort (with the corrected invariant) performs at most n\u00b7H + O(n) comparisons.\n\nWhy useful here. This formalizes that TimSort is instance-adaptive to the distribution of run lengths (Shannon entropy): when a few long runs dominate, H is small and TimSort\u2019s comparison count is linear.\n\nObstacles / caveats\n- The argument relies on the corrected Python TimSort (additional #5-condition). The original/Java variants need extra technical work; without the fix, the invariant can fail.\n- The constant in front of H can be sharpened: a refined potential-argument shows merge cost \u2264 (3/2)\u00b7n\u00b7H + O(n), and Buss\u2013Knop provide matching lower bounds for TimSort; constant 1 is not generally attainable for TimSort merges.\n\nNext steps\n- Extract precise constants hidden in O(n) for starting/ending sequences.\n- Formalize the token accounting as a potential function to get the 3/2\u00b7n\u00b7H + O(n) merge-cost bound.\n- Discuss the Java variant and how to repair it to inherit the same entropy bound.\n\nReferences (for the verifier)\n- Auger\u2013Jug\u00e9\u2013Nicaud\u2013Pivoteau: On the Worst-Case Complexity of TimSort (sections proving O(n+nH) and 1.5\u00b7nH+O(n)).\n- Buss\u2013Knop: lower bounds for TimSort\u2019s merge cost and non-optimality up to factor 3/2.\n"
}