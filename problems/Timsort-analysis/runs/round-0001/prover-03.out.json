{
  "content": "Ideas\n\n- Goal: Show that Python\u2019s TimSort (with the corrected merge_collapse condition r_4 \u2264 r_3 + r_2) sorts an array with run-lengths \u2113_1,\u2026,\u2113_r using at most O(n + n H) comparisons, where H = \u2211_i (\u2113_i/n) log_2(n/\u2113_i).\n- Caveat: The exact inequality \u201c\u2264 n H + O(n)\u201d with leading constant 1 is false for TimSort; there are inputs with \u2265 (3/2) n H \u2212 O(n) merge cost (Buss\u2013Knop; Auger\u2013Jug\u00e9\u2013Nicaud\u2013Pivoteau). The correct statement is: comparisons \u2264 c n H + O(n) with some absolute constant c, and in fact c = 3/2 is tight. We prove here the big-O bound O(n + n H). Why useful: establishes entropy adaptivity up to a constant factor, the standard target in adaptive sorting.\n\nSetup and Notation\n\n- Input of length n decomposes greedily into monotone runs of lengths r_1,\u2026,r_\u03c1 (\u03c1 = number of runs). Define H = \u2211_{i=1}^\u03c1 (r_i/n) log_2(n/r_i).\n- Merge cost of merging runs of lengths a,b is a+b. With standard stable merging, comparisons \u2264 a+b, so total comparisons \u2264 total merge cost + O(n) (for run detection and constant overhead).\n- We analyze the main loop of TimSort through the translated rules (#1 push, #2 merge R2,R3 if r_1 > r_3, #3 merge R1,R2 if r_1 \u2265 r_2, #4 merge R1,R2 if r_1+r_2 \u2265 r_3, #5 merge R1,R2 if r_2+r_3 \u2265 r_4). The corrected Python version includes #5.\n\nLower Bound (why useful: optimality benchmark)\n\n- Claim: Any comparison-based algorithm needs at least n H \u2212 O(n) comparisons for inputs whose run decomposition has lengths r_1,\u2026,r_\u03c1.\n- Sketch: Count arrays compatible with the given run partition; Stirling bounds imply log_2 |C| \u2265 n H \u2212 O(n). Hence some input in the class requires \u2265 n H \u2212 O(n) comparisons.\n\nStack Growth Invariant (why useful: bounds starting-sequence costs)\n\n- Lemma 1 (Fibonacci growth): After each collapse, for all i \u2265 1 with indices valid, r_{i+2} > r_{i+1} + r_i and r_{i+1} > r_i. Proof: Direct induction over cases #1\u2013#5; each case preserves the inequalities when merges stop.\n- Corollary 1: When about to push a new run (i.e., no merges pending), r_i \u2264 2^{(i+1\u2212j)/2} r_j for all i \u2264 j. In particular, deeper runs grow at least geometrically up the stack.\n\nDecomposition of Execution (why useful: separates easy O(n) part)\n\n- Split each iteration into a starting sequence (#1 then as many #2 as possible) and an ending sequence (a nonempty block of #3/#4/#5/#2 merges, but never #1).\n\nStarting Sequences are Cheap\n\n- Lemma 2: The total merge cost incurred by all starting sequences is O(n). More precisely, if a starting sequence begins by pushing a run R of length r and then performs k\u22121 merges of type #2 (merging R_2,R_3,\u2026), its cost is \u2264 \u03b3 r for a constant \u03b3 = 2 \u2211_{j\u22651} j / 2^{j/2}.\n- Proof idea: After the push, the last #2 implies r > r_k and Corollary 1 yields r_k \u2265 2^{(k\u22121\u2212i)/2} r_i. Sum C \u2264 \u2211_{i=1}^k (k+1\u2212i) r_i \u2264 r \u00b7 2 \u2211_{j=1}^k j 2^{\u2212j/2} < \u03b3 r. Summing over runs gives O(n).\n\nEnding Sequences: Token Accounting\n\n- We credit tokens per element to pay for merges:\n  - Credit rule: When a run is pushed or when an element\u2019s height decreases due to a merge in an ending sequence, credit that element with 2 c-tokens and 1 s-token.\n  - Spending rule: For a merge in the ending sequence: \n    - #2: every element of R_1 and R_2 pays 1 c-token.\n    - #3: every element of R_1 pays 2 c-tokens.\n    - #4/#5: every element of R_1 pays 1 c-token and every element of R_2 pays 1 s-token.\n- Lemma 3 (nonnegativity): No element\u2019s token balances ever become negative. Why: Each time #4/#5 spends an s-token from R_2, the very next step must be another merge (same ending sequence), which decreases the height of the new top run and re-credits one s-token to those elements before any further s-token is spent; c-tokens are credited exactly when heights decrease and cover the #2/#3/#4/#5 costs by case.\n\nBounding the Number of Credits per Element (why useful: turns tokens into an entropy sum)\n\n- Lemma 4 (stack height bound): Let a run R of length r be pushed; when its starting sequence ends, the stack height h satisfies h \u2264 4 + 2 log_2(n/r). Proof: Corollary 1 gives r_3 \u2264 2^{2\u2212h/2} n while the last #2 ensures r \u2264 r_3.\n- Consequence: Each element gets at most O(h) events (one push + at most h\u22121 height decreases during ending merges while it remains on the stack), so at most O(h) credits. Thus each element receives \u2264 2h c-tokens and \u2264 h s-tokens.\n- Summing over elements in runs of length r: tokens \u2264 \u0398(r (1 + log(n/r))). Summed over all runs: \u0398(\u2211_i r_i (1 + log(n/r_i))) = \u0398(n + n H).\n- Since token spending exactly equals the merge costs of ending sequences (by the spending rules), ending sequences contribute O(n + n H) merge cost.\n\nMain Theorem (entropy-adaptive upper bound)\n\n- Theorem: For inputs with run-lengths r_1,\u2026,r_\u03c1, Python TimSort performs at most O(n + n H) comparisons.\n- Proof summary: (i) Computing run decomposition costs O(n) comparisons. (ii) Total merge cost in starting sequences is O(n) (Lemma 2). (iii) Total merge cost in ending sequences is O(n + n H) by the token scheme and Lemmas 3\u20134. (iv) Final forced merges can be absorbed into the main-loop analysis (e.g., by a sentinel run) or add O(n). Since comparisons per merge \u2264 merge cost, total comparisons \u2264 O(n + n H).\n\nObstacles / Corrections\n\n- The stronger claim \u201c\u2264 n H + O(n)\u201d (leading constant 1) is false for TimSort. Known constructions force \u2265 (3/2) n log n (hence \u2265 (3/2) n H) merge cost. The refined analysis (potential function \u03d5(r) = (3/2) r log_2 r and grouping merges) shows an upper bound of (3/2) n H + O(n), and Buss\u2013Knop give matching lower bounds: the constant 3/2 is tight.\n\nExamples / Sanity Checks\n\n- All runs length 2: starting sequences alone do all work; costs O(n). H = \u0398(log \u03c1) with \u03c1 \u2248 n/2, so bound O(n + n H) is trivial.\n- One huge run and many tiny ones: ending sequences dominate; the height bound yields per-element O(log(n/r)) credits, summing to O(n + n H).\n\nNext Steps (if stronger bound desired)\n\n- Refine the ending-sequence analysis by a potential \u03a6(run of length r) = (3/2) r log_2 r. Show that (i) many merges are \u201cbalanced\u201d in the sense cost \u2264 \u0394\u03a6, (ii) the few \u201cunbalanced\u201d ones can be paired so that cost \u2264 \u0394\u03a6(pair) + O(r) for the initiating run, and (iii) total additive O(n). This yields the sharp (3/2) n H + O(n) bound.\n\nReferences (for the verifier)\n\n- Auger, Jug\u00e9, Nicaud, Pivoteau: On the Worst-Case Complexity of TimSort (sections proving O(n + n H) and 1.5 n H + O(n)).\n- Munro, Wild: Nearly-Optimal Mergesorts (for the lower bound n H \u2212 O(n) and context).\n"
}