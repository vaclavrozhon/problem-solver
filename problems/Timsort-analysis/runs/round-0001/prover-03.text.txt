Ideas

- Goal: Show that Python’s TimSort (with the corrected merge_collapse condition r_4 ≤ r_3 + r_2) sorts an array with run-lengths ℓ_1,…,ℓ_r using at most O(n + n H) comparisons, where H = ∑_i (ℓ_i/n) log_2(n/ℓ_i).
- Caveat: The exact inequality “≤ n H + O(n)” with leading constant 1 is false for TimSort; there are inputs with ≥ (3/2) n H − O(n) merge cost (Buss–Knop; Auger–Jugé–Nicaud–Pivoteau). The correct statement is: comparisons ≤ c n H + O(n) with some absolute constant c, and in fact c = 3/2 is tight. We prove here the big-O bound O(n + n H). Why useful: establishes entropy adaptivity up to a constant factor, the standard target in adaptive sorting.

Setup and Notation

- Input of length n decomposes greedily into monotone runs of lengths r_1,…,r_ρ (ρ = number of runs). Define H = ∑_{i=1}^ρ (r_i/n) log_2(n/r_i).
- Merge cost of merging runs of lengths a,b is a+b. With standard stable merging, comparisons ≤ a+b, so total comparisons ≤ total merge cost + O(n) (for run detection and constant overhead).
- We analyze the main loop of TimSort through the translated rules (#1 push, #2 merge R2,R3 if r_1 > r_3, #3 merge R1,R2 if r_1 ≥ r_2, #4 merge R1,R2 if r_1+r_2 ≥ r_3, #5 merge R1,R2 if r_2+r_3 ≥ r_4). The corrected Python version includes #5.

Lower Bound (why useful: optimality benchmark)

- Claim: Any comparison-based algorithm needs at least n H − O(n) comparisons for inputs whose run decomposition has lengths r_1,…,r_ρ.
- Sketch: Count arrays compatible with the given run partition; Stirling bounds imply log_2 |C| ≥ n H − O(n). Hence some input in the class requires ≥ n H − O(n) comparisons.

Stack Growth Invariant (why useful: bounds starting-sequence costs)

- Lemma 1 (Fibonacci growth): After each collapse, for all i ≥ 1 with indices valid, r_{i+2} > r_{i+1} + r_i and r_{i+1} > r_i. Proof: Direct induction over cases #1–#5; each case preserves the inequalities when merges stop.
- Corollary 1: When about to push a new run (i.e., no merges pending), r_i ≤ 2^{(i+1−j)/2} r_j for all i ≤ j. In particular, deeper runs grow at least geometrically up the stack.

Decomposition of Execution (why useful: separates easy O(n) part)

- Split each iteration into a starting sequence (#1 then as many #2 as possible) and an ending sequence (a nonempty block of #3/#4/#5/#2 merges, but never #1).

Starting Sequences are Cheap

- Lemma 2: The total merge cost incurred by all starting sequences is O(n). More precisely, if a starting sequence begins by pushing a run R of length r and then performs k−1 merges of type #2 (merging R_2,R_3,…), its cost is ≤ γ r for a constant γ = 2 ∑_{j≥1} j / 2^{j/2}.
- Proof idea: After the push, the last #2 implies r > r_k and Corollary 1 yields r_k ≥ 2^{(k−1−i)/2} r_i. Sum C ≤ ∑_{i=1}^k (k+1−i) r_i ≤ r · 2 ∑_{j=1}^k j 2^{−j/2} < γ r. Summing over runs gives O(n).

Ending Sequences: Token Accounting

- We credit tokens per element to pay for merges:
  - Credit rule: When a run is pushed or when an element’s height decreases due to a merge in an ending sequence, credit that element with 2 c-tokens and 1 s-token.
  - Spending rule: For a merge in the ending sequence: 
    - #2: every element of R_1 and R_2 pays 1 c-token.
    - #3: every element of R_1 pays 2 c-tokens.
    - #4/#5: every element of R_1 pays 1 c-token and every element of R_2 pays 1 s-token.
- Lemma 3 (nonnegativity): No element’s token balances ever become negative. Why: Each time #4/#5 spends an s-token from R_2, the very next step must be another merge (same ending sequence), which decreases the height of the new top run and re-credits one s-token to those elements before any further s-token is spent; c-tokens are credited exactly when heights decrease and cover the #2/#3/#4/#5 costs by case.

Bounding the Number of Credits per Element (why useful: turns tokens into an entropy sum)

- Lemma 4 (stack height bound): Let a run R of length r be pushed; when its starting sequence ends, the stack height h satisfies h ≤ 4 + 2 log_2(n/r). Proof: Corollary 1 gives r_3 ≤ 2^{2−h/2} n while the last #2 ensures r ≤ r_3.
- Consequence: Each element gets at most O(h) events (one push + at most h−1 height decreases during ending merges while it remains on the stack), so at most O(h) credits. Thus each element receives ≤ 2h c-tokens and ≤ h s-tokens.
- Summing over elements in runs of length r: tokens ≤ Θ(r (1 + log(n/r))). Summed over all runs: Θ(∑_i r_i (1 + log(n/r_i))) = Θ(n + n H).
- Since token spending exactly equals the merge costs of ending sequences (by the spending rules), ending sequences contribute O(n + n H) merge cost.

Main Theorem (entropy-adaptive upper bound)

- Theorem: For inputs with run-lengths r_1,…,r_ρ, Python TimSort performs at most O(n + n H) comparisons.
- Proof summary: (i) Computing run decomposition costs O(n) comparisons. (ii) Total merge cost in starting sequences is O(n) (Lemma 2). (iii) Total merge cost in ending sequences is O(n + n H) by the token scheme and Lemmas 3–4. (iv) Final forced merges can be absorbed into the main-loop analysis (e.g., by a sentinel run) or add O(n). Since comparisons per merge ≤ merge cost, total comparisons ≤ O(n + n H).

Obstacles / Corrections

- The stronger claim “≤ n H + O(n)” (leading constant 1) is false for TimSort. Known constructions force ≥ (3/2) n log n (hence ≥ (3/2) n H) merge cost. The refined analysis (potential function ϕ(r) = (3/2) r log_2 r and grouping merges) shows an upper bound of (3/2) n H + O(n), and Buss–Knop give matching lower bounds: the constant 3/2 is tight.

Examples / Sanity Checks

- All runs length 2: starting sequences alone do all work; costs O(n). H = Θ(log ρ) with ρ ≈ n/2, so bound O(n + n H) is trivial.
- One huge run and many tiny ones: ending sequences dominate; the height bound yields per-element O(log(n/r)) credits, summing to O(n + n H).

Next Steps (if stronger bound desired)

- Refine the ending-sequence analysis by a potential Φ(run of length r) = (3/2) r log_2 r. Show that (i) many merges are “balanced” in the sense cost ≤ ΔΦ, (ii) the few “unbalanced” ones can be paired so that cost ≤ ΔΦ(pair) + O(r) for the initiating run, and (iii) total additive O(n). This yields the sharp (3/2) n H + O(n) bound.

References (for the verifier)

- Auger, Jugé, Nicaud, Pivoteau: On the Worst-Case Complexity of TimSort (sections proving O(n + n H) and 1.5 n H + O(n)).
- Munro, Wild: Nearly-Optimal Mergesorts (for the lower bound n H − O(n) and context).
