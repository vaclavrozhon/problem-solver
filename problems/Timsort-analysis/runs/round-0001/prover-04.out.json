{
  "content": "# Goal, status, and corrected target\n\n- Requested claim: \u201cTimSort never uses more than n\u00b7H + O(n) comparisons, where H = \u03a3 (\u2113_i/n) log2(n/\u2113_i) for the run-lengths \u2113_i.\u201d\n- This claim with leading constant 1 is false. There are inputs on which TimSort\u2019s merge cost is \u2265 (3/2)\u00b7n\u00b7log2 n + O(n) (Buss\u2013Knop 2018), and since H \u2264 log2 n, this implies total comparisons \u2265 (3/2)\u00b7n\u00b7H \u2212 O(n) on those inputs. So TimSort cannot satisfy an upper bound with leading constant 1.\n- Correct statements from the literature (Python TimSort core, with De Gouw et al. fix):\n  1) TimSort runs in O(n + nH) time (Auger\u2013Jug\u00e9\u2013Nicaud\u2013Pivoteau). This is an asymptotic bound with some (unspecified) constant.\n  2) A refined, tight bound: total merge cost \u2264 (3/2)\u00b7n\u00b7H + O(n), and this factor 3/2 is optimal for TimSort (A\u2013J\u2013N\u2013P (Thm. 1.5), Buss\u2013Knop\u2019s lower bound family).\n- Below I give (i) a clean, checkable proof that TimSort uses O(n + nH) comparisons, and (ii) the sharper (3/2)\u00b7n\u00b7H + O(n) merge-cost upper bound, plus a matching lower bound family showing the factor 3/2 cannot be improved.\n\n\n## Setup and notation\n\n- Input array of length n is greedily decomposed into \u03c1 maximal monotone runs of lengths r_1,\u2026,r_\u03c1 (allowing decreasing runs, reversed stably).\n- Entropy H = \u03a3_i (r_i/n) log2(n/r_i) = log2 n \u2212 (1/n) \u03a3_i r_i log2 r_i.\n- Merge model: merging runs of sizes a,b costs at most a+b\u22121 comparisons, and we define merge-cost M(a,b)=a+b. Galloping can only reduce comparisons, so comparison bounds follow from merge-cost bounds up to O(\u03c1) = O(n).\n- Counting runs costs O(n) comparisons.\n\n\n# Part I. TimSort uses O(n + nH) comparisons\n\nI outline the standard amortized analysis (Auger\u2013Jug\u00e9\u2013Nicaud\u2013Pivoteau), organized as auditable lemmas.\n\n### Invariants and stack growth\n\n- TimSort maintains a stack of runs R_1,\u2026,R_h (top is R_1), with lengths r_1,\u2026,r_h.\n- Invariant (Fibonacci growth in the \u201cback\u201d of the stack): whenever the inner loop is about to push a run (i.e., all immediate-merge conditions fail), for all i\u22653,\n  r_i + r_{i+1} < r_{i+2} and r_1 < r_2 and r_1 + r_2 < r_3.\n  Sketch of proof: straightforward case-check of the four merge cases (#2\u2013#5); every update preserves the inequalities.\n- Consequence (exponential growth): for all i\u2264j\u2264h,\n  r_i \u2264 2^{(i+1\u2212j)/2} r_j.\n- Stack-height bound after the \u201cstarting sequence\u201d for a just-pushed run R of length r: h \u2264 4 + 2 log2(n/r).\n  Proof: from r = r_1 \u2264 r_3 \u2264 2^{2\u2212h/2} n.\n\nWhy useful: with h controlled in terms of log(n/r), each element \u201cpays\u201d at most O(log(n/r)) in the ensuing amortization.\n\n### Splitting events: starting vs ending sequences\n\nEach iteration (push of a run) decomposes into:\n- Starting sequence: a maximal suffix of #2 merges (absorbing below the newcomer). \n- Ending sequence: a sequence of merges (#3/#4/#5) that repairs the invariant; no #1 occurs inside.\n\n### Cost of starting sequences is linear\n\nLemma A (Starting sequences): the total merge cost over all starting sequences is O(n).\n- Proof sketch: Suppose pushing R of size r triggers k\u22121 merges #2 with R_1,\u2026,R_k. The cost is C \u2264 \u03a3_{i=1}^k (k+1\u2212i) r_i. From the growth bound, r \u2265 r_k \u2265 2^{(k\u22121\u2212i)/2} r_i, so C/r \u2264 2 \u03a3_{j=1}^k j\u00b72^{\u2212j/2} = O(1). Summing over pushes (\u03a3 r = n) gives O(n).\n\nWhy useful: Starting sequences contribute only linear overhead; the entropy term arises solely from ending sequences.\n\n### Token amortization for ending sequences\n\nScheme:\n- Each element receives tokens whenever its height decreases in an ending-sequence merge: 2 \u201cC-tokens\u201d and 1 \u201cS-token\u201d. Tokens are spent to pay merge cost: \n  - In case #2 (if it appears inside an ending sequence): both R_1 and R_2 pay C-tokens.\n  - In #3: R_1 pays 2 C-tokens.\n  - In #4/#5: R_1 pays 1 C-token, R_2 pays 1 S-token.\n- Claim: token balances never go negative throughout an ending sequence (every time an S-token is spent, the next merge in the same ending sequence immediately credits it back; C-tokens are covered by the height-decrease credits).\n\nHence the total cost of ending sequences is upper-bounded by the number of tokens minted. An element at height h can only be credited O(h) times; with the height bound h \u2264 4 + 2 log2(n/r) when R is pushed, each element pays O(log(n/r)). Summing over all elements in the run R gives O(r log(n/r)). Summing over all runs:\n\n\u03a3_{runs R} O(|R| log(n/|R|)) = O(\u03a3 r_i log(n/r_i)) = O(n + nH).\n\nCombining with starting sequences and run detection yields total comparisons:\n\nComparisons \u2264 O(n) [run detection] + (total merge-cost) \u2212 (\u03c1\u22121) \u2264 O(n) + O(n + nH) = O(n + nH).\n\nRemark: The \u201c\u2212(\u03c1\u22121)\u201d is because each merge needs \u2264 a+b\u22121 comparisons.\n\n\n# Part II. Sharp worst-case constant 3/2 for TimSort\n\nTo pin down the best possible leading constant \u03ba for TimSort we use potentials and show \u03ba=3/2 is both achievable (upper bound) and necessary (lower bound family).\n\n### Upper bound: potential method\n\nDefine potential of a run of length r as pot(r) = (3/2) r log2 r. The potential of a configuration is the sum.\n\nKey lemmas (Auger\u2013Jug\u00e9\u2013Nicaud\u2013Pivoteau):\n- Balanced merge lemma: If two runs are within a factor \u03c6^2 of each other (\u03c6 the golden ratio), then the potential increase of merging them is \u2265 their merge cost. \n- Grouping lemma: Any ending sequence can be partitioned into groups (either a single merge or a pair of consecutive merges, one of which must be case #3) such that each group\u2019s total merge cost \u2264 total potential increase for that group, except possibly the first group of an ending sequence, which may exceed by \u2264 r, where r is the length of the just-pushed run. \n- Summing over all ending sequences, the \u201c\u2264 r\u201d slack sums to at most n.\n\nTherefore, the total merge cost of ending sequences \u2264 \u0394pot + n. Since potentials telescope and pot(final single run) \u2212 pot(initial runs) = \u2212(3/2) \u03a3_i r_i log2(r_i) = (3/2) n\u00b7(log2 n \u2212 H) = (3/2) n\u00b7log2 n \u2212 (3/2) n H, the ending-sequence cost is \u2264 (3/2) n\u00b7log2 n \u2212 (3/2) n H + n.\n\nAccounting for starting sequences (O(n)) gives a total merge-cost upper bound \u2264 (3/2) n H + O(n). Translating to comparisons adds only O(n), so comparisons \u2264 (3/2) n H + O(n).\n\n### Lower bound family for TimSort (factor 3/2 is necessary)\n\nBuss\u2013Knop (and also in A\u2013J\u2013N\u2013P) construct a recursive family of run-length sequences R(n) for which TimSort\u2019s merge cost satisfies\n\nc(R(n)) \u2265 (3/2)\u00b7n\u00b7log2 n \u2212 O(n).\n\nSince H \u2264 log2 n always, this implies c(R(n)) \u2265 (3/2)\u00b7n\u00b7H \u2212 O(n). Hence no bound of the form \u201cTimSort \u2264 (1+o(1))\u00b7n\u00b7H\u201d can hold, and the constant 3/2 is optimal for TimSort.\n\n\n# Why the user\u2019s claim (constant 1) cannot hold\n\n- There exist arrays with merge-cost \u2265 (3/2)\u00b7n\u00b7log2 n + O(n) under TimSort. Since H \u2264 log2 n, this \u2265 (3/2)\u00b7n\u00b7H \u2212 O(n). Thus for infinitely many n, TimSort\u2019s comparisons exceed n\u00b7H by an asymptotic factor \u2265 1.5, up to lower-order terms.\n\n\n# Summary of proven results suitable for output.md\n\n- Theorem (coarse). Let r_1,\u2026,r_\u03c1 be run lengths and H = \u03a3 (r_i/n) log2(n/r_i). TimSort performs O(n + nH) comparisons. Sketch: starting sequences cost O(n); ending sequences paid by a token scheme: each element contributes O(log(n/|R|)) tokens \u2192 O(\u03a3 r_i log(n/r_i)) = O(n + nH) total. Run detection costs O(n); each merge uses \u2264 a+b\u22121 comparisons.\n\n- Theorem (sharp). TimSort\u2019s merge cost is \u2264 (3/2)\u00b7n\u00b7H + O(n), and there exist inputs where it is \u2265 (3/2)\u00b7n\u00b7H \u2212 O(n). Hence comparisons \u2264 (3/2)\u00b7n\u00b7H + O(n), and the constant 3/2 is optimal for TimSort.\n\n\n# Obstacles and checks\n\n- The invariant maintenance needs a (routine) case-check (#2\u2013#5) to ensure r_i + r_{i+1} < r_{i+2} holds \u201cfar enough down.\u201d This is local and verifiable.\n- The token accounting: verify that S-tokens are always reimbursed by the next merge within the same ending sequence; C-tokens cover the other cases as heights decrease. This is a direct induction on the inner loop.\n- Galloping only reduces comparisons, so all bounds hold verbatim for real implementations.\n\n\n# Next steps (for the verifier)\n\n- Put the coarse O(n + nH) proof into proofs.md with the three lemmas: (i) invariant and growth, (ii) starting-sequence bound, (iii) token amortization for ending sequences.\n- Add the sharp (3/2)\u00b7n\u00b7H + O(n) bound via potential pot(r) = (3/2) r log2 r and the grouping lemma; include the Buss\u2013Knop lower bound construction summary.\n- Add a short note in output.md: the best constant for TimSort is 3/2; if one wants constant 1, use Huffman-based or powersort/peeksort merges (Munro\u2013Wild) that achieve n\u00b7H + O(n)."
}