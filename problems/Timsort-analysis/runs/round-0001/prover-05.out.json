{
  "content": "# Ideas\n- Goal: Prove Python\u2019s (patched) TimSort uses at most n H + O(n) comparisons on an input whose natural run decomposition has lengths \u2113_1,\u2026,\u2113_r, where H = \u2211_i (\u2113_i/n) log_2(n/\u2113_i).\n- Strategy: Work with merge-cost (= sum of sizes of merged pairs), then convert to comparisons. Split the main loop into starting-sequences (cheap, O(n) total) and ending-sequences (amortize via tokens per element). Use the stack invariant to bound the stack height after the starting-sequence of each pushed run, then sum per-element token charges to obtain O(\u2211 \u2113_i log(n/\u2113_i)) = O(nH).\n\n# Setup and model\n- Consider Python TimSort (post-2017 patch) whose merge-collapsing rule is the translated Algorithm with cases #1\u2013#5 as in the Notes (Auger\u2013Jug\u00e9\u2013Nicaud\u2013Pivoteau). We analyze only comparisons; detecting runs costs O(n).\n- Standard merge between two runs of sizes a and b uses \u2264 a+b\u22121 comparisons; thus total comparisons \u2264 total merge-cost + O(#merges) = total merge-cost + O(n). So an O(nH) bound on merge-cost implies O(nH + n) comparisons.\n\n# Invariants and consequences\nClaim I (Fibonacci growth). At any time (during main loop), for the stack S = (R_1,\u2026,R_h) (top to bottom, lengths r_1,\u2026,r_h), we have r_{i}+r_{i+1} < r_{i+2} for all i \u2265 3, and r_1<r_2, r_1+r_2<r_3, r_2+r_3<r_4 whenever no more merges are immediately triggered.\n- Proof sketch: direct induction by case analysis over updates #1\u2013#5 (as in paper). Each update either shifts indices or appends a new top run at #1 when no collapse conditions hold; both preserve the inequalities.\nCorollary (exponential growth). Just before pushing a new run, for all i \u2264 j \u2264 h, r_i \u2264 2^{(i+1\u2212j)/2} r_j. In particular, r_i strictly increases with i, and each move downward by 2 multiplies by \u22652.\n\n# Decomposition of an iteration\n- Each iteration of the main loop (triggered by pushing a new run, case #1) consists of:\n  1) a starting-sequence: maximal consecutive #2 merges (merge R_2 with R_3 repeatedly while r_1>r_3), possibly none;\n  2) an ending-sequence: then a sequence of merges of type #3, #4, or #5 (and possibly interspersed #2), but no #1, until no rule applies.\n\n# Cost of starting-sequences is O(n)\nLemma S (starting-sequence bound). If a run R of length r is pushed on a stack S and its starting-sequence performs k\u22121 merges #2 of runs R_1,\u2026,R_k beneath R, then the total merge-cost there is \u2264 \u03b3 r where \u03b3 = 2 \u2211_{j\u22651} j 2^{\u2212j/2} (a fixed constant). Summing over all pushed runs yields O(n).\n- Proof: With C = \u2211_{i=1}^k (k+1\u2212i) r_i, and using r>r_k (because the last #2 fails afterwards) and the exponential growth corollary on the pre-push stack (so r_k \u2265 2^{(k\u22121\u2212i)/2} r_i), we get\n  C/r \u2264 \u2211_{i=1}^k (k+1\u2212i) 2^{(i+1\u2212k)/2} = 2 \u2211_{j=1}^k j 2^{\u2212j/2} < \u03b3.\n\n# Token scheme for ending-sequences\nWe pay merge-costs in ending-sequences by tokens assigned to elements.\n- Credits: When a run is pushed (or its height later decreases due to an ending-sequence merge), each element receives 2 c-tokens and 1 s-token.\n- Charges per ending merge:\n  - Case #2: every element of R_1 and R_2 pays 1 c-token.\n  - Case #3: every element of R_1 pays 2 c-tokens.\n  - Case #4 or #5: every element of R_1 pays 1 c-token, every element of R_2 pays 1 s-token.\n- Accounting invariant: Balances of c- and s-tokens never go negative.\n  Proof sketch: For #2\u2013#5, the c-tokens spent are matched by c-credits given when heights drop. For s-tokens (#4/#5), merging R_1 and R_2 makes R_2\u2019s elements topmost; by the post-merge sizes (r\u0304_1\u2265r\u0304_2 or r\u0304_1+r\u0304_2\u2265r\u0304_3) the next operation is another merge; that height drop re-credits an s-token to those elements before they spend another; induct on the steps of the ending-sequence.\n\n# Bounding tokens per element via stack height\nLet R be a run of length r just pushed; let h be the stack height right after its starting-sequence.\nLemma H (height bound). h \u2264 4 + 2 log_2(n/r).\n- Proof: After the starting-sequence, r\u0304_1=r and r\u0304_1\u2264r\u0304_3; by the corollary applied to the pre-push tail, r\u0304_3 \u2264 2^{2\u2212h/2} r\u0304_h \u2264 2^{2\u2212h/2} n, hence r \u2264 2^{2\u2212h/2} n.\n\nConsequence: Each element of R undergoes at most h height decreases during ending-sequences (so receives at most O(h) credits and can spend at most O(h) tokens). Since we credit at most 3 tokens per height decrease (2 c, 1 s), each element of R pays O(h) tokens total.\n\n# Summing costs of ending-sequences\n- Every token pays one unit of merge-cost. Therefore, total ending-sequence merge-cost \u2264 \u2211_runs \u2113 \u00b7 O(h_run) \u2264 C \u2211_runs \u2113 (1 + log(n/\u2113)), using Lemma H with a global constant C.\n- Note \u2211_i \u2113_i log(n/\u2113_i) = n H (base-2 logs) and \u2211_i \u2113_i = n. Hence ending-sequences cost O(n + nH).\n\n# Putting it together (main theorem)\nTheorem (Entropy adaptivity). Python TimSort uses at most n H + O(n) comparisons, where H = \u2211_i (\u2113_i/n) log_2(n/\u2113_i).\n- Proof: Run detection costs O(n). Starting-sequences cost O(n) (Lemma S). Ending-sequences cost O(n+nH) by token accounting and Lemma H. Each merge uses \u2264 size\u22121 comparisons, so comparisons \u2264 merge-cost + O(n). Total \u2264 nH + O(n).\n\n# Examples/checks\n- Two runs of lengths (x, n\u2212x): H = (x/n)log(n/x) + ((n\u2212x)/n)log(n/(n\u2212x)) \u2264 1, so bound gives O(n). TimSort performs one merge.\n- r equal runs: H = log_2 r, bound gives O(n log r), as expected.\n\n# Obstacles and assumptions\n- The extra collapse condition (#5) in Python\u2019s patched TimSort is used to ensure the invariant and the s-token re-credit (next action is a merge). Without it (e.g., older Java variant), the argument needs significant technical repairs.\n- We abstracted galloping; it only reduces comparisons. Converting merge-cost to comparisons incurs at most O(#merges)=O(n).\n\n# Next steps\n- Tighten constants to show merge-cost \u2264 (3/2) n H + O(n), following the potential method (pot(r)=3r log_2 r /2) and balanced/unbalanced merge grouping (as in AJNP and Buss\u2013Knop).\n- Optional: formalize the invariant preservation proof via a small mechanized checker for case updates; explore robustness to tuning parameters (minrun)."
}