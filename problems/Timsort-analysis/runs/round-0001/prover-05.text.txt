# Ideas
- Goal: Prove Python’s (patched) TimSort uses at most n H + O(n) comparisons on an input whose natural run decomposition has lengths ℓ_1,…,ℓ_r, where H = ∑_i (ℓ_i/n) log_2(n/ℓ_i).
- Strategy: Work with merge-cost (= sum of sizes of merged pairs), then convert to comparisons. Split the main loop into starting-sequences (cheap, O(n) total) and ending-sequences (amortize via tokens per element). Use the stack invariant to bound the stack height after the starting-sequence of each pushed run, then sum per-element token charges to obtain O(∑ ℓ_i log(n/ℓ_i)) = O(nH).

# Setup and model
- Consider Python TimSort (post-2017 patch) whose merge-collapsing rule is the translated Algorithm with cases #1–#5 as in the Notes (Auger–Jugé–Nicaud–Pivoteau). We analyze only comparisons; detecting runs costs O(n).
- Standard merge between two runs of sizes a and b uses ≤ a+b−1 comparisons; thus total comparisons ≤ total merge-cost + O(#merges) = total merge-cost + O(n). So an O(nH) bound on merge-cost implies O(nH + n) comparisons.

# Invariants and consequences
Claim I (Fibonacci growth). At any time (during main loop), for the stack S = (R_1,…,R_h) (top to bottom, lengths r_1,…,r_h), we have r_{i}+r_{i+1} < r_{i+2} for all i ≥ 3, and r_1<r_2, r_1+r_2<r_3, r_2+r_3<r_4 whenever no more merges are immediately triggered.
- Proof sketch: direct induction by case analysis over updates #1–#5 (as in paper). Each update either shifts indices or appends a new top run at #1 when no collapse conditions hold; both preserve the inequalities.
Corollary (exponential growth). Just before pushing a new run, for all i ≤ j ≤ h, r_i ≤ 2^{(i+1−j)/2} r_j. In particular, r_i strictly increases with i, and each move downward by 2 multiplies by ≥2.

# Decomposition of an iteration
- Each iteration of the main loop (triggered by pushing a new run, case #1) consists of:
  1) a starting-sequence: maximal consecutive #2 merges (merge R_2 with R_3 repeatedly while r_1>r_3), possibly none;
  2) an ending-sequence: then a sequence of merges of type #3, #4, or #5 (and possibly interspersed #2), but no #1, until no rule applies.

# Cost of starting-sequences is O(n)
Lemma S (starting-sequence bound). If a run R of length r is pushed on a stack S and its starting-sequence performs k−1 merges #2 of runs R_1,…,R_k beneath R, then the total merge-cost there is ≤ γ r where γ = 2 ∑_{j≥1} j 2^{−j/2} (a fixed constant). Summing over all pushed runs yields O(n).
- Proof: With C = ∑_{i=1}^k (k+1−i) r_i, and using r>r_k (because the last #2 fails afterwards) and the exponential growth corollary on the pre-push stack (so r_k ≥ 2^{(k−1−i)/2} r_i), we get
  C/r ≤ ∑_{i=1}^k (k+1−i) 2^{(i+1−k)/2} = 2 ∑_{j=1}^k j 2^{−j/2} < γ.

# Token scheme for ending-sequences
We pay merge-costs in ending-sequences by tokens assigned to elements.
- Credits: When a run is pushed (or its height later decreases due to an ending-sequence merge), each element receives 2 c-tokens and 1 s-token.
- Charges per ending merge:
  - Case #2: every element of R_1 and R_2 pays 1 c-token.
  - Case #3: every element of R_1 pays 2 c-tokens.
  - Case #4 or #5: every element of R_1 pays 1 c-token, every element of R_2 pays 1 s-token.
- Accounting invariant: Balances of c- and s-tokens never go negative.
  Proof sketch: For #2–#5, the c-tokens spent are matched by c-credits given when heights drop. For s-tokens (#4/#5), merging R_1 and R_2 makes R_2’s elements topmost; by the post-merge sizes (r̄_1≥r̄_2 or r̄_1+r̄_2≥r̄_3) the next operation is another merge; that height drop re-credits an s-token to those elements before they spend another; induct on the steps of the ending-sequence.

# Bounding tokens per element via stack height
Let R be a run of length r just pushed; let h be the stack height right after its starting-sequence.
Lemma H (height bound). h ≤ 4 + 2 log_2(n/r).
- Proof: After the starting-sequence, r̄_1=r and r̄_1≤r̄_3; by the corollary applied to the pre-push tail, r̄_3 ≤ 2^{2−h/2} r̄_h ≤ 2^{2−h/2} n, hence r ≤ 2^{2−h/2} n.

Consequence: Each element of R undergoes at most h height decreases during ending-sequences (so receives at most O(h) credits and can spend at most O(h) tokens). Since we credit at most 3 tokens per height decrease (2 c, 1 s), each element of R pays O(h) tokens total.

# Summing costs of ending-sequences
- Every token pays one unit of merge-cost. Therefore, total ending-sequence merge-cost ≤ ∑_runs ℓ · O(h_run) ≤ C ∑_runs ℓ (1 + log(n/ℓ)), using Lemma H with a global constant C.
- Note ∑_i ℓ_i log(n/ℓ_i) = n H (base-2 logs) and ∑_i ℓ_i = n. Hence ending-sequences cost O(n + nH).

# Putting it together (main theorem)
Theorem (Entropy adaptivity). Python TimSort uses at most n H + O(n) comparisons, where H = ∑_i (ℓ_i/n) log_2(n/ℓ_i).
- Proof: Run detection costs O(n). Starting-sequences cost O(n) (Lemma S). Ending-sequences cost O(n+nH) by token accounting and Lemma H. Each merge uses ≤ size−1 comparisons, so comparisons ≤ merge-cost + O(n). Total ≤ nH + O(n).

# Examples/checks
- Two runs of lengths (x, n−x): H = (x/n)log(n/x) + ((n−x)/n)log(n/(n−x)) ≤ 1, so bound gives O(n). TimSort performs one merge.
- r equal runs: H = log_2 r, bound gives O(n log r), as expected.

# Obstacles and assumptions
- The extra collapse condition (#5) in Python’s patched TimSort is used to ensure the invariant and the s-token re-credit (next action is a merge). Without it (e.g., older Java variant), the argument needs significant technical repairs.
- We abstracted galloping; it only reduces comparisons. Converting merge-cost to comparisons incurs at most O(#merges)=O(n).

# Next steps
- Tighten constants to show merge-cost ≤ (3/2) n H + O(n), following the potential method (pot(r)=3r log_2 r /2) and balanced/unbalanced merge grouping (as in AJNP and Buss–Knop).
- Optional: formalize the invariant preservation proof via a small mechanized checker for case updates; explore robustness to tuning parameters (minrun).