{
  "content": "# Ideas\n\n- Correct statement: An \u201cO(n H)\u201d upper bound without additive terms is false for TimSort (H can be 0 for one run; scanning still costs \u0398(n)). The right target is comparisons \u2264 C1 n + C2 n H for universal constants. I provide a self-contained proof of comparisons = O(n + n H) for Python TimSort (with the de Gouw fix r2 + r3 \u2265 r4), plus a short counterexample to \u201cO(n H)\u201d without additive n.\n- Small refinement: Give explicit constant \u03b3 for the total cost of all starting sequences; compute \u03b3 exactly.\n- Accounting detail: comparisons \u2264 merge-cost \u2212 (r \u2212 1) + (n \u2212 1) \u2264 merge-cost + n; we only need to bound merge-cost.\n\n# Examples (counterexample to O(n H) without +n)\n\n- Single run: Array already nondecreasing; run decomposition consists of a single run of length n, so H = 0. TimSort must detect the run, requiring n \u2212 1 adjacent comparisons; hence comparisons \u2265 n \u2212 1. Therefore, an upper bound \u201ccomparisons \u2264 C n H\u201d is impossible (RHS = 0), but comparisons \u2264 O(n + n H) holds.\n\n# Main self-contained proof of comparisons \u2264 O(n + n H)\n\nSetup. Let \u21131,\u2026,\u2113r be the maximal-run lengths, \u2211\u2113i = n, and H = \u2211i (\u2113i/n) log2(n/\u2113i). Consider the Python TimSort core with stack S = (R1,\u2026,Rh) (top to bottom) and lengths (r1,\u2026,rh). After pushing a run, repeatedly apply:\n- (#2) if h \u2265 3 and r1 > r3 then merge (R2,R3);\n- (#3) else if h \u2265 2 and r1 \u2265 r2 then merge (R1,R2);\n- (#4) else if h \u2265 3 and r1 + r2 \u2265 r3 then merge (R1,R2);\n- (#5) else if h \u2265 4 and r2 + r3 \u2265 r4 then merge (R1,R2);\nelse stop; finally force-collapse to one run. Merging runs of sizes a,b costs at most a + b \u2212 1 comparisons; we upper bound by a + b.\n\nInvariant (Fibonacci-type growth). At quiescence (no rule applies): r1 < r2, r1 + r2 < r3, r2 + r3 < r4, and for all 3 \u2264 i \u2264 h \u2212 2: ri + ri+1 < ri+2. Consequently, for i \u2264 j \u2264 h, ri \u2264 2^{(i+1\u2212j)/2} rj. Proof: merges shift indices and preserve tail inequalities; at quiescence, negations of rules give the first three; chaining yields the 2-ary growth.\n\nIteration split. For each push of a run R (length r), the ensuing updates split into: starting sequence = maximal block of (#2) merges; ending sequence = subsequent merges until next push.\n\nStarting sequences (O(n) total). Suppose a starting sequence merges k \u2265 2 preexisting runs R1,\u2026,Rk. Its merge cost C \u2264 \u2211i=1..k (k + 1 \u2212 i) ri. Since the final (#2) ensures r > rk, and by exponential-decay, rk \u2265 2^{(k\u22121\u2212i)/2} ri, we get\nC / r \u2264 \u2211i (k + 1 \u2212 i) 2^{(i+1\u2212k)/2} = 2 \u2211_{j\u22651} j 2^{\u2212j/2} =: \u03b3.\nWith \u2211_{j\u22651} j x^j = x/(1 \u2212 x)^2 at x = 2^{-1/2} = 1/\u221a2, we obtain \u03b3 = 2 \u00b7 (2^{\u22121/2})/(1 \u2212 2^{\u22121/2})^2 = 4/(\u221a2 \u2212 1)^2 \u2248 16.4853. Summing over all pushes (\u2211r = n), total starting-sequence merge cost \u2264 \u03b3 n.\n\nEnding sequences via tokens (amortized). Each element holds two token types, c and s, both worth one merge-cost unit.\n- Credits: On pushing its run, each element gets 2 c and 1 s. Whenever an ending-merge reduces its stack height, it gets +2 c. (s-tokens are handled below.)\n- Charges: #2: every element of R1 and R2 pays 1 c; #3: every element of R1 pays 2 c; #4/#5: every element of R1 pays 1 c and every element of R2 pays 1 s.\nNo deficits. For c, each height drop credits 2 c covering charges (#2/#3/#4/#5). For s, only R2 pays 1 s in #4/#5; that merge must be followed immediately by another ending merge (by the rules), which reduces their height and re-credits 1 s before any further s-charge. Induction over merges shows non-negativity.\n\nHeight bound after the starting sequence. Let h be the height when R (length r) finishes its starting sequence. Then r = r1 \u2264 r3, and by exponential growth on the tail r3 \u2264 2^{2 \u2212 h/2} n. Hence h \u2264 4 + 2 log2(n/r).\n\nEnding-sequence cost bound. Each element experiences at most h height drops, thus accrues \u2264 2 h c and h s credits; by the height bound, that is O(1 + log(n/r)) per element. Tokens cover all ending-sequence merge cost, so total ending cost \u2264 C \u2211r (1 + log(n/r)) = O(n + \u2211 \u2113i log(n/\u2113i)) = O(n + n H).\n\nComparisons vs merge-cost. Over all merges, comparisons \u2264 (sum over merges of (a + b \u2212 1)) + (n \u2212 1) \u2264 (merge-cost) \u2212 (r \u2212 1) + (n \u2212 1) \u2264 (merge-cost) + n. Therefore comparisons \u2264 \u03b3 n + O(n + n H) = O(n + n H).\n\nLower bound. Any comparison sort must use \u2265 log2(n!/(\u220f \u2113i!)) = n H \u2212 O(n) comparisons (Stirling). Hence the bound is optimal up to constants.\n\n# Obstacles\n\n- The strict \u201cO(n H)\u201d (no additive n) is impossible because H can be 0 while comparisons \u2265 n \u2212 1 (run detection); also TimSort\u2019s worst-case leading constant is \u2265 3/2 on families with H \u2248 log n.\n\n# Next steps\n\n- Promote the above to proofs.md with explicit constants (\u03b3 \u2248 16.49 for starting sequences; comparisons \u2264 merge-cost + n). Optionally, add the refined tight upper bound comparisons \u2264 (3/2) n H + O(n) via the potential method (as in the cited analysis)."
}