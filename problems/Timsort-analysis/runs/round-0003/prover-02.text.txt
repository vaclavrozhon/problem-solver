# Ideas

- Correct statement: An “O(n H)” upper bound without additive terms is false for TimSort (H can be 0 for one run; scanning still costs Θ(n)). The right target is comparisons ≤ C1 n + C2 n H for universal constants. I provide a self-contained proof of comparisons = O(n + n H) for Python TimSort (with the de Gouw fix r2 + r3 ≥ r4), plus a short counterexample to “O(n H)” without additive n.
- Small refinement: Give explicit constant γ for the total cost of all starting sequences; compute γ exactly.
- Accounting detail: comparisons ≤ merge-cost − (r − 1) + (n − 1) ≤ merge-cost + n; we only need to bound merge-cost.

# Examples (counterexample to O(n H) without +n)

- Single run: Array already nondecreasing; run decomposition consists of a single run of length n, so H = 0. TimSort must detect the run, requiring n − 1 adjacent comparisons; hence comparisons ≥ n − 1. Therefore, an upper bound “comparisons ≤ C n H” is impossible (RHS = 0), but comparisons ≤ O(n + n H) holds.

# Main self-contained proof of comparisons ≤ O(n + n H)

Setup. Let ℓ1,…,ℓr be the maximal-run lengths, ∑ℓi = n, and H = ∑i (ℓi/n) log2(n/ℓi). Consider the Python TimSort core with stack S = (R1,…,Rh) (top to bottom) and lengths (r1,…,rh). After pushing a run, repeatedly apply:
- (#2) if h ≥ 3 and r1 > r3 then merge (R2,R3);
- (#3) else if h ≥ 2 and r1 ≥ r2 then merge (R1,R2);
- (#4) else if h ≥ 3 and r1 + r2 ≥ r3 then merge (R1,R2);
- (#5) else if h ≥ 4 and r2 + r3 ≥ r4 then merge (R1,R2);
else stop; finally force-collapse to one run. Merging runs of sizes a,b costs at most a + b − 1 comparisons; we upper bound by a + b.

Invariant (Fibonacci-type growth). At quiescence (no rule applies): r1 < r2, r1 + r2 < r3, r2 + r3 < r4, and for all 3 ≤ i ≤ h − 2: ri + ri+1 < ri+2. Consequently, for i ≤ j ≤ h, ri ≤ 2^{(i+1−j)/2} rj. Proof: merges shift indices and preserve tail inequalities; at quiescence, negations of rules give the first three; chaining yields the 2-ary growth.

Iteration split. For each push of a run R (length r), the ensuing updates split into: starting sequence = maximal block of (#2) merges; ending sequence = subsequent merges until next push.

Starting sequences (O(n) total). Suppose a starting sequence merges k ≥ 2 preexisting runs R1,…,Rk. Its merge cost C ≤ ∑i=1..k (k + 1 − i) ri. Since the final (#2) ensures r > rk, and by exponential-decay, rk ≥ 2^{(k−1−i)/2} ri, we get
C / r ≤ ∑i (k + 1 − i) 2^{(i+1−k)/2} = 2 ∑_{j≥1} j 2^{−j/2} =: γ.
With ∑_{j≥1} j x^j = x/(1 − x)^2 at x = 2^{-1/2} = 1/√2, we obtain γ = 2 · (2^{−1/2})/(1 − 2^{−1/2})^2 = 4/(√2 − 1)^2 ≈ 16.4853. Summing over all pushes (∑r = n), total starting-sequence merge cost ≤ γ n.

Ending sequences via tokens (amortized). Each element holds two token types, c and s, both worth one merge-cost unit.
- Credits: On pushing its run, each element gets 2 c and 1 s. Whenever an ending-merge reduces its stack height, it gets +2 c. (s-tokens are handled below.)
- Charges: #2: every element of R1 and R2 pays 1 c; #3: every element of R1 pays 2 c; #4/#5: every element of R1 pays 1 c and every element of R2 pays 1 s.
No deficits. For c, each height drop credits 2 c covering charges (#2/#3/#4/#5). For s, only R2 pays 1 s in #4/#5; that merge must be followed immediately by another ending merge (by the rules), which reduces their height and re-credits 1 s before any further s-charge. Induction over merges shows non-negativity.

Height bound after the starting sequence. Let h be the height when R (length r) finishes its starting sequence. Then r = r1 ≤ r3, and by exponential growth on the tail r3 ≤ 2^{2 − h/2} n. Hence h ≤ 4 + 2 log2(n/r).

Ending-sequence cost bound. Each element experiences at most h height drops, thus accrues ≤ 2 h c and h s credits; by the height bound, that is O(1 + log(n/r)) per element. Tokens cover all ending-sequence merge cost, so total ending cost ≤ C ∑r (1 + log(n/r)) = O(n + ∑ ℓi log(n/ℓi)) = O(n + n H).

Comparisons vs merge-cost. Over all merges, comparisons ≤ (sum over merges of (a + b − 1)) + (n − 1) ≤ (merge-cost) − (r − 1) + (n − 1) ≤ (merge-cost) + n. Therefore comparisons ≤ γ n + O(n + n H) = O(n + n H).

Lower bound. Any comparison sort must use ≥ log2(n!/(∏ ℓi!)) = n H − O(n) comparisons (Stirling). Hence the bound is optimal up to constants.

# Obstacles

- The strict “O(n H)” (no additive n) is impossible because H can be 0 while comparisons ≥ n − 1 (run detection); also TimSort’s worst-case leading constant is ≥ 3/2 on families with H ≈ log n.

# Next steps

- Promote the above to proofs.md with explicit constants (γ ≈ 16.49 for starting sequences; comparisons ≤ merge-cost + n). Optionally, add the refined tight upper bound comparisons ≤ (3/2) n H + O(n) via the potential method (as in the cited analysis).