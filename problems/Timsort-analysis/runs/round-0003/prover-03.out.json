{
  "content": "Ideas\n\n- Target (coarse entropy adaptivity). For Python\u2019s patched TimSort (with the extra rule \u201c#5: r_2 + r_3 \u2265 r_4\u201d), the number of comparisons on inputs with greedy run-lengths \u2113_1,\u2026,\u2113_r and entropy H = \u2211(\u2113_i/n) log_2(n/\u2113_i) is O(n + nH). Comparisons \u2264 merge cost + O(n) (run detection + constant overhead), so it suffices to bound total merge cost by O(n + nH).\n- Decomposition. Split each iteration into a starting sequence (#1 then maximal #2\u2019s) and an ending sequence (begins with #3/#4/#5 and contains no #1). Two central lemmas (proved below) complete the coarse bound: (i) Starting sequences contribute O(n). (ii) Ending sequences\u2019 cost \u2264 sum over elements of O(height-changes) = O(\u2211 \u2113_i (1 + log(n/\u2113_i))) = O(n + nH), by a token scheme and a stack-height bound.\n\nProof nuggets (to add to proofs.md)\n\n- Lemma 1 (Fibonacci growth invariant). After collapse, for all legal i, r_{i+2} > r_{i+1} + r_i and r_{i+1} > r_i. In particular, when about to push, r_i \u2264 2^{(i+1\u2212j)/2} r_j for i \u2264 j. Proof: case analysis over #1\u2013#5; if pushing (#1), none of #2\u2013#5 holds; merges shift indices and preserve the strict inequalities.\n- Lemma 2 (Starting sequences cost O(n) with explicit constant). If pushing a run R of length r triggers k\u22121 merges of type #2 (merging runs R_2,\u2026,R_k), then total cost C \u2264 \u2211_{i=1}^k (k+1\u2212i) r_i. From the last #2, r > r_k, and the growth corollary gives r \u2265 r_k \u2265 2^{(k\u22121\u2212i)/2} r_i, hence\n  C/r \u2264 2 \u2211_{j=1}^k j 2^{\u2212j/2} < \u03b3 with \u03b3 = 2\u00b7q/(1\u2212q)^2 and q = 2^{\u22121/2} \u2248 0.7071; numerically \u03b3 \u2248 16.48. Summing over all pushes (\u2211 r = n) yields total starting cost \u2264 \u03b3 n = O(n).\n- Lemma 3 (Height bound after a starting sequence). Let a run of length r be pushed; when its starting sequence finishes, the stack height h satisfies h \u2264 4 + 2 log_2(n/r). Reason: runs r_3,\u2026,r_h below are untouched, so r_3 \u2264 2^{2\u2212h/2} r_h \u2264 2^{2\u2212h/2} n, and no further #2 holds, so r \u2264 r_3.\n- Lemma 4 (Ending sequences: token accounting). Credit rule: when a run is pushed, or when an element\u2019s height decreases due to a merge in an ending sequence, credit that element 2 c-tokens and 1 s-token. Spending: #2 spends 1 c from each element of R_1,R_2; #3 spends 2 c from each element of R_1; #4/#5 spend 1 c from each element of R_1 and 1 s from each element of R_2. Nonnegativity: After any #2/#3/#4/#5, every charged element\u2019s height decreases, so the post-merge credit (2 c + 1 s) covers the c-spend. For s-tokens: merges #4/#5 must be followed by another merge (see Lemma 5), which decreases the new top height and re-credits an s-token before any further s-spend. Hence s-balances never go negative.\n- Lemma 5 (Follow-up merges after #4/#5). If #4 merges R_1 and R_2, the new top lengths are r_1\u2032 = r_1 + r_2 and r_2\u2032 = r_3, so #3 (r_1\u2032 \u2265 r_2\u2032) holds immediately. If #5 merges R_1 and R_2, then r_1\u2032 + r_2\u2032 = (r_1 + r_2) + r_3 \u2265 r_1 + (r_2 + r_3) \u2265 r_1 + r_4 > r_4 = r_3\u2032, so #4 holds. Thus #4/#5 always force at least one more merge next.\n- Summation. Each element in a run of length r experiences at most O(h) height decreases after the starting sequence, with h \u2264 4 + 2 log_2(n/r) (Lemma 3). Thus per element tokens minted = O(1 + log(n/r)), and ending-sequence cost = O(\u2211 r (1 + log(n/r))) = O(n + nH). Adding starting sequences (O(n)) and run detection (O(n)) yields total merge cost O(n + nH). Because merging a,b needs \u2264 a+b\u22121 comparisons, total comparisons are also O(n + nH).\n\nExamples (sanity checks)\n\n- After #4, #3 next: take (from top) r_1=5, r_2=6, r_3=10. Since 5+6 \u2265 10, #4 merges 5 and 6 \u2192 new top 11, next 10; #3 (11 \u2265 10) merges again. This illustrates Lemma 5 and s-token re-crediting.\n- Extreme distribution. One run of size n and many of size 1: H = O(1). The height bound gives h = O(1) for the big run\u2019s elements; total cost is linear as predicted.\n\nObstacles\n\n- The original target \u201c\u2264 nH + O(n) comparisons\u201d with leading constant 1 is false for TimSort (Buss\u2013Knop show \u2265 (3/2) n log n merge cost on some inputs). We keep the coarse O(n + nH) and, if needed, the sharp (3/2)\u00b7nH + O(n).\n- Our s-token argument relies only on the fact that neither #4 nor #5 can end an ending sequence (Lemma 5), which holds in Python\u2019s patched TimSort. We restrict to that variant; galloping does not increase comparisons.\n\nNext steps\n\n- Move Lemmas 1\u20135 and the O(n + nH) proof into proofs.md with explicit constants \u03b3 and the conversion to comparisons. Note reliance on the Python-patched merge_collapse (includes #5).\n- Optionally add the refined (3/2)\u00b7nH + O(n) bound via potential \u03a6(r) = (3/2) r log_2 r: (i) balanced-merge lemma (sizes within \u03c6^2), (ii) forbidden patterns in ending sequences, (iii) grouping unbalanced merges so each group\u2019s cost \u2264 \u0394\u03a6 plus at most r for the first group; total slack O(n)."
}