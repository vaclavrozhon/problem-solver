=== SYSTEM ===
You are a strict mathematical verifier & research manager.

You are being run in a loop with provers. Your tasks:
- Audit their ideas for correctness and rigor.
- Curate files (notes.md, proofs.md, and output.md).
- Suggest the next steps.

Inputs: 
- task statement
- outputs of provers
- notes.md (informal progress & exploratory work so far)
- proofs.md (rigorous proofs & detailed mathematics so far)
- output.md (main results summary so far)

**Three-tier file system guidelines**:
- **notes.md**: Readable running log of exploratory work, intuitions, failed attempts, ideas. Prefer appending. Replace only if old notes are clearly misleading. Should be self-contained.
- **proofs.md**: Complete rigorous mathematical proofs with full details. Think twice before adding anything. Contains proven lemmas, theorems, and detailed derivations. Only add mathematically verified content here.
- **output.md**: This is the only file that the user is supposed to read. It has to be self-contained. It should contain ONLY the main problem results, and a discussion why they are relevant. It is extremely selective. Think twice before promoting proofs from proofs.md to output.md. Only if the result marks clear progress on the problem, and only if the proof in proofs.md has been thoroughly vetted by provers and you, it can join output.md. Each result must be followed by a rigorous, self-contained proof that properly defines all objects of interest and is essentially publication-ready. 

Tasks:
1) Audit rigor: identify unjustified steps, false statements, missing lemmas; produce minimal counterexamples.
2) Triage value: separate genuine progress from noise.
3) Guide next steps: propose what provers should try next.

**Return strictly JSON**:
{
  "feedback_md": "Detailed critique of prover outputs (≥200 words). Include concrete next-step suggestions.",
  "summary_md": "Concise summary of this round's work for summarizer.",
  "verdict": "promising|uncertain|unlikely",
  "blocking_issues": ["List of issues preventing progress"],
  "per_prover": [{"prover_id": "01", "brief_feedback": "feedback", "score": "promising|uncertain|unlikely"}],
  "notes_update": {"action": "append|replace", "content": "Markdown for notes.md"},
  "proofs_update": {"action": "append|replace", "content": "Markdown for proofs.md"}, 
  "output_update": {"action": "append|replace", "content": "Markdown for output.md"}
}

**Verdict definitions**:
- **promising**: Significant mathematical progress, publishable results, or clear path to solution
- **uncertain**: Some progress but with unresolved issues or incomplete reasoning  
- **unlikely**: No meaningful progress, fundamentally flawed approaches, or just exploratory work

**Update guidelines**:
- Use null for any update field you don't want to modify
- notes_update: Include informal reasoning, failed attempts, intuitions, exploratory work
- proofs_update: Only include complete, rigorous proofs that you have verified
- output_update: Only include main problem results that directly advance toward the solution


=== USER ===
=== Task (txt) ===

prove that timsort adapts to the entropy of the distribution of run lengths. I.e.,  prove that it never uses more than <math> O(n\mathcal{H} ) </math> comparisons, where <math>\mathcal{H} = \sum_{i=1}^{r} \frac{\ell_i}{n} \log_2\left({\frac n{\ell_i}}\right)</math>, for <math>\ell_1,\ldots,\ell_r</math> the lengths of the runs in the input.


The goal is a fully self-contained proof. 


=== Notes ===

# Research Notes



We target entropy adaptivity of TimSort on inputs decomposed into runs of lengths ℓ1,…,ℓr with Σℓi=n and H=Σ (ℓi/n) log2(n/ℓi).

Key status:
- The statement “TimSort uses ≤ n H + O(n) comparisons (leading constant 1)” is false. Buss–Knop exhibited run-length sequences that force ≥ (3/2 − o(1)) n log2 n comparisons for TimSort; for these inputs, H ≈ log2 n.
- True: Python TimSort (with the de Gouw et al. fix, i.e., the extra guard r2 + r3 ≥ r4) achieves comparisons ≤ O(n + n H). We now have a self-contained proof.
- Stronger/tight: comparisons ≤ (3/2) n H + O(n) is provable via a potential-function analysis; this is tight. We will formalize this later.

Proof strategy for O(n + nH):
1) Model TimSort’s core stack rules (top=R1, below=R2,…): After pushing a new run, repeatedly apply (in order):
   #2: if h≥3 and r1>r3 merge (R2,R3); #3: if h≥2 and r1≥r2 merge (R1,R2); #4: if h≥3 and r1+r2≥r3 merge (R1,R2); #5 (fix): if h≥4 and r2+r3≥r4 merge (R1,R2). Finally collapse.
   Cost model: merge(a,b) costs a+b; comparisons ≤ total merge cost + O(n) for scanning.
2) Fibonacci-type invariant: At quiescence (before the next push), r1<r2, r1+r2<r3, r2+r3<r4, and for i≥3: ri+ri+1<ri+2. Hence exponential growth down the stack: ri ≤ 2^{(i+1−j)/2} rj.
3) Decompose each iteration into a starting sequence (maximal consecutive #2’s after push) and an ending sequence (subsequent merges until next push). Show starting sequences have total cost O(n): for a starting sequence that merges k preexisting runs R1,…,Rk, the cost is ≤ γ r with a universal constant γ, using r>rk and the exponential growth bound.
4) Token accounting for ending sequences: Each element receives 2 c-tokens and 1 s-token upon push and whenever its height drops during an ending merge. Charge merges as follows: #2: top two runs pay 1 c each; #3: top run pays 2 c; #4/#5: top run pays 1 c, second run pays 1 s. Show no deficits: s-tokens used by R2 in #4/#5 are immediately re-credited on the subsequent merge (the ending sequence must continue), and c-credits on height drops cover c-charges.
5) Stack-height bound for a run of length r after its starting sequence: h ≤ 4 + 2 log2(n/r), since r1=r ≤ r3 ≤ 2^{2−h/2} n by the exponential-growth bound. Thus each element accrues O(h)=O(1+log(n/r)) credits, paying for O(h) merge cost. Summing over all elements yields O(Σ r (1+log(n/r)))=O(n+nH).
6) Add O(n) for run detection/overhead. Hence comparisons ≤ O(n+nH).

Lower bound: Any comparison sort needs ≥ nH − O(n) comparisons when the run partition is known, by counting n!/(ℓ1!…ℓr!) and Stirling.

Next steps:
- Polish constants and edge cases (empty/singleton runs, final-collapse folding), then promote the O(n + nH) result to output.md.
- Optionally, finalize the potential-function proof for the tight (3/2) nH + O(n) upper bound and add it to proofs.md.
- If the goal truly requires a leading constant 1, switch to a nearly optimal stable mergesort (e.g., powersort/peeksort): comparisons ≤ nH + O(n).


=== Current Output ===

# Main Results



# Entropy-Adaptivity of TimSort

We consider Python’s TimSort (with the de Gouw et al. repair, i.e., the extra guard r2 + r3 ≥ r4) in the comparison model. The input array of length n is greedily decomposed into maximal monotone runs; decreasing runs are reversed (stably), so all runs are nondecreasing. Let the run lengths be ℓ1,…,ℓr with Σℓi = n, and define the binary entropy of the run-length distribution
H = Σ_{i=1}^r (ℓ_i/n) log2(n/ℓ_i).
We count comparisons; merging runs of lengths a and b needs at most a + b − 1 comparisons; detecting runs and loop overhead costs O(n). We upper bound by the “merge cost” M = Σ(a+b) over all merges and add O(n) at the end.

Theorem (Entropy adaptivity). Python’s TimSort performs at most C n + C′ n H comparisons for universal constants C, C′. Equivalently, comparisons = O(n + nH).

Algorithmic core (abstracted rules). TimSort maintains a stack of runs R1,…,Rh (top to bottom) with lengths r1,…,rh. After pushing a new run, repeatedly apply the first applicable rule:
(#2) if h ≥ 3 and r1 > r3 then merge (R2,R3);
(#3) else if h ≥ 2 and r1 ≥ r2 then merge (R1,R2);
(#4) else if h ≥ 3 and r1 + r2 ≥ r3 then merge (R1,R2);
(#5) else if h ≥ 4 and r2 + r3 ≥ r4 then merge (R1,R2);
otherwise stop. After all pushes, force-collapse until one run remains.

1) Fibonacci-type growth invariant. At any quiescent moment (no rule applies), the stack satisfies r1 < r2, r1 + r2 < r3, r2 + r3 < r4, and for all i ≥ 3: ri + ri+1 < ri+2. Proof: If quiescent, the negations of the rules yield the first three inequalities; merges only shift indices and preserve ri + ri+1 < ri+2 for i ≥ 3. Consequently, ri+2 ≥ 2 ri, hence for i ≤ j ≤ h: ri ≤ 2^{(i+1−j)/2} rj.

2) Iteration decomposition. For each push of a run R (length r), split ensuing updates into: starting sequence = maximal block of (#2)-merges; ending sequence = the later merges until the next push.

3) Starting sequences cost O(n). Fix a starting sequence that merges the top k pre-existing runs R1,…,Rk (k ≥ 2). Its total merge cost is C ≤ Σ_{i=1}^k (k+1−i) ri. The last (#2) merge ensures r > rk; applying the exponential-decay bound to the pre-existing stack yields rk ≥ 2^{(k−1−i)/2} ri. Thus C/r ≤ Σ (k+1−i) 2^{(i+1−k)/2} = 2 Σ_{j≥1} j 2^{−j/2} =: γ, a universal constant. Summing over all pushes (Σ r = n) gives O(n) total.

4) Token scheme for ending sequences. We amortize ending merges with per-element tokens (each pays 1 unit of merge cost), of two kinds: c-tokens and s-tokens.
Credits: When a run is pushed, each element receives 2 c-tokens and 1 s-token. Whenever an ending-sequence merge lowers an element’s stack height, it receives again 2 c-tokens; additionally, in #4/#5 merges, the elements of R2 will be re-credited 1 s-token on the immediately following ending merge (see below).
Charges: For a merge in the ending sequence:
- #2 (merge R2,R3): each element of R1 and R2 pays 1 c-token. Since r1 > r3, r1 + r2 ≥ r2 + r3 covers the cost.
- #3 (merge R1,R2): each element of R1 pays 2 c-tokens; cost ≤ 2 r1 as r1 ≥ r2.
- #4/#5 (merge R1,R2): each element of R1 pays 1 c-token and each element of R2 pays 1 s-token; cost equals r1 + r2.
No deficits. In #2/#3/#4/#5, the height of elements in the top run (and sometimes the second) drops, and each drop credits 2 c-tokens, covering c-charges. For s-tokens, only #4/#5 charge R2. After such a merge, the resulting top run necessarily triggers another ending merge immediately (by #3 or #4/#5), reducing its height and re-crediting 1 s-token before any subsequent s-charge. Hence no element’s c- or s-balance becomes negative (induction over merges).

5) Stack-height bound after the starting sequence. Let h be the stack height when the starting sequence of R (length r) ends. At that time, r = r1 ≤ r3, while for the tail r3 ≤ 2^{2−h/2} n by the exponential bound. Thus r ≤ 2^{2−h/2} n, i.e., h ≤ 4 + 2 log2(n/r).

6) Ending-sequence cost. Each height drop re-credits 2 c-tokens, so each element accrues O(h) credits; the s-token is re-credited at most once per height drop. Therefore, each element of a run of length r contributes O(1 + log(n/r)) tokens during its ending sequence (by the height bound). Summing over all elements gives O(Σ r (1 + log(n/r))) = O(n + Σ r log(n/r)) = O(n + n H) for the ending-sequence merge cost.

Conclusion. Starting sequences cost O(n); ending sequences cost O(n + nH). Adding O(n) for the run scan and control, we obtain comparisons ≤ C n + C′ n H.

Lower bound (optimality up to constants). Any comparison sort needs at least n H − O(n) comparisons on inputs with fixed run partition ℓ1,…,ℓr (distinct keys). Indeed, there are n!/(ℓ1!⋯ℓr!) compatible permutations; Stirling’s bounds give log2(n!) − Σ log2(ℓi!) = nH − O(n).

Remarks. (i) The stricter “≤ nH + O(n)” with leading constant 1 is false for TimSort: there are families with ≥ (3/2 − o(1)) n log2 n comparisons and H ≈ log2 n. (ii) A refined potential-function analysis yields the tight upper bound comparisons ≤ (3/2) nH + O(n) for TimSort.



=== Prover 1 Output ===

## Ideas
- The target “TimSort uses O(n H) comparisons” is false in general (H can be 0 for a single run, yet TimSort needs Θ(n) comparisons to scan). The correct entropy-adaptive statement is comparisons ≤ C n + C′ n H.
- I give a self-contained proof for Python TimSort (with the de Gouw–et al. guard r2 + r3 ≥ r4) that comparisons ≤ O(n + nH), by amortizing merges via a token scheme and using a Fibonacci-type stack-growth invariant.

## Model and setup
- Input: n elements decomposed greedily into r maximal monotone runs of lengths ℓ1,…,ℓr (Σℓi=n). Let H = Σ (ℓi/n) log2(n/ℓi).
- We analyze TimSort’s core stack rules. The stack holds lengths r1,…,rh (top to bottom). After pushing a run, repeatedly do the first applicable:
  (2) if h≥3 and r1>r3: merge (R2,R3);
  (3) else if h≥2 and r1≥r2: merge (R1,R2);
  (4) else if h≥3 and r1+r2≥r3: merge (R1,R2);
  (5) else if h≥4 and r2+r3≥r4: merge (R1,R2);
  else stop. After all pushes, force-collapse.
- Cost model: merging runs of sizes a,b costs a+b; comparisons ≤ total merge cost + O(n) (run detection and control).

## Lemmas and proofs
Lemma 1 (quiescent invariant). At any quiescent point (no rule applies): r1<r2, r1+r2<r3, r2+r3<r4, and for all i≥3, ri+ri+1<ri+2.
Proof. If quiescent, the negations of (2)–(5) yield the first three. A merge shifts indices down and preserves ri+ri+1<ri+2 for i≥3 by induction. ∎

Corollary 2 (exponential growth). For 1≤i≤j≤h: ri ≤ 2^{(i+1−j)/2} rj. In particular, for the top run r=r1, when the starting sequence ends (see below), h ≤ 4+2 log2(n/r).
Proof. From Lemma 1, ri+2 ≥ 2 ri hence ri ≤ 2^{−k} ri+2k ≤ 2^{−k} ri+2k+1. For the height: r1=r ≤ r3 ≤ 2^{2−h/2} n. ∎

Definition (starting/ending sequences). After pushing a run R (length r), the maximal initial block of (2)-merges is the starting sequence; following merges up to the next push form the ending sequence.

Lemma 3 (starting sequences are linear). The total merge cost of all starting sequences is O(n).
Proof. A starting sequence that merges the top k preexisting runs R1,…,Rk has cost C ≤ Σ_{i=1}^k (k+1−i) ri. Since the last (2) requires r>rk and by Corollary 2, rk ≥ 2^{(k−1−i)/2} ri, we get C/r ≤ 2 Σ_{j≥1} j 2^{−j/2} =: γ. Summing over all pushes (Σ r = n) gives O(n). ∎

Token scheme for ending sequences. Two token types per element: c- and s-tokens.
- Credits: upon push, each element gets 2 c + 1 s; whenever an ending-merge lowers an element’s height, it gets again 2 c; for s, see below.
- Charges: (2) merge (R2,R3): each element of R1 and R2 pays 1 c (r1+r2 ≥ r2+r3). (3): each element of R1 pays 2 c (cost ≤ 2 r1). (4)/(5): each element of R1 pays 1 c and each element of R2 pays 1 s (cost = r1+r2).

Lemma 4 (no deficits). No element ever runs out of c- or s-tokens during ending sequences.
Proof. c: every height drop immediately credits 2 c, covering c-charges in the same/next step. s: only (4)/(5) charge R2; the next ending merge is forced immediately by the triggering condition, lowering the height of the new top run and re-crediting 1 s before any further s-charge. Induction on merges. ∎

Lemma 5 (height bound per element). If R has length r, when its starting sequence ends the stack height h ≤ 4 + 2 log2(n/r). Hence each element of R experiences O(h) height drops and accrues O(1+log(n/r)) credits.
Proof. Corollary 2 yields the bound. Each height drop credits anew; the number of drops per element is O(h). ∎

## Main theorem and proof
Theorem. TimSort performs at most O(n + nH) comparisons on inputs with run lengths ℓ1,…,ℓr.
Proof. Starting sequences cost O(n) (Lemma 3). Ending sequences cost at most the total number of tokens spent; per element this is O(1+log(n/r)) (Lemmas 4–5). Summing over all runs gives O(Σ r (1+log(n/r))) = O(n + nH). Adding O(n) overhead and using comparisons ≤ merge cost + O(n) completes the proof. ∎

## Lower bound (optimal up to constants)
Proposition. Any comparison sort needs ≥ nH − O(n) comparisons on inputs with fixed run lengths ℓ1,…,ℓr (distinct keys).
Proof. The number of compatible permutations is n!/(ℓ1!⋯ℓr!). Taking logs and applying Stirling yields nH − O(n). ∎

## Obstacles
- The stronger “O(n H)” (without the additive n) is false: a single already-sorted run has H=0 but TimSort uses Θ(n) comparisons to detect the run and terminate. Moreover, the best-possible leading constant in front of nH for TimSort is 3/2, not 1: there are families with ≥ (3/2 − o(1)) n log2 n comparisons (H≈log2 n).

## Next steps
- If desired, I can supply the refined tight bound comparisons ≤ (3/2) nH + O(n) via a potential function Φ(r) = (3/2) r log2 r (grouping merges and showing balancedness), and recall the matching lower-bound family.
- Clean integration into output: state the O(n + nH) theorem with the above lemmas and remark why O(nH) cannot hold for TimSort.


=== Prover 2 Output ===

# Ideas

- Correct statement: An “O(n H)” upper bound without additive terms is false for TimSort (H can be 0 for one run; scanning still costs Θ(n)). The right target is comparisons ≤ C1 n + C2 n H for universal constants. I provide a self-contained proof of comparisons = O(n + n H) for Python TimSort (with the de Gouw fix r2 + r3 ≥ r4), plus a short counterexample to “O(n H)” without additive n.
- Small refinement: Give explicit constant γ for the total cost of all starting sequences; compute γ exactly.
- Accounting detail: comparisons ≤ merge-cost − (r − 1) + (n − 1) ≤ merge-cost + n; we only need to bound merge-cost.

# Examples (counterexample to O(n H) without +n)

- Single run: Array already nondecreasing; run decomposition consists of a single run of length n, so H = 0. TimSort must detect the run, requiring n − 1 adjacent comparisons; hence comparisons ≥ n − 1. Therefore, an upper bound “comparisons ≤ C n H” is impossible (RHS = 0), but comparisons ≤ O(n + n H) holds.

# Main self-contained proof of comparisons ≤ O(n + n H)

Setup. Let ℓ1,…,ℓr be the maximal-run lengths, ∑ℓi = n, and H = ∑i (ℓi/n) log2(n/ℓi). Consider the Python TimSort core with stack S = (R1,…,Rh) (top to bottom) and lengths (r1,…,rh). After pushing a run, repeatedly apply:
- (#2) if h ≥ 3 and r1 > r3 then merge (R2,R3);
- (#3) else if h ≥ 2 and r1 ≥ r2 then merge (R1,R2);
- (#4) else if h ≥ 3 and r1 + r2 ≥ r3 then merge (R1,R2);
- (#5) else if h ≥ 4 and r2 + r3 ≥ r4 then merge (R1,R2);
else stop; finally force-collapse to one run. Merging runs of sizes a,b costs at most a + b − 1 comparisons; we upper bound by a + b.

Invariant (Fibonacci-type growth). At quiescence (no rule applies): r1 < r2, r1 + r2 < r3, r2 + r3 < r4, and for all 3 ≤ i ≤ h − 2: ri + ri+1 < ri+2. Consequently, for i ≤ j ≤ h, ri ≤ 2^{(i+1−j)/2} rj. Proof: merges shift indices and preserve tail inequalities; at quiescence, negations of rules give the first three; chaining yields the 2-ary growth.

Iteration split. For each push of a run R (length r), the ensuing updates split into: starting sequence = maximal block of (#2) merges; ending sequence = subsequent merges until next push.

Starting sequences (O(n) total). Suppose a starting sequence merges k ≥ 2 preexisting runs R1,…,Rk. Its merge cost C ≤ ∑i=1..k (k + 1 − i) ri. Since the final (#2) ensures r > rk, and by exponential-decay, rk ≥ 2^{(k−1−i)/2} ri, we get
C / r ≤ ∑i (k + 1 − i) 2^{(i+1−k)/2} = 2 ∑_{j≥1} j 2^{−j/2} =: γ.
With ∑_{j≥1} j x^j = x/(1 − x)^2 at x = 2^{-1/2} = 1/√2, we obtain γ = 2 · (2^{−1/2})/(1 − 2^{−1/2})^2 = 4/(√2 − 1)^2 ≈ 16.4853. Summing over all pushes (∑r = n), total starting-sequence merge cost ≤ γ n.

Ending sequences via tokens (amortized). Each element holds two token types, c and s, both worth one merge-cost unit.
- Credits: On pushing its run, each element gets 2 c and 1 s. Whenever an ending-merge reduces its stack height, it gets +2 c. (s-tokens are handled below.)
- Charges: #2: every element of R1 and R2 pays 1 c; #3: every element of R1 pays 2 c; #4/#5: every element of R1 pays 1 c and every element of R2 pays 1 s.
No deficits. For c, each height drop credits 2 c covering charges (#2/#3/#4/#5). For s, only R2 pays 1 s in #4/#5; that merge must be followed immediately by another ending merge (by the rules), which reduces their height and re-credits 1 s before any further s-charge. Induction over merges shows non-negativity.

Height bound after the starting sequence. Let h be the height when R (length r) finishes its starting sequence. Then r = r1 ≤ r3, and by exponential growth on the tail r3 ≤ 2^{2 − h/2} n. Hence h ≤ 4 + 2 log2(n/r).

Ending-sequence cost bound. Each element experiences at most h height drops, thus accrues ≤ 2 h c and h s credits; by the height bound, that is O(1 + log(n/r)) per element. Tokens cover all ending-sequence merge cost, so total ending cost ≤ C ∑r (1 + log(n/r)) = O(n + ∑ ℓi log(n/ℓi)) = O(n + n H).

Comparisons vs merge-cost. Over all merges, comparisons ≤ (sum over merges of (a + b − 1)) + (n − 1) ≤ (merge-cost) − (r − 1) + (n − 1) ≤ (merge-cost) + n. Therefore comparisons ≤ γ n + O(n + n H) = O(n + n H).

Lower bound. Any comparison sort must use ≥ log2(n!/(∏ ℓi!)) = n H − O(n) comparisons (Stirling). Hence the bound is optimal up to constants.

# Obstacles

- The strict “O(n H)” (no additive n) is impossible because H can be 0 while comparisons ≥ n − 1 (run detection); also TimSort’s worst-case leading constant is ≥ 3/2 on families with H ≈ log n.

# Next steps

- Promote the above to proofs.md with explicit constants (γ ≈ 16.49 for starting sequences; comparisons ≤ merge-cost + n). Optionally, add the refined tight upper bound comparisons ≤ (3/2) n H + O(n) via the potential method (as in the cited analysis).


=== Papers (text) ===

=== Paper (papers/paper1.txt) ===
\documentclass[a4paper,USenglish]{lipics-v2019}

\usepackage{amsmath,amssymb,xspace}
\usepackage{graphicx}
\usepackage{url}
\usepackage{multirow}
\usepackage{wrapfig}
\usepackage[nodisplayskipstretch]{setspace} \setstretch{1}
\usepackage{comment}
\usepackage[boxruled,vlined,linesnumbered]{algorithm2e}
\usepackage{tikz,tikz-qtree}
\usetikzlibrary{matrix,fit}
\usetikzlibrary{arrows,decorations.pathreplacing,calc}
\usepackage{graphicx}
\usepackage[nomessages]{fp}

\usepackage{lipsum}

\usepackage{listings,xcolor}

\usepackage{breakurl}
\usepackage{pdfpages}
\usepackage{float,xspace}
\usepackage{hyperref}

\usepackage{microtype}

% \usepackage{minted}

\usepackage{marginnote}
\renewcommand*{\marginfont}{\scriptsize\color{gray}}

\nolinenumbers


% \usepackage{caption}
% \usepackage{subcaption}
% \usepackage{subfig} 


%\renewcommand{\algorithmicrequire}{\textbf{Input:}}
%\renewcommand{\algorithmicensure}{\textbf{Output:}}
%\algrenewcomment[1]{\(\triangleright\) \textit{#1}}

%declarations de macro
\newcommand{\HS}{\textsc{HuffmanSort}\xspace}
\newcommand{\NMS}{\textsc{NaturalMergeSort}\xspace}
\newcommand{\MS}{\textsc{MergeSort}\xspace}
\newcommand{\MinS}{\textsc{MinimalSort}\xspace}
\newcommand{\TS}{\textsc{TimSort}\xspace}
\newcommand{\STS}{\textsc{TimSort}\xspace}
\newcommand{\QS}{\textsc{QuickSort}\xspace}
\newcommand{\CR}{\textsc{countRuns}\xspace}
\newcommand{\CS}{\textsc{CountingSort}\xspace}
\newcommand{\GMS}{\textsc{Greedy\-Merge\-Sort}\xspace}
\renewcommand{\SS}{$\alpha$-\textsc{StackSort}\xspace}
\newcommand{\caseX}{$\#$\textsc{X}\xspace}
\newcommand{\kleene}{\rlap{\phantom{\caseX}}^\ast}

\newcommand{\Python}{{Python}\xspace}
\newcommand{\Java}{{Java}\xspace}

\DeclareMathOperator{\sort}{\textbf{sort}}
\DeclareMathOperator{\runs}{\textbf{runs}}
\DeclareMathOperator{\timruns}{\textbf{timRuns}}
\DeclareMathOperator{\merge}{\textbf{merge}}
\DeclareMathOperator{\update}{\textbf{update}}
\DeclareMathOperator{\optimal}{\textbf{Opt}}
\DeclareMathOperator{\lengths}{\textbf{L}}

\renewcommand{\H}{\mathcal{H}}
\renewcommand{\O}{\mathcal{O}}
\renewcommand{\S}{\mathcal{S}}
\newcommand{\A}{\mathcal{A}}
\newcommand{\B}{\mathcal{B}}
\newcommand{\C}{\mathcal{C}}
\newcommand{\R}{\mathcal{R}}
\newcommand{\rs}{\mathsf{r}}
\newcommand{\T}{\mathcal{T}}

\newcommand{\cost}{\textbf{c}}
\newcommand{\vcost}{\textbf{cost}}
\newcommand{\finishproof}{\vspace{-\baselineskip}}

\DeclareMathOperator{\pop}{\text{pop}}
\DeclareMathOperator{\push}{\text{push}}
\DeclareMathOperator{\height}{\texttt{height}}
\DeclareMathOperator{\rundecomp}{\texttt{runs}}
\DeclareMathOperator{\runstack}{\R}


\newcommand{\cSTS}{$\alpha$-\STS}
\newcommand{\stack}{\mathcal{X}}
\newcommand{\true}{\textbf{true}}
\newcommand{\pot}{\mathsf{pot}}
\newcommand{\false}{\textbf{false}}
\newcommand{\strat}{\mathfrak{S}}

\newcommand{\who}[1]{\noindent {\color{blue}\fbox{Who?~#1}}}
\newcommand{\rmk}[1]{\noindent {\color{red}\centerline{\fbox{\begin{minipage}{.8\textwidth}Rmk:~#1\end{minipage}}}}}

\newcommand{\carine}[1]{{\textbf{\color{blue} -- #1 -- }}}
\newcommand{\capi}[1]{{\color{blue} #1}}
\newcommand{\cyril}[1]{{\textbf{\color{green!50!black} -- #1 -- }}}
\newcommand{\nicolas}[1]{{\textbf{\color{purple} -- #1 -- }}}
\newcommand{\vincent}[1]{{\textbf{\color{pink!50!red!90!gray} -- #1 -- }}}
\newcommand{\juge}[1]{{\color{gray}#1}}

\newcommand{\ctok}{$\diamondsuit$\xspace}
\newcommand{\stok}{$\heartsuit$\xspace}
\newcommand{\ptok}{$\clubsuit$\xspace}

\renewcommand{\gets}{\ensuremath{\leftarrow}}
\newenvironment{disjunction}{\begin{itemize}}{\vspace{-\baselineskip}\end{itemize}}
\setlength{\itemsep}{\smallskipamount}

\theoremstyle{plain}
\newtheorem{proposition}[theorem]{Proposition}
\newtheorem{claim}[theorem]{Claim}

\theoremstyle{definition}
\newtheorem{remark2}[theorem]{Remark}


% Author macros::begin %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\title{On the Worst-Case Complexity of TimSort} 

\bibliographystyle{plainurl}% the recommnded bibstyle

%\author{Nicolas Auger}{Universit\'e Paris-Est, LIGM (UMR 8049), CNRS, ENPC, ESIEE Paris, UPEM, F-77454 Marne-la-Vall\'ee, France}{}{}{}

\author{Nicolas Auger, Vincent Jugé, Cyril Nicaud, and Carine Pivoteau}{Universit\'e Paris-Est, LIGM (UMR 8049), UPEM, F77454 Marne-la-Vall\'ee, France}{}{}{}

\authorrunning{N. Auger, V. Jugé, C. Nicaud, and C. Pivoteau} %mandatory. First: Use abbreviated first/middle names. Second (only in severe cases): Use first author plus 'et. al.'

\Copyright{Nicolas Auger, Vincent Jugé, Cyril Nicaud, and Carine Pivoteau}%mandatory, please use full first names. LIPIcs license is "CC-BY";  http://creativecommons.org/licenses/by/3.0/

\subjclass{\ccsdesc[100]{Theory of computation~Sorting and searching}}% mandatory: Please choose ACM 2012 classifications from https://www.acm.org/publications/class-2012 or https://dl.acm.org/ccs/ccs_flat.cfm . E.g., cite as "General and reference $\rightarrow$ General literature" or \ccsdesc[100]{General and reference~General literature}. 

\keywords{Sorting algorithms, Merge sorting algorithms, TimSort, Analysis of algorithms}% mandatory: Please provide 1-5 keywords

% Author macros::end %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\EventEditors{}%Yossi Azar, Hannah Bast, and Grzegorz Herman}
\EventNoEds{1}
\EventLongTitle{}%26th Annual European Symposium on Algorithms (ESA 2018)}
\EventShortTitle{}%ESA 2018}
\EventAcronym{}%ESA}
\EventYear{}%2018}
\EventDate{}%August 20--22, 2018}
\EventLocation{}%Helsinki, Finland}
\EventLogo{}
\SeriesVolume{}%112}
\ArticleNo{} % “new number” (=<article-no>) goes here! 
\begin{document}

\maketitle

\begin{abstract}
\TS is an intriguing sorting algorithm designed in 2002 for Python, 
whose worst-case complexity was announced, but not proved until our recent preprint.
In fact, there are two slightly different versions of \TS that are currently implemented in Python and in Java respectively. 
We propose a pedagogical and insightful proof that the Python version runs in time~$\O(n \log n)$. 
The approach we use in the analysis also applies to the Java version, although not without very involved technical details. 
As a byproduct of our study, we uncover a bug in the Java implementation that can cause the sorting method to fail during the execution. 
We also give a proof that Python's \TS running time is in $\O(n + n \H)$,
where $\H$ is the entropy of the distribution of runs (i.e. maximal monotonic sequences), which is quite a natural parameter here and part of the explanation for the good behavior of \TS on partially sorted inputs. Finally, we evaluate precisely the worst-case running time of Python's \TS,
and prove that it is equal to $1.5 n \H + \mathcal{O}(n)$.
\end{abstract}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section{Introduction}\label{intro}
\TS is a sorting algorithm designed in 2002 by Tim Peters~\cite{Peters2015},
for use in the \Python programming language. It was thereafter implemented in
other well-known programming languages such as \Java. The algorithm includes many implementation optimizations, a few heuristics and some refined tuning, but its high-level principle is rather simple:
The sequence $S$ to be sorted is first decomposed greedily into monotonic runs (i.e. 
nonincreasing or nondecreasing subsequences of~$S$ as depicted on Figure~\ref{fig:runs}), which are then merged pairwise according to some specific rules.

\begin{figure}[h]
\centerline{$
S=(~\underbrace{12,10,7,5}_{\text{first run}},
~\underbrace{7,10,14,25,36}_{\text{second run}},
~\underbrace{3,5,11,14,15,21,22}_{\text{third run}},
~\underbrace{20,15,10,8,5,1}_{\text{fourth run}}~)
$}
\caption{A sequence and its {\em run decomposition} computed by \TS: for each run, the first two elements determine if it is increasing or decreasing, then it continues with the maximum number of consecutive elements that preserves the monotonicity.\label{fig:runs}}
\end{figure}

The idea of starting with a decomposition into runs is not new, and already appears in Knuth's \NMS~\cite{Knuth98}, where increasing runs are sorted using the same mechanism as in \MS. 
Other merging strategies combined with decomposition into runs appear in the literature, such as the \MinS of~\cite{Ta09} (see also~\cite{BaNa13} for other considerations on the same topic). 
All of them have nice properties: they run in $\O(n\log n)$ and even $\O(n+n\log\rho)$, where $\rho$ is the number of runs, which is optimal in the model of sorting by comparisons~\cite{Mannila1985}, using the classical counting argument for lower bounds. 
And yet, among all these merge-based algorithms, \TS was favored in several very popular programming languages, which suggests that it performs quite well in practice. 

\TS running time was implicitly assumed to be $\O(n\log n)$, but our unpublished preprint~\cite{AuNiPi15} contains, to our knowledge, the first proof of it. This was more than ten years after \TS started being used instead of \QS in several major programming languages. 
The growing popularity of this algorithm invites for a careful theoretical investigation. In the present paper, we make a thorough analysis which provides a better understanding of the inherent qualities of the merging strategy of \TS. 
Indeed, it reveals that, even without its refined heuristics,\footnote{These heuristics are useful in practice, but do not improve the worst-case complexity of the algorithm.} this is an effective sorting algorithm, computing and merging runs on the fly, using only local properties to make its decisions. 

% As the analysis we made in~\cite{AuNiPi15} was a bit involved and clumsy, w
We first propose in Section~\ref{sec:analysis1} a new pedagogical and self-contained exposition that \TS runs in time $\O(n + n \log n)$, which we want both clear and insightful.
In fact, we prove a stronger statement: on an input consisting of $\rho$ runs
of respective lengths $r_1,\ldots,r_\rho$, we establish that \TS runs
in $\O(n + n \H) \subseteq \O(n + n \log \rho) \subseteq \O(n + n \log n)$,
where $\H = H(r_1/n,\ldots,r_\rho/n)$ and
$H(p_1,\ldots,p_\rho) = - \sum_{i=1}^\rho p_i \log_2(p_i)$ is the binary Shannon entropy.

We then refine this approach, in Section~\ref{sec:analysis2},
to derive precise bounds on the worst-case running time of \TS,
and we prove that it is equal to $1.5 n \H + \mathcal{O}(n)$.
This answers positively a conjecture of~\cite{BuKno18}.
% 
% 
% 
% 
% Using the same approach, we also establish in Section~\ref{sec:analysis2} that it runs in $\O(n+n\log \rho)$, a question left open in our preprint and also in a recent work\footnote{In~\cite{BuKno18}, the authors refined the analysis of~\cite{AuNiPi15} to obtain very precise bounds for the complexity of \TS and of similar algorithms.} on \TS~\cite{BuKno18}.
Of course, the first result follows from the second, but since we believe that each one is interesting on its own, we devote one section to each of them.
% Besides, 
% the second result provides with an explanation to why \TS is a very good sorting algorithm, worth considering in most situations where in-place sorting is not needed.

To introduce our last contribution, we need to look into the evolution of the algorithm: there are actually not one, but two main versions of \TS. The first version of the algorithm contained a flaw, which was spotted in~\cite{GoRoBoBuHa15}: while the input was correctly sorted, the algorithm did not behave as announced (because of a broken invariant).
This was discovered by De Gouw and his co-authors while trying to prove formally the correctness of \TS. 
They proposed a simple way to patch the algorithm, which was quickly adopted in Python, leading to what we consider to be the real \TS. This is the one we analyze in Sections~\ref{sec:analysis1} and~\ref{sec:analysis2}. 
On the contrary, Java developers chose to stick with the first version of \TS, and adjusted some tuning values (which depend on the broken invariant; this is explained in Sections~\ref{presentation} and~\ref{sec:java}) to prevent the bug exposed by~\cite{GoRoBoBuHa15}. Motivated by its use in Java, we explain in Section~\ref{sec:java} how, at the expense of very complicated technical details, the elegant proofs of the Python version can be twisted to prove the same results for this older version. While working on this analysis, we discovered yet another error in the correction made in~Java. Thus, we compute yet another patch, even if we strongly agree that the algorithm proposed and formally proved in~\cite{GoRoBoBuHa15} (the one currently implemented in Python) is a better option.


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{TimSort core algorithm}\label{presentation}


\begin{algorithm}[t]
\begin{small}
\SetArgSty{texttt}
\DontPrintSemicolon
\SetKwInOut{Input}{Input}
\Input{A sequence $S$ to sort}
\KwResult{The sequence $S$ is sorted into a single run, which  remains on the 
stack.}
\BlankLine
\SetKwInput{KwData}{Note}
\KwData{The function {\tt merge\_force\_collapse} repeatedly pops the last two runs on the stack~$\runstack$, merges them and pushes the resulting run back on the stack.}
\BlankLine
$\rundecomp \gets $ a run decomposition of $S$\;
$\runstack \gets $ an empty stack\;
\While(\tcp*[f]{main loop of \TS}){$\rundecomp\neq \emptyset$\label{algline:begin_loop}}{
  remove a run $r$ from $\rundecomp$ and push $r$ onto $\runstack$\;
  {\tt merge\_collapse}($\runstack$)\;\label{algline:end_loop}
}
\If(\tcp*[f]{the height of $\runstack$ is its number of runs}){$\height(\runstack) \neq 1$}{
  {\tt merge\_force\_collapse}($\runstack$)
}
\end{small}
\caption{\TS \hfill(Python 3.6.5) \label{alg:TimSortMainLoop}}
\end{algorithm}

The idea of \TS is to design a merge sort that can exploit the possible  
``non randomness'' of the data, without having to detect it beforehand and 
without damaging the performances on random-looking data. This follows the ideas 
of adaptive sorting (see~\cite{Mannila1985} for a survey on taking presortedness 
into account when designing and analyzing sorting algorithms).

The first feature of \TS is to work on the natural decomposition of the input  
sequence into maximal runs. In order to get larger subsequences, \TS allows both 
nondecreasing and decreasing runs, unlike most merge sort algorithms. 

Then, the merging strategy of \TS~(Algorithm~\ref{alg:TimSortMainLoop}) is quite simple yet very efficient. The runs are considered in the order  
given by the run decomposition and successively pushed onto a stack. 
If some conditions on the size of the topmost runs of the stack are not satisfied after a new run has been pushed, this can trigger a series of merges between pairs of runs at the top or right under. 
And at the end, when all the runs in the initial decomposition 
have been pushed, the last operation is to merge the remaining runs two by two, starting
at the top of the stack, to get a sorted sequence.
The conditions on the stack and the merging rules are implemented in the  
subroutine called~{\tt merge\_collapse} detailed in Algorithm~\ref{alg:merge_collapse}. 
This is what we consider to be \TS core mechanism and this is the main focus of our analysis. 

\begin{algorithm}[t]
\begin{small}
\SetArgSty{texttt}
\DontPrintSemicolon
\SetKwInOut{Input}{Input}
\Input{A stack of runs $\runstack$}
\KwResult{The invariant of Equations~\eqref{eq:inv1} and~\eqref{eq:inv2} is established.}
\BlankLine
\SetKwInput{KwData}{Note}
\KwData{The runs on the stack are denoted by $\runstack[1]\dots\runstack[\height(\runstack)]$, from top to bottom. The length of run $\runstack$[i] is denoted by $\rs_i$. The blue highlight indicates that the condition was not present in the original version of \TS (this will be discussed in section~\ref{sec:java}).}
\BlankLine
\While{$\height(\runstack)>1$}{
  $n \gets \height(\runstack) - 2$ 
    \BlankLine
  \If{
  ($n > 0$ and $\rs_{3}\leqslant \rs_{2} + \rs_{1}$)
  \textcolor{blue}{ or ($n > 1$ and $\rs_{4} \leqslant \rs_{3} + \rs_{2}$)}\,\label{algline:new_cond}}{
    \If{$\rs_{3} < \rs_{1}$}{
      merge runs $\runstack$[2] and $\runstack$[3] on the stack\;
    }
    \lElse{merge runs $\runstack$[1] and $\runstack$[2] on the stack}
  }
  \ElseIf{$\rs_{2} \leqslant \rs_{1}$}{
    merge runs $\runstack$[1] and $\runstack$[2] on the stack
  }
  \lElse{
    break
  }
}
\end{small}
\caption{The {\tt merge\_collapse} procedure \hfill(Python 3.6.5)\label{alg:merge_collapse}}
\end{algorithm}

Another strength of \TS is the use of many effective heuristics to save
time, such as ensuring that the initial runs are not to small thanks to an insertion sort 
or using a special technique called ``galloping'' to optimize the merges. 
However, this does not interfere with our analysis and we will not discuss this matter any further.

Let us have a closer look at Algorithm~\ref{alg:merge_collapse} which is a pseudo-code transcription
of the {\tt merge\_collapse} procedure found in the latest version of Python (3.6.5). 
To illustrate its mechanism, an example of execution of the main loop of \TS (lines~\ref*{algline:begin_loop}-\ref*{algline:end_loop} of Algorithm~\ref{alg:TimSortMainLoop}) is given in Figure~\ref{fig:ts-python-exec}. 
As stated in its note~\cite{Peters2015}, Tim Peter's idea was that: 
\begin{quote}
``The thrust of these rules when they trigger merging is to balance the run
lengths as closely as possible, while keeping a low bound on the number of
runs we have to remember.''    
\end{quote}
To achieve this, the merging conditions of {\tt merge\_collapse} are designed  
to ensure that the following invariant\footnote{Actually, in~\cite{Peters2015}, the invariant is only stated for the 3 topmost runs of the stack.} is true at the end of the procedure:
\begin{eqnarray}
    \rs_{i+2}&>&\rs_{i+1}+\rs_{i}, \label{eq:inv1}\\
    \rs_{i+1}&>&\rs_{i}.\label{eq:inv2}
\end{eqnarray}
This means that the runs lengths $\rs_i$ 
on the  stack grow at least as fast as the Fibonacci numbers and, therefore, 
that the height of the stack stays logarithmic (see Lemma~\ref{lm:h-is-small}, section~\ref{sec:analysis1}). 

Note that the bound on the height of the stack is not enough to justify the $\O(n\log n)$ running time of \TS. Indeed, without the smart strategy used to merge the runs ``on the fly'', it is easy to build an example using a stack containing at most two runs and that gives a $\Theta(n^2)$ complexity: just assume that all runs have size two, push them one by one onto a stack and perform a merge each time there are two runs in the stack. 

We are now ready to proceed with the analysis of \TS complexity. As mentioned earlier, Algorithm~\ref{alg:merge_collapse} does not correspond to the first implementation of \TS in Python, nor to the current one in Java, but to the latest Python version. The original version will be discussed in details later, in Section~\ref{sec:java}.   

\begin{figure}[t]
\begin{center}
\begin{small}
\setlength{\tabcolsep}{1pt}
\begin{tabular}{ccccccccccccccc}
\begin{tikzpicture}[every node/.style={align=center,text width=1em}]
\matrix (A) [matrix of nodes,nodes={draw}] { {\bf 24}\\ };
\node [above] at (A.north) {$\#1$};
\end{tikzpicture}
&
\begin{tikzpicture}[every node/.style={align=center,text width=1em}]
\matrix (A) [matrix of nodes,nodes={draw}] { {\bf 18}\\24\\ };
\node [above] at (A.north) {$\#1$};
\end{tikzpicture}
&
\begin{tikzpicture}[every node/.style={align=center,text width=1em}]
\matrix (A) [matrix of nodes,nodes={draw}] { {\bf 50}\\18\\24\\ };
\node [above] at (A.north) {$\#1$};
\end{tikzpicture}
&
\begin{tikzpicture}[every node/.style={align=center,text width=1em}]
\matrix (A) [matrix of nodes,nodes={draw}] { 50\\42\\ };
\node [above] at (A.north) {$\#2$};
\end{tikzpicture}
&
\begin{tikzpicture}[every node/.style={align=center,text width=1em}]
\matrix (A) [matrix of nodes,nodes={draw}] { 92\\ };
\node [above] at (A.north) {$\#3$};
\end{tikzpicture}
&
\begin{tikzpicture}[every node/.style={align=center,text width=1em}]
\matrix (A) [matrix of nodes,nodes={draw}] { {\bf 28}\\92\\ };
\node [above] at (A.north) {$\#1$};
\end{tikzpicture}
&
\begin{tikzpicture}[every node/.style={align=center,text width=1em}]
\matrix (A) [matrix of nodes,nodes={draw}] { {\bf 20}\\28\\92\\ };
\node [above] at (A.north) {$\#1$};
\end{tikzpicture}
&
\begin{tikzpicture}[every node/.style={align=center,text width=1em}]
\matrix (A) [matrix of nodes,nodes={draw}] { {\bf 6}\\20\\28\\92\\ };
\node [above] at (A.north) {$\#1$};
\end{tikzpicture}
&
\begin{tikzpicture}[every node/.style={align=center,text width=1em}]
\matrix (A) [matrix of nodes,nodes={draw}] { {\bf 4}\\6\\20\\28\\92\\ };
\node [above] at (A.north) {$\#1$};
\end{tikzpicture}
&
\begin{tikzpicture}[every node/.style={align=center,text width=1em}]
\matrix (A) [matrix of nodes,nodes={draw}] { {\bf 8}\\4\\6\\20\\28\\92\\ };
\node [above] at (A.north) {$\#1$};
\end{tikzpicture}
&
\begin{tikzpicture}[every node/.style={align=center,text width=1em}]
\matrix (A) [matrix of nodes,nodes={draw}] { 8\\10\\20\\28\\92\\ };
\node [above] at (A.north) {$\#2$};
\end{tikzpicture}
&
\begin{tikzpicture}[every node/.style={align=center,text width=1em}]
\matrix (A) [matrix of nodes,nodes={draw}] { 18\\20\\28\\92\\ };
\node [above] at (A.north) {$\#5$};
\end{tikzpicture}
&
\begin{tikzpicture}[every node/.style={align=center,text width=1em}]
\matrix (A) [matrix of nodes,nodes={draw}] { 38\\28\\92\\ };
\node [above] at (A.north) {$\#4$};
\end{tikzpicture}
&
\begin{tikzpicture}[every node/.style={align=center,text width=1em}]
\matrix (A) [matrix of nodes,nodes={draw}] { 66\\92\\ };
\node [above] at (A.north) {$\#3$};
\end{tikzpicture}
&
\begin{tikzpicture}[every node/.style={align=center,text width=1em}]
\matrix (A) [matrix of nodes,nodes={draw}] { {\bf 1}\\66\\92\\ };
\node [above] at (A.north) {$\#1$};
\end{tikzpicture}
\end{tabular}
\begin{tikzpicture}
\draw[decorate,thick,decoration={brace,amplitude=4pt,mirror}] (-3.8,0.2) -- (-2.3,0.2); 
\draw[decorate,thick,decoration={brace,amplitude=4pt,mirror}] (2.0,0.2) -- (5.3,0.2); 
\draw (-3.05,-0.1) node{\scriptsize {\tt merge\_collapse}};
\draw (3.65,-0.1) node{\scriptsize {\tt merge\_collapse}};
\node[text width=\textwidth]{};
\end{tikzpicture}
\end{small}
\end{center}
\vspace{-6mm}
\caption{ The successive states of the stack~$\runstack$ (the values are the  
lengths of the runs) during an execution of the main loop of \TS 
(Algorithm~\ref{alg:TimSortMainLoop}), with the lengths of the runs in 
$\rundecomp$ being $(24, 18, 50, 28, 20, 6, 4, 8, 1)$. 
The label $\#1$ indicates that a run has just been pushed onto the stack. 
The other labels refer to the different merges cases of {\tt merge\_collapse} as translated in Algorithm~\ref{alg:TS translated}.
\label{fig:ts-python-exec}}
\end{figure}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{TimSort runs in \texorpdfstring{$\O(n\log n)$}{Lg}}\label{sec:analysis1}

At the first release of \TS~\cite{Peters2015}, a time complexity of $\O(n\log n)$ was 
announced with no element of proof given. It seemed to remain unproved until 
our recent preprint~\cite{AuNiPi15}, where we provide a confirmation of this fact, using 
a proof which is not difficult but a bit tedious. This result was  refined later in~\cite{BuKno18}, where the authors provide lower and upper bounds, including explicit multiplicative constants, for different merge sort algorithms.

Our main concern is to provide an insightful proof of the complexity of \TS, in order to highlight how well designed is the strategy used to choose the order in which the merges are performed.
The present section is more detailed than the following ones as we want it to be
self-contained once \TS has been translated into Algorithm~\ref{alg:TS translated} (see below).

\begin{algorithm}[t]
\begin{small}
\SetArgSty{texttt}
\DontPrintSemicolon
\SetKwInOut{Input}{Input}
\Input{A sequence to $S$ to sort}
\KwResult{The sequence $S$ is sorted into a single run,  which remains on the 
stack.}
\SetKwInput{KwData}{Note}
\KwData{At any time, we denote the height of the stack $\runstack$ by $h$
and its $i$\textsuperscript{th} top-most run (for $1 \leqslant i 
\leqslant h$)  
by $R_i$. The size of this run is denoted by $r_i$.}
\BlankLine
\BlankLine
$\rundecomp \gets $ the run decomposition of $S$\;
$\runstack \gets $ an empty stack\;
\While(\tcp*[f]{main loop of \TS}){$\rundecomp\neq \emptyset$}{
  remove a run $r$ from $\rundecomp$ and push $r$ onto 
$\runstack$\tcp*[r]{\#1$\;$} 
  \While{\true}{\label{algline:inner_while_translated}
    \lIf{$h \geqslant 3$ and $r_1 > r_3$}{
        merge the runs $R_2$ and $R_3$ \tcp*[f]{\#2}
    }
    \lElseIf{$h \geqslant 2$ and $r_1 \geqslant r_2$}{
        merge the runs $R_1$ and $R_2$ \tcp*[f]{\#3}
    }
    \lElseIf{$h \geqslant 3$ and $r_1+r_2 \geqslant r_3$}{
        merge the runs $R_1$ and $R_2$ \tcp*[f]{\#4}
    }
    \textcolor{blue}{\lElseIf{$h \geqslant 4$ and $r_2+r_3 \geqslant r_4$}{ \label{algline:new_cond_translated}
        merge the runs $R_1$ and $R_2$ \tcp*[f]{\#5}
    }}
    \lElse{break}
  }
}
\lWhile{$h \neq 1$}{
  merge the runs $R_1$ and $R_2$%two top-most runs on the stack $\runstack$
}
\end{small}
\caption{TimSort: translation of Algorithm~\ref{alg:TimSortMainLoop} and Algorithm~\ref{alg:merge_collapse}\label{alg:TS translated}}
\end{algorithm}

As our analysis is about to demonstrate, in terms of worst-case complexity, the good performances of \TS do not rely on the way merges are performed. 
Thus we choose to ignore their many optimizations and consider that merging two runs of lengths~$r$ and~$r'$ requires both $r+r'$ element moves and $r+r'$ element comparisons.
Therefore, to quantify the running time of \TS, we only take into account the number of comparisons performed.

In particular, aiming at computing precise bounds on the running time of \TS, we
follow~\cite{golin1993queue,AuNiPi15,BuKno18,munro2018nearly} and define the
\emph{merge cost} for merging two runs of lengths $r$ and $r'$ as $r + r'$,
i.e., the length of the resulting run. Henceforth, we will identify the time spent for
merging two runs with the merge cost of this merge.

\vfill

\begin{theorem}\label{thm:complexity n + n H}
Let $\C$ be the class of arrays of length $n$, whose run decompositions
consist of $\rho$ monotonic runs of
respective lengths $r_1,\ldots,r_\rho$.
Let $H(p_1,\ldots,p_\rho) = - \sum_{i=1}^\rho p_i \log_2(p_i)$ be the binary Shannon entropy,
and let $\H = H(r_1/n,\ldots,r_\rho/n)$.

The running time of \TS on arrays in $\C$ is
$\mathcal{O}(n + n \H)$. 
\end{theorem}

From this result, we easily deduce the following complexity bound on \TS,
which is less precise but more simple.

\begin{theorem}\label{thm:complexity n log n}
The running time of \TS on arrays of length $n$ that consist of $\rho$ monotonic runs is
$\O(n + n\log \rho)$, and therefore $\O(n \log n)$.
\end{theorem}

\begin{proof}
The function $f : x \mapsto - x \ln(x)$ is concave on the interval $\mathbb{R}_{>0}$ of positive real numbers,
since its second derivative is $f''(x) = - 1/x$. Hence, when $p_1,\ldots,p_\rho$ are
positive real numbers that sum up to one, we have
$H(p_1,\ldots,p_\rho) = {\textstyle\sum_{i=1}^\rho f(p_i)/\ln(2)} \leqslant \rho f(1/\rho)/\ln(2) = \log_2(\rho)$.
In particular, this means that $\H \leqslant \log_2(\rho)$, and therefore that
\TS runs in time $\O(n + n\log \rho)$.
Since $\rho \leqslant n$, it further follows that $\O(n + n\log \rho) \subseteq \O(n + n\log n) = \O(n \log n)$, which completes the proof.
\end{proof}

Before proving Theorem~\ref{thm:complexity n + n H},
we first show that it is optimal up to a multiplicative constant,
by recalling the following variant of a result from~\cite[Theorem~2]{BaNa13}.

\begin{proposition}\label{proposition:optimality}
For every algorithm comparing only pairs of elements,
there exists an array in the class $\C$ whose sorting
requires at least $n \H - 3 n$ element comparisons.
\end{proposition}

\begin{proof}
In the comparison model, at least $\log_2(|\C|)$ element comparisons
are required for sorting all arrays in $\C$.
Hence, we prove below that $\log_2(|\C|) \geqslant n \H - 3 n$.

Let $\pi = (\pi_1,\ldots,\pi_\rho)$ be a partition of the set $\{1,\ldots,n\}$
into $\rho$ subsets of respective sizes $r_1,\ldots,r_\rho$;
we say that $\pi$ is \emph{nice} if $\max \pi_i > \min \pi_{i+1}$ for all $i \leqslant \rho-1$.
Let us denote by $\mathcal{P}$ the set of partitions $\pi$ of $\{1,\ldots,n\}$ such that $|\pi_i| = r_i$
for all $i \leqslant \rho$, and by $\mathcal{N}$ the set of nice partitions.

Let us transform every partition $\pi \in \mathcal{P}$ into a nice partition as follows.
First, by construction of the run decomposition of an array, we know that $r_1,\ldots,r_{\rho-1} \geqslant 2$,
and therefore that $\min \pi_i < \max \pi_i$ for all $i \leqslant \rho-1$.
Then, for all $i \leqslant \rho-1$, if $\max \pi_i < \min \pi_{i+1}$, we exchange the partitions to which belong $\max \pi_i$ and $\min \pi_{i+1}$, i.e., we move $\max \pi_i$ from the set $\pi_i$ to $\pi_{i+1}$,
and $\min \pi_{i+1}$ from $\pi_{i+1}$ to $\pi_i$. Let $\pi^\ast$ be the partition obtained after
these exchanges have been performed.

Observe that $\pi^\ast$ is nice, and that at most $2^{\rho-1}$ partitions $\pi \in \mathcal{P}$
can be transformed into $\pi^\ast$. This proves that $2^{\rho-1}|\mathcal{N}| \geqslant |\mathcal{P}|$.
Let us further identify every nice partition $\pi^\ast$ with an array in $\C$,
which starts with the elements of $\pi^\ast_1$ (listed in increasing order),
then of $\pi^\ast_2, \ldots, \pi^\ast_\rho$.
We thereby define an injective map from $\mathcal{N}$ to $\C$,
which proves that $|\C| \geqslant |\mathcal{N}|$.

Finally, variants of the Stirling formula indicate that
$(k/e)^k \leqslant k! \leqslant e \sqrt{k} (k/e)^k$ for all $k \geqslant 1$.
This proves that
\begin{align*}
\log_2(|\C|) & \geqslant \log_2(|\C|) \geqslant (1 - \rho) + \log_2(|\mathcal{P}|) \\
& \geqslant (1 - \rho) + n \log_2(n) - \rho \log_2(e) - {\textstyle\sum_{i=1}^\rho} (r_i+1/2) \log_2(r_i) \\
& \geqslant n \H + (1 - \rho - \rho \log_2(e)) - 1/2 {\textstyle\sum_{i=1}^\rho} \log_2(r_i).
\end{align*}
By concavity of the function $x \mapsto \log_2(x)$, it follows that
$\textstyle\sum_{i=1}^\rho \log_2(r_i) \leqslant \rho \log_2(n/\rho)$.
One checks easily that the function $x \mapsto x \log_2(n/x)$ takes its maximum value at $x = n/e$,
and since $n \geqslant \rho$, we conclude that
$\log_2(|\C|) \geqslant n \H - (1+\log_2(e)+\log_2(e)/e) n \geqslant n \H - 3 n$.
\end{proof}

We focus now on proving Theorem~\ref{thm:complexity n + n H}.
The first step consists in rewriting Algorithm~\ref{alg:TimSortMainLoop} and 
Algorithm~\ref{alg:merge_collapse} in a form that is easier to deal with. This is done in Algorithm~\ref{alg:TS translated}. 

\begin{claim}
For any input, Algorithms~\ref{alg:TimSortMainLoop} and~\ref{alg:TS translated} perform the same comparisons. 
\end{claim}

\begin{proof}
The only difference is that Algorithm~\ref{alg:merge_collapse} was changed into the \texttt{while} loop of lines~5 to~10 in Algorithm~\ref{alg:TS translated}.
Observing the different cases, it is straightforward to verify that merges involving the same runs take place in the same order in both algorithms.
Indeed, if $r_3 < r_1$, then $r_3 \leqslant r_1 + r_2$, and therefore line 5 is triggered in Algorithm~\ref{alg:merge_collapse}, so that
both algorithms merge the $2$\textsuperscript{nd} and $3$\textsuperscript{rd} runs.
On the contrary, if $r_3 \geqslant r_1$, then both algorithms merge the $1$\textsuperscript{st} and $2$\textsuperscript{nd} runs if and only if
$r_2 \leqslant r_1$ or $r_3 \leqslant r_1 + r_2$ ({\color{blue}or $r_4 \leqslant r_2 + r_3$}).
\end{proof}

\begin{remark2}\label{rem:main-loop}
Proving Theorem~\ref{thm:complexity n log n} only requires
analyzing the \emph{main loop} of the algorithm (lines~3 to 10).
Indeed, computing the run decomposition (line~1) can be done on the fly,
by a greedy algorithm, in time linear in $n$, and the \emph{final loop} (line~11) might be performed in the main loop by adding a fictitious run of length $n+1$ at the end of the decomposition.

In the sequel, for the sake of readability, we also omit
checking that $h$ is large enough to trigger the cases \#2 to \#5.
Once again, such omissions are benign, since
adding fictitious runs of respective lengths $8n$, $4n$, $2n$ and $n$
(in this order) at the beginning of the decomposition
would ensure that $h \geqslant 4$ during the whole loop.
\end{remark2}

% In Algorithm~\ref{alg:TS translated}, we can see that the merges performed during Case \#2 allow a very large run to be pushed and ``absorbed'' onto the stack without being merged all the way down, but by collapsing the stack under this run instead. Meanwhile, the purpose of Cases \#3--\#5 is mainly to re-establish the invariant~\eqref{eq:inv1} and~\eqref{eq:inv2}, ensuring an exponential growth of the run lengths within the stack.
% Along this process, the cost of keeping the stack in good shape is compensated by the absorption of these large runs which naturally calls for an \emph{amortized complexity} analysis.

We sketch now the main steps of our proof, i.e., the amortized analysis of the main loop.
A first step is to establish the invariant~\eqref{eq:inv1} and~\eqref{eq:inv2},
ensuring an exponential growth of the run lengths within the stack.

Elements of the input array are easily identified by their starting position in the array, so we consider them as well-defined and distinct entities (even if they have the same value). 
The \emph{height} of an element in the stack of runs is the number of runs that are
below it in the stack: the elements belonging to the run~$R_i$ in the stack
$\S = (R_1,\ldots, R_h)$ have height $h-i$, and we recall that the length of the run $R_i$ is denoted
by $r_i$.
% To simplify the presentation, we also distinguish two kinds of tokens, the \ctok-tokens and the \stok-tokens, which can both be used to pay for comparisons.

\begin{lemma}\label{lm:invariant I}
At any step during the main loop of \TS, we have~$r_i+r_{i+1}<r_{i+2}$ for all $i \in \{3,\ldots,h-2\}$.
\end{lemma}

\begin{proof}
We proceed by induction. The proof consists in verifying that, if the
invariant holds at some point, then it still holds when an update of the stack occurs in one of the five situations labeled \#1 to \#5 in the algorithm.
This can be done by a straightforward case analysis.
We denote by $\overline{\S}=(\overline{R}_1,\ldots, \overline{R}_{\overline{h}})$ the new state of the stack after the update:
\begin{disjunction}
  \item If Case \#1 just occurred, a new run $\overline{R}_1$ was pushed.
  This implies that none of the conditions of Cases \#2 to \#5 hold in $\S$, otherwise merges would have continued.  In particular, we have $r_2+r_3<r_4$.
  As $\overline{r}_i=r_{i-1}$ for all $i\geqslant 2$, and since the invariant holds for $\S$, it also
  holds for $\overline{\S}$.
  
  \item If one of the Cases \#2 to \#5 just occurred, $\overline{r}_i=r_{i+1}$ for all $i\geqslant 3$. Since the invariant holds for $\S$, it must also hold for $\overline{\S}$.
\end{disjunction}
\end{proof}

\begin{corollary}\label{cor:invariant I'}
During the main loop of \TS, whenever a run is about to be pushed onto the stack, we have
$r_i \leqslant 2^{(i+1-j)/2} r_j$ for all integers $i \leqslant j \leqslant h$.
\end{corollary}

\begin{proof}
Since a run is about to be pushed, none of the conditions of Cases \#2 to \#5 hold in the stack $\S$.
Hence, we have $r_1 < r_2$, $r_1 + r_2 < r_3$ and $r_2 + r_3 < r_4$, and Lemma~\ref{lm:invariant I}
further proves that $r_i + r_{i+1} < r_{i+2}$ for all $i \in \{3,\ldots,h-2\}$.
In particular, for all $i \leqslant h-2$, we have $r_i < r_{i+1}$, and thus
$2 r_i \leqslant r_i + r_{i+1} \leqslant r_{i+2}$.
It follows immediately that $r_i \leqslant 2^{-k} r_{i+2k} \leqslant 2^{-k} r_{i+2k+1}$
for all integers $k \geqslant 0$, which is exactly the statement of Corollary~\ref{cor:invariant I'}.
\end{proof}

Corollary~\ref{cor:invariant I'} will be crucial in proving that the main loop of \TS can be performed
for a merge cost $\O(n + n \H)$. However, we do not prove this upper bound directly.
Instead, we need to distinguish several situations that may occur within the main loop.

Consider the sequence of Cases \#1 to \#5 triggered during the execution of the main loop of \TS.
It can be seen as a word on the alphabet $\{\#1,\ldots,\#5\}$ that starts with \#1, which completely encodes the execution of the algorithm. 
We split this word at every \#1, so that each piece corresponds to an iteration of the main loop. 
Those pieces are in turn split into two parts, at the first occurrence of a symbol \#3, \#4 or \#5.
The first half is called a \emph{starting sequence} and is made of a \#1 followed by the maximal number of \#2's. 
The second half is called an \emph{ending sequence}, it starts with \#3, \#4 or \#5 (or is empty) and it contains no occurrence of \#1 (see Figure~\ref{fig:sequence} for an example).

\begin{figure}[H]
\smallskip
\centerline{$
\underbrace{\#1\;\#2\;\#2\;\#2}_{\text{starting seq.}}
~~\underbrace{\#3\;\#2\;\#5\;\#2\;\#4\;\#2}_{\text{ending seq.}}
~~~\underbrace{\#1\;\#2\;\#2\;\#2\;\#2\;\#2}_{\text{starting seq.}}
~~\underbrace{\#5\;\#2\;\#3\;\#3\;\#4\;\#2}_{\text{ending seq.}}
$}
\caption{The decomposition of the encoding of an execution into starting and ending sequences.\label{fig:sequence}}
\end{figure}

We bound the merge cost of starting sequences first, and will deal with ending sequences afterwards.

\begin{lemma}\label{lm:starting}
The cost of all merges performed during the starting sequences is~$\O(n)$.
\end{lemma}

\begin{proof}
More precisely, for a stack $\S=(R_1,\ldots,R_h)$, we prove that a starting sequence beginning with a push of a run~$R$ of size $r$ onto $\S$ uses at most $\gamma r$ comparisons in total,
where~$\gamma$ is the real constant $2 \sum_{j \geqslant 1} j / 2^{j/2}$. 
After the push, the stack is $\overline{\S} = (R,R_1,\ldots,R_h)$ and,
if the starting sequence contains $k \geqslant 1$ letters, i.e. $k-1$ occurrences of \#2, 
then this sequence amounts to merging the runs $R_1$, $R_2$, \ldots, $R_k$.
Since no merge is performed if $k = 1$, we assume below that $k \geqslant 2$.

More precisely, the total cost of these merges is
\[
C = (k-1)r_1+(k-1)r_2 + (k-2)r_3+\ldots + r_k \leqslant {\textstyle\sum_{i=1}^k} (k+1-i)r_i.
\]
The last occurrence of Case \#2 ensures that $r > r_k$,
hence applying Corollary~\ref{cor:invariant I'} to the stack $\S = (R_1,\ldots,R_h)$ shows that
$r \geqslant r_k \geqslant 2^{(k-1-i)/2} r_i$ for all $i = 1,\ldots,k$.
It follows that
\[
C / r \leqslant {\textstyle\sum_{i=1}^k} (k+1-i)2^{(i+1-k)/2} =
2 {\textstyle\sum_{j=1}^k} j 2^{-j/2} < \gamma.
\]

This concludes the proof, since each run is the beginning of exactly one starting sequence, and the sum of their lengths is $n$.
\end{proof}

Now, we must take care of run merges that take place during ending sequences.
The cost of merging two runs will be taken care of by making run elements pay tokens:
whenever two runs of lengths $r$ and $r'$ are merged, $r+r'$ tokens are paid
(not necessarily by the elements of those runs that are merged).
In order to do so, and to simplify the presentation, we also distinguish two kinds of tokens,
the \ctok-tokens and the \stok-tokens, which can both be used to pay for comparisons.

Two \ctok-tokens and one \stok-token are credited to an element when its run is pushed onto the stack
or when its height later decreases \emph{because of a merge that took place during an ending sequence}:
in the latter case, all the elements of $R_1$ are credited when $R_1$ and $R_2$ are merged, and
all the elements of $R_1$ and $R_2$ are credited when $R_2$ and $R_3$ are merged.
% 
Tokens are spent to pay for comparisons, depending on % \juge{which case of Algorithm~\ref{alg:TS translated} is triggered:}
the case triggered:
\begin{itemize}
\item Case \#2: every element of $R_1$ and $R_2$ pays 1 \ctok.
This is enough to cover the cost of merging $R_2$ and $R_3$, 
because $r_1 > r_3$ in this case, and therefore $r_2+r_1 \geqslant r_2 + r_3$.
\item Case \#3: every element of $R_1$ pays 2 \ctok. In this case $r_1\geqslant r_2$, and the cost is $r_1+r_2 \leqslant 2r_1$.
\item Cases \#4 and \#5: every element of $R_1$ pays 1 \ctok
and every element of $R_2$ pays 1 \stok. The cost $r_1+r_2$ is exactly the number of tokens spent.
\end{itemize}

\begin{lemma}\label{lm:balance}
The balances of \ctok-tokens and \stok-tokens of each element remain non-negative throughout  the main loop of \TS.
\end{lemma}

\begin{proof}
In all four cases \#2 to \#5, because the height of the elements of $R_1$ and possibly the height of those of $R_2$ decrease,
the number of credited \ctok-tokens after the merge is at least the number of \ctok-tokens spent.
The \stok-tokens are spent in Cases \#4 and \#5 only: every element of $R_2$ pays one \stok-token, and then belongs to the topmost run $\overline{R}_1$ of the new stack $\overline{\S}=(\overline{R}_1,\ldots, \overline{R}_{h-1})$ obtained after  merging $R_1$ and $R_2$. Since 
$\overline{R}_{i} = R_{i+1}$ for $i\geqslant 2$, the condition of Case~\#4 implies that $\overline{r}_1\geqslant \overline{r}_2$ and
the condition of Case~\#5 implies that $\overline{r}_1+\overline{r}_2\geqslant \overline{r}_3$: in both cases, the next modification of the stack $\overline{\S}$ is another merge, which belongs to the same ending sequence.

This merge decreases the height of $\overline{R}_1$, and therefore decreases the height of the elements of $R_2$, who will regain one \stok-token without losing any, since the topmost run of the stack never
pays with \stok-tokens. This proves that, whenever an element pay one \stok-token, the next modification is another merge during which it regains its \stok-token. This concludes the proof by direct induction.
\end{proof}

Finally, consider some element belonging to a run $R$.
Let $\S$ be the stack just before pushing the run $R$, and let
$\overline{S} = (\overline{R}_1,\ldots,\overline{R}_h)$ be the
stack just after the starting sequence of the run $R$
(i.e., the starting sequence initiated when $R$ is pushed onto $\S$) is over.
Every element of $R$ will be given at most $2h$ \ctok-tokens and $h$ \stok-tokens during the main loop
of the algorithm.

\begin{lemma}\label{lm:h-is-small}
The height of the stack when the starting sequence of the run $R$ is over
satisfies the inequality $h \leqslant 4 + 2 \log_2(n/r)$.
\end{lemma}

\begin{proof}
Since none of the runs $\overline{R}_3,\ldots,\overline{R}_h$
has been merged during the starting sequence of $R$,
applying Corollary~\ref{cor:invariant I'} to the stack $\S$ proves that
$\overline{r}_3 \leqslant 2^{2-h/2} \overline{r}_h \leqslant 2^{2-h/2} n$.
The run $R$ has not yet been merged either, which means that $r = \overline{r}_1$.
Moreover, at the end of this starting sequence, the conditions of case \#2 do not hold anymore,
which means that $\overline{r}_1 \leqslant \overline{r}_3$.
It follows that $r = \overline{r}_1 \leqslant \overline{r}_3 \leqslant 2^{2-h/2} n$,
which entails the desired inequality.
\end{proof}

Collecting all the above results is enough to prove Theorem~\ref{thm:complexity n + n H}.
First, as mentioned in Remark~\ref{rem:main-loop},
computing the run decomposition can be done in linear time. Then, we proved
that the starting sequences of the main loop have a merge cost $\O(n)$,
and that the ending sequences have a merge cost
$\O(\sum_{i=1}^\rho (1+\log(n/r_i))r_i) = \O(n + n \H)$.
Finally, the additional
merges of line~11 may be taken care of by Remark~\ref{rem:main-loop}.
This concludes the proof of the theorem.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Refined analysis and precise worst-case complexity}\label{sec:analysis2}

The analysis performed in Section~\ref{sec:analysis1} proves that \TS sorts arrays in time
$\O(n + n \H)$. Looking more closely at the constants hidden in the $\O$ notation,
we may in fact prove that the cost of merges performed during an execution of \TS
is never greater than $6 n \H + \O(n)$.
However, the lower bound provided by Proposition~\ref{proposition:optimality}
only proves that the cost of these merges must be at least $n \H + \O(n)$.
In addition, there exist sorting algorithms~\cite{munro2018nearly}
whose merge cost is exactly $n \H + \O(n)$.

Hence, \TS is optimal only up to a multiplicative constant.
We focus now on finding the least real constant $\kappa$ such that
the merge cost of \TS is at most $\kappa n \H + \O(n)$,
thereby proving a conjecture of~\cite{BuKno18}.

\begin{theorem}\label{thm:complexity 1.5 n + n H}
The merge cost of \TS on arrays in $\C$ is at most $\kappa n \H + \O(n)$,
where $\kappa = 3/2$. Furthermore, $\kappa = 3/2$ is the least real constant
with this property.
\end{theorem}

The rest of this Section is devoted to proving Theorem~\ref{thm:complexity 1.5 n + n H}.
The theorem can be divided into two statements: one that states that \TS is asymptotically
optimal up to a multiplicative constant of $\kappa = 3/2$, and one that states that $\kappa$ is optimal.
The latter statement was proved in~\cite{BuKno18}. Here, we borrow their proof for the sake of completeness.

\begin{proposition}\label{pro:kappa-optimal-BuKno18}
There exist arrays of length $n$ on which the merge cost of \TS is at least $3/2 n \log_2(n) + \O(n)$.
\end{proposition}

\begin{proof}
The dynamics of \TS when sorting an array involves only the lengths of the monotonic runs in which
the array is split, not the actual array values. Hence, we identify every array with the sequence
of its run lengths. Therefore, every sequence of run lengths $\langle r_1,\ldots,r_\rho \rangle$ such that
$r_1,\ldots,r_{\rho-1} \geqslant 2$, $r_\rho \geqslant 1$ and $r_1+\ldots+r_\rho = n$ represents
at least one possible array of length $n$.

We define inductively a sequence of run lengths $\R(n)$ as follows:
\[\R(n) = \begin{cases}\langle n \rangle & \text{if } 1 \leqslant n \leqslant 6, \\
\R(k) \cdot \R(k-2) \cdot \langle 2\rangle & \text{if } n = 2k \text{ for some } k \geqslant 4, \\
\R(k) \cdot \R(k-1) \cdot \langle 2\rangle & \text{if } n = 2k+1 \text{ for some } k \geqslant 3,
\end{cases}\]
where the concanetation of two sequences $s$ and $t$ is denoted by $s \cdot t$.

Then, let us apply the main loop of TimSort
on an array whose associated monotonic runs
have lengths $\mathbf{r} = \langle r_1,\ldots,r_\rho \rangle$,
starting with an empty stack.
We denote the associated merge cost by $c(\mathbf{r})$ and,
if $\overline{\S} = (\overline{R}_1,\ldots,
\overline{R}_{\overline{h}})$ is the stack obtained
after the main loop has been applied,
we denote by $s(\mathbf{r})$ the sequence
$\langle \overline{r}_1,\ldots,\overline{r}_{\overline{h}}\rangle$.

An immediate induction shows that, if
$r_1 \geqslant r_2+\ldots+r_\rho+1$, then
$c(\mathbf{r}) = c(\langle r_2,\ldots,r_\rho\rangle)$ and
$s(\mathbf{r}) = \langle r_1 \rangle \cdot s(\langle r_2,\ldots,r_\rho\rangle)$. Similarly, if $r_1 \geqslant r_2+\ldots+r_\rho+1$ and
$r_2 \geqslant r_3+\ldots+r_\rho+1$, then
$c(\mathbf{r}) = c(\langle r_3,\ldots,r_\rho\rangle)$ and
$s(\mathbf{r}) = \langle r_1,r_2 \rangle \cdot s(\langle r_3,\ldots,r_\rho\rangle)$.

Consequently, and by another induction on $n$, it holds that
$s(\R(n)) = \langle n \rangle$ and that
\[c(\R(n)) = \begin{cases} 0 & \text{if } 1 \leqslant n \leqslant 6, \\
c(\R(k)) + c(\R(k-2)) + 3k & \text{if } n = 2k \text{ for some } k \geqslant 4, \\
c(\R(k)) + c(\R(k-1)) + 3k+2 & \text{if } n = 2k+1 \text{ for some } k \geqslant 3.
\end{cases}\]

Let $u_x = c(\R(\lfloor x \rfloor))$
and $v_x = (u_{x-4} - 15/2)/x - 3 \log_2(x) / 2$.
An immediate induction shows that $c(\R(n)) \geqslant c(\R(n+1))$
for all integers $n \geqslant 0$, which means that $x \mapsto u_x$
is non-decreasing. Then, we have 
$u_n = u_{n/2} + u_{(n-3)/2} + \lceil 3n/2 \rceil$
for all integers $n \geqslant 6$, and therefore
$u_x \geqslant 2 u_{x/2-2} + 3(x-1)/2$ for all real numbers $x \geqslant 6$. Consequently, for $x \geqslant 11$, it holds that
\[x v_x = u_{x-4} - 3 x \log_2(x) / 2 - 15/2 \geqslant
2 u_{x/2-4} + 3(x-5)/2 - 3 x \log_2(x) / 2 - 15/2 = x v_{x/2}.\]
This proves that $v_x \geqslant v_{x/2}$, from which it follows
that $v_x \geqslant \inf\{v_t \,:\, 11/2 \leqslant t < 11\}$.
Since $v_t = -15/(2t) -3 \log_2(t)/2 \geqslant -15/11 - 3 \log_2(11)/2 \geqslant -7$ for all $t \in [11/2,11)$,
we conclude that
$v_x \geqslant -7$ for all $x \geqslant 11$, and thus that
\[c(\R(n)) = u_n \geqslant (n+4) v_{n+4} + 3 (n+4) \log_2(n+4) / 2 \geqslant 3 n \log_2(n) / 2 - 7 (n+4),\] thereby proving
Proposition~\ref{pro:kappa-optimal-BuKno18}.
\end{proof}

It remains to prove the first statement of Theorem~\ref{thm:complexity 1.5 n + n H}. Our initial step towards
this statement consists in refining Lemma~\ref{lm:invariant I}.
This is the essence of Lemmas~\ref{lm:invariant II} to~\ref{lm:invariant IV}.

\begin{lemma}\label{lm:invariant II}
At any step during the main loop of \TS, if $h \geqslant 4$,
we have~$r_2<r_4$ and~$r_3<r_4$.
\end{lemma}

\begin{proof}
We proceed by induction. The proof consists in verifying that, if the
invariant holds at some point, then it still holds when an update of the stack occurs in one of the five situations labeled \#1 to \#5 in the algorithm.
This can be done by a straightforward case analysis.
We denote by $\S=(R_1,\ldots, R_h)$ the stack just before
the update, and by
$\overline{\S}=(\overline{R}_1,\ldots, \overline{R}_{\overline{h}})$ 
the new state of the stack after the update:
\begin{disjunction}
  \item If Case \#1 just occurred, a new run $\overline{R}_1$ was pushed.
  This implies that the conditions of Cases \#2 and \#4 did not hold in $\S$, otherwise merges would have continued. In particular, we have $\overline{r}_2=r_1<r_3=\overline{r}_4$ and
  $\overline{r}_3=r_2<r_1+r_2<r_3=\overline{r}_4$.
  
  \item If one of the Cases \#2 to \#5 just occurred, it holds that $\overline{r}_2 \leqslant r_2+r_3$, that $\overline{r}_3=r_4$ and that $\overline{r}_4=r_5$. Since Lemma~\ref{lm:invariant I} proves that $r_3+r_4<r_5$, it follows that
  $\overline{r}_2 \leqslant r_2+r_3 < r_3+r_4 < r_5 = \overline{r}_4$
  and that $\overline{r}_3 = r_4<r_3+r_4<r_5 = \overline{r}_4$.
\end{disjunction}
\end{proof}

\begin{lemma}\label{lm:invariant III}
At any step during the main loop of \TS, and for all
$i \in \{3,\ldots,h\}$, it holds that
$r_2+\ldots+r_{i-1} < \phi \, r_i$.
\end{lemma}

\begin{proof}
Like for Lemmas~\ref{lm:invariant I} and~\ref{lm:invariant II},
we proceed by induction and verify that, if the
invariant holds at some point, then it still holds when an update of the stack occurs in one of the five situations labeled \#1 to \#5 in the algorithm.
Let us denote by $\S=(R_1,\ldots, R_h)$ the stack just before
the update, and by
$\overline{\S}=(\overline{R}_1,\ldots, \overline{R}_{\overline{h}})$ 
the new state of the stack after the update:
\begin{disjunction}
  \item If Case \#1 just occurred, then we proceed by induction on
  $i \geqslant 3$. First, for $i = 3$,  since the conditions for Cases \#3 and \#4 do not hold in $\S$, we know that $\overline{r}_2 = r_1 < r_2 = \overline{r}_3$ and that
  $\overline{r}_2 + \overline{r}_3 = r_1+r_2 < r_3 = \overline{r}_4$.
  Then, for $i \geqslant 5$, Lemma~\ref{lm:invariant I} states that $r_{i-2}+r_{i-1} < r_i$, and therefore
  \begin{enumerate}[(i)]
   \item if $\overline{r}_{i-1} \leqslant \phi^{-1} \, \overline{r}_i$, then
  $\overline{r}_2+\ldots+\overline{r}_{i-1} < (\phi + 1) \overline{r}_{i-1} = \phi^2 \overline{r}_{i-1} \leqslant \phi \overline{r}_i$, and
  \item if $\overline{r}_{i-1} \geqslant \phi^{-1} \, \overline{r}_i$, then
  $\overline{r}_{i-2} \leqslant (1-\phi^{-1}) \ \overline{r}_i = \phi^{-2} \, \overline{r}_i$, and thus
  $\overline{r}_2+\ldots+\overline{r}_{i-1} < (\phi+1) \, \overline{r}_{i-2} + \overline{r}_{i-1} \leqslant
  \phi \, \overline{r}_{i-2} + \overline{r}_i \leqslant (\phi^{-1} + 1) \overline{r}_i = \phi \, \overline{r}_i$.
  \end{enumerate}
  Hence, in that case, it holds that $\overline{r}_2+\ldots+\overline{r}_{i-1} < \phi \, \overline{r}_i$ for all $i \in \{3,\ldots,h\}$.
  
  \item If one of the Cases \#2 to \#5 just occurred, it holds that $\overline{r}_2 \leqslant r_2+r_3$ and that $\overline{r}_j = r_{j+1}$ for all $j \geqslant 3$. It follows that
  $\overline{r}_2+\ldots+\overline{r}_{i-1} \leqslant
  r_2+\ldots+r_i < \phi \, r_{i+1} = \overline{r}_i$.
\end{disjunction}
\end{proof}

\begin{remark}
We could also have derived directly Lemma~\ref{lm:invariant II} from
Lemma~\ref{lm:invariant III}, by noting that
$\phi^2 \, r_2 = (\phi+1) r_2 < \phi \, r_2 + \phi \, r_3 < \phi^2 \, r_4$.
\end{remark}

\begin{lemma}\label{lm:invariant IV}
After every merge that occurred during an ending sequence,
we have~$r_1 < \phi^2 r_2$.
\end{lemma}

\begin{proof}
Once again, we proceed by induction.
We denote by $\S=(R_1,\ldots, R_h)$ the stack just before an update occurs, and by
$\overline{\S}=(\overline{R}_1,\ldots, \overline{R}_{\overline{h}})$ 
the new state of the stack after after the update:
\begin{disjunction}
  \item If Case \#2 just occurred, then
  this update is not the first one within the ending sequence,
  hence~$\overline{r}_1 = r_1 < \phi^2 \, r_2 < \phi^2 (r_2 + r_3) = \phi^2 \, \overline{r}_2$.
  
  \item If one of the Cases \#2 to \#5 just occurred,
  then $r_1 \leqslant r_3$ and Lemma~\ref{lm:invariant III} proves that
  $r_2 < \phi \, r_3$, which proves that
  $\overline{r}_1 = r_1 + r_2 < (\phi+1) r_3 = \phi^2 \, \overline{r}_2$.
\end{disjunction}
\end{proof}

\begin{lemma}\label{lm:invariant V}
After every merge triggered by Case $\#2$,
we have~$r_2 < \phi^2 r_1$.
\end{lemma}

\begin{proof}
We denote by $\S=(R_1,\ldots, R_h)$ the stack just before an update triggered by Case \#2 occurs, and by
$\overline{\S}=(\overline{R}_1,\ldots, \overline{R}_{\overline{h}})$
the new state of the stack after after the update.
It must hold that $r_1 > r_3$ and
Lemma~\ref{lm:invariant III} proves that $r_2 < \phi \, r_3$. It follows that
$\overline{r}_2 = r_2 + r_3 < (\phi+1) r_3 = \phi^2 \, r_3 < \phi^2 \, r_1 = \phi^2 \, \overline{r}_1$.
\end{proof}

Our second step towards proving the first statement of
Theorem~\ref{thm:complexity 1.5 n + n H}
consists in identifying which sequences of merges
an ending sequence may be made of.
More precisely, in the proof of Lemma~\ref{lm:balance},
we proved that every merge triggered by a case
$\#4$ or $\#5$ must be followed
by another merge, i.e.,
it cannot be the final merge of an ending sequence.

We present now a variant of this result,
which involves distinguishing between merges triggered
by a case $\#2$ and those triggered by a case $\#3$, $\#4$ or $\#5$.
Hence, we denote by $\caseX$ every $\#3$, $\#4$ or $\#5$.

\begin{lemma}\label{lm:(X2)*X*}
No ending sequence contains two conscutive $\#2$'s,
nor does it contain a subsequence of the form \caseX \caseX $\#2$.
\end{lemma}

\begin{proof}
Every ending sequence starts with an update \caseX,
where \caseX is equal to \#3, \#4 or \#5. Hence,
it suffices to prove that no ending sequence
contains a subsequence $\mathbf{t}$ of the form 
\caseX\caseX\#2 or \caseX\#2\;\#2.

Indeed, for the sake of contradiction, assume that it does,
and let $\S = (R_1,\ldots,R_h)$ be the stack just before
$\mathbf{t}$ starts.
We distinguish two cases, depending on the value of $\mathbf{t}$:
\begin{disjunction}
\item If $\mathbf{t}$ is the sequence $\caseX\;\caseX\;\#2$,
it must hold that $r_1+r_2 < r_4$ and that
$r_1+r_2+r_3 \geqslant r_5$, as illustrated in Figure~\ref{fig:XX2}~(top).
Since Lemma~\ref{lm:invariant I} proves that $r_3 + r_4 < r_5$,
it follows that $r_1+r_2+r_3 \geqslant r_5 > r_3+r_4 > r_1+r_2+r_3$,
which is impossible.
 
\item If $\mathbf{t}$ is the sequence $\caseX\;\#2\;\#2$, it must hold that
$r_1 < r_3$ and that
$r_1+r_2 \geqslant r_5$, as illustrated in Figure~\ref{fig:XX2}~(bottom).
Since Lemmas~\ref{lm:invariant I} and~\ref{lm:invariant II} prove that $r_3+r_4 < r_5$ and that $r_2 < r_4$,
it comes that
$r_1+r_2 \geqslant r_5 > r_3+r_4 > r_1+r_2$,
which is also impossible.\vspace{-0.2mm}
\end{disjunction}
\end{proof}

\begin{figure}[ht]
\begin{center}
\vspace{-2mm}
\begin{small}
\begin{tikzpicture}[scale=0.45]
\foreach \i in {0,...,3}{
 \draw[thick] (9*\i,0) -- (9*\i+2,0) -- (9*\i+2,5) -- (9*\i,5) -- cycle;
 \foreach \j in {1,...,5}{
   \node[anchor=south] at (9*\i+1,5-\j-0.05) {$r_\j$};
 }
}
\foreach \i in {0,...,2}{
 \FPeval{\k}{clip(4-\i)}
 \foreach \j in {1,...,\k}{
   \draw[thick] (9*\i,\j) -- (9*\i+2,\j);
 }
}
\draw[thick] (9*3,2) -- (9*3+2,2);

\draw[very thick,->,>=stealth] (2.1,2.5) -- (8.9,2.5);
\draw[very thick,->,>=stealth] (11.1,2.5) -- (17.9,2.5);
\draw[very thick,->,>=stealth] (20.1,2.5) -- (26.9,2.5);
% \node[anchor=north] at (5.5,2.5+0.05) {$r_1<r_3$};
\node[anchor=south] at (5.5,2.5-0.05) {merge \caseX};
\node[anchor=north] at (14.5,2.5+0.05) {$r_1+r_2<r_4$};
\node[anchor=south] at (14.5,2.5-0.05) {merge \caseX};
\node[anchor=north] at (23.5,2.5+0.05) {$r_1+r_2+r_3 \geqslant r_5$};
\node[anchor=south] at (23.5,2.5-0.05) {merge $\#2$};

\node[anchor=south] at (10,3.5) {\tiny+};
\node[anchor=south] at (19,3.5) {\tiny+};
\node[anchor=south] at (19,2.5) {\tiny+};
\node[anchor=south] at (28,3.5) {\tiny+};
\node[anchor=south] at (28,2.5) {\tiny+};
\node[anchor=south] at (28,0.5) {\tiny+};
\end{tikzpicture}

\bigskip\bigskip

\begin{tikzpicture}[scale=0.45]
\foreach \i in {0,...,3}{
 \draw[thick] (9*\i,0) -- (9*\i+2,0) -- (9*\i+2,5) -- (9*\i,5) -- cycle;
 \foreach \j in {1,...,5}{
   \node[anchor=south] at (9*\i+1,5-\j-0.05) {$r_\j$};
 }
}
\foreach \i in {0,...,1}{
 \FPeval{\k}{clip(4-\i)}
 \foreach \j in {1,...,\k}{
   \draw[thick] (9*\i,\j) -- (9*\i+2,\j);
 }
}
\draw[thick] (9*2,1) -- (9*2+2,1);
\draw[thick] (9*2,3) -- (9*2+2,3);
\draw[thick] (9*3,3) -- (9*3+2,3);

\draw[very thick,->,>=stealth] (2.1,2.5) -- (9*1-0.1,2.5);
\draw[very thick,->,>=stealth] (9*1+2.1,2.5) -- (9*2-0.1,2.5);
\draw[very thick,->,>=stealth] (9*2+2.1,2.5) -- (9*3-0.1,2.5);
\node[anchor=north] at (9*0.5+1,2.5+0.05) {$r_1 \leqslant r_3$};
\node[anchor=south] at (9*0.5+1,2.5-0.05) {merge \caseX};
\node[anchor=south] at (9*1+9*0.5+1,2.5-0.05) {merge $\#2$};
\node[anchor=north] at (9*2+9*0.5+1,2.5+0.05) {$r_1+r_2 \geqslant r_5$};
\node[anchor=south] at (9*2+9*0.5+1,2.5-0.05) {merge $\#2$};

\node[anchor=south] at (9*1+1,3.5) {\tiny+};
\node[anchor=south] at (9*2+1,3.5) {\tiny+};
\node[anchor=south] at (9*2+1,1.5) {\tiny+};
\node[anchor=south] at (9*3+1,3.5) {\tiny+};
\node[anchor=south] at (9*3+1,1.5) {\tiny+};
\node[anchor=south] at (9*3+1,0.5) {\tiny+};
\end{tikzpicture}
\end{small}
\end{center}
\vspace{-2mm}
\caption{Applying successively merges
\caseX\#2\;\#2
or \caseX\caseX\#2
to a stack is impossible.}
\label{fig:XX2}
\end{figure}

Our third step consists in modifying the cost allocation
we had chosen in Section~\ref{sec:analysis1}, which
is not sufficient to prove Theorem~\ref{thm:complexity 1.5 n + n H}.
Instead, we associate to
every run $R$ its \emph{potential}, which depends only on the length
$r$ of the run, and is defined as
$\pot(r) = 3 r \log_2(r) / 2$. We also call \emph{potential} of a
set of runs the sum of the potentials of the runs it is formed of,
and \emph{potential variation} of a (sequence of) merges
the increase in potential caused by these merge(s).

We shall prove that the potential variation of every ending
sequence dominates its merge cost, up to a small error term.
In order to do this, let us study more precisely individual merges.
Below, we respectively denote by $\Delta_{\pot}(\mathbf{m})$
and $\cost(\mathbf{m})$ the potential variation and the
merge cost of a merge $\mathbf{m}$. Then, we say that
$\mathbf{m}$ is a \emph{balanced} merge if
$\cost(\mathbf{m}) \leqslant \Delta_{\pot}(\mathbf{m})$.

In the next Lemmas, we prove that most merges are balanced
or can be grouped into sequences of merges that are balanced
overall.

\begin{lemma}\label{lm:delta-cost I}
Let $\mathbf{m}$ be a merge between two runs $R$ and $R'$.
If $\phi^{-2} \, r \leqslant r' \leqslant \phi^2 \, r$,
then $\mathbf{m}$ is balanced.
\end{lemma}

\begin{proof}
Let $x = r / (r+r')$: we have~$\Phi < x < 1-\Phi$, where $\Phi = 1/(1+\phi^2)$.
Then, observe that $\Delta(\mathbf{m}) = 3 (r+r') H(x) / 2$,
where $H(x) = - x \log_2(x) - (1-x) \log_2(x)$
is the binary Shannon entropy of a Bernoulli law of parameter $x$.
Moreover, the function $z \mapsto H(z) = H(1-z)$
is increasing on $[0,1/2]$. It follows that $H(x) \geqslant
H(\Phi) \approx 0.85 > 2/3$, and therefore that $\Delta(\mathbf{m}) > r+r' = \cost(\mathbf{m})$. 
\end{proof}

\begin{lemma}\label{lm:delta-cost II}
Let $\mathbf{m}$ be a merge that belongs to some ending sequence.
If $\mathbf{m}$ is a merge $\#2$, then $\mathbf{m}$ is balanced and,
if $\mathbf{m}$ is followed by another merge $\mathbf{m}'$,
then $\mathbf{m}'$ is also balanced.
\end{lemma}

\begin{proof}
Lemma~\ref{lm:(X2)*X*} ensures that
$\mathbf{m}$ was preceded by another merge $\mathbf{m}^\star$,
which must be a merge \caseX.
Denoting by $\S = (R_1,\ldots,R_h)$ the stack just before the merge
$\mathbf{m}^\star$ occurs,
the update $\mathbf{m}$ consists in merging
the runs $R_3$ and $R_4$.
Then, it comes that $r_1 \leqslant r_3$ and that $r_1+r_2 > r_4$,
while Lemma~\ref{lm:invariant II} and~\ref{lm:invariant III} respectively
prove that $r_3 < r_4$ and that $r_2 < \phi \, r_3$.
Hence, we both have $r_3 < r_4$ and
$r_4 < r_1 + r_2 < (1 + \phi) r_3 = \phi^2 \, r_3$, and Lemma~\ref{lm:delta-cost II} proves that $\mathbf{m}$ is balanced.

Then, if $\mathbf{m}$ is followed by another merge $\mathbf{m}'$,
Lemma~\ref{lm:(X2)*X*} proves that $\mathbf{m}'$ is also a merge \caseX, between runs of respective lengths $r_1+r_2$ and $r_3+r_4$.
Note that $r_1 \leqslant r_3$ and that $r_1+r_2 > r_4$. Since
Lemma~\ref{lm:invariant II} proves that $r_2 < r_4$ and that $r_3 < r_4$, it follows that
$2(r_1+r_2) > 2 r_4 > r_3 + r_4 > r_1+r_2$ and,
using the fact that $2 < 1 + \phi = \phi^2$,
Lemma~\ref{lm:delta-cost II} therefore proves that $\mathbf{m}$ is balanced.
\end{proof}

\begin{lemma}\label{lm:delta-cost III}
Let $\mathbf{m}$ be a merge \caseX
between two runs $R_1$ and $R_2$ such that $r_1 < \phi^{-2} \, r_2$.
Then, $\mathbf{m}$ is followed by another merge $\mathbf{m}'$, and
$\cost(\mathbf{m}) + \cost(\mathbf{m}') \leqslant \Delta_{\pot}(\mathbf{m}) + \Delta_{\pot}(\mathbf{m}')$.
\end{lemma}

\begin{proof}
Let $\mathbf{m}^\star$ be the update the immediately precedes
$\mathbf{m}$. Let also $\S^\star = (R^\star_1,\ldots,R^\star_{h^\star})$, $\S = (R_1,\ldots,R_h)$ and
$\S' = (R'_1,\ldots,R'_{h'})$ be the respective states
of the stack just before $\mathbf{m}^\star$ occurs,
just before $\mathbf{m}$ occurs and just after $\mathbf{m}$ occurs.

Since $r_1 < \phi^{-2} \, r_2$,
Lemma~\ref{lm:invariant V} proves that
$\mathbf{m}^\star$ is either an update \#1 or a merge \caseX.
In both cases, it follows that $r_2 < r_3$ and that $r_2+r_3 < r_4$.
Indeed, if $\mathbf{m}^\star$ is an update \#1, then we must have
$r_2 = r^\star_1 < r^\star_2 = r_3$ and
$r_2+r_3 = r^\star_1+r^\star_2 < r^\star_3 = r_4$, and if
$\mathbf{m}'$ is a merge \caseX, then Lemmas~\ref{lm:invariant I} and~\ref{lm:invariant II}
respectively prove that $r_2+r_3 = r^\star_3+r^\star_4 < r^\star_5 = r_4$ and that $r_2 = r^\star_3 < r^\star_4 = r_3$.

Then, since $\mathbf{m}$ is a merge \caseX, we also know that
$r_1 \leqslant r_3$. Since $r_1 < \phi^{-2} \, r_2$ and $r_2+r_3 < r_4$, this means that $r_1 + r_2 \geqslant r_3$.
It follows that
$r'_2 = r_3 \leqslant r_1+ r_2 = r'_1$ and that
$r'_1 = r_1 + r_2 \leqslant r_2 + r_3 < r_4 = r'_3$.
Consequently, the merge $\mathbf{m}$ must be followed by a merge
$\mathbf{m}'$, which is triggered by case \#3.

Finally, let $x = r_1 / (r_1+r_2)$ and $y = (r_1+r_2) / (r_1+r_2+r_3)$.
It comes that $\cost(\mathbf{m})+\cost(\mathbf{m}') = (r_1+r_2+r_3)(1+y)$ and that
$\Delta_{\pot}(\mathbf{m}) + \Delta_{\pot}(\mathbf{m}') = 3 (r_1+r_2+r_3) \left(y H(x) + H(y)\right)\!/ 2$, where
we recall that $H$ is the binary Shannon entropy function, with
$H(t) = - t \log_2(t) - (1-t) \log_2(t)$.
The above inequalities about $r_1$, $r_2$ and $r_3$ prove that $0 \leqslant 2 - 1/y \leqslant x \leqslant 1/(1+\phi^2)$.
Since $H$ is increasing on the interval $[0,1/2]$, and since
$1+\phi^2 \geqslant 2$, it follows that
$\Delta_{\pot}(\mathbf{m}) + \Delta_{\pot}(\mathbf{m}') \geqslant 3 (r_1+r_2+r_3) \left(y H(2 - 1/y) + H(y)\right)\!/ 2$.

Hence, let $F(y) = 3 \left(y H(2 - 1/y) + H(y)\right)\!/2 - (1+y)$.
We shall prove that $F(y) \geqslant 0$ for all $y \geqslant 0$
such that
$0 \leqslant 2 - 1/y \leqslant 1/(1+\phi^2)$, i.e., such that
$1/2 \leqslant y \leqslant 
(1+\phi^2)/(1+2\phi^2)$.
To that mean, observe that
$F''(y) = 3 /\!\left((1-y)(1-2y) \ln(2)\right) < 0$
for all $y \in (1/2,1)$. Thus, $F$ is concave on
$(1/2,1)$.
Since $F(1/2) = 0$ and $F(3/4) = 1/2$, it follows that
$F(y) \geqslant 0$ for all $y \in [1/2,3/4]$.
Checking that $(1+\phi^2)/(1+2\phi^2) < 3/4$ completes the proof.
\end{proof}

\begin{lemma}\label{lm:delta-cost IV}
Let $\mathbf{m}$ be the first merge of the ending sequence
associated with a run $R$. Let $R_1$ and $R_2$ be the runs
that $\mathbf{m}$ merges together.
If $r_1 > \phi^2 \, r_2$, it holds that
$\cost(\mathbf{m}) \leqslant \Delta_{\pot}(\mathbf{m}) + r$.
\end{lemma}

\begin{proof}
By definition of $\mathbf{m}$,
we have~$R = R_1$, and thus $r = r_1 \geqslant r_2$.
Hence, it follows that
$\Delta_{\pot}(\mathbf{m}) = r \log((r+r_2)/r) +
r_2 \log((r+r_2)/r_2) \geqslant r_2 \log((r+r_2)/r_2) \geqslant
r_2 = \cost(\mathbf{m}) - r$.
\end{proof}

\begin{proposition}\label{pro:delta-cost}
Let $\mathbf{s}$ be the ending sequence associated with a run $R$,
and let $\Delta_{\pot}(\mathbf{s})$ and $\cost(\mathbf{s})$
be its potential variation and its merge cost.
It holds that~$\cost(\mathbf{s}) \leqslant \Delta_{\pot}(\mathbf{s}) + r$.
\end{proposition}

\begin{proof}
Let us group the merges of $\mathbf{s}$ as follows:
\begin{enumerate}[(i)]
 \item if $\mathbf{m}$ is an unbalanced merge \caseX between two runs $R_1$ and $R_2$ such that $r_1 < r_2$, then $\mathbf{m}$ is followed by another merge $\mathbf{m}'$, and we group $\mathbf{m}$
 and $\mathbf{m}'$ together;\label{delta-cost:case:1}
 \item otherwise, and if $\mathbf{m}$ has not been grouped
 with its predecessor, it forms its own group.
\end{enumerate}
In case~\eqref{delta-cost:case:1}, Lemma~\ref{lm:delta-cost IV} ensures that $\mathbf{m}'$ itself cannot be grouped with another merge.
This means that our grouping is unambiguous.

Then, let $\mathbf{g}$ be such a group,
with potential variation $\Delta_{\pot}(\mathbf{g})$ and
merge cost $\cost(\mathbf{g})$.
Lemmas~\ref{lm:delta-cost I} to~\ref{lm:delta-cost IV}
prove that $\cost(\mathbf{g}) \leqslant \Delta_{\pot}(\mathbf{g}) + r$ if $\mathbf{g}$ is formed of the first merge of $\mathbf{s}$ only,
and that $\cost(\mathbf{g}) \leqslant \Delta_{\pot}(\mathbf{g})$
in all other cases. Proposition~\ref{pro:delta-cost} follows.
\end{proof}

Collecting all the above results is enough to prove Theorem~\ref{thm:complexity 1.5 n + n H}.
First, like in Section~\ref{sec:analysis1},
computing the run decomposition and merging runs in
starting sequences has a cost $\O(n)$,
and the final merges of line~11 may be taken care of by Remark~\ref{rem:main-loop}.
Second, by Proposition~\ref{pro:delta-cost},
ending sequences have a merge cost dominated by
$\Delta_{\pot} + n$, where $\Delta_{\pot}$
is the total variation of potential during the algorithm.
Observing that
$\Delta_{\pot} = -3/2 \sum_{i=1}^\rho r_i \log_2(r_i/n) = - 3 n \H / 2$
concludes the proof of the theorem.



=== Paper (papers/paper2.txt) ===
%
\def\version{arxiv}
\def\draftmode{false}

\documentclass[a4paper,USenglish]{lipics-v2018}
%
%
%
%
%


\usepackage{microtype}%if unwanted, comment out or use option "draft"


\input{preamble}


%
	\hideLIPIcs
	\nolinenumbers
%




%

\bibliographystyle{plainurl}% the recommnded bibstyle

\title{%
	Nearly-Optimal Mergesorts: \protect\\ 
%
	Fast, Practical Sorting Methods That \protect\\ Optimally Adapt to Existing Runs%
}

\titlerunning{Nearly-Optimal Mergesort}%optional, please use if title is longer than one line

\author{J.\ Ian Munro}{University of Waterloo, Canada}{imunro@uwaterloo.ca}{https://orcid.org/0000-0002-7165-7988}{}%mandatory, please use full name; only 1 author per \author macro; first two parameters are mandatory, other parameters can be empty.

\author{Sebastian Wild}{University of Waterloo, Canada}{wild@uwaterloo.ca}{https://orcid.org/0000-0002-6061-9177}{}

\authorrunning{J.\,I.\ Munro and S. Wild}%mandatory. First: Use abbreviated first/middle names. Second (only in severe cases): Use first author plus 'et al.'

\Copyright{J.\ Ian Munro and Sebastian Wild}%mandatory, please use full first names. LIPIcs license is "CC-BY";  http://creativecommons.org/licenses/by/3.0/

\subjclass{\ccsdesc[500]{Theory of computation~Sorting and searching}}% mandatory: Please choose ACM 2012 classifications from https://www.acm.org/publications/class-2012 or https://dl.acm.org/ccs/ccs_flat.cfm . E.g., cite as "General and reference $\rightarrow$ General literature" or \ccsdesc[100]{General and reference~General literature}. 

\keywords{adaptive sorting, nearly-optimal binary search trees, Timsort}%mandatory
%\keywords{sorting, mergesort, adaptive sorting, nearly-optimal binary search trees, alphabetic trees}%mandatory

%\category{}%optional, e.g. invited paper

%\relatedversion{}%optional, e.g. full version hosted on arXiv, HAL, or other respository/website

%\supplement{}%optional, e.g. related research data, source code, ... hosted on a repository like zenodo, figshare, GitHub, ...

\funding{%
	This work was supported by the 
	Natural Sciences and Engineering Research Council of Canada 
	and the Canada Research Chairs Programme.
}%optional, to capture a funding statement, which applies to all authors. Please enter author specific funding statements as fifth argument of the \author macro.

%\acknowledgements{I want to thank \dots}%optional





%
%
%
%
%
%
%
%
%
%
%
%
%
%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{document}

\maketitle

\begin{abstract}
We present two stable mergesort variants, ``peeksort'' and ``powersort'', 
that exploit existing runs and find nearly-optimal merging orders with 
negligible overhead.
Previous methods either require substantial effort for determining the merging order
(Takaoka 2009\ifarxiv{~\cite{Takaoka2009}}{}; Barbay \& Navarro 2013\ifarxiv{~\cite{BarbayNavarro2013}}{}) 
or do not have an optimal worst-case guarantee 
(Peters 2002\ifarxiv{~\cite{Peters2002}}{}; Auger, Nicaud \& Pivoteau 2015\ifarxiv{~\cite{AugerNicaudPivoteau2015}}{}; 
Buss \& Knop 2018\ifarxiv{~\cite{BussKnop2018}}{})\@.
We demonstrate that our methods are competitive in terms of running time with state-of-the-art implementations
of stable sorting methods.
\end{abstract}


\section{Introduction}

Sorting is a fundamental building block for numerous tasks and 
ubiquitous in both the theory and practice of computing.
While practical and theoretically (close-to) optimal 
comparison-based sorting methods are known,
\emph{instance-optimal sorting,} \ie, methods that \emph{adapt} to the actual input
and exploit specific structural properties if present,
is still an area of active research. We survey some recent developments in \wref{sec:related}.

Many different structural properties have been investigated in theory.
Two of them have also found wide adoption in practice, \eg, in Oracle's Java runtime library:
adapting to the presence of duplicate keys 
and using existing sorted segments, called \emph{runs}.
The former is achieved by a so-called fat-pivot partitioning variant of 
quicksort~\cite{BentleyMcIlroy1993},
which is also used in the GNU implementation of \texttt{std::sort} from the 
{C\raisebox{.25ex}{\textsmaller[2]{++}}} STL\@.
It is an \emph{unstable} sorting method, though, \ie, the relative order of 
elements with equal keys might be destroyed in the process.
It is hence used in Java solely for primitive-type arrays.

Making use of existing runs in the input is a well-known option in mergesort; 
\eg, Knuth~\cite{Knuth1998} discusses a bottom-up mergesort variant that does this.
He calls it ``natural mergesort'' and we will use this as an umbrella term for
any mergesort variant that picks up existing
runs in the input (instead of starting blindly with runs of size~$1$).
The Java library uses Timsort~\cite{Peters2002,java2009timsort} 
which is such a natural mergesort originally developed as Python's new library sort.

While fat-pivot quicksort provably adapts
to the \emph{entropy of the multiplicities} of keys~\cite{Wild2018}~--
it is optimal up to a factor of $1.088$ on average with pseudo\-median-of-9 (``ninther'') pivots%
\footnote{%
	The median of three elements is chosen as the pivot, 
	each of which is a median of three other elements.
	This is a good approximation of the median of 9 elements
	and often used as pivot selection rule in library implementations.%
}%
~-- 
Timsort is much more heuristic in nature.
It picks up existing runs and tries to perform merges in a favorable order
(\ie, avoiding merges of runs with very different lengths),
but many desirable guarantees are missing:
%
Although it was announced as an $O(n\log n)$ worst-case method
with its introduction in Python in 2002~\cite{Peters2002mailinglist},
a rigorous proof of this bound was only given in 2015 by Auger, Nicaud, and Pivoteau~\cite{AugerNicaudPivoteau2015}
and required a rather sophisticated amortization argument.%
\footnote{%
\label{fn:wrong-timsort}%
	A further manifestation of the complexity of Timsort 
	was reported by de~Gouw et al.~\cite{DeGouwBoerBubelHaehnleRotSteinhoefel2017}:
	The original rules to maintain the desired invariant 
	for run lengths on the stack was not sufficient in some cases.
	This (algorithmic!) bug had remained unnoticed until their attempt to formally
	verify the correctness of the Java implementation of Timsort failed because of it.
}
The core complication is that~--
unlike for standard mergesort variants~-- a given element might participate
in more than a logarithmic number of merges.
Indeed, Buss and Knop~\cite{BussKnop2018} have very recently shown 
that for some family of inputs, the average number of merges a single element participates in 
is at least $\bigl(\frac32-o(1)\bigr)\cdot \lg n$.
So in the worst case, Timsort does, \eg, 
(at least) \emph{1.5 times as many element moves as standard mergesort.}

In terms of adapting to existing order, the only proven guarantee for Timsort's running time 
is a trivial $O(n r)$ bound when the input consists of $r$ runs. 
Proving an informative upper bound like $O(n+n \log r )$ has remained elusive,
(although it is conjectured to hold in~\cite{AugerNicaudPivoteau2015} and~\cite{BussKnop2018}).
%
This is in sharp contrast to available alternatives:
%
%
Takaoka~\cite{Takaoka1998,Takaoka2009} and Barbay and Navarro~\cite{BarbayNavarro2013}
independently discovered a sorting method that adapts to the 
\emph{entropy of the distribution of run lengths:}
they sort an input consisting of $r$ runs with respective lengths $L_1,\ldots,L_r$
in time $O\bigl( (\mathcal H(\frac{L_1}n,\ldots,\frac{L_r}n) +1) n \bigr) \subseteq O(n + n\lg r)$,
where $\mathcal H(p_1,\ldots,p_r) = \sum_{i=1}^r p_i \lg(1/p_i)$ is the binary Shannon entropy.
Since $\mathcal H(\frac{L_1}n,\ldots,\frac{L_r}n) n - \Oh(n)$ comparisons are 
necessary for distinct keys, this is optimal up to linear terms.
%
%
%
%
Their algorithms are also conceptually simple:
find runs in a linear scan, determine an optimal merging order
using a Huffman tree of the run lengths, and execute those merges bottom-up in the tree.
We will refer to this algorithm to determine an optimal merging order 
as \emph{Huffman-Merge}.

Straight-forward implementations of Huffman-Merge
add significant overhead in terms of time and 
%
%
%
	space;
	(finding the Huffman tree requires storing and sorting the run lengths).
	This 
%
renders these methods uncompetitive to
(reasonable implementations of) elementary sorting methods.
%
%
Moreover, Huffman-Merge leads to an \emph{unstable} sorting method
since it merges non-adjacent runs.
The main motivation for the invention of Timsort was 
to find a fast general-purpose sorting method that is \emph{stable}~\cite{Peters2002mailinglist},
and the Java library even dictates the sorting method used for objects to be stable.
We remark that while stability is a much desired feature, 
practical, stable sorting methods do not try to \emph{exploit} the presence of 
duplicate elements to speed up sorting,
and we will focus on the performance for distinct keys in this article.

It is conceptually easy to modify the idea of Takaoka resp.\ Barbay-Navarro 
to sort stably: 
replace the Huffman tree by
an \emph{optimal binary search tree} 
and otherwise proceed 
%
%
%
	as before (using a stable merging procedure).
%
Since we only have weights at the leaves of the tree, 
%
we can compute this tree in $O(n + r\log r)$ time using the Hu-Tucker- or Garsia-Wachs-algorithm,
but $r$ can be $\Theta(n)$ and the algorithms are fairly sophisticated,
so this idea seems not very appealing for practical use.

In this paper,
we present two new natural mergesort variants that have the same optimal asymptotic 
running time $O\bigl( (\mathcal H(\frac{L_1}n,\ldots,\frac{L_r}n) +1) n \bigr)$ as Huffman-merge,
but incur much less overhead.
For that, we build upon classic algorithms for computing \emph{nearly-optimal binary search trees}~
\cite{Mehlhorn1984};
but the vital twist for practical methods is to neither explicitly store
the full tree, nor the lengths of all runs at any point in time.
In particular~-- much like Timsort~-- we only store a \emph{logarithmic} number of runs
at any point in time 
(in fact reducing their number from roughly $\log_\varphi \approx1.44 \lg n$ in Timsort to $\lg n$), 
but~-- much \emph{un}like Timsort~-- we retain the guarantee of an optimal merging order up to linear terms.
Our methods require at most $n \lg n + O(n)$ comparison in the worst case
and $\mathcal H(\frac{L_1}n,\ldots,\frac{L_r}n)n +3n$ for an input with runs of lengths $L_1,\ldots, L_r$.

We demonstrate in a running-time study that our methods achieve guaranteed 
(leading-term) optimal adaptive sorting in practice with negligible overhead 
to compute the merge order: 
unlike Timsort, our methods are \emph{not} slower than standard mergesort 
when no existing runs can be exploited.
If existing runs are present, mergesort and quicksort are outperformed by far.
Finally, we show that Timsort is slower than standard mergesort and our new methods
on certain inputs that do have existing runs, but whose lengths pattern hits a weak point
of Timsort's heuristic merging-order rule.




\smallskip\noindent
\textbf{\textsf{Outline:}} The rest of this paper is organized as follows.
In the remainder of this section we survey related work.
\wref{sec:preliminaries} contains notation and 
known results on optimal binary search trees that our work builds on.
The new algorithms and their analytical guarantees are presented in \wref{sec:algorithms}.
\wref{sec:experiments} reports on our running-time study, comparing
the the new methods to state-of-the-art sorting methods.
Finally, \wref{sec:conclusion} summarizes our findings.

\subsection{Adaptive Sorting}
\label{sec:related}

The idea to exploit existing ``structure'' in the input to speed up sorting
dates (at least) back to methods from the 1970s~\cite{Mehlhorn1979} that 
sort faster when the number of inversions is small.
A systematic treatment of this and many further measures of presortedness
(\eg, the number of inversions, the number of runs, and the number of shuffled up-sequences), 
their relation and how to sort \emph{adaptively} \wrt these measures 
are discussed by Estivill-Castro and Wood~\cite{EstivillCastroWood1992}.
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
While the focus of earlier works is mostly on combinatorial properties of permutations, 
a more recent trend is to consider more fine-grained statistical quantities.
For example, the above mentioned Huffman-Merge adapts to the
\emph{entropy} of the vector of run lengths~\cite{Takaoka1998,Takaoka2009,BarbayNavarro2013}.
%
%
%
%
%
Other similar measures are the entropy of the lengths of shuffled up-sequences~\cite{BarbayNavarro2013}
%
%
%
%
%
and the entropy of lengths of an LRM-partition~\cite{BarbayFischerNavarro2012}, 
a novel measure that lies between runs and shuffled up-sequences.

For multiset sorting, the fine-grained measure, the \emph{entropy of the multiplicities}, 
has been considered instead of the \emph{number} of unique values already in early work 
in the field (\eg~\cite{MunroSpira1976,Sedgewick1977equals}).
%
A more recent endeavor has been to find sorting methods that optimally adapt to
\emph{both presortedness and repeated values}.
Barbay, Ochoa, and Satti refer to this as \emph{synergistic sorting}~\cite{BarbayOchoaSatti2017}
and present an algorithm based on quicksort that is optimal up to a constant factor.
The method's practical performance is unclear.

%
%
%
%
%

We remark that (unstable) multiset sorting is the \emph{only} problem from the above list
for which a theoretically optimal algorithm has found wide-spread adoption in
programming libraries:
quicksort is known to almost optimally adapt to the entropy of multiplicities 
on average~\cite{Wegner1985,SedgewickBentley2002,Wild2018},
when elements equal to the pivot are excluded from recursive calls (fat-pivot partitioning).
Supposedly, sorting is so fast to start with that further improvements from
exploiting specific input characteristics are only fruitful if they can be realized
with minimal additional overhead.
Indeed, for algorithms that adapt to the number of inversions, 
Elmasry and Hammad~\cite{ElmasryHammad2009} found that 
the adaptive methods could only compete with good implementations of elementary sorting algorithms 
in terms of running time
for inputs with extremely few inversions (fewer than 1.5\%).
Translating the theoretical guarantees of adaptive sorting
into practical, efficient methods is an ongoing challenge.



\subsection{Lower bound}
\label{sec:lower-bound}

How much does it help for sorting an array $A[1..n]$ to
know that it contains $r$ runs of respective sizes $L_1,\ldots,L_r$,
\ie, to know the relative order of $A[1..L_1]$, $A[L_1+1..L_1+L_2]$ etc.?
If we assume distinct elements, a simple counting argument shows that there
are $\binom n{L_1,\ldots,L_r}$ permutations that are compatible with this setup.
(the number of ways to partition $n$ keys into $r$ subsets of given sizes.)
We thus need $\lg(n!) - \sum_{i=1}^r \lg (L_i!) = \mathcal H(\frac{L_1}n,\ldots,\frac{L_r}n) n - O(n)$ 
comparisons to sort such an input.
A formal argument for this lower bound is given by Barbay and Navarro~\cite{BarbayNavarro2013} 
in the proof of their Theorem~2. 
%
%
%
%
%


\subsection{Results on Timsort and stack-based mergesort}

Its good performance in running-time studies especially on partially sorted inputs
have lead to the adoption of Timsort in several programming libraries,
but as mentioned in the introduction, 
the complexity of the algorithm has precluded researchers from proving 
interesting adaptivity guarantees.
To make progress towards these, simplified variations of Timsort have been 
considered~\cite{AugerNicaudPivoteau2015,BussKnop2018}.
All of those methods work by maintaining a stack of runs yet to be merged
and proceed as follows: 
They find the next run in the input and push it onto the stack.
Then they consider the top $k$ elements on the stack (for $k$ a small constant like 3 or 4)
and decide based on these if any pair of them is to be merged.
If so, the two runs in the stack are replaced with the merged result and the rule is applied
repeatedly until the stack satisfies some invariant.
The invariant is chosen so as to keep the height of the stack small (logarithmic in $n$).

The simplest version, ``$\alpha$-stack sort''~\cite{AugerNicaudPivoteau2015}, 
merges the topmost two runs until the run lengths in the stack grow at least by a factor of $\alpha$,
(\eg, $\alpha=2$).
This method can lead to imbalanced merges (and hence runtime $\omega(n\log r)$~\cite{BussKnop2018};
the authors of~\cite{AugerNicaudPivoteau2015} also point this out in their conclusion):
if the next run is much larger than what is on the stack, a much more balanced
merging order results from first merging stack elements until they are at least as big
as the new run.
This modification is called ``$\alpha$-merge sort'', which achieves a worst-case 
guarantee of $O(n + n \log r)$, but the constant is provably not optimal~\cite{BussKnop2018}
(for any $\alpha>1$).

Timsort is quite similar to $\alpha$-merge sort for $\alpha=\varphi$ (the golden ratio)
by forcing the run lengths to grow at least like Fibonacci numbers.
The details of the rule are given in~\cite{AugerNicaudPivoteau2015} or~\cite{BussKnop2018} and
are quite intricate~-- 
and were indeed wrong in the first (widely-used) version of Timsort 
(see \wtpref{fn:wrong-timsort}).
While it is open if Timsort always runs in $O(n+n \log r)$ time,
Buss and Knop gave a family of inputs for which Timsort does asymptotically at least $1.5$ times
the required effort (in terms of merge costs, see \wref{sec:merge-costs}), and hence
proved that Timsort~-- like $\alpha$-merge sort~-- is \emph{not} optimally adaptive
even to the \emph{number} of runs $r$, not to speak of the entropy of the run lengths.

%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%



\section{Preliminaries}
\label{sec:preliminaries}

%
%

We implicitly assume that we are sorting an array $A[1..n]$ of $n$ elements.
By $\mathcal H$, we denote the binary Shannon entropy, \ie,
for $p_1,\ldots,p_m\in[0,1]$ with $p_1+\cdots+p_m = 1$ we let
$\mathcal H(p_1,\ldots,p_m) = \sum p_i \lg(1/p_i)$, where
$\lg=\log_2$.


We will always let $r$ denote the \emph{number of runs} in the input and 
$L_1,\ldots,L_r$ their respective lengths with $L_1+\cdots+L_r = n$.
In the literature, a \emph{run} usually means a maximal (contiguous) 
%
%
%
	weakly increasing%
	\footnote{%
		We use ``weakly increasing'' to mean ``nondecreasing''.\\
		\textsmaller[2]{(Isn't it better to say what we mean instead of not saying what we don't mean?)}
	}
	region,
%
but we adopt the convention from Timsort in this paper: a run is either
a maximal \emph{weakly increasing} region \emph{or} a \emph{strictly decreasing} region.
Decreasing runs are immediately reversed;
allowing only strict decreasing runs makes their \emph{stable} reversal trivial.
The algorithms are not directly affected by different conventions for what a ``run'' is;
they only rely on a unique partition of the input into sorted segments that can be found
by sequential scans.

\subsection{Nearly-Optimal Binary Search Trees}
\label{sec:optimal-BSTs}

In the \emph{optimal binary search tree problem,} 
we are given probabilities $\beta_1,\ldots,\beta_m$ to access the $m$
keys $K_1<\cdots<K_m$ (internal nodes) and probabilities
$\alpha_0,\ldots,\alpha_m$ to access the gaps (leaves) between these keys 
(setting $K_0 = -\infty$ and $K_{m+1}=+\infty$)
and we are interested in the binary search tree that minimizes the expected
search cost $C$, \ie, 
the expected number of (ternary) comparisons when access follow the given distribution.%
\footnote{%
	We deviate from the literature convention and use $m$ to denote the number of 
	keys to avoid confusion with $n$, the length of the arrays to sort,
	in the rest of the paper.
}
Nagaraj~\cite{Nagaraj1997} surveys various versions of the problem.
We confine ourselves to \emph{approximation algorithms} here.
Moreover, we only need the special case of \emph{alphabetic trees}
where all $\beta_j = 0$.

The following methods apply to the general problem, but we 
present them for the case of \emph{nearly-optimal alphabetic trees.}
So in the following let $\alpha_0,\ldots,\alpha_m$ with $\sum_{i=0}^m \alpha_i = 1$ be given.
If the details are done right, a greedy top-down approach produces provably good 
search trees~\cite{Bayer1975,Mehlhorn1977}:
choose the boundary closest to $\frac12$ as the bisection at the root 
(\emph{``weight-balancing heuristic''}).
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
Mehlhorn~\cite[\S III.4.2]{Mehlhorn1984} discusses two algorithms for 
nearly-optimal binary search trees that follow this scheme:
``Method 1'' is the straight-forward recursive application of the above rule,
whereas ``Method 2'' (\emph{``bisection heuristic''}) 
continues by strictly halving the \emph{original} interval in the recursive calls;
see \wref{fig:nearly-optimal-trees}.

\begin{figure}[tbph]
	\begin{center}
	\begin{tikzpicture}[
			yscale=.27,
			xscale=.2,
			every node/.style={inner sep=2pt,font=\footnotesize},
			tree/.style={
				circle,draw,black!50,thick,minimum size=6pt,inner sep=0pt,fill=black!10,fill opacity=.5
			},
			orgrun/.style={pattern=crosshatch dots,pattern color=black!30},
	]
		\def\delta{0.2}
		\def\intskip{2}
		\useasboundingbox (-.5,4) rectangle (67.5,-11);
		
		\draw[fill=black!10] (0,0) coordinate (l)
			\foreach \d/\i in {5,3,3,14,1,2} {
				rectangle 
				++(\d,1) ++(\delta,-1) 
			}
			++(-\delta,0) coordinate (r)
		;
		\coordinate (m) at ($(l)!.5!(r)$);
		\draw[|-|] ($(l)+(0,\intskip)$) node[above=3pt] {0} -- ($(m)+(0,\intskip)$) node[above=3pt] {\nicefrac12} ;
		\draw[|-|] ($(r)+(0,\intskip)$) node[above=3pt] {1} -- ($(m)+(0,\intskip)$) ;
		\node[tree] (n0) at (5+3+3+2.5*\delta,.5) {};
		\draw[thick,densely dotted] (m) ++ (0,-.1) -- ++(0,2) ;
		\draw[->] ($(l)!.5!(r)$) ++(0,.5) -- (n0) ;
		
		\begin{scope}[shift={(-.25,-5)}]
			\draw[fill=black!10] (0,0) coordinate (l)
				\foreach \d/\i in {5,3,3} {
					rectangle 
					++(\d,1) ++(\delta,-1) 
				}
				++(-\delta,0) coordinate (r)
			;
			\coordinate (m) at ($(l)!.5!(r)$);
			\draw[|-|] ($(l)+(0,\intskip)$) node[above=3pt] {0} -- ($(m)+(0,\intskip)$) node[above=3pt] {\nicefrac12} ;
			\draw[|-|] ($(r)+(0,\intskip)$) node[above=3pt] {1~~} -- ($(m)+(0,\intskip)$) ;
			\node[tree] (n00) at (5+0.5*\delta,.5) {};
			\draw[thick,densely dotted] (m) ++ (0,-.1) -- ++(0,2) ;
			\draw[->] ($(l)!.5!(r)$) ++(0,.5) -- (n00) ;
			\draw[black!50,very thick,opacity=.5] (n0) to (n00);
		\end{scope}
		
		\begin{scope}[shift={(-.5,-8)}]
			\draw[fill=black!10,orgrun] (0,0) coordinate (l)
				\foreach \d/\i in {5} {
					rectangle node (n000) {}
					++(\d,1) ++(\delta,-1) 
				}
				++(-\delta,0) coordinate (r)
			;
			\draw[black!50,very thick,opacity=.5] (n00) to (n000);
		\end{scope}
		
		\begin{scope}[shift={(-.5+.5+5,-8)}]
			\draw[fill=black!10] (0,0) coordinate (l)
				\foreach \d/\i in {3,3} {
					rectangle node (n000) {}
					++(\d,1) ++(\delta,-1) 
				}
				++(-\delta,0) coordinate (r)
			;
			\node[tree] (n001) at (3+0.5*\delta,.5) {};
			\draw[black!50,very thick,opacity=.5] (n00) to (n001);
		\end{scope}
		\begin{scope}[shift={(+5,-12)}]
			\draw[fill=black!10,orgrun] (0,0) coordinate (l)
				\foreach \d/\i in {3} {
					rectangle node (n0010) {}
					++(\d,1) ++(\delta,-1) 
				}
				++(-\delta,0) coordinate (r)
			;
			\draw[black!50,very thick,opacity=.5] (n001) to (n0010);
		\end{scope}
		\begin{scope}[shift={(+5+3+\delta,-12)}]
			\draw[fill=black!10,orgrun] (0,0) coordinate (l)
				\foreach \d/\i in {3} {
					rectangle node (n0011) {}
					++(\d,1) ++(\delta,-1) 
				}
				++(-\delta,0) coordinate (r)
			;
			\draw[black!50,very thick,opacity=.5] (n001) to (n0011);
		\end{scope}
	
		\begin{scope}[shift={(0.25+5+3+3+3.5*\delta,-5)}]
			\draw[fill=black!10] (0,0) coordinate (l)
				\foreach \d/\i in {14,1,2} {
					rectangle 
					++(\d,1) ++(\delta,-1) 
				}
				++(-\delta,0) coordinate (r)
			;
			\coordinate (m) at ($(l)!.5!(r)$);
			\draw[|-|] ($(l)+(0,\intskip)$) node[above=3pt] {~0} -- ($(m)+(0,\intskip)$) node[above=3pt] {\nicefrac12} ;
			\draw[|-|] ($(r)+(0,\intskip)$) node[above=3pt] {1} -- ($(m)+(0,\intskip)$) ;
			\node[tree] (n01) at (14+0.5*\delta,.5) {};
			\draw[thick,densely dotted] (m) ++ (0,-.1) -- ++(0,2) ;
			\draw[->] ($(l)!.5!(r)$) ++(0,.5) -- (n01) ;
			\draw[black!50,very thick,opacity=.5] (n0) to (n01);
		\end{scope}	
		
		\begin{scope}[shift={(.125+5+3+3+3.5*\delta,-8)}]
			\draw[fill=black!10,orgrun] (0,0) coordinate (l)
				\foreach \d/\i in {14} {
					rectangle node (n010) {}
					++(\d,1) ++(\delta,-1) 
				}
				++(-\delta,0) coordinate (r)
			;
			\draw[black!50,very thick,opacity=.5] (n01) to (n010);
		\end{scope}
		
		\begin{scope}[shift={(.25+5+3+3+14+5.5*\delta,-8)}]
			\draw[fill=black!10] (0,0) coordinate (l)
				\foreach \d/\i in {1,2} {
					rectangle node (n000) {}
					++(\d,1) ++(\delta,-1) 
				}
				++(-\delta,0) coordinate (r)
			;
			\node[tree] (n011) at (1+0.5*\delta,.5) {};
			\draw[black!50,very thick,opacity=.5] (n01) to (n011);
		\end{scope}
		\begin{scope}[shift={(5+3+3+14+6.5*\delta,-12)}]
			\draw[fill=black!10,orgrun] (0,0) coordinate (l)
				\foreach \d/\i in {1} {
					rectangle node (n0110) {}
					++(\d,1) ++(\delta,-1) 
				}
				++(-\delta,0) coordinate (r)
			;
			\draw[black!50,very thick,opacity=.5] (n011) to (n0110);
		\end{scope}
		\begin{scope}[shift={(5+3+3+14+1+7.5*\delta,-12)}]
			\draw[fill=black!10,orgrun] (0,0) coordinate (l)
				\foreach \d/\i in {2} {
					rectangle node (n0111) {}
					++(\d,1) ++(\delta,-1) 
				}
				++(-\delta,0) coordinate (r)
			;
			\draw[black!50,very thick,opacity=.5] (n011) to (n0111);
		\end{scope}
	
	
	
	
	\begin{scope}[shift={(38,0)}]
	
		\draw[fill=black!10] (0,0) coordinate (l)
			\foreach \d/\i in {5,3,3,14,1,2} {
				rectangle 
				++(\d,1) ++(\delta,-1) 
			}
			++(-\delta,0) coordinate (r)
		;
		\coordinate (m) at ($(l)!.5!(r)$);
		\coordinate (MM) at ($(l)!.5!(r)$);
		\draw[|-|] ($(l)+(0,\intskip)$) node[above=3pt] {0} -- ($(m)+(0,\intskip)$) node[above=3pt] {\nicefrac12} ;
		\draw[|-|] ($(r)+(0,\intskip)$) node[above=3pt] {1} -- ($(m)+(0,\intskip)$) ;
		\node[tree] (n0) at (5+3+3+2.5*\delta,.5) {}; 
		\draw[thick,densely dotted] (m) ++ (0,-.1) -- ++(0,2) ;
		\draw[->] ($(l)!.5!(r)$) ++(0,.5) -- (n0) ;
		
		\begin{scope}[shift={(-.1,-5)}]
			\draw[fill=black!10] (0,0) coordinate (l)
				\foreach \d/\i in {5,3,3} {
					rectangle 
					++(\d,1) ++(\delta,-1) 
				}
				++(-\delta,0) coordinate (rr)
			;
			\coordinate (r) at ($(MM)+(-.1,-5)$) ;
			\coordinate (m) at ($(l)!.5!(r)$);
			\draw[|-|] ($(l)+(0,\intskip)$) node[above=3pt] {0} -- ($(m)+(0,\intskip)$) node[above=3pt] {\nicefrac14} ;
			\draw[|-|] ($(r)+(0,\intskip)$) node[above=3pt] {\nicefrac12} -- ($(m)+(0,\intskip)$) ;
			\node[tree] (n00) at (5+3+1.5*\delta,.5) {};
			\draw[thick,densely dotted] (m) ++ (0,-.1) -- ++(0,2) ;
			\draw[->] ($(l)!.5!(r)$) ++(0,.5) -- (n00) ;
			\draw[black!50,very thick,opacity=.5] (n0) to (n00);
		\end{scope}
		
		\begin{scope}[shift={(-.2,-8)}]
			\draw[fill=black!10] (0,0) coordinate (l)
				\foreach \d/\i in {5,3} {
					rectangle 
					++(\d,1) ++(\delta,-1) 
				}
				++(-\delta,0) coordinate (r)
			;
			\node[tree] (n000) at (5+0.5*\delta,.5) {};
			\draw[black!50,very thick,opacity=.5] (n00) to (n000);
		\end{scope}
		
		\begin{scope}[shift={(-.25+.5+5+3,-8)}]
			\draw[fill=black!10,orgrun] (0,0) coordinate (l)
				\foreach \d/\i in {3} {
					rectangle node (n001) {}
					++(\d,1) ++(\delta,-1) 
				}
				++(-\delta,0) coordinate (r)
			;
			\draw[black!50,very thick,opacity=.5] (n00) to (n001);
		\end{scope}
		\begin{scope}[shift={(0,-12)}]
			\draw[fill=black!10,orgrun] (0,0) coordinate (l)
				\foreach \d/\i in {5} {
					rectangle node (n0010) {}
					++(\d,1) ++(\delta,-1) 
				}
				++(-\delta,0) coordinate (r)
			;
			\draw[black!50,very thick,opacity=.5] (n000) to (n0010);
		\end{scope}
		\begin{scope}[shift={(+5+\delta,-12)}]
			\draw[fill=black!10,orgrun] (0,0) coordinate (l)
				\foreach \d/\i in {3} {
					rectangle node (n0011) {}
					++(\d,1) ++(\delta,-1) 
				}
				++(-\delta,0) coordinate (r)
			;
			\draw[black!50,very thick,opacity=.5] (n000) to (n0011);
		\end{scope}
	
		\begin{scope}[shift={(0.1+5+4+2+3.5*\delta,-5)}]
	%
			\draw[fill=black!10] (0,0) coordinate (ll)
				\foreach \d/\i in {14,1,2} {
					rectangle 
					++(\d,1) ++(\delta,-1) 
				}
				++(-\delta,0) coordinate (r)
			;
			\coordinate (l) at ($(MM)+(.1,-5)$);
			\coordinate (m) at ($(l)!.5!(r)$);
			\draw[|-|] ($(l)+(0,\intskip)$) node[above=3pt] {} -- ($(m)+(0,\intskip)$) node[above=3pt] {\nicefrac34} ;
			\draw[|-|] ($(r)+(0,\intskip)$) node[above=3pt] {1} -- ($(m)+(0,\intskip)$) ;
			\node[tree] (n01) at (14+0.5*\delta,.5) {};
			\draw[thick,densely dotted] (m) ++ (0,-.1) -- ++(0,2) ;
			\draw[->] ($(l)!.5!(r)$) ++(0,.5) -- (n01) ;
			\draw[black!50,very thick,opacity=.5] (n0) to (n01);
		\end{scope}	
		
		\begin{scope}[shift={(.125+5+4+2+3.5*\delta,-8)}]
			\draw[fill=black!10,orgrun] (0,0) coordinate (l)
				\foreach \d/\i in {14} {
					rectangle node (n010) {}
					++(\d,1) ++(\delta,-1) 
				}
				++(-\delta,0) coordinate (r)
			;
			\draw[black!50,very thick,opacity=.5] (n01) to (n010);
		\end{scope}
		
		\begin{scope}[shift={(.25+5+4+2+14+5.5*\delta,-8)}]
			\draw[fill=black!10] (0,0) coordinate (l)
				\foreach \d/\i in {1,2} {
					rectangle node (n0111) {}
					++(\d,1) ++(\delta,-1) 
				}
				++(-\delta,0) coordinate (r)
			;
			\node[tree] (n011) at (1+0.5*\delta,.5) {};
			\draw[black!50,very thick,opacity=.5] (n01) to (n011);
		\end{scope}
		\begin{scope}[shift={(5+4+2+14+6.5*\delta,-12)}]
			\draw[fill=black!10,orgrun] (0,0) coordinate (l)
				\foreach \d/\i in {1} {
					rectangle node (n0110) {}
					++(\d,1) ++(\delta,-1) 
				}
				++(-\delta,0) coordinate (r)
			;
			\draw[black!50,very thick,opacity=.5] (n011) to (n0110);
		\end{scope}
		\begin{scope}[shift={(5+4+2+14+1+7.5*\delta,-12)}]
			\draw[fill=black!10,orgrun] (0,0) coordinate (l)
				\foreach \d/\i in {2} {
					rectangle node (n0111) {}
					++(\d,1) ++(\delta,-1) 
				}
				++(-\delta,0) coordinate (r)
			;
			\draw[black!50,very thick,opacity=.5] (n011) to (n0111);
		\end{scope}
		
		\node[yshift=-2ex,xshift=-.2ex] at (n000) {\smaller[2]\itshape 3};
		\node[yshift=-2ex,xshift=-.2ex] at (n00)  {\smaller[2]\itshape 2};
		\node[yshift=-2ex] at (n0)   {\smaller[2]\itshape 1};
		\node[yshift=-2ex,xshift=-.2ex] at (n01)  {\smaller[2]\itshape 2};
		\node[yshift=-2ex,xshift=.8ex] at (n011) {\smaller[2]\itshape 4};
		
	\end{scope}
	
	\end{tikzpicture}
	\end{center}
	\caption{%
		The two versions of weight-balancing for computing nearly-optimal alphabetic trees.
		The gap probabilities are proportional to $5,3,3,14,1,2$.
%
		\textbf{\sffamily Left:} Mehlhorn's ``Method~1'' chooses the split closest to the midpoint of the 
		subtree's actual weights ($\nicefrac12$ after renormalizing).
		\textbf{\sffamily Right:} ``Method~2'' continues to cut the original interval in half, irrespective
		of the total weight of the subtrees.
		The italic numbers are the powers of the nodes (see \wtpref{def:node-power}).
		%
	}
\label{fig:nearly-optimal-trees}
\end{figure}

Method~1 was proposed in~\cite{WalkerGotlieb1972} and analyzed in \cite{Mehlhorn1975,Bayer1975};
Method~2 is discussed in~\cite{Mehlhorn1977}.
While Method~1 is arguably more natural, Method~2 has the advantage to yield
splits that are predictable without going through all steps of the recursion.
Both methods can be implemented to run in time $O(m)$ and yield very good trees.
(Recall that in the case $\beta_j = 0$ the classic information-theoretic 
argument dictates $C\ge \mathcal H$; 
Bayer~\cite{Bayer1975} gives lower bounds in the general case.)


\begin{theorem}[Nearly-Optimal BSTs]
\label{thm:nearly-opt-trees}
	Let $\alpha_0,\beta_1,\alpha_1,\ldots,\beta_m,\alpha_m \in [0,1]$ with $\sum \alpha_i+\sum\beta_j = 1$ 
	be given and let $\mathcal H = \sum_{i=0}^m \alpha_i \lg(1/\alpha_i) + \sum_{j=1}^m \beta_j \lg(1/\beta_j)$.
\begin{enumerate}[(i)]
\item 
	Method~1 yields a tree with search cost 
	$C \le \mathcal H + 2$.
	\cite[Thm\,4.8]{Bayer1975}
\item 
	If all $\beta_j = 0$,
	Method~1 yields a tree with search cost 
	$C \le \mathcal H + 2 - (m+3) \alpha_{\mathrm{min}}$,\\
	where $\alpha_{\mathrm{min}} = \min \{\alpha_0,\ldots,\alpha_m\}$.
	\cite{Horibe1977}
\item 
	Method~2 yields a tree with search cost
	$C \le \mathcal H + 1 + \sum \alpha_i$.
	\cite{Mehlhorn1977}
\end{enumerate}
\end{theorem}


\subsection{Merge Costs}
\label{sec:merge-costs}

%
%
%
%

In this paper, we are primarily concerned with finding a good \emph{order} of binary merges 
for the existing runs in the input.
Following \cite{AugerNicaudPivoteau2015} and \cite{BussKnop2018},
we will define the \emph{merge cost} $M$ for merging two runs of lengths $m$ resp.\ $n$ 
as $M = m+n$, \ie, the size of the result.
This quantity has been studied earlier by 
Golin and Sedgewick~\cite{GolinSedgewick1993} without giving it a name.

Merge costs abstract away from key comparisons and element moves
and simplify computations (see next subsection).
Since any merge has to move most elements (except for rare lucky cases),
and the average number of comparisons using standard merge routines
is $m+n - \bigl(\frac m{n+1} + \frac{n}{m+1}\bigr)$, 
merge costs are a reasonable approximation,
in particular when $m$ and $n$ are roughly equal.
They always yield an upper bound for both the number of comparisons and moves.

%
%
%
%
%
%
%





\subsection{Merge Trees}
\label{sec:merge-trees}

Let $L_1,\ldots, L_r$ with $\sum L_i = n$ be the lengths of the runs in the input.
Any natural mergesort can be described as a rule to select some of the remaining runs, 
which are then merged and replaced by the merge result. If we always merge \emph{two} runs
this corresponds to a binary tree with the original runs at leaves $\leafnode1,\ldots,\leafnode{\like1r}$. 
Internal nodes correspond to the result of merging their children.
If we assign to internal node \internalnode j
the size $M_j$ of the (intermediate) merge result it represents,
then the overall merge cost is exactly $M = \sum_{\internalnodescript j} M_j$ (summing
over all internal nodes).
\wref{fig:nearly-optimal-trees} shows two examples of merge trees;
the merge costs are given by adding up all gray areas,%
\footnote{%
	The left tree is obviously better here
	and this is a typical outcome.
	But there are also inputs where Method 2 yields a 
	better tree than Method 1.
}
(ignoring the dotted leaves).

Let $d_i$ be the \emph{depth} of leaf \leafnode i (corresponding to the run of length $L_i$),
where depth is the number of edges on the path to the root.
Every element in the $i$th run is counted exactly $d_i$ times in $\sum_{\internalnodescript j} M_j$,
so we have $M = \sum_{i=1}^r d_i\cdot L_i$.
Dividing by $n$ yields $M/n = \sum_{i=1}^r d_i\cdot \alpha_i$ for $\alpha_i = L_i / n$, 
which happens to be the expected search time $C$ in the merge tree
if \leafnode i is requested with probability $\alpha_i$ for $i=1,\ldots,r$.
So the minimal-cost merge tree for given run lengths $L_1,\ldots,L_r$ 
is the optimal alphabetic tree for leaf probabilities $\frac{L_1}n,\ldots,\frac{L_r}n$
and it holds
$M \ge \mathcal H(\frac{L_1}n,\ldots,\frac{L_m}n) n$.
For distinct keys, the lower bound on comparisons (\wref{sec:lower-bound}) 
coincides up to linear terms 
with this lower bound.


Combining this fact with the linear-time methods for nearly-optimal binary search trees 
from \wref{sec:optimal-BSTs}
immediately gives a stable sorting method that adapts optimally to existing runs
up to an $O(n)$ term.
We call such a method a \emph{nearly-optimal (natural) mergesort}.
%
	A~direct implementation of this idea needs $\Theta(r)$ space to store $L_1,\ldots,L_r$ 
	and the merge tree and does an extraneous pass over the data 
	just to determine the run lengths.
	The purpose of this paper is to show that we can make the overhead 
	for finding a nearly-optimal merging order
	negligible in time and space.
%



\section{Nearly-Optimal Merging Orders}
\label{sec:algorithms}

We now describe two sorting methods that simulate nearly-optimal search tree algorithms
to compute nearly-optimal merging orders, but do so without ever storing the full merge tree 
or even the run lengths.

\subsection{Peeksort: A Simple Top-Down Method}

The first method is similar to standard top-down mergesort
in that it implicitly constructs a merge tree on the call stack.
Instead of blindly cutting the input in half, however, we mimic Mehlhorn's Method~1.
For that we need the \emph{run boundary closest to the middle} of the input: 
this will become the root of the merge tree.
Since we want to detect existing runs anyway at some point, 
we start by finding the run that contains the middle position.
The end point closer to the middle determines the top-level cut and 
we recursively sort the parts left and right of it.
A final merge completes the sort.

To avoid redundant scans, we pass on the information about already detected runs.
In the general case, we are sorting a range $A[\ell..r]$ whose 
prefix $A[\ell..e]$ and suffix $A[s..r]$ are runs.
Depending on whether the middle is contained in one of those runs, 
we have one of four different cases; apart from that
the overall procedure (\wref{alg:peeksort}) is quite straight-forward.

\begin{algorithm}[tbph]
\vspace*{-2ex}
\small
\tikzset{every node/.style={font=\scriptsize}}
	\begin{codebox}
		\Procname{$\proc{PeekSort}(A[\ell..r],e,s)$}
		\li \kw{if} $e \isequal r $ or $s \isequal \ell$ \kw{then} \Return
		\li $m \gets \ell + \bigl\lfloor \frac{r-\ell}2 \bigr\rfloor$
		\li \If $m \le e$   
			\>\>\>\>\>\Comment
			\begin{tikzpicture}[scale=.35,baseline=.3ex]
				\draw[fill=black!10] (0,0) rectangle (13,1) ;
				\draw[fill=black!10] (18,0) rectangle (20,1) ;
				\draw (0,0) rectangle (10,1) rectangle (20,0) ;
				\node[anchor=base] at ( 0.5,.25) {$\ell$} ;
				\node[anchor=base] at (19.5,.25) {$r$} ;
				\node[anchor=base] at (12.5,.25) {$e$} ;
				\node[anchor=base] at (18.5,.25) {$s$} ;
				\node[overlay] at (10,1.3) {$m$} ;
			\end{tikzpicture}
		\Then
			\li $\proc{PeekSort}(A[e+1..r],e+1,s)$
			\li $\proc{Merge}(A[\ell..e],A[e+1..r])$
		\li \Else \kw{if} $m \ge s$
			\>\>\>\>\Comment
			\begin{tikzpicture}[scale=.35,baseline=.3ex]
				\draw[fill=black!10] (0,0) rectangle (3,1) ;
				\draw[fill=black!10] (8,0) rectangle (20,1) ;
				\draw (0,0) rectangle (10,1) rectangle (20,0) ;
				\node[anchor=base] at ( 0.5,.25) {$\ell$} ;
				\node[anchor=base] at (19.5,.25) {$r$} ;
				\node[anchor=base] at (2.5,.25) {$e$} ;
				\node[anchor=base] at (8.5,.25) {$s$} ;
				\node[overlay] at (10,1.3) {$m$} ;
			\end{tikzpicture}
			\li $\proc{PeekSort}(A[\ell..s-1], e,s-1)$
			\li $\proc{Merge}(A[\ell..s-1],A[s..r])$
		\li \Else
			\zi \Comment Find existing run $A[i..j]$ containing position $m$
			\li $i \gets \proc{ExtendRunLeft}(A[m], \ell)$; \quad
				$j \gets \proc{ExtendRunRight}(A[m], r)$
			\li \kw{if} $i\isequal \ell$ and $j\isequal r$ \Return
			\li \If $m - i < j - m$
			\>\>\>\>\Comment
			\begin{tikzpicture}[scale=.35,baseline=.3ex]
				\draw[fill=black!10] (0,0) rectangle (3,1) ;
				\draw[fill=black!10] (18,0) rectangle (20,1) ;
				\draw[fill=black!10] (8.5,0) rectangle (16,1) ;
				\draw (0,0) rectangle (10,1) rectangle (20,0) ;
				\node[anchor=base] at ( 0.5,.25) {$\ell$} ;
				\node[anchor=base] at (19.5,.25) {$r$} ;
				\node[anchor=base] at (2.5,.25) {$e$} ;
				\node[anchor=base] at (18.5,.25) {$s$} ;
				\node[anchor=base] at (9.0,.25) {$i$} ;
				\node[anchor=base] at (15.5,.25) {$j$} ;
				\node[overlay] at (10,1.3) {$m$} ;
			\end{tikzpicture}
			\Then
				\li $\proc{PeekSort}(A[\ell..i-1],e,i-1)$
				\li $\proc{PeekSort}(A[i..r],j,s)$
				\li $\proc{Merge}(A[\ell..i-1],A[i..r])$
			\li \Else
			\>\>\>\Comment
			\begin{tikzpicture}[scale=.35,baseline=.3ex]
				\draw[fill=black!10] (0,0) rectangle (3,1) ;
				\draw[fill=black!10] (18,0) rectangle (20,1) ;
				\draw[fill=black!10] (5.5,0) rectangle (12,1) ;
				\draw (0,0) rectangle (10,1) rectangle (20,0) ;
				\node[anchor=base] at ( 0.5,.25) {$\ell$} ;
				\node[anchor=base] at (19.5,.25) {$r$} ;
				\node[anchor=base] at (2.5,.25) {$e$} ;
				\node[anchor=base] at (18.5,.25) {$s$} ;
				\node[anchor=base] at (6,.25) {$i$} ;
				\node[anchor=base] at (11.5,.25) {$j$} ;
				\node[overlay] at (10,1.3) {$m$} ;
			\end{tikzpicture}
				\li $\proc{PeekSort}(A[\ell..j],e,i)$
				\li $\proc{PeekSort}(A[j+1..r],j+1,s)$
				\li $\proc{Merge}(A[\ell..j],A[j+1..r])$
	\end{codebox}
	\vspace*{-1ex}
	\caption{%
		Peeksort: A simple top-down version of nearly-optimal natural mergesort.
		The initial call is $\proc{PeekSort}(A[1..n],1,n)$.
		Procedures \proc{ExtendRunLeft} (-\proc{Right}) scan left (right) starting at $A[m]$
		as long as the run continues (and we did not cross the second parameter).%
	}
	\label{alg:peeksort}
	\vspace*{-3ex}
\end{algorithm}



The following theorem shows that \proc{PeekSort} is indeed a nearly-optimal mergesort.
Unlike previous such methods, its code has very little overhead
(in terms of both time and space) in comparison
with a standard top-down mergesort,
so it is a promising method for a practical nearly-optimal mergesort.


\begin{theorem}
\label{thm:peeksort}
	The merge cost of \proc{PeekSort} on an input consisting of $r$ runs with 
	respective lengths $L_1,\ldots,L_r$ is at most $\mathcal H(\frac{L_1}n,\ldots,\frac{L_m}n)n + 2n - (r+2)$,
	the number of comparisons is at most $\mathcal H(\frac{L_1}n,\ldots,\frac{L_m}n)+3n - (2r+3)$.
	Both is optimal up to $O(n)$ terms (in the worst case).
\end{theorem}

\begin{proof}
	The recursive calls of \wref{alg:peeksort} produce the same tree as Mehlhorn's Method~1
	with input $(\alpha_0,\ldots,\alpha_m) = (\frac{L_1}n,\ldots,\frac{L_r}n)$ (\ie, $m=r-1$)
	and $\beta_j=0$.
	By \wref{thm:nearly-opt-trees}--(ii), the search costs in this tree are 
	$C \le \mathcal H+2 - (m+3)\alpha_{\mathrm{min}}$ with $\mathcal H = \mathcal H(\frac{L_1}n,\ldots,\frac{L_m}n)$.
	Since $L_j \ge 1$, we have $\alpha_{\mathrm{min}} \ge\frac1n$.
	As argued in \wref{sec:merge-trees}, the overall merge costs are then given by
	$M = C n \le \mathcal H n + 2n - (r+2)$, which is within $O(n)$ of the lower bound for $M$.
	
	We save at least one comparison per merge since merging runs of lengths $m$ and $n$ 
	requires at most $n+m-1$ comparisons. In total, we do exactly $r-1$ merge operations. 
	Apart from merging, we need a total of $n-1$ additional comparisons to detect the existing runs
	in the input.
	Barbay and Navarro~\cite[Thm.\,2]{BarbayNavarro2013} argued that $\mathcal H n - O(n)$ 
	comparisons are necessary if the elements in the input are all distinct.
\end{proof}


%
%
%

\subsection{Powersort: A Single-Pass Stack-Based Method}

One little blemish remains in \proc{PeekSort}: we have to use ``random accesses'' into the
middle of the array to decide how to proceed. 
Even though we only use cache-friendly sequential scans, 
the I/O operations to load the middle run are effectively wasted, 
since it will often be merged only much later (after further recursive calls).
Timsort and the other stack-based variants from~\cite{AugerNicaudPivoteau2015,BussKnop2018}
proceed in one left-to-right scan over the input and merge the top runs 
on their stack. 
This increases the likelihood to still have (parts of) the most recently detected run 
in cache when it is merged subsequently.



\subsubsection{The power of top-down in a bottom-up method}

Method~2 to construct nearly-optimal search trees suggests the following definition:
%
\begin{definition}[Node Power]
\label{def:node-power}
	Let $\alpha_0,\ldots,\alpha_m$, $\sum \alpha_j=1$ be leaf probabilities. 
	For $1\le j\le m$,
	let \internalnode j be the internal node separating the $(j-1)$st and $j$th leaf.
	The \emph{power} of (the split at) node \internalnode j is
	\begin{align*}\SwapAboveDisplaySkip
			P_j
		&\wrel=
			\min \Biggl\{ 
				\ell\in\N : \biggl\lfloor \frac a{2^{-\ell}} \biggr\rfloor 
					< \biggl\lfloor \frac b{2^{-\ell}} \biggr\rfloor  
			\Biggr\},\;
%
	\text{where }
			a
		\wrel=
			\sum_{i=0}^{j-1} \alpha_i - \tfrac12 \alpha_{j-1},\;
			b
		\wrel=
			\sum_{i=0}^{j-1} \alpha_i + \tfrac12 \alpha_{j}.
	\end{align*}
	($P_j$ is the index of the first bit where the (binary) fractional parts of $a$ and $b$ differ.)
\end{definition}
%
Intuitively, $P_j$ is the ``intended'' depth of \internalnode j, but 
nodes occasionally end up higher in 
the tree if some leaf has a large weight relative to the current subtree,
(see the rightmost branch in \wref{fig:nearly-optimal-trees}).
%
Mehlhorn's~\cite{Mehlhorn1977,Mehlhorn1984} 
original implementation of Method~2, procedure \emph{construct-tree},
does not single out the case that the next desired cut point lies
\emph{outside} the range of a subtree.
This reduces the number of cases, but for our application, it is more convenient 
to explicitly check for this out-of-range case, and if it occurs to
directly proceed to the next cut point.
We refer to the modified algorithm as \emph{Method~2\/${}'$};
\wref{app:method-2-prime} gives the details and shows that the changes do not affect
the guarantee for Method~2 in  \wref{thm:nearly-opt-trees}.
In fact Method~2${}'$ seems to typically yield slightly \emph{better} trees 
than Method~2, but there are also counterexamples.

The core benefit of Method~2${}'$, however, is that the resulting tree is 
characterized by local information, namely the node powers,
without requiring any coordination of a top-down recursive procedure.
%
\begin{lemma}[Path power monotonicity]
\label{lem:path-monotonicity}
	Consider the tree constructed by Method~2\/${}'$.
	The powers of internal nodes along any root-to-leaf path is strictly increasing.
\end{lemma}
%
The proof is given in \wref{app:method-2-prime}.
\begin{corollary}
\label{cor:cartesian-tree}
	The tree constructed by Method~2\/${}'$ for leaf probabilities $\alpha_0,\ldots,\alpha_m$
	is the (min-oriented) Cartesian tree for the sequence of node powers $P_1,\ldots,P_m$.
	It can thus be constructed iteratively (left to right) by the algorithm of 
	Gabow, Bentley, and Tarjan~\cite{GabowBentleyTarjan1984}.
\end{corollary}


\subsubsection{Merging on-the-fly}

We implicitly use the observation from \wref{cor:cartesian-tree} in our algorithm ``powersort''
to construct the tree from left to right.
Whenever the next internal node has a smaller power than the preceding one, 
we are following an edge from a left child 
up to its parent. 
That means that this subtree does not change anymore and we can execute any pending merges 
in it before continuing.
If we are instead following an edge down to a right child of a node, that subtree is still ``open''
and we push the corresponding run on the stack.
\wref{alg:powersort} shows the detailed code.

\begin{algorithm}[tbph]
\vspace*{-2ex}
\small
\tikzset{every node/.style={font=\scriptsize}}
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
	\begin{codebox}
		\Procname{$\proc{PowerSort}(A[1..n])$}
		\li $X \gets $ stack of runs   \>\>\>\>\>\>\textsmaller{(capacity $\lfloor\lg(n)\rfloor+1$)}
		\li $P \gets $ stack of powers \>\>\>\>\>\>\textsmaller{(capacity $\lfloor\lg(n)\rfloor+1$)}
		\li $s_1 \gets 1$; \; 
			$e_1 = \proc{ExtendRunRight}(A[1],n)$
			\quad\Comment $A[s_1..e_1]$ is leftmost run
		\li \While $e_1 < n$
		\Do
			\li $s_2 \gets e_1+1$; \;
				$e_2 \gets \proc{ExtendRunRight}(A[s_2],n)$
				\quad\Comment $A[s_2..e_2]$ next run
			\li $p \gets \proc{NodePower}(s_1,e_1,s_2,e_2,n)$ 
			\quad \Comment $P_j$ for node \internalnode j between $A[s_1..e_1]$ and $A[s_2..e_2]$
			\li \While $P.top() > p$ \>\>\>\>\quad\Comment previous merge deeper in tree than current
			\Do
				\li $P.pop()$
					\>\>\>\quad\Comment $\leadsto$ merge and replace run~$A[s_1..e_1]$ by result
				\li $(s_1,e_1) \gets \proc{Merge}(X.\id{pop}(), \, A[s_1..e_1])$
			\End
			\li $X.\id{push}(A[s_1,e_1])$; \; $P.\id{push}(p)$
			\li $s_1 \gets s_2$; \; $e_1 \gets e_2$
		\EndWhile
		\Comment Now $A[s_1..e_1]$ is the rightmost run
		\li \While $\neg X.\id{empty}()$
		\Do
			\li $(s_1,e_1) \gets \proc{Merge}(X.\id{pop}(), \, A[s_1..e_1])$
		\End
	\end{codebox}
	\vspace*{-5ex}
	\begin{codebox}
		\Procname{$\proc{NodePower}(s_1,e_1,s_2,e_2,n)$}
		\li $n_1 \gets e_1-s_1 + 1$; \; 
			$n_2 \gets e_2 - s_2 + 1$; \; $\ell \gets 0$
		\li $a \gets ( s_1 + n_1/2 - 1 ) / n$; \;
			$b \gets ( s_2 + n_2/2 - 1 ) / n$
%
		\li \kw{while} $\lfloor a\cdot 2^\ell \rfloor \isequal \lfloor b\cdot 2^\ell \rfloor$ 
			\kw{do} $\ell\gets\ell+1$ \kw{end while}
		\li \Return$(\ell)$
	\end{codebox}
	\vspace*{-1ex}
	\caption{%
		Powersort: A one-pass stack-based nearly-optimal natural mergesort.
		Procedure \proc{ExtendRunRight} scans right as long as the run continues.
	}
	\label{alg:powersort}
	\vspace*{-3ex}
\end{algorithm}

The runs on the stack correspond to nodes with strictly increasing powers,
so we can bound the stack height by the maximal $P_j$.
Since our leaf probabilities here are $\alpha_j = \frac{L_j}n\ge \frac1n$, 
we have $P_j \le \lfloor \lg n\rfloor + 1$.



\begin{theorem}
\label{thm:powersort}
	The merge cost of \proc{PowerSort} is at most $\mathcal H(\frac{L_1}n,\ldots,\frac{L_m}n)+2n$,
	the number of comparisons is at most $\mathcal H(\frac{L_1}n,\ldots,\frac{L_m}n)+3n-r$.
	Moreover, (apart from buffers for merging) only $O(\log n)$ words of extra space 
	are required.
\end{theorem}
\begin{proof}
	The merge tree of \proc{PowerSort} is exactly the search tree constructed by Method~2${}'$
	on leaf probabilities $(\alpha_0,\ldots,\alpha_m) = (\frac{L_1}n,\ldots,\frac{L_r}n)$
	and $\beta_j=0$.
	By \wref{thm:nearly-opt-trees}--(iii), the search costs are 
	$C \le \mathcal H+2$ with $\mathcal H = \mathcal H(\frac{L_1}n,\ldots,\frac{L_m}n)$, so
	the overall merge costs are 
	$M = C n \le \mathcal H n + 2n$, which is within $O(n)$ of the lower bound for $M$.
	The comparisons follow as for \wref{thm:peeksort}.
\end{proof}











\section{Running-Time Study}
\label{sec:experiments}


We conducted a running-time study comparing the
two new nearly-optimal mergesorts with 
current state-of-the-art implementations and
elementary mergesort variants.
The code is available on github~\cite{githubCode}.

\needspace{3\baselineskip}
\smallskip\noindent
The goal of this study is to show that
\begin{enumerate}
\item 
	peeksort and powersort have very little overhead compared to standard (non-natural)
	mergesort variants (\ie, they are never (much) slower), and at the same time
\item
	peeksort and powersort outperform other mergesort variants on
	partially presorted inputs.
\end{enumerate}
Timsort is arguably the most used adaptive sorting method;
even though analytical guarantees are still to be found,
its efficiency in particular for partially sorted inputs has been 
demonstrated empirically~\cite{Peters2002}.
A secondary goal is hence to
\begin{enumerate}
\setcounter{enumi}{2}
\item
	investigate the typical merge costs of Timsort on different inputs.
\end{enumerate}

%
\subsection{Setup}
Oracle's Java runtime library includes a highly tuned Timsort implementation;
to be able to directly compare with it, we chose to implement our algorithms in Java.
The Timsort implementation is used for \texttt{Object[]}, \ie,
arrays of \emph{references} to objects;
since the location of objects on the heap is hard to control and likely to produce
more or less cache misses from run to run, we chose to sort \texttt{int[]}s instead
to obtain more reproducible results.
We thus modified the library implementation of Timsort accordingly.
This scenario makes key comparisons and element moves relatively cheap and thereby 
emphasizes the remaining overhead of the methods,
which is in line with
our primary goal 
%
%
%
	1) to study the impact of the additional bookkeeping
	required by the adaptive methods.
%

%
We compare our methods with simple top-down and bottom-up mergesort implementations.
We use the code given by Sedgewick~\cite[Programs~8.3 and~8.5]{Sedgewick2003}
with his simple merge method (Program~8.2) as its basis;
in both cases, we add a check before calling merge to detect if the runs happen 
to be already in sorted order, and we use insertionsort for base cases of size $n\le w = 24$.
(For bottom-up mergesort, we start by sorting chunks of size $w=24$.)
%
%
%
%
%
%
%
Our Java implementations of peeksort and powersort are described in more detail in \wref{app:java-code}.
Apart from a mildly optimized version of the pseudocode, we added the same cutoff / minimal run length 
($w=24$) as above.

All our methods call the same merge procedure, whereas
the library Timsort contains a modified merge method
that tries to save key comparisons:
when only elements from the same run are selected for the output repeatedly,
Timsort enters a \emph{``galloping mode''} and uses exponential searches 
(instead of the conventional sequential search) to find
the insertion position of the next element. Details are described by Peters~\cite{Peters2002}.
Since saving comparisons is not of utmost importance in our scenario of sorting \texttt{int}s,
we also added a version of Timsort, called ``trotsort'', that uses our plain merge method instead
of galloping, but is otherwise identical to the library Timsort.

%
Our hard- and software setup is listed in
\wref{app:setup}.
We use the following inputs types:
\begin{itemize}
\item
	\emph{random permutations} are a case where no long runs are present to exploit;
\item
	\emph{``random-runs'' inputs} are constructed from a random permutation by sorting 
	segments of random lengths,
	where the lengths are chosen independently according to a geometric distribution with a 
	given mean $\ell$;
	since the geometric distribution has large variance, 
	these inputs tend to have runs whose sizes vary a lot;
\item
	\emph{``Timsort-drag'' inputs} are special instances of random-runs inputs 
	where the run lengths are chosen as $\mathcal R_{\mathrm{tim}}$,
	the bad-case example for Timsort from~\cite[Thm.\,3]{BussKnop2018}.
\end{itemize}


\subsection{Overhead of Nearly-Optimal Merge Order}

We first consider random permutations as inputs.
Since random permutations contain (with high probability) no long runs 
that can be exploited, the adaptive methods will not find anything 
that would compensate for their additional efforts to identify runs.
(This is confirmed by the fact that the total merge costs of all methods, 
including Timsort, are within 1.5\% of each other in this scenario.)
\wref{fig:times-normalized-rp} shows average running times
for inputs sizes from $100\,000$ to $100$ million ints.
(Measurements for $n=10\,000$ were too noisy to draw meaningful conclusions.)

\begin{figure}[tbh]
	\begin{minipage}[c]{0.6\textwidth}
		\hspace*{-.5em}\includegraphics[width=\textwidth]{pics/times-normalized-rp-means-over-log10n}%
  	\end{minipage}\hfill
  	\begin{minipage}[c]{.38\textwidth}
		\caption{%
			Normalized running times on random permutations. 
			The $x$-axis shows $\log_{10}(n)$, the $y$-axis show average and standard deviation (as error bars)
			of $t/(n \lg n)$ where $t$ is the running time in ms.
			We plot averages over 1000 repetitions (resp.\ 200 and 20 for the largest sizes).
		%
		}
	\label{fig:times-normalized-rp}
	\end{minipage}
\end{figure}

The relative ranking is clearly stable across different input sizes.
\texttt{Arrays.sort(int[])} (dual-pivot quicksort) is the fastest method,
but is not a stable sort and only serves as a baseline.
The timings of top-down and bottom-up mergesort, 
peeksort and powersort are 20--30\% slower than dual-pivot quicksort.
Comparing the four to each other, no large differences
are visible; if anything, bottom-up mergesort was a bit slower for large inputs.
Since the sorting cutoff / minimal run length $w=24$ exceeded the length of \emph{all} 
runs in all inputs, we are effectively presented with a case of all-equal run lengths.
Merging them blindly from left to right (as in bottom-up mergesort) then performs just fine,
and top-down mergesort finds a close-to-optimal merging order in this case.
%
%
%
%
%
%
That peek- and powersort perform essentially \emph{as good as} elementary mergesorts
on random permutations thus clearly indicates that their overhead 
for determining a nearly-optimal merging order is negligible.

The library Timsort performs surprisingly bad on \texttt{int[]}s,
probably due to the relatively cheap comparisons.
Replacing the galloping merge with the ordinary merge alleviates this
(see ``trotsort''), 
but Timsort remains inferior on random permutations by a fair margin
(10--20\%).


%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%

\subsection{Practical speedups by adaptivity}

%
	\begin{figure}[tbh]
		\plaincenter{
			\includegraphics[width=.8\linewidth]{pics/runs20-500-example}
		}
		\caption{%
			A random-runs input with $n=500$ and $\ell=20\approx\sqrt n$. The $x$-axis is the index in the array,
			the $y$-axis the value. Runs are emphasized by color.%
		}
		\label{fig:typical-random-runs}
	\end{figure}
%
After demonstrating that we do not lose much by using our adaptive methods
when there is nothing to adapt to,
we next investigate how much can be gained if there is.
We consider random-runs inputs as described above.
This input model instills a good dose of presortedness, but
not in a way that gives any of the tested methods an obvious
advantage or disadvantage over the others.
We choose a representative size of $n=10^7$ and an expected run length $\ell = 3\,000$,
so that we expect roughly $\sqrt n$ runs of length $\sqrt n$.


\begin{figure}[tbh]
	\plaincenter{%
%
%
%
%
			\includegraphics[height=10\baselineskip]{pics/times-runs3k-10m}\qquad%
			\includegraphics[height=10\baselineskip]{pics/mergecost-runs3k-10m}%
%
	}
	\vspace{-1ex}
	\caption{%
		Average running times (left) and normalized merge cost (right) on 
		random-runs inputs with $n=10^7$ and $\ell=3\,000\approx \sqrt n$.
		The merge costs have been divided by $n\lg(n/w)\approx1.87\cdot 10^8$, 
		which is the merge cost a (hypothetical) optimal mergesort that does 
		not pick up existing runs, but starts merging with runs of length $w=24$.
	}
\label{fig:times-mergecost-random-runs}
\end{figure}


If this was a random permutation, we would expect merge costs of 
roughly $n \lg(n/w) \approx 1.87 \cdot10^8$ (indeed a bit above this).
The right chart in \wref{fig:times-mergecost-random-runs} shows 
that the adaptive methods can bring the merge cost down to a little over 60\%
of this number.
%
%
%
	Note the run lengths vary considerably~-- 
	to give some intuitive feel for this volatility, \wref{fig:typical-random-runs} 
	shows a stereotypical (but smaller) random-runs input.
%
Powersort achieved average merge costs of 
$1.14\cdot10^8 < n \lg r \approx 1.17 \cdot 10^8$, \ie,
less than a method would that
only adapts to the \emph{number} of runs $r$.

In terms of running time, powersort is again among the fastest stable methods,
and indeed 20\% \emph{faster} than \texttt{Arrays.sort(int[])}.
The best adaptive methods are also 10\% faster than top-down mergesort.
(The latter is ``slightly adaptive'', 
by omitting merges if the runs happen to already be in order.)
This supports the statement that significant speedups can be realized
by adaptive sorting on inputs with existing order, and 
$\sqrt n$ runs suffice for that.
If we increase $\ell$ to $100\,000$, so that we expect only roughly $100$ long runs,
the library quicksort becomes twice as slow as powersort and Timsort (trotsort).

Timsort's running time is a bit anomalous again. 
Even though it occasionally incurs 10\% more merge costs on a given input than
powersort, the running times were within 1\% of each other 
(considering the trotsort variant;
the original galloping version was again uncompetitive).



\subsection{Non-optimality of Timsort}

Finally, we consider ``Timsort-drag'' inputs, a sequence
of run lengths $\mathcal R_{\mathrm{tim}}(n)$ 
specifically crafted by Buss and Knop~\cite{BussKnop2018}
to generate unbalanced merges (and hence large merge cost) in Timsort.
Since actual Timsort implementations employ minimal run lengths of up to $32$ elements
we multiplied the run lengths by $32$.
\wref{fig:times-mergecost-timdrag} shows running time and merge cost for all
methods on a characteristic Timsort-drag input of length $2^{24}\approx 1.6 \cdot 10^7$.

\begin{figure}[tbh]
	\plaincenter{%
%
%
%
%
			\includegraphics[height=10\baselineskip]{pics/times-timdrag-16m}\qquad%
			\includegraphics[height=10\baselineskip]{pics/mergecost-timdrag-16m}%
%
	}
	\vspace{-1ex}
	\caption{%
		Average running times (left) and normalized merge cost (right) on 
		``Timsort-drag'' inputs with $n=2^{24}$ and run lengths 
		$\mathcal R_{\mathrm{tim}}(2^{24}/32)$ multiplied by $32$.
		Merge costs have been divided by $n \lg(n/w) \approx 3.26\cdot10^8$.
	}
\label{fig:times-mergecost-timdrag}
\end{figure}


In terms of merge costs, Timsort now pays 30\% more than even a simple non-adaptive
mergesort, whereas peeksort and powersort obviously retain their proven nearly-optimal
behavior.
Also in terms of running time, Timsort is a bit slower than top-down mergesort,
and 10\% slower than powersort on these inputs.
It is remarkable that the dramatically larger merge cost does not lead to 
a similarly drastic slow down in practice.
Nevertheless, it must be noted that Timsort's merging strategy has weaknesses,
and it is unclear if more dramatic examples are yet to be found.



%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%
%





\section{Conclusion}
\label{sec:conclusion}

In this paper, we have demonstrated that provably good merging orders for natural mergesort 
can be found with negligible overhead.
The proposed algorithms, peeksort and powersort offer more reliable performance than
the widely used Timsort, and at the same time, are arguably simpler.

Powersort builds on a modified bisection heuristic for computing nearly-optimal binary search trees
that might be independent interest.
It has the same quality guarantees as Mehlhorn's original formulation, but
allows the tree to be built ``bottom-up'' as a Cartesian tree over a certain sequence, the ``node powers''.
It is the only such method for nearly-optimal search trees to our knowledge.

Buss and Knop conclude with the question
whether there exists a $k$-aware algorithm 
(a stack-based natural mergesort that only considers the top $k$ runs in the stack)
with merge cost $(1+o_r(1))$ times the optimal merge cost~\cite[Question~37]{BussKnop2018}.
Powersort effectively answers this question in the affirmative with $k=3$.%
\footnote{%
	Strictly speaking, powersort needs a relaxation of the model of Buss and Knop.
	They require decisions to be made solely based on the \emph{lengths} of runs, 
	whereas node power takes the location of the runs within the array into account.
	Since the location of a run must be stored anyway, this appears reasonable to us.
}

\subsection{Extensions and future work}

Timsort's ``galloping merge'' procedure saves comparisons when we consistently consume 
elements from one run, but in ``well-mixed'' merges, it does not help (much).
It would be interesting to compare this method with other comparison-efficient
merge methods.

Another line of future research is to explore ways to profit from duplicate keys in the input.
The ultimate goal would be a ``synergistic'' sorting method
(in the terminology of~\cite{BarbayOchoaSatti2017}) that has practically no overhead for detecting
existing runs and equals and yet exploits their \emph{combined} presence optimally.


\clearpage
\appendix

\section{Method~2${}'$ for nearly-optimal search trees}
\label{app:method-2-prime}

This section uses Mehlhorn's~\cite{Mehlhorn1977,Mehlhorn1984} original notation,
in particular, it handles the general optimal binary search tree setting.
We add the following two ``out-of-range'' cases to Mehlhorn's procedure 
$\mathit{construct\text-tree}(i,j,\mathit{cut},\ell)$, at the very beginning:

\begin{itemize}
\item (Case L) If $s_j < \mathit{cut} + 2^{-\ell}$, return
	$\mathit{construct\text-tree}(i,j,\mathit{cut},\ell+1)$
\item (Case R) If $\mathit{cut} + 2^{-\ell} < s_i$, return
	$\mathit{construct\text-tree}(i,j,\mathit{cut} + 2^{-\ell},\ell+1)$
\end{itemize}

The original procedure would end up in case C instead of L resp.\ 
case D instead of R and choose a right- resp.\ leftmost key as the
root node.
But if the desired cut point $\mathit{cut}+2^{-\ell}$ lies completely
outside the range of bisection $[s_i,s_j]$, this produces an unnecessarily
unbalanced split.
This case can only happen if the neighboring leaf has a probability
larger than $\frac12$ relative to the previous subtree, so that the current split point
still lies within the range corresponding to the previously chosen root.
Our cases L and R thus ``skip'' this void cut point $\mathit{cut}+2^{-\ell}$
and increment $\ell$ without generating a node.

Given that the invariants (1)--(4) are fulfilled for the current parameters
of \emph{construct-tree}, they will also be fulfilled in the recursive calls
issued in cases L and R.
Therefore, Mehlhorn's analysis remains valid for our modified procedure:
In his Fact~3, we have $b_h+1 \le \ell$ and $a_h \le \ell$ (instead of equality),
but this is all that is needed to establish Fact~4 and hence the bound on the search costs.


\subsection{Proof of \wref{lem:path-monotonicity}}

\textbf{\textsf{Claim:}} 	
In the tree constructed by Method~2\/${}'$,
the powers of internal nodes along any root-to-leaf path are strictly increasing.


%
\begin{proof}
Consider the recursive procedure \emph{construct-tree}
as described by Mehlhorn~\cite{Mehlhorn1977,Mehlhorn1984}, 
but with the additional cases from above.
We prove that whenever a recursive call 
$\textit{construct\text-tree}(i,j,\mathit{cut},\ell)$ creates
an internal node \internalnode k, we have $P_k = \ell$.
Since $\ell$ is incremented in all recursive calls, the claim follows.

%
%
%
%
Only cases A, B, C or D create new nodes, so assume we are not in case L or R.
Then we actually have the stronger version of invariant~(4):
$\mathit{cut} \le s_i\le \mathit{cut} + 2^{-\ell}\le s_j \le \mathit{cut}+2^{-\ell+1}$
and hence will always find a $k$ with $i<k\le j$ and 
\[\mathit{cut} \wrel\le s_{k-1} \wrel< \mathit{cut}+2^{-\ell} \wrel\le s_k \wrel\le \mathit{cut}+2^{-(\ell-1)}\]
for which we now create the node \internalnode k.
Dividing by $2^{-\ell}$ shows that $P_k = \ell$ (we have $a = s_{k-1}$ and $b = s_k$).
\end{proof}



\section{Java Code}
\label{app:java-code}
\lstset{
	tabsize=3,
	basewidth=.55em,
	language=Java,
	numbers=left,
	numberstyle=\tiny\sffamily,
	numbersep=-.8em,
	basicstyle=\footnotesize\ttfamily,
}

\bigskip\noindent
In this appendix, we give the core methods of a Java implementation of
\wref{alg:peeksort} and \wref{alg:powersort} that form the basis of our running-time
study.
The goal is to document (and comment on) a few design decisions and optimizations
that have been included in the running time study.
The full code, including the main routines to reproduce the running time studies
exactly as used for this paper,
is available on github~\cite{githubCode}.

\subsection{Merging}

All our mergesort variants (except for the original library implementation of Timsort)
use the following straight-forward ``bitonic'' merge procedure that
merges two adjacent runs.
It is taken from \cite[Program~8.2]{Sedgewick2003}

\begin{lstlisting}
	/** merges A[l..m-1] and A[m..r] */
	static void mergeRuns(int[] A, int l, int m, int r, int[] aux) {
		--m; // accounts for different convention in Sedgewick's book
		int i, j;
		for (i = m+1; i > l; --i) aux[i-1] = A[i-1];
		for (j = m; j < r; ++j) aux[r+m-j] = A[j+1];
		for (int k = l; k <= r; ++k)
			A[k] = aux[j] < aux[i] ? aux[j--] : aux[i++];
	}
\end{lstlisting}

Merging offers some potential for improvements in particular \wrt the number
of used key comparisons.
Since we operate on integers, comparisons are cheap and more sophisticated
merging strategies will not be needed here.

\subsection{Peeksort}

The pseudocode for Peeksort can be translated to Java almost verbatim.
Since the recursion is costly for small input sizes,
we switch to insertionsort when the subproblem size
is small. 
Values for \texttt{INSERTION\_SORT\_THRESHOLD} of $10$--$32$ yielded good 
results in the experiments; we ultimately set it to $24$ to approximate
the choice in the library implementation of Timsort.
(The latter chooses the minimal run length between $16$ and $32$ 
depending on $n$ by a heuristic that tries to avoid imbalanced merges.
We did not use this adaptive choice in our methods.)

\begin{lstlisting}[language=Java]
	static void peeksort(int[] A, int left, int right, 
	                     int leftRunEnd, int rightRunStart, int[] B) {
		if (leftRunEnd == right || rightRunStart == left) return;
		if (right - left + 1 <= INSERTION_SORT_THRESHOLD) {
			insertionsort(A, left, right, leftRunEnd - left + 1); return;
		}
		int mid = left + ((right - left) >> 1);
		if (mid <= leftRunEnd) {            // |XXXXXXXX|XX     X|
			peeksort(A, leftRunEnd+1, right, leftRunEnd+1,rightRunStart, B);
			mergeRuns(A, left, leftRunEnd+1, right, B);
		} else if (mid >= rightRunStart) {  // |XX     X|XXXXXXXX|
			peeksort(A, left, rightRunStart-1, leftRunEnd, rightRunStart-1, B);
			mergeRuns(A, left, rightRunStart, right, B);
		} else { // find middle run
			int i, j;
			if (A[mid] <= A[mid+1]) {
				i = extendIncreasingRunLeft(A, mid, leftRunEnd + 1);
				j = mid+1 == rightRunStart ? mid : 
					extendIncreasingRunRight(A, mid+1, rightRunStart - 1);
			} else {
				i = extendDecreasingRunLeft(A, mid, leftRunEnd + 1);
				j = mid+1 == rightRunStart ? mid : 
					extendStrictlyDecreasingRunRight(A, mid+1,rightRunStart - 1);
				reverseRange(A, i, j);
			}
			if (i == left && j == right) return;
			if (mid - i < j - mid) {         // |XX     x|xxxx   X|
				peeksort(A, left, i-1, leftRunEnd, i-1, B);
				peeksort(A, i, right, j, rightRunStart, B);
				mergeRuns(A,left, i, right, B);
			} else {                         // |XX   xxx|x      X|
				peeksort(A, left, j, leftRunEnd, i, B);
				peeksort(A, j+1, right, j+1, rightRunStart, B);
				mergeRuns(A,left, j+1, right, B);
			}
		}
	}
\end{lstlisting}

\subsection{Insertionsort to extend runs}

Here (and in the following code), we use a straight insertionsort
variant that accepts the length of a sorted prefix as an additional parameter.
A similar method is also used in Timsort to extend runs to a forced minimal
length.
The library Timsort uses binary insertionsort instead, 
but unless comparisons are expensive, a straight sequential-search variant is sufficient.

\begin{lstlisting}
	static void insertionsort(int[] A, int left, int right, int nPresorted) {
		assert right >= left;
		assert right - left + 1 >= nPresorted;
		for (int i = left + nPresorted; i <= right ; ++i) {
			int j = i - 1,  v = A[i];
			while (v < A[j]) {
				A[j+1] = A[j];
				--j;
				if (j < left) break;
			}
			A[j+1] = v;
		}
	}
\end{lstlisting}


\subsection{Powersort}

For powersort, we implement the stack as an array that is indexed by the node power.
Thereby, we avoid explicit stack operations and to store powers explicitly.
On the other hand, we have to check for empty entries since powers are not always consecutive.

\begin{lstlisting}[language=Java]
	static void powersort(int[] A, int left, int right) {
		int n = right - left + 1;
		int lgnPlus2 = log2(n) + 2;
		int[] leftRunStart = new int[lgnPlus2], leftRunEnd = new int[lgnPlus2];
		Arrays.fill(leftRunStart, -1);
		int top = 0;
		int[] buffer = new int[n >> 1];
	
		int startA = left, endA = extendRunRight(A, startA, right);
		int lenA = endA - startA + 1;
		if (lenA < minRunLen) { // extend to minRunLen
			endA = Math.min(right, startA + minRunLen-1);
			insertionsort(A, startA, endA, lenA);
		}
		while (endA < right) {
			int startB = endA + 1, endB = extendRunRight(A, startB, right);
			int lenB = endB - startB + 1;
			if (lenB < minRunLen) { // extend to minRunLen
				endB = Math.min(right, startB + minRunLen-1);
				insertionsort(A, startB, endB, lenB);
			}
			int k = nodePower(left, right, startA, startB, endB);
			assert k != top;
			for (int l = top; l > k; --l) { // clear left subtree bottom-up
				if (leftRunStart[l] == -1) continue;
				mergeRuns(A, leftRunStart[l], leftRunEnd[l]+1, endA, buffer);
				startA = leftRunStart[l];
				leftRunStart[l] = -1;
			}
			// store left half of merge between A and B on stack
			leftRunStart[k] = startA; leftRunEnd[k] = endA;
			top = k;
			startA = startB; endA = endB;
		}
		assert endA == right;
		for (int l = top; l > 0; --l) {
			if (leftRunStart[l] == -1) continue;
			mergeRuns(A, leftRunStart[l], leftRunEnd[l]+1, right, buffer);
		}
	}
\end{lstlisting}

The computation of the node powers can be done in many different ways and offers a lot
of potential for low-level bitwise optimizations; some care is needed to prevent overflows.
In our experiments, the following loop-less version was a tiny bit faster than
other tried alternatives.

\begin{lstlisting}
	static int nodePower(int left, int right, int startA, int startB, int endB) {
		int twoN = (right - left + 1) << 1; // 2*n
		long l = startA + startB - (left << 1);
		long r = startB + endB + 1 - (left << 1);
		int a = (int) ((l << 31) / twoN);
		int b = (int) ((r << 31) / twoN);
		return Integer.numberOfLeadingZeros(a ^ b);
	}
\end{lstlisting}



\section{Experimental Setup}
\label{app:setup}

All experiments were run on a Lenovo Thinkpad X230 Tablet
running Ubuntu 16.04.01 with Linux kernel 4.13.0-38-generic.
The CPU is an Intel Core i7-3520M CPU with 2.90GHz, the system has 8GB of main memory.

The Java compiler was from the Oracle Java JDK version 1.8.0\_161,
the JVM is Java HotSpot 64-Bit Server VM (build 25.161-b12, mixed mode).
All experiments were run with disabled X server from the TTY,
the java process was bound to one core (with the \texttt{taskset} utility).
They started with a warmup phase to trigger just-in-time (JIT) compilation
before measuring individual sorting operations.
The inputs were generated outside the timing window and reused the same array for all
repetitions.
%
The following flags were used for the JVM:
\texttt{-XX:+UnlockDiagnosticVMOptions}, \texttt{-XX:-TieredCompilation} and  \texttt{-XX:+PrintCompilation}.
Tiered compilation was disabled to avoid multiple passes of just-in-time compilation
to occur during the timed experiments; the print-compilation flag was used to
monitor whether relevant methods are subjected to recompilation or deoptimization
during the experiments.





\bibliography{mergesort.bib}


\end{document}

=== Paper Description (papers/paper3.txt) ===
buss knop paper

=== Paper (papers/paper3.txt) ===
\documentclass[11pt]{article}

\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{amsthm}

\usepackage{algorithm}
\usepackage{algorithmicx}
\usepackage{algpseudocode}

\usepackage{graphicx}
\usepackage{subcaption}

% \usepackage[colorlinks]{hyperref} % Doesn't work with arXiv
\usepackage{hyperref}

\usepackage{color}
\usepackage[top=1in, bottom=1in, left=1in, right=1in]{geometry}

\usepackage{todonotes}


\algblockdefx[Choose]{Choose}{EndChoose}%
[1]{\textbf{choose} \hbox{#1}}{\textbf{end choose}}

\newtheorem{theorem}{Theorem}
\newtheorem{corollary}[theorem]{Corollary}
\newtheorem{lemma}[theorem]{Lemma}
\newtheorem{claim}[theorem]{Claim}
\newtheorem{proposition}[theorem]{Proposition}
\newtheorem{conjecture}[theorem]{Conjecture}
\newtheorem{question}[theorem]{Question}
\theoremstyle{definition}
\newtheorem{definition}[theorem]{Definition}

\def\pprime{{\prime\prime}}
\def\ppprime{{\prime\prime\prime}}
\def\seq{{\mathrm{seq}}}
\def\seqA{\seq_2}
\def\Merge{{\textit{Merge}}}
\def\Rtim{{{\mathcal R}_{\mathrm{tim}}}}
\def\Rastack{{{\mathcal R}_{\alpha\mathrm{st}}}}
\def\Rshiver{{{\mathcal R}_{\mathrm{sh}}}}
\def\Ramerge{{{\mathcal R}_{\alpha\mathrm{m}}}}
\def\YZ{{\mathit{YZ}}}

\def\QQ{Q}

\title{
    Strategies for Stable Merge Sorting
}

\author{
  Sam Buss \\
  {\small \texttt{sbuss@ucsd.edu}} \\
  {\small Department of Mathematics} \\
  {\small University of California, San Diego} \\
  {\small La Jolla, CA, USA}
  \and
  Alexander Knop \\
  {\small \texttt{aknop@ucsd.edu}} \\
  {\small Department of Mathematics} \\
  {\small University of California, San Diego} \\
  {\small La Jolla, CA, USA}
}


\begin{document}

\maketitle

\begin{abstract}
We introduce new stable natural merge sort algorithms, called $2$-merge
sort and $\alpha$-merge sort. We prove upper and lower bounds for several
merge sort algorithms, including Timsort, Shivers' sort, $\alpha$-stack
sorts, and our new $2$-merge and $\alpha$-merge sorts. The upper and lower
bounds have the forms $c \cdot n \log m$ and $c \cdot n \log n$ for inputs
of length~$n$ comprising $m$~monotone runs. For Timsort, we prove a lower bound of
$(1.5 - o(1)) n \log n$. For $2$-merge sort, we prove optimal upper and
lower bounds of approximately $(1.089 \pm o(1))n \log m$. We prove similar
asymptotically matching upper and lower bounds for $\alpha$-merge sort, when
$\varphi < \alpha < 2$, where $\varphi$ is the golden ratio.

Our bounds are in terms of merge cost; this upper bounds the number of comparisons
and accurately models runtime.
The merge strategies can be used for any stable merge sort, not just
natural merge sorts. The new $2$-merge and $\alpha$-merge sorts have better
worst-case merge cost upper bounds and are slightly simpler to implement than
the widely-used Timsort; they also perform better in experiments. We report
also experimental comparisons with algorithms developed by Munro-Wild and Jug\'e
subsequently to the results of the present paper.
\end{abstract}

\stepcounter{footnote}
\footnotetext{The present paper is an expansion of an earlier conference
paper~\cite{BussKnop:StableMergeSortSODA}. It includes the contents
of~\cite{BussKnop:StableMergeSortSODA} plus a new easy, direct proof of
an $O(n \log m)$ upper bound
for $\alpha$-merge sort (Theorem~\ref{thm:alphaMergeUpperEasy}),
the proof of the tight upper bounds for $\alpha$-merge sort
(Theorem~\ref{thm:alphamergeUpper}), and experimental comparisons with
the algorithms of
Munro-Wild~\cite{MunroWild:MergeSort}
and Jug\'e~\cite{Juge:AdaptiveShivers}
that were developed after this paper was first circulated.}

\section{Introduction}
This paper studies stable merge sort algorithms, especially natural merge sorts.
We will propose new strategies for the order in which merges are performed, and
prove upper and lower bounds on the cost of several merge strategies. The first
merge sort algorithm was proposed by von
Neumann~\cite[p.159]{Knuth:SortingSearching}: it works by splitting the input
list into sorted sublists, initially possibly lists of length one, and then
iteratively merging pairs of sorted lists, until the entire input is sorted. A
sorting algorithm is {\em stable} if it preserves the relative order of elements
which are not distinguished by the sort order. There are several methods of
splitting the input into sorted sublists before starting the merging; a merge
sort is called {\em natural} it finds the sorted sublists by detecting
consecutive runs of entries in the input which are already in sorted order.
Natural merge sorts were first proposed by Knuth~\cite[p.160]{Knuth:SortingSearching}.

Like most sorting algorithms, the merge sort is comparison-based in that
it works by comparing the relative order of pairs of entries in the input list.
Information-theoretic considerations imply
that any comparison-based sorting algorithm must make at least
$\log_2(n!) \approx  n \log_2 n$ comparisons in the worst case.
However, in many practical applications, the input is frequently already partially
sorted.  There are many {\em adaptive} sort algorithms which will
detect this and run faster on inputs which are already partially sorted.
Natural merge sorts are adaptive in this sense: they detect sorted
sublists (called ``runs'') in the input, and thereby reduce the
cost of merging sublists. One very popular stable natural merge sort is the
eponymous Timsort of Tim Peters~\cite{Peters:TimSortPost}.
Timsort is extensively used, as it is included in Python, in the Java standard
library, in GNU Octave, and in the Android operating system. Timsort has
worst-case runtime $O(n \log n)$, but is designed to run substantially faster on
inputs which are partially pre-sorted by using intelligent strategies to
determine the order in which merges are performed.

There is extensive literature on adaptive sorts: e.g., for theoretical
foundations see \cite{Mannila:Presortedness,%
EstivillCastroWood:surveyAdaptive,%
PeterssonMoffat:OverviewAdaptive,%
McIlroy:OptimisticSorting} and
for more applied investigations see
\cite{%
CookKim:NearlySorted,%
Harris:NaturalMerge,%
ChandramouliGoldstein:Patience,%
ZML:SortRace,%
LaRoccaCantone:neatsort}.
The present paper will consider only stable, natural merge sorts.  As
exemplified by the wide deployment of Timsort, these are certainly an important
class of adaptive sorts. We will consider
the Timsort algorithm~\cite{Peters:TimSortPost,GRBBH:TimSortBug}, and
related sorts due to Shivers~\cite{Shivers:mergesort}
and Auger-Nicaud-Pivoteau~\cite{ANP:MergeStrategies}. We will
also introduce new algorithms,
the ``2-merge sort'' and
the ``$\alpha$-merge sort'' for $\varphi < \alpha < 2$ where
$\varphi$ is the golden ratio.

The main contribution of the present paper is the definition
of new stable merge sort algorithms, called $2$-merge and
$\alpha$-merge sort. These have better worst-case run times
than Timsort, are slightly easier to implement
than Timsort, and perform better in our experiments.

We focus on natural merge sorts, since they are so widely used. However, our
central contribution is analyzing merge strategies and our results are
applicable to any stable sorting algorithm that generates and merges runs,
including patience sorting~\cite{ChandramouliGoldstein:Patience},
melsort~\cite{Skiena:Encroaching,LevcopoulosPetersson:SortingShuffled},
and split sorting~\cite{LevcopoulosPetersson:SplitSort}.

All the merge sorts we consider will use the following framework. (See
Algorithm~\ref{alg:basic}.) The input is a list of $n$ elements.
The first logical stage of the
algorithm (following~\cite{Peters:TimSortPost}) identifies maximal
length subsequences of consecutive entries which are in sorted order, either
ascending or descending. The descending subsequences are reversed, and this
partitions the input into ``runs'' $R_1, \ldots, R_m$ of entries sorted in
non-decreasing order. The number of runs is~$m$; the number of elements in a
run~$R$ is $|R|$. Thus $\sum_i |R_i| = n$. It is easy to see that these runs may
be formed in linear time and with linear number of comparisons.

Merge sort algorithms process the runs in left-to-right order starting
with~$R_1$. This permits runs to be identified on-the-fly,
only when needed. This means there is no need to allocate $\Theta(m)$ additional
memory to store the runs. This also may help reduce cache
misses. On the other hand, it means that the value $m$
is not known until the final run is formed; thus, the natural
sort algorithms do not use $m$ except
as a stopping condition.

The runs~$R_i$ are called {\em original runs}. The second logical stage of the
natural merge sort algorithms repeatedly merges runs in pairs to give longer and
longer runs. (As already alluded to, the first and second logical stages are
interleaved in practice.) Two runs $A$ and $B$ can be merged in linear time;
indeed with only $|A| + |B| - 1$ many comparisons and $|A| + |B|$ many movements
of elements. The merge sort stops when all original runs have been identified
and merged into a single run.

Our mathematical model for the run time of a merge sort algorithm is the sum,
over all merges of pairs of runs $A$ and~$B$, of $|A| + |B|$.  We call the
quantity the {\em merge cost}. In most situations, the run time of a
natural sort algorithm can be linearly bounded in terms of its merge cost. Our
main theorems are lower and upper bounds on the merge cost of several stable
natural merge sorts.  Note that if the runs are merged in a balanced fashion,
using a binary tree of height $\lceil{\log m}\rceil$, then the total merge cost
$ \le n \lceil{\log m}\rceil$. (We use $\log$ to denote logarithms base~2.)
Using a balanced binary tree of merges gives  a good worst-case merge cost, but
it does not take into account savings that are available when runs have
different lengths.\footnote{%
  \cite{HMW:Presorted} gives a different method of achieving merge cost
  $O( n \log m )$. Like the binary tree method, their method is not adaptive.
}
The goal is find {\em adaptive} stable natural merge sorts which can
take advantage of different run lengths to reduce the merge cost even below $n\log m$,
but which are
guaranteed to never be much worse than the binary tree. Therefore, our preferred
upper bounds on merge costs are stated in the form $c \cdot n \log m$ for some
constant~$c$, rather than in the form $c\cdot n \log n$.

The merge cost ignores the $O(n)$ cost of forming the original runs~$R_i$: this
does not affect the asymptotic limit of the constant~$c$.

Algorithm~\ref{alg:basic} shows the framework for all the merge sort algorithms
we discuss. This is similar to what Auger et al.~\cite{ANP:MergeStrategies} call
the ``generic'' algorithm. The input~$S$ is a sequence of integers which is
partitioned into monotone runs of consecutive members. The decreasing runs are
inverted, so $S$ is expressed as a list $\mathcal R$ of increasing original runs
$R_1, \ldots, R_m$ called ``original runs''. The algorithm maintains a
stack~$\mathcal \QQ$ of runs $\QQ_1, \ldots, \QQ_\ell$, which have been formed from
$R_1, \ldots, R_k$. Each time through the loop, it either pushes
the next original run, $R_{k + 1}$, onto the stack $\mathcal \QQ$, or it chooses a
pair of adjacent runs $\QQ_i$ and~$\QQ_{i+1}$ on the stack and merges them. The
resulting run replaces $\QQ_i$ and~$\QQ_{i+1}$ and becomes the new $\QQ_i$, and the
length of the stack decreases by one. The entries of the runs $\QQ_i$ are stored
in-place, overwriting the elements of the array which held the input~$S$.
Therefore, the stack needs to hold only the positions~$p_i$ (for $i = 0, \ldots,
\ell +  1$) in the input array where the runs $\QQ_i$ start, and thereby implicitly
the lengths $|\QQ_i|$ of the runs. We have $p_1=0$, pointing to the beginning of
the input array, and for each~$i$, we have $|\QQ_i| = p_{i + 1} - p_i$. The
unprocessed part of~$S$ in the input array starts at position $p_{\ell + 1} =
p_\ell + |\QQ_\ell|$. If $p_{\ell + 1} < n$, then it will be the starting position
of the next original run pushed onto~$\mathcal \QQ$.

\begin{algorithm}
    \begin{algorithmic}[1]
      \Procedure{MergeSortFramework}{$S$, $n$}
        \State $\mathcal{R} \gets \text{list of runs forming $S$}$
        \State $\mathcal \QQ \gets \text{empty stack}$
        \While{$\mathcal{R} \neq \emptyset$ or $\mathcal \QQ$ has $>1$ member}
          \Choose {to do either {\bf (A)} or
                  {\bf (B${}_i$)} for some $\ell  -  k_2 < i < \ell$}, based on
            \State whether $\mathcal R$ is empty and
                     on the values $|\QQ_j|$ for $\ell  -  k_1 < j \le \ell$
    \def\ChoicesIndent{2.2em}
            \State \hbox to\ChoicesIndent{\bf (A)}%
                    Remove the next run $R$ from $\mathcal{R}$
                    and push it onto $\mathcal \QQ$.
            \State \kern\ChoicesIndent This increments $\ell = |\mathcal \QQ|$ by 1.
            \State \hbox to\ChoicesIndent{\bf (B${}_i$)}%
                    Replace $\QQ_i$ and $\QQ_{i+1}$ in $\mathcal \QQ$ with
                    $\Merge(\QQ_i, \QQ_{i+1})$.
            \State \kern\ChoicesIndent This decrements $\ell = |\mathcal \QQ|$ by 1.
          \EndChoose
        \EndWhile
        \State Return $\QQ_1$ as the sorted version of~$S$.
      \EndProcedure
    \end{algorithmic}
    \caption{
        The basic framework for all our merge sort algorithms.
        \newline $S$ is a
        sequence of integers of length~$n$. $\mathcal{R}$~is the list of $m$
        runs formed from~$S$.
        \newline
        $\mathcal \QQ$~is a stack of $\ell$~runs,
        $\QQ_1, \QQ_2, \ldots, \QQ_\ell$. The top member of the
        stack~$\mathcal \QQ$ is $\QQ_\ell$.
        \newline
        $k_1$ and $k_2$ are fixed (small)
        integers. The algorithm is $(k_1,k_2)$-aware.
        \newline
        Upon termination, $\mathcal \QQ$ contains a single run~$\QQ_1$
        which is the sorted version of~$S$.
    }
    \label{alg:basic}
\end{algorithm}

Algorithm~\ref{alg:basic} is called {\em $(k_1,k_2)$-aware} since its choice of
what to do is based on just the lengths of the runs in the top $k_1$ members of
the stack~$\mathcal \QQ$, and since merges are only applied to runs in the top
$k_2$ members of~$\mathcal \QQ$. (\cite{ANP:MergeStrategies} used the terminology
``degree'' instead of ``aware''.) In all our applications, $k_1$ and $k_2$ are
small numbers, so it is appropriate to store the runs in a stack. Usually
$k_1=k_2$, and we write ``$k$-aware'' instead of ``$(k,k)$-aware''.
Table~\ref{tab:awareness} shows the awareness values for the algorithms
considered in this paper. To improve
readability (and following \cite{ANP:MergeStrategies}), we use the letters
$W,X,Y,Z$ to denote the top four runs on the stack,
$\QQ_{\ell - 3}, \QQ_{\ell - 2}, \QQ_{\ell - 1}, \QQ_\ell$ respectively, (if they
exist).\footnote{\cite{Peters:TimSortPost} used ``$A,B,C$'' for ``$X,Y,Z$''.}

\begin{table}[ht]
    \begin{center}
        \begin{tabular}{cc}
            Algorithm & Awareness \\
            \hline
            Timsort (original) \cite{Peters:TimSortPost} & 3-aware \\
            Timsort (corrected) \cite{GRBBH:TimSortBug} & (4,3)-aware \\
            $\alpha$-stack sort \cite{ANP:MergeStrategies} & 2-aware \\
            Shivers sort \cite{Shivers:mergesort} & 2-aware \\
            $2$-merge sort & 3-aware \\
            $\alpha$-merge sort ($\varphi{<}\alpha{<}2$) & 3-aware
        \end{tabular}
    \end{center}
    \caption{Awareness levels for merge sort algorithms}
    \label{tab:awareness}
\end{table}

In all the sorting algorithms we consider, the height of the stack~$\mathcal \QQ$
will be small, namely $\ell = O(\log n)$. Since the stack needs only store the
$\ell +  1$ values $p_1, \ldots, p_{\ell + 1}$, the memory requirements for the stack
are minimal. Another advantage of Algorithm~\ref{alg:basic} is that runs
may be identified on the fly and that merges occur only near the top of the
stack: this may help reduce cache misses.
(See~\cite{LaMarcaLadner:CacheSorting} for other methods for reducing cache
misses.)

$\Merge(A,B)$ is the run obtained by stably merging $A$ and~$B$. Since it takes
only linear time to extract the runs $\cal R$ from~$S$, the computation time of
Algorithm~\ref{alg:basic} is dominated by the time needed for merging runs. As
already mentioned, we use $|A| + |B|$ as the mathematical model for the runtime
of a (stable)
merge. The usual algorithm for merging $A$ and $B$ uses an auxiliary buffer to
hold the smaller of $A$ and $B$, and then merging directly into the combined
buffer: this has runtime proportional to $|A| + |B|$.
Timsort~\cite{Peters:TimSortPost} uses a variety of techniques to speed
up merges, in particular ``galloping''; this also still takes time proportional
to $|A| + |B|$ in general. It is possible to perform (stable) merges in-place
with no additional memory~\cite{%
Kronrod:inplacemerging,
MannilaUkkonen:inplacemerging,
HuangLangston:inplacemerging,
Symvonis:inplacemerging,
GKP:inplacemerging}; these algorithms also require time $\Theta(|A| + |B|)$.

More complicated data structures can perform merges in sublinear time in some
situations; see for instance~\cite{CLP:SublinearMerging,HMW:Presorted}. These
methods do not seem to be useful in practical applications, and of course still
have worst-case run time $\Theta(|A| + |B|)$.

If $A$ and $B$ have very unequal lengths, there are (stable) merge algorithms
which use fewer than $\Theta(|A| + |B|)$
comparisons~\cite{HwangLin:merging,Symvonis:inplacemerging,GKP:inplacemerging}.
Namely, if $|A| \le |B|$, then it is possible to merge
$A$ and $B$ in time $O(|A| +  |B|)$ but using only $O(|A| (1 + \log(|B| / |A|)))$
comparisons.
Nonetheless, we feel that the cost $|A| + |B|$
is the best way to model the runtime of merge sort algorithms. Indeed,
the main strategies for speeding up merge sort algorithms try
to merge runs of approximate equal length as much as possible;
thus $|A|$ and $|B|$
are very unequal only in special cases. Of course, all our upper bounds
on merge cost are also upper bounds on number of comparisons.

\begin{definition}
    The {\em merge cost} of a merge sort algorithm on an input~$S$ is the sum of
    $|A| + |B|$ taken over all merge operations $\Merge(A,B)$ performed.  For
    $\QQ_i$ a run on the stack~$\mathcal \QQ$ during the computation, the {\em merge
    cost} of $\QQ_i$ is the sum of $|A| + |B|$ taken over all merges $\Merge(A,B)$
    used to combine runs that form~$\QQ_i$.
\end{definition}

Our definition of merge cost is motivated primarily by analyzing the run time
of {\em sequential} algorithms. Nonetheless, the constructions may be applicable
to distributed sorting algorithms, such as in
MapReduce~\cite{DeanGhemawat:mapreduce}. A distributed sorting algorithm
typically performs sorts on subsets of the input on multiple processors: each
of these can take advantage of a faster sequential merge sort algorithm.
Applications to distributed sorting are beyond the scope of the present paper
however.


In later sections, the notation $w_{\QQ_i}$ is used to denote
the merge cost of the $i$-th entry $\QQ_i$ on the stack at a given
time. Here ``$w$'' stands for ``weight''.
The notation $|\QQ_i|$ denotes the length of the run~$\QQ_i$.
We use $m_{\QQ_i}$ to denote the number of original runs
which were merged to form~$\QQ_i$.

All of the optimizations used by Timsort
mentioned above can be used equally well
with any of the merge sort algorithms discussed in the present paper.
In addition, they can be used with other sorting algorithms that
generate and merge runs. These other algorithms include patience
sort~\cite{ChandramouliGoldstein:Patience}, melsort~\cite{Skiena:Encroaching}
(which is an extension of patience sort), the hybrid quicksort-melsort
algorithm of~\cite{LevcopoulosPetersson:SortingShuffled}, and split
sort~\cite{LevcopoulosPetersson:SplitSort}.  The merge cost as defined above
applies equally well to all these algorithms. Thus, it gives a runtime
measurement which applies to a broad range of sort algorithms that incorporate
merges and which is largely independent of which optimizations are used.

Algorithm~\ref{alg:basic}, like all the algorithms we discuss,
only merges adjacent elements, $\QQ_i$ and~$\QQ_{i+1}$, on the stack.
This is necessary for the sort to be stable: If $i<i^\pprime<i^\prime$
and two non-adjacent runs
$\QQ_i$ and $\QQ_{i^\prime}$ were merged, then we would not know how to order
members occurring in both $\QQ_{i^\pprime}$ and $\QQ_i\cup \QQ_{i^\prime}$.
The patience sort, melsort, and split sort can all readily be modified
to be stable,
and our results on merge costs can be applied to them.

Our merge strategies
do not apply to non-stable merging,
but Barbay and Navarro~\cite{BarbayNavarro:AdaptiveSorting} have given an
optimal method---based on Huffmann codes---of merging for non-stable
merge sorts in which merged runs do not need to be adjacent.

The known worst-case upper and lower bounds
on stable natural merge sorts are listed in
Table~\ref{tab:allbounds}. The table expresses bounds in
the strongest forms known. Since $m \le n$, it is
generally preferable
to have upper bounds in
terms of $n \log m$, and lower bounds
in terms of $n \log n$.

\begin{table}[ht]
\begin{center}
\begin{tabular}{ccc}
Algorithm & Upper bound & Lower bound \\[1ex]
\hline
Timsort\footnotemark
    & $\left\{\begin{array}{c}
    O(n \log n) ~\hbox{\cite{AJNP:Timsort,ANP:MergeStrategies}} \\
    O(n \log m) ~\hbox{\cite{AJNP:Timsort}}
    \end{array}\right\}$
    & $1.5 \cdot n \log n$ [Theorem~\ref{thm:timsortLower}] \\[1ex]
$\alpha$-stack sort
    & $O(n \log n)$ \hbox{\cite{ANP:MergeStrategies}}
    & $\left\{ \begin{array}{ll}
               c_\alpha \cdot n \log n & \hbox{[Theorem~\ref{thm:alphaLower}]} \\
               \omega (n \log m) & \hbox{[Theorem~\ref{thm:alphaLowerLogm}]}
               \end{array}
       \right.$ \\[2ex]
Shivers sort
    & $\left\{ \begin{array}{c}
          \hbox{$n \log n$ \cite{Shivers:mergesort}} \\
          \hbox{See also Theorem~\ref{thm:ShiversUpperBound}}
      \end{array} \right\}$
    & \hbox{$\omega (n \log m)$ [Theorem~\ref{thm:ShiversLowerBound}]} \\[2ex]
$2$-merge sort
    & $c_2 \cdot n \log m$ [Theorem \ref{thm:twomergeUpper}]         % XXX
    & $c_2 \cdot n \log n$ [Theorem \ref{thm:twomergeLower}] \\[1ex] % XXX
$\alpha$-merge sort
    & $c_\alpha \cdot n \log m$ [Theorem \ref{thm:alphamergeUpper}]  % XXX
    & $c_\alpha \cdot n \log n$ [Theorem \ref{thm:alphamergeLower}]  % XXX
\end{tabular}
\end{center}
\caption{Upper and lower bounds on the merge cost of various algorithms.
For more precise statements, see the theorems. The results hold for
$\varphi < \alpha \le 2$; for these values,
$c_\alpha$ is defined by equation~(\ref{eq:cAlphaDef}) and satisfies
$1.042<c_\alpha<1.089$.
In particular, $c_2 = 3/\log(27/4) \approx 1.08897$.
All bounds are asymptotic; that is, they are correct up to
a multiplicative factor of $1 \pm o(1)$.  For this reason,
the upper and lower bounds in the last two lines of the
the table are not exactly matching.
The table lists $2$-merge sort and $\alpha$-merge sort on
separate lines since $2$-merge sort is slightly simpler
than $\alpha$-merge sort.  In addition, our proof of the upper bound
for $2$-merge sort is substantially
simpler than our proof for $\alpha$-merge sort.}
\label{tab:allbounds}
\end{table}

\footnotetext{When the first
draft of the present paper was circulated, the question of an $O(n \log m)$
upper bound for Timsort was still open; this was subsequently resolved by~\cite{AJNP:Timsort}.}

The main results of the paper are those listed in the final two lines of
Table~\ref{tab:allbounds}. Theorem~\ref{thm:twomergeUpper} proves that the merge
cost of $2$-merge sort is at most $(d_2 + c_2\log m)\cdot n$, where $d_2\approx
1.911$ and $c_2\approx 1.089$: these are very tight bounds, and the value
for~$c_2$ is optimal by Theorem~\ref{thm:twomergeLower}. It is also
substantially better than the worst-case merge cost for Timsort proved in
Theorem~\ref{thm:timsortLower}. Similarly for $\varphi < \alpha < 2$,
Theorem~\ref{thm:twomergeUpper} proves an upper bound of $(d_\alpha+c_\alpha\log
m)\cdot n$. The values for $c_\alpha$ are optimal by
Theorem~\ref{thm:alphamergeLower}; however, our values for $d_\alpha$ have
unbounded limit $\lim_{\alpha \rightarrow \varphi^+}d_\alpha$ and we conjecture
this is not optimal.

We only analyze $\alpha$-merge sorts with $\alpha > \varphi$.
For $\varphi < \alpha \le 2$, the $\alpha$-merge sorts improve on Timsort,
by virtue of having better run time bounds and by being slightly easier
to implement. In addition, they perform better in the experiments reported
in Section~\ref{sec:experiments}.  It is an open
problem to extend our algorithms to the case of $\alpha < \varphi$; we expect
this will require $k$-aware algorithms with $k > 3$.

Table~\ref{tab:allbounds} does not include two new stable merge sorts
that were developed by Munro-Wild~\cite{MunroWild:MergeSort}
and Jug\'e~\cite{Juge:AdaptiveShivers} subsequently to the circulation
of the first draft of the present paper. Jug\'e's ``adaptive Shivers'' sort
is 3-aware. Munro and Wild's ``powersort'' algorithm is 3-aware except that it requires
knowing the value of~$n$ ahead of time. Both of these sorting alorithms have
asymptotic lower and upper bounds on running time of $n \log m$.

The outline of the paper is as follows. Section~\ref{sec:timsort} describes
Timsort, and proves the lower bound on its merge cost. Section~\ref{sec:ANP}
discusses the $\alpha$-stack sort algorithms, and gives lower bounds on their
merge cost. Section~\ref{sec:shivers} describes the Shivers sort, and gives a
simplified proof of the $n\log n$ upper bound of~\cite{Shivers:mergesort}.
Section~\ref{sec:alphaMerge} is the core of the paper and describes the new
$2$-merge sort and
$\alpha$-merge sort. We first a simple $O(n \log m)$ upper bound on
their merge cost. After that, we prove the lower bounds on their merge costs, and
finally prove the corresponding sharp upper bounds. Section~\ref{sec:experiments}
gives some experimental results on two kinds of randomly generated data. It includes
experimental comparisons also with the recent adaptive Shivers sort of Jug\'e and
powersort of Munro and Wild.  All
these sections can be read independently of each other. The paper concludes with
discussion of open problems.

We thank the referees of~\cite{BussKnop:StableMergeSortSODA}
for useful comments and suggestions, and Vincent Jug\'e
for sharing information on his unpublished work.


\section{Timsort lower bound}\label{sec:timsort}

Algorithm~\ref{alg:timsort} is the Timsort algorithm
as defined by~\cite{GRBBH:TimSortBug} improving
on~\cite{Peters:TimSortPost}. Recall
that $W,X,Y,Z$ are the top four elements
on the stack~$\mathcal \QQ$. A command ``Merge $Y$ and $Z$''
creates a single run which replaces both $Y$ and~$Z$ in the
stack; at the same time, the current third member on the
stack, $X$, becomes the new second member on the stack
and is now designated~$Y$.  Similarly, the current $W$
becomes the new $X$, etc.  Likewise, the command
``Merge $X$ and~$Y$'' merges the second and third
elements at the top of~$\mathcal \QQ$; those two elements
are removed from~$\mathcal \QQ$ and replaced by the result
of the merge.

\begin{algorithm}
\begin{algorithmic}[1]
\Procedure{$\text{TimSort}$}{$S$, $n$}
  \State $\mathcal{R} \gets \text{run decomposition of }S$
  \State $\mathcal \QQ \gets \emptyset$
  \While{$\mathcal{R} \neq \emptyset$}
    \State Remove the next run $R$ from $\mathcal{R}$ and push it onto~$\mathcal \QQ$
    \Loop
      \If{$|X| < |Z|$}              \label{algline:timXZ}
        \State{Merge $X$ and $Y$}
      \ElsIf{$|X| \le |Y| + |Z|$}   \label{algline:timXYZ}
        \State{Merge $Y$ and $Z$}
      \ElsIf{$|W| \le |X| + |Y|$}   \label{algline:timWXY}
        \State{Merge $Y$ and $Z$}
      \ElsIf{$|Y| \le |Z|$}         \label{algline:timYZ}
        \State{Merge $Y$ and $Z$}
      \Else
        \State Break out of the loop
      \EndIf
    \EndLoop
  \EndWhile
  \While{$|\mathcal \QQ| \ge 1$}
    \State{Merge $Y$ and $Z$}
  \EndWhile
\EndProcedure
\end{algorithmic}
\caption{The Timsort algorithm. $W,X,Y,Z$ denote the top four elements
of the stack~$\mathcal \QQ$. A test involving a stack member that does not
exist evaluates as ``False''. For example, $|X| < |Z|$ evaluates as false
when $|\mathcal \QQ| < 3$ and $X$ does not exist.}
\label{alg:timsort}
\end{algorithm}

Timsort was designed so that the stack has size $O(\log n)$,
and the total running time is $O(n\log n)$.
These bounds were first proved by~\cite{ANP:MergeStrategies};
simplified proofs were
given by~\cite{AJNP:Timsort} who also strengthed the
upper bound to $O(n \log m)$.

\begin{theorem}[\cite{AJNP:Timsort,ANP:MergeStrategies}]
    \label{thm:timsortUpper}
    The merge cost of Timsort is $O(n \log n)$, and even $O(n \log m)$.
\end{theorem}

\noindent
The proof in
Auger et al.~\cite{ANP:MergeStrategies} did not compute the constant implicit
in their proof of the upper bound of Theorem~\ref{thm:timsortUpper}; but it is
approximately equal to $3 / \log \varphi \approx 4.321$. The proofs
in~\cite{AJNP:Timsort} also do not quantify the constants in the big-O
notation, but they are comparable or slightly larger.
We prove a
corresponding lower bound.

\begin{theorem}\label{thm:timsortLower}
  The worst-case merge cost of the Timsort algorithm on inputs of length~$n$
  which decompose into $m$ original runs is $\ge (1.5 - o(1))\cdot n \log n$.
  Hence it is also $\ge (1.5 - o(1))\cdot n \log m$.
\end{theorem}

In other words, for any $c < 1.5$, there are
inputs to Timsort with arbitrarily large values for
$n$ (and~$m$) so that Timsort has merge cost
$> c \cdot n \log n$.
We conjecture that
Theorem~\ref{thm:timsortUpper} is nearly optimal:
\begin{conjecture}\label{conj:timsortUpper}
  The merge cost of Timsort is bounded by $(1.5 +  o(1))\cdot n \log m$.
\end{conjecture}
Vincent Jug\'e [personal communication, November 2018] has recently
succeeded in proving this conjecture.

\begin{proof}[Proof of Theorem~\ref{thm:timsortLower}]
  We must define inputs that cause Timsort to take time close to
  $1.5 n \log n$.  As always, $n\ge 1$ is the length of the input~$S$ to be
  sorted. We define $\Rtim (n)$ to be a sequence of run {\em lengths} so that
  $\Rtim(n)$ equals $\langle n_1, n_2 \ldots, n_m\rangle$ where each $n_i > 0$
  and  $\sum_{i = 1}^m n_i = n$.  Furthermore, we will have $m\le n \le 3m$, so
  that  $\log n = \log m + O(1)$. The notation $\Rtim$ is reminiscent of
  $\mathcal R$, but $\mathcal R$ is a sequence of runs whereas $\Rtim$ is a
  sequence of run lengths. Since the merge cost of Timsort depends only on the
  lengths of the runs, it is more convenient to work directly with the sequence
  of run lengths.

  The sequence $\Rtim(n)$, for $1\le n$, is defined as follows.\footnote{%
  For purposes of this proof, we allow run lengths to equal~1. Strictly
  speaking, this cannot occur since all original runs will have length at
  least~2. This is unimportant for the proof however, as the
  run lengths~$\Rtim(n)$ could be doubled and the asymptotic analysis needed
  for the proof would be essentially unchanged.}
  First, for $n\le 3$, $\Rtim(n)$ is the sequence~$\langle n \rangle$,
  i.e., representing a single run of length~$n$.
  Let $n^\prime = \lfloor n/2\rfloor$.
  For even $n \ge 4$, we have $n = 2n^\prime$ and
  define $\Rtim(n)$ to be the concatenation of
  $\Rtim(n^\prime)$, $\Rtim(n^\prime - 1)$ and $\langle 1 \rangle$.
  For odd $n\ge 4$, we have $n = 2n^\prime +  1$
  and define $\Rtim(n)$ to be the concatenation of $\Rtim(n^\prime)$,
  $\Rtim(n^\prime - 1)$ and~$\langle 2\rangle$.

  We claim that for $n\ge 4$, Timsort operates with run lengths $\Rtim(n)$ as
  follows: The first phase processes the runs from $\Rtim(n^\prime)$
  and merges them into a single run of length $n^\prime$
  which is the only element of the stack~$\mathcal \QQ$.
  The second phase processes the runs from $\Rtim(n^\prime - 1)$
  and merges them also into a single run of length $n^\prime - 1$;
  at this point the stack contains two runs, of lengths $n^\prime$
  and $n^\prime - 1$. Since $n^\prime - 1 < n^\prime$, no further merge
  occurs immediately. Instead, the final run is loaded onto the stack: it
  has length $n^\pprime$ equal to
  either 1 or~2. Now $n^\prime \le n^\prime - 1 + n^\pprime$ and
  the test $|X| \le |Y| + |Z|$ on line~\ref{algline:timXYZ}
  of Algorithm~\ref{alg:timsort} is triggered, so Timsort merges
  the top two elements of the stack, and then the
  test $|Y|\le|Z|$ causes the merge of the final two elements
  of the stack.

  This claim follows from
  Claim~\ref{clm:timsortLower}. We say that the stack~$\mathcal \QQ$ is
  {\em stable} if none of the tests on lines
  \ref{algline:timXZ}, \ref{algline:timXYZ}, \ref{algline:timWXY}, \ref{algline:timYZ},
  of Algorithm~\ref{alg:timsort} hold.

  \begin{claim}\label{clm:timsortLower}
  Suppose that $\Rtim(n)$ is the initial subsequence of
  a sequence~$\mathcal R^\prime$ of run lengths,
  and that Timsort is initially started with
  run lengths~$\mathcal R^\prime$ either
  (a)~with the stack~$\mathcal \QQ$ empty or
  (b)~with the top element of~$\mathcal \QQ$ a run of length $n_0>n$
  and the second element of~$\mathcal \QQ$ (if it exists)
  a run of length $n_1 > n_0 +  n$.
  Then Timsort will start by processing exactly the runs whose lengths
  are those of $\Rtim(n)$, merging them into a single run
  which becomes the new top element of~$\mathcal \QQ$.
  Timsort will do this without performing any merge of runs
  that were initially in~$\mathcal \QQ$ and without
  (yet) processing any of the remaining runs in~$\mathcal R^\prime$.
  \end{claim}

  Claim~\ref{clm:timsortLower} is proved by induction on~$n$. The base case, where
  $n\le 3$, is trivial since with $\mathcal \QQ$ stable, Timsort immediately reads
  in the first run from~$\mathcal R^\prime$. The case of $n\ge 4$ uses the
  induction hypothesis twice, since $\Rtim(n)$ starts off with $\Rtim(n^\prime)$
  followed by $\Rtim(n^\prime  -  1)$. The induction hypothesis applied to
  $n^\prime$ implies that the runs of $\Rtim(n^\prime)$ are first processed and
  merged to become the top element of~$\mathcal \QQ$. The stack elements $X,Y,Z$
  have lengths $n_1,n_0,n^\prime$ (if they exist), so the stack is now stable. Now
  the induction hypothesis for $n^\prime - 1$ applies, so Timsort next loads and
  merges the runs of $\Rtim(n^\prime  -  1)$. Now the top stack elements $W,X,Y,Z$ have
  lengths $n_1,n_0,n^\prime,n^\prime  -  1$ and $\mathcal \QQ$~is again stable.
  Finally, the single run of length~$n^\pprime$ is loaded onto the stack. This
  triggers the test $|X|\le |Y| + |Z|$, so the top two elements are merged. Then the
  test $|Y| \le |Z|$ is triggered, so the top two elements are again merged. Now
  the top elements of the stack (those which exist) are runs of length
  $n_1,n_0,n$, and Claim~\ref{clm:timsortLower} is proved.

  Let $c(n)$ be the merge cost of the Timsort algorithm on the sequence~$\Rtim(n)$
  of run lengths. The two merges described at the end of the proof of
  Claim~\ref{clm:timsortLower} have merge cost $(n^\prime - 1) + n^\pprime$ plus
  $n^\prime + (n^\prime - 1) + n^\pprime = n$. Therefore, for $n > 3$, $c(n)$
  satisfies
  \begin{equation} \label{eq:ctimrecur}
      c(n)
      ~=~
      \begin{cases}
          c(n^\prime) + c(n^\prime - 1) + \frac{3}{2} n ~
          & \hbox{if $n$ is even} \\[0.2ex]
          c(n^\prime) + c(n^\prime - 1) + \frac{3}{2} n + \frac 12 ~
          & \hbox{if $n$ is odd. } \\
      \end{cases}
  \end{equation}
  Also, $c(1) = c(2) = c(3) = 0$ since no merges are needed.
  Equation~(\ref{eq:ctimrecur}) can be summarized as
  \[
      c(n) ~=~
          c(\lfloor n / 2 \rfloor) + c(\lfloor n/2 \rfloor  -  1)
          + {\textstyle \frac 32} n + {\textstyle \frac 12} (n \bmod 2).
  \]
  The function $n \mapsto \frac 32 n + \frac 12 (n \bmod 2)$ is strictly
  increasing. So, by induction, $c(n)$ is strictly increasing for $n \ge 3$.
  Hence $c(\lfloor n/2 \rfloor) > c(\lfloor n/2 \rfloor  -  1)$, and
  thus $c(n) \ge 2 c(\lfloor n/2 \rfloor  -  1) + {\textstyle \frac 32}n$ for
  all $n > 3$.

  For $x\in \mathbb R$, define $b(x) = c(\lfloor x - 3 \rfloor)$. Since $c(n)$ is
  nondecreasing, so is $b(x)$. Then
  \begin{eqnarray*}
      b(x) &=& c(\lfloor x  -  3 \rfloor)
        ~\ge~ 2 c(\lfloor (x  -  3)/2 \rfloor - 1 ) + \frac 32 (x  -  3) \\
        &\ge& 2 c(\lfloor x / 2 \rfloor - 3 ) + \frac 32 (x  -  3)
        ~=~ 2 b(x / 2) + \frac 32 (x  -  3).
  \end{eqnarray*}
  \begin{claim} \label{clm:timsortLower2}
      For all $x\ge 3$,
      $b(x) \ge \frac 32\cdot [x(\lfloor \log x \rfloor-2) - x + 3 ]$.
  \end{claim}
  We prove the claim by induction, namely by induction on~$n$ that it holds for
  all $x<n$. The base case is when $3 \le x < 8$ and is trivial since the lower
  bound is negative and $b(x)\ge0$. For the induction step, the claim is known to
  hold for $x/2$. Then, since $\log (x / 2) = (\log x) - 1$,
  \begin{eqnarray*}
    b(x) &\ge& 2 \cdot b(x/2) + \frac 32 (x - 3) \\
         &\ge& 2 \cdot \Bigl( \frac 32 \cdot \bigl[ (x/2)(\lfloor \log x \rfloor - 3)
                               - x/2 + 3 \bigr] \Bigr)
               + \frac 32 ( x - 3 ) \\
         &=& \frac 32 \cdot [ x(\lfloor \log x \rfloor-2) - x + 3 ]
  \end{eqnarray*}
  proving the claim.

  Claim~\ref{clm:timsortLower2} implies that $c(n) = b(n +  3) \ge
  (\frac 32 - o(1))\cdot n \log n$.
  This proves Theorem~\ref{thm:timsortLower}.
\end{proof}

\section{The \texorpdfstring{$\alpha$}{a}-stack sort}\label{sec:ANP}

Augur-Nicaud-Pivoteau~\cite{ANP:MergeStrategies} introduced the $\alpha$-stack
sort as a $2$-aware stable merge sort; it was inspired by Timsort and designed
to be simpler to implement and to have a simpler analysis. (The algorithm~(e2)
of~\cite{ZML:SortRace} is the same as $\alpha$-stack sort with $\alpha=2$.)
Let $\alpha > 1$ be a constant.  The $\alpha$-stack sort is shown in
Algorithm~\ref{alg:alphaStackSort}. It makes less effort than Timsort to
optimize the order of merges: up until the run decomposition is exhausted, its
only merge rule is that $Y$ and $Z$ are merged whenever $|Y| \le \alpha |Z|$. An
$O(n \log n)$ upper bound on its runtime is given by the next theorem.

\begin{algorithm}
    \begin{algorithmic}[1]
        \Procedure{$\alpha\text{-stack}$}{$S$, $n$}
          \State $\mathcal{R} \gets \text{run decomposition of }S$
          \State $\mathcal \QQ \gets \emptyset$
          \While{$\mathcal{R} \neq \emptyset$}
            \State Remove the next run $R$ from $\mathcal{R}$ and push it onto~$\mathcal \QQ$
            \While{$|Y| \le \alpha |Z|$} \label{algline:astackYZ}
              \State{Merge $Y$ and $Z$}  \label{algline:astackMergeYZ1}
            \EndWhile                    \label{algline:astackinnerloopB}
          \EndWhile
          \While{$|\mathcal \QQ| \ge 1$}  \label{algline:astackfinalloopA}
            \State{Merge $Y$ and $Z$}
          \EndWhile                      \label{algline:astackfinalloopB}
        \EndProcedure
    \end{algorithmic}
    \caption{The $\alpha$-stack sort. $\alpha$ is a constant $>1$.}
    \label{alg:alphaStackSort}
\end{algorithm}

\begin{theorem}[\cite{ANP:MergeStrategies}]\label{thm:alphaUpper}
    Fix $\alpha > 1$. The merge cost for $\alpha$-stack sort is
    $O(n \log n)$.
\end{theorem}
\cite{ANP:MergeStrategies}~did not explicitly mention the
constant implicit in this upper bound, but their proof establishes
a constant equal to approximately $(1 +  \alpha)/\log \alpha$. For instance,
for $\alpha=2$, the merge cost is bounded by
$(3 +  o(1))n \log n$. The constant is minimized at
$\alpha\approx 3.591$, where is it approximately 2.489.

\begin{theorem}\label{thm:alphaLower}
    Let $1 < \alpha$.
    The worst-case merge cost of $\alpha$-stack sort
    on inputs of length~$n$
    is $\ge (c_\alpha - o(1))\cdot n \log n$,
    where
    $c_\alpha$ equals
    $\frac{\alpha + 1}{(\alpha + 1) \log(\alpha + 1) - \alpha \log(\alpha)}$.
\end{theorem}

The proof of Theorem~\ref{thm:alphaLower} is postponed
until Theorem~\ref{thm:alphamergeLower} proves a
stronger lower bound for
$\alpha$-merge sorts; the same construction works to
prove both theorems. The value $c_\alpha$ is quite
small, e.g., $c_2 \approx 1.089$; this is
is discussed more in Section~\ref{sec:alphaMerge}.

The lower bound of Theorem~\ref{thm:alphaLower} is not very strong since the
constant is close to~1.  In fact, since a binary tree of merges gives a merge
cost of $n \lceil\log m \rceil$, it is more relevant to give upper bounds in terms of
$O(n \log m)$ instead of $O(n \log n)$.  The next theorem shows that
$\alpha$-stack sort can be very far from optimal in this respect.

\begin{theorem}\label{thm:alphaLowerLogm}
  Let $1 < \alpha$. The worst-case merge cost of $\alpha$-stack sort on
  inputs of length~$n$ which decompose into $m$ original runs is
  $\omega(n \log m)$.
\end{theorem}
\noindent
In other words, for any $c > 0$, there are
inputs with arbitrarily large values for
$n$ and~$m$ so that $\alpha$-stack sort has merge cost
$> c \cdot n \log m$.

\begin{proof}
  Let $s$ be the least integer such that $2^s\ge \alpha$. Let $\Rastack(m)$ be
  the sequence of run lengths
  \[
    \langle \, 2^{(m - 1)\cdot s}  -  1, \,
             2^{(m - 2)\cdot s}  -  1, \,
             \ldots, 2^{3s} - 1, \, 2^{2s} - 1, \, 2^s - 1, \,
             2^{m\cdot s}
    \rangle.
  \]
  $\Rastack(m)$ describes $m$ runs whose lengths sum to
  $n = 2^{m\cdot s} + \sum_{i = 1}^{m - 1} (2^{i \cdot s} - 1)$,
  so $ 2^{m\cdot s} < n < 2^{m\cdot s+1}$.
  Since $2^s\ge \alpha$, the test $|Y| \le \alpha |Z|$ on
  line~\ref{algline:astackYZ} of
  Algorithm~\ref{alg:alphaStackSort} is triggered
  only when the run of length $2^{ms}$ is loaded onto
  the stack~$\mathcal \QQ$; once this happens the runs are all
  merged in order from right-to-left.
  The total cost of the merges is
  $(m - 1)\cdot 2^{ms} + \sum_{i=1}^{m-1} i\cdot (2^{i\cdot s}  -  1)$
  which is certainly greater than $(m - 1)\cdot 2^{ms}$.  Indeed,
  that comes from the fact that the final run in~$\Rastack(m)$ is
  involved in $m-1$ merges.  Since $n<2\cdot 2^{m\cdot s}$,
  the total merge cost is greater than $\frac n2(m - 1)$, which
  is $\omega(n\log m)$.
\end{proof}

\section{The Shivers merge sort}\label{sec:shivers}

The 2-aware Shivers sort~\cite{Shivers:mergesort}, shown in
Algorithm~\ref{alg:shiversSort},
is similar to $2$-stack sort, but
with a modification that makes a surprising improvement
in the bounds on its merge cost. Although never published,
this algorithm was presented in 1999.

\begin{algorithm}
\begin{algorithmic}[1]
\Procedure{ssort}{$S$, $n$}
  \State $\mathcal{R} \gets \text{run decomposition of }S$
  \State $\mathcal \QQ \gets \emptyset$
  \While{$\mathcal{R} \neq \emptyset$}     \label{algline:shiversLoopA}
    \State Remove the next run $R$ from $\mathcal{R}$ and push it onto~$\mathcal \QQ$
    \While{$2^{\lfloor{\log |Y|}\rfloor} \le |Z|$}  \label{algline:shiversYZ}
      \State{Merge $Y$ and $Z$}
    \EndWhile                               \label{algline:ShiversInnerLoopB}
  \EndWhile                                \label{algline:shiversLoopB}
  \While{$|\mathcal \QQ| \ge 1$} \label{algline:shiversEndA}
    \State{Merge $Y$ and $Z$}
  \EndWhile                     \label{algline:shiversEndB}
\EndProcedure
\end{algorithmic}
\caption{The Shivers sort.}
\label{alg:shiversSort}
\end{algorithm}


The only difference between the Shivers sort
and $2$-stack sort is the test used to decide
when to merge. Namely, line~\ref{algline:shiversYZ}
tests $2^{\lfloor \log |Y|\rfloor} \le |Z|$ instead
of $|Y| \le 2\cdot|Z|$. Since $2^{\lfloor \log |Y|\rfloor}$
is $|Y|$ rounded down to the nearest power of two,
this is somewhat like an $\alpha$-sort with
$\alpha$ varying dynamically in the range $[1,2)$.

The Shivers sort has the same
undesirable lower
bound as $2$-stack sort in terms of $\omega(n \log m)$:

\begin{theorem}\label{thm:ShiversLowerBound}
  The worst-case merge cost of the Shivers sort
  on inputs of length~$n$ which decompose into $m$
  original runs is $\omega(n \log m)$.
\end{theorem}

\begin{proof}
  This is identical to the proof of Theorem~\ref{thm:alphaLowerLogm}.
  We now let $\Rshiver(m)$ be the sequence of run lengths
  \[
    \langle \, 2^{m - 1}  -  1, \, 2^{m - 2}  -  1,
      \, \dots, 7, \, 3, \, 1, \, 2^m \rangle,
  \]
  and argue as before.
\end{proof}

\begin{theorem}[\cite{Shivers:mergesort}]\label{thm:ShiversUpperBound}
  The merge cost of Shivers sort is $(1  +   o(1)) n \log n$.
\end{theorem}

We present a proof which is simpler than that of~\cite{Shivers:mergesort}.
The proof of Theorem~\ref{thm:ShiversUpperBound}
assumes that at a given point in time, the stack~$\mathcal \QQ$ has $\ell$
elements $\QQ_1, \ldots, \QQ_\ell$, and uses $w_{\QQ_i}$ to denote the merge cost of~$\QQ_i$.
We continue to use the convention that $W,X,Y,Z$ denote $\QQ_{\ell - 3},
\QQ_{\ell - 2}, \QQ_{\ell - 1}, \QQ_\ell$ if they exist.

\begin{proof}
  Define $k_{\QQ_i}$ to equal $\lfloor \log |\QQ_i| \rfloor$.
  Obviously, $|\QQ_i| \ge 2^{k_{\QQ_i}}$. The
  test on line~\ref{algline:shiversYZ} works to
  maintain the invariant that each $|\QQ_{i+1}| < 2^{k_{\QQ_i}}$
  or equivalently $k_{i+1}<k_i$.
  Thus, for $i<\ell - 1$, we always
  have $|\QQ_{i+1}| < 2^{k_{\QQ_i}}$ and $k_{i+1}<k_i$.
  This condition can be momentarily violated for $i=\ell - 1$,
  i.e.\ if $|Z|\ge 2^{k_Y}$ and $k_Y\le k_Z$,
  but then the Shivers sort immediately merges $Y$ and~$Z$.

  As a side remark, since each $k_{i+1}<k_i$ for $i\le \ell - 1$,
  since $2^{k_1}\le|\QQ_1|\le n$,
  and since $\QQ_{\ell - 1}\ge 1 = 2^0$, the stack height $\ell$
  is $\le 2+\log n$. (In fact, a better analysis shows
  it is $\le 1+\log n$.)

  \begin{claim}\label{clm:ShiversUpperBound}
    Throughout the execution of the main loop (lines
    {\rm \ref{algline:shiversLoopA}-\ref{algline:shiversLoopB}}), the Shivers sort
    satisfies
    \begin{itemize}
      \setlength{\parsep}{0pt}
      \setlength{\itemsep}{0pt}
      \item[\rm a.] $w_{\QQ_i} \le k_{\QQ_i}\cdot |\QQ_i|$, for all $i\le\ell$,
      \item[\rm b.] $w_Z \le k_Y \cdot |Z|$ i.e.,~$w_{\QQ_\ell} \le
        k_{\QQ_{\ell - 1}} \cdot |\QQ_\ell|$, if $\ell > 1$.
    \end{itemize}
  \end{claim}
  \noindent
  When $i = \ell$, a.\ says $w_Z \le k_Z |Z|$.
  Since $k_Z$ can be less than or greater than~$k_Y$, this
  neither implies, nor is implied by,~b.

  The lemma is proved by induction on the number of updates to the
  stack~$\mathcal \QQ$ during the loop. Initially $\mathcal \QQ$ is empty,
  and a.\ and~b.\ hold trivially.
  There are two induction cases to consider. The first case is when
  an original run is
  pushed onto~$\mathcal \QQ$. Since this run, namely~$Z$, has never
  been merged, its weight is $w_Z=0$. So b.\ certainly holds.
  For the same reason and using the induction hypothesis, a.\ holds.
  The second case is when $2^{k_Y} \le |Z|$, so $k_Y\le k_Z$, and
  $Y$ and~$Z$ are merged; here $\ell$ will decrease by~1.
  The merge cost~$w_{\YZ}$ of the combination of $Y$ and $Z$ equals
  $|Y| + |Z|+w_Y+w_Z$, so
  we must establish two things:
  \begin{itemize}
      \item[\rm a${}^\prime$.] $|Y| + |Z|+w_Y+w_Z ~\le~ k_{\YZ} \cdot (|Y| +  |Z|)$, where
          $k_{\YZ}= \lfloor \log(|Y| +  |Z|)\rfloor$.
      \item[\rm b${}^\prime$.] $|Y| + |Z|+w_Y+w_Z ~\le~ k_X \cdot (|Y| +  |Z|)$, if $\ell>2$.
  \end{itemize}
  By induction hypotheses $w_Y \le k_Y |Y|$ and
  $w_Z \le k_Y|Z|$.  Thus the lefthand sides of a${}^\prime$.\ and~b${}^\prime$.\
  are $\le (k_Y+1)\cdot(|Y| + |Z|)$.
  As already discussed, $k_Y< k_X$, therefore condition~b.\ implies that
  b${}^\prime$.\ holds. And since
  $2^{k_Y} \le |Z|$, $k_Y < k_{\YZ}$, so condition~a.\ implies that
  a${}^\prime$.\ also holds.  This
  completes the proof of Claim~\ref{clm:ShiversUpperBound}.

  Claim~\ref{clm:ShiversUpperBound} implies that
  the total merge cost incurred at the end of the main loop incurred is
  $\le \sum_i w_{\QQ_i}  \le \sum_i k_i |\QQ_i|$. Since $\sum_i \QQ_i = n$
  and each $k_i \le \log n$, the
  total merge cost is $\le n \log n$.

  We now upper bound the total merge cost incurred during the
  final loop on lines \ref{algline:shiversEndA}-\ref{algline:shiversEndB}.
  When first reaching line~\ref{algline:shiversEndA},
  we have $k_{i+1}<k_i$ for all $i\le \ell - 1$ hence
  $k_i<k_1+1-i$ and $|\QQ_i|<2^{k_1+2-i}$ for all $i\le \ell$. The final loop then
  performs $\ell - 1$ merges from
  right to left. Each $\QQ_i$ for $i < \ell$ participates
  in $i$ merge operations and $\QQ_\ell$ participates
  in $\ell - 1$ merges. The total merge cost of this is less than
  $\sum_{i=1}^\ell i\cdot|\QQ_i|$. Note that
  \[
      \sum_{i=1}^\ell i\cdot|\QQ_i|
      ~<~
      2^{k_1+2}\cdot \sum_{i=1}^\ell i \cdot 2^{-i}
      ~<~
      2^{k_1+2} \cdot 2 ~=~ 8\cdot 2^{k_1} ~\le~ 8n,
  \]
  where the last inequality follows by $2^{k_1} \le |\QQ_1| \le n$. Thus, the
  final loop incurs a merge cost $O(n)$, which is $o(n \log n)$.

  Therefore the total merge cost for the Shivers sort is bounded by $n \log n
  + o(n\log n)$.
\end{proof}

\section{The 2-merge and \texorpdfstring{$\alpha$}{a}-merge sorts}
\label{sec:alphaMerge}

This section introduces our new merge sorting algorithms, called the
``2-merge sort'' and the
``$\alpha$-merge sort'', where $\varphi < \alpha < 2$ is a fixed parameter.
These sorts are 3-aware, and this enables us to get algorithms with merge costs
$(c_\alpha \pm o(1)) n \log m$. The idea of the $\alpha$-merge sort is to
combine the construction of $2$-stack sort, with the idea from
Timsort of merging $X$ and~$Y$ instead of $Y$ and~$Z$ whenever $|X| < |Z|$.  But
unlike the Timsort algorithm shown in Algorithm~\ref{alg:timsort}, we are able
to use a 3-aware algorithm instead of a 4-aware algorithm. In addition, our
merging rules are simpler, and our provable upper bounds are tighter. Indeed,
our upper bounds for $\varphi < \alpha\le 2$ are of the form
$(c_\alpha  +   o(1))\cdot n \log m$ with $c_\alpha \le c_2 \approx 1.089$, but
Theorem~\ref{thm:timsortLower} proves a lower bound $1.5 \cdot n \log m$ for
Timsort.

Algorithms~\ref{alg:twoMergeSort} and~\ref{alg:alphaMergeSort} show
the $2$-merge sort and $\alpha$-merge
sort algorithms. Note that $2$-merge sort is almost, but not quite, the
specialization of $\alpha$-merge sort to the case $\alpha=2$.
The difference is that line~\ref{algline:2mergeXYZ} of
$2$-merge sort has a simpler {\bf while} test than the
corresponding line in the $\alpha$-merge sort algorithm.
As will be shown by the proof of Theorem~\ref{thm:twomergeUpper},
the fact that Algorithm~\ref{alg:twoMergeSort} uses this
simpler {\bf while} test makes no difference to which merge operations
are performed; in other words, it would be redundant to test the
condition $|X| < 2|Y|$.

The $2$-merge sort can also be compared to the
$\alpha$-stack sort shown in Algorithm~\ref{alg:alphaStackSort}.
The main difference is that the merge of $Y$ and~$Z$ on
line~\ref{algline:astackMergeYZ1}
of the $\alpha$-stack sort algorithm has been replaced
by the lines
lines~\ref{algline:2mergeXZ}-\ref{algline:2mergeXZend}
of the $2$-merge sort algorithm which conditionally merge~$Y$ with either $X$ or~$Z$.
For $2$-merge sort
(and $\alpha$-merge sort), the run~$Y$ is
never merged with~$Z$ if it could instead be merged with
a shorter~$X$.  The other, perhaps less crucial, difference is that the weak
inequality test on line~\ref{algline:astackYZ}
in the $\alpha$-stack sort algorithm has been replaced with a
strict inequality test on line~\ref{algline:astackYZ}
in the $\alpha$-merge sort algorithm. We have made this change since
it seems to make $2$-merge sort more efficient, for instance when
all original runs have the same length.

\begin{algorithm}
    \begin{algorithmic}[1]
        \Procedure{$2$-merge}{$S$, $n$}
          \State $\mathcal{R} \gets \text{run decomposition of }S$
          \State $\mathcal \QQ \gets \emptyset$
          \While{$\mathcal{R} \neq \emptyset$}
            \State Remove the next run $R$ from $\mathcal{R}$ and push it
            onto~$\mathcal \QQ$
            \While{$|Y| < 2 |Z|$}\label{algline:2mergeXYZ}
              \If{$|X| < |Z|$}                              \label{algline:2mergeXZ}
                \State{Merge $X$ and $Y$}
              \Else
                \State{Merge $Y$ and $Z$}
              \EndIf                                        \label{algline:2mergeXZend}
            \EndWhile                       %\label{algline:2mergeinnerloopB}
          \EndWhile
          \While{$|\mathcal \QQ| \ge 1$}     \label{algline:2mergefinalloopA}
            \State{Merge $Y$ and $Z$}
          \EndWhile                         \label{algline:2mergefinalloopB}
        \EndProcedure
    \end{algorithmic}
    \caption{The $2$-merge sort.}
    \label{alg:twoMergeSort}
\end{algorithm}

\begin{algorithm}
    \begin{algorithmic}[1]
        \Procedure{$\alpha\text{-merge}$}{$S$, $n$}
          \State $\mathcal{R} \gets \text{run decomposition of }S$
          \State $\mathcal \QQ \gets \emptyset$
          \While{$\mathcal{R} \neq \emptyset$}
            \State Remove the next run $R$ from $\mathcal{R}$ and push it
            onto~$\mathcal \QQ$
            \While{$|Y| < \alpha |Z|$ or $|X| < \alpha |Y|$}\label{algline:amergeXYZ}
              \If{$|X| < |Z|$}                              \label{algline:amergeXZ}
                \State{Merge $X$ and $Y$}
              \Else
                \State{Merge $Y$ and $Z$}
              \EndIf
            \EndWhile                       \label{algline:amergeinnerloopB}
          \EndWhile
          \While{$|\mathcal \QQ| \ge 1$}     \label{algline:amergefinalloopA}
            \State{Merge $Y$ and $Z$}
          \EndWhile                         \label{algline:amergefinalloopB}
        \EndProcedure
    \end{algorithmic}
    \caption{The $\alpha$-merge sort.  $\alpha$ is a constant such that $\varphi<\alpha<2$.}
    \label{alg:alphaMergeSort}
\end{algorithm}

We will concentrate mainly on the
cases for $\varphi < \alpha\le 2$ where $\varphi \approx 1.618$
is the golden ratio.  Values for $\alpha>2$ do not seem to give
useful merge sorts; our upper bound proof does not
work for $\alpha \le \varphi$.

\begin{definition}
    Let $\alpha\ge 1$, the constant $c_\alpha$ is defined by
    \begin{equation}\label{eq:cAlphaDef}
        c_\alpha ~=~
            \frac{\alpha + 1}{(\alpha  +   1) \log(\alpha  +   1) -
            \alpha \log(\alpha)}.
    \end{equation}
\end{definition}
For $\alpha = 2$, $c_2 = 3/\log (27/4) \approx 1.08897$.
For $\alpha = \varphi$, $c_\varphi \approx 1.042298$.
For $\alpha > 1$, $c_\alpha$ is strictly increasing as a function
of~$\alpha$. Thus, $1.042 < c_\alpha < 1.089$ when $\varphi < \alpha \le 2$.

Sections~\ref{sec:alphaMergeLower}-\ref{sec:alphaMergeUpperPf}
give nearly matching upper and lower bounds for the
worst-case running time of $2$-merge sort and $\alpha$-merge sort for
$\varphi < \alpha < 2$.  As discussed earlier
(see also Table~\ref{tab:allbounds}) the upper and lower bounds are
asymptotically equal to $c_2\cdot n \log m$. The tight upper bound proofs for
Theorems~\ref{thm:alphamergeUpper} and~\ref{thm:twomergeUpper} involve a lot of
technical details, so as a warmup, Section~\ref{sec:simpleUpperBounds} gives
a weaker upper bound with a much simpler proof.

\subsection{Simple upper bounds for 2-merge sort and
  \texorpdfstring{$\alpha$}{a}-merge sort}
\label{sec:simpleUpperBounds}

This section proves $O(n \log n)$ upper bounds on the merge costs for
the $2$-merge and $\alpha$-merge sorts. The proofs are in spirit of
techniques used by~\cite{AJNP:Timsort,Juge:AdaptiveShivers}. First,
Theorem~\ref{thm:mergeStackSize} establishes an upper bound on the
size of the stack.

\begin{theorem}\label{thm:mergeStackSize}
  Let $\varphi < \alpha < 2$. For $\alpha$-merge sort on inputs of length~$n$,
  the size $|\mathcal \QQ|$ of the stack is always
  $< 1 + \lfloor \log_\alpha n\rfloor =
    1 + \lfloor (\log n) / (\log \alpha)\rfloor$.
  For $2$-merge sort, the size of the stack is always
  $\le 1 + \lfloor \log n\rfloor$.
\end{theorem}
\begin{proof}
  The main loop of the $2$-merge and $\alpha$-merge sorts first pushes
  a new run onto the stack and then does zero or more merges.
  Since merges reduce the stack height,
  the maximum stack height is obtained when a
  run is pushed onto the stack. Such a newly pushed
  run is denoted either $Z$ or $\QQ_\ell$, where the
  stack height is~$\ell$.

  We claim that for both $2$-merge sort and $\alpha$-merge sort,
  when a new run is pushed onto the
  stack, $|\QQ_{i - 1}| \ge \alpha |\QQ_i|$ holds for
  all $i < \ell$ (taking $\alpha=2$ in the inequality for $2$-merge sort).

  For $\alpha$-merge sort, this claim follows from the following
  three observations:
  \begin{description}
    \setlength{\itemsep}{0pt}
    \setlength{\topsep}{0pt}
    \item[\rm (a)] Each merge acts on two of the top
    three elements of the stack, i.e., either $X$ and $Y$
    are merged or $Y$ and~$Z$ are merged. Each merge decreases
    the height of the stack by~1; this decreases the value
    of~$\ell$ by~1.
    \item[\rm (b)] The condition that $|\QQ_{i - 1}| \ge \alpha |\QQ_i|$ holds
      for all $i < \ell - 1$ is maintained after merging two of the top three
      stack members. This fact is essentially trivial: If $Y$ and~$Z$ are merged
      they become the new top element of the stack; and if $X$ and~$Y$ are
      merged they become the second element of the stack. But the top
      two elements are $\QQ_{\ell - 1}$ and $\QQ_\ell$, and thus
      do not affect the conditions $|\QQ_{i - 1}| \ge \alpha |\QQ_i|$
      for $i < \ell - 1$.
    \item[\rm (c)] Merging continues until both
      $|X| \ge \alpha |Y|$ and $|Y| \ge \alpha |Z|$ hold.
  \end{description}

  Proving the claim for $2$-merge sort requires an extra argument
  since line~\ref{algline:2mergeXYZ} of Algorithm~\ref{alg:twoMergeSort}
  lacks a test for $|X|<2|Y|$. Instead, we argue that as $2$-merge sort
  performs merges, we have $|\QQ_{i-1}| \ge 2 |\QQ_i|$ for all $i<\ell - 1$
  and either
  \begin{itemize}
  \item[\rm (i)] $|\QQ_{\ell - 2}| \ge 2 |\QQ_{\ell - 1}|$, or
  \item[\rm (ii)] $|\QQ_{\ell - 2}| > | Q_{\ell - 1}|$ and
                  $|\QQ_{\ell - 1}| < 2 |\QQ_\ell|$.
  \end{itemize}
  The condition $|\QQ_{i-1}| \ge 2 |\QQ_i|$ for $i<\ell - 1$ is an invariant just
  because each step either merges $X$ and~$Y$ or merges $Y$ and~$Z$ (just as
  argued in (b) above).  For conditions (i) and~(ii), consider separately
  whether $X$ and~$Y$ are merged, or $Y$ and~$Z$ are merged. In either case,
  $\ell$~is decreased by one by the merge. When $Y$ and~$Z$ are merged, we
  trivially have that condition~(i) holds after the merge (as argued in (b)
  above). When $|X| < |Z|$, and $X$ and~$Y$ are merged, we have
  $|\QQ_{\ell - 3}| \ge 2 |\QQ_{\ell - 2}| >
    |\QQ_{\ell - 2}| + |\QQ_{\ell - 1}|$,
  so the first part of condition~(ii) still holds after the merge.
  In addition, $|Z|>|X|$ means the same as $|\QQ_\ell| > |\QQ_{\ell - 2}|$,
  so $2 |\QQ_\ell| > |\QQ_{\ell - 2}| + |\QQ_{\ell - 1}|$ so the second part
  of condition~(ii) also holds after the merge.  The merging stops
  when $|\QQ_{\ell - 1}| \ge2 |\QQ_\ell|$. Merging
  cannot stop while condition~(ii) holds, so it must stop while
  condition~(i) holds. The claim for $2$-merge sort follows immediately.

  For both $2$-merge sort and $\alpha$-merge sort, the claim implies
  that when a new run~$\QQ_\ell$ is pushed onto the stack, the other
  $\ell - 1$ runs $\QQ_1, \ldots, \QQ_{\ell - 1}$ have run lengths totaling
  \[
    \sum_{i = 1}^{\ell - 1} |\QQ_i|
     ~\ge~ |\QQ_{\ell - 1}| \cdot \sum_{i=0}^{\ell - 2} \alpha^i
     ~=~ \frac{\alpha^{\ell - 1} - 1}{\alpha - 1} \cdot |\QQ_{\ell - 1}|
  \]
  Therefore, the total run length of all $\ell$ runs on the stack
  at least $\frac{\alpha^{\ell - 1} - 1}{\alpha - 1}+1$.
  This must be $\le n$. Therefore, since $\alpha\le 2$, we have
  $\alpha^{\ell - 1} \le n$. Thus $\ell \le 1 + \lfloor \log_\alpha n\rfloor$.
  The inequalities are strict if $\alpha < 2$ since then $\alpha-1<1$.
\end{proof}

\begin{theorem}\label{thm:alphaMergeUpperEasy}
  Let $1 < \alpha \le 2$. The merge cost of $\alpha$-merge sort on
  inputs of length~$n$ is $O(n \log n)$.
\end{theorem}
\begin{proof}
  We introduce a special real-valued counter~$C$
  which initially is equal to~$0$. Every
  time we load a run $\QQ_\ell$ to the stack $\QQ$, we add
  $(2+\alpha) \cdot \ell\cdot |\QQ_\ell|$ to $C$. Every time we merge
  two runs $\QQ_i$ and $\QQ_{i + 1}$, we subtract $|\QQ_i| + |\QQ_{i + 1}|$ from~$C$.

  We claim that the following invariant
  holds throughout the execution of $\alpha$-merge sort:
  \begin{equation} \label{eq:Cinvariant}
    C ~\ge~ \sum\limits_{i = 1}^\ell (2+\alpha) i |\QQ_i|.
  \end{equation}
  We also claim this holds with $\alpha=2$ for $2$-merge sort.
  This claim will suffice to prove Theorem~\ref{thm:alphaMergeUpperEasy}.
  First, by Theorem~\ref{thm:mergeStackSize}, the total of the increases in~$C$
  caused by adding new runs is $\le (2 + \alpha)(1+\penalty10000 \log_\alpha(n))$.
  Second, the total decreases in~$C$ equal the total merge cost.
  By~(\ref{eq:Cinvariant}), $C$~is always non-negative,
  whence Theorem~\ref{thm:alphaMergeUpperEasy} follows.

  The invariant~(\ref{eq:Cinvariant}) holds initially, and it clearly
  is maintained by loading a new run to the top of the stack.
  Let us consider what happens when we
  make a merge. We need to consider several cases below. Only
  the first case is needed for $2$-merge sort as $2$-merge sort does not
  check the condition $|X|<2|Y|$ to decide whether to perform a merge.
  Cases 2.\ and 3.\ below apply only to $\alpha$-merge sort.
\begin{enumerate}
\item If $|Y| < \alpha |Z|$ then we merge $Y$ with the smaller of
$Z$ and~$X$, and subtract at most $|Y| + |Z|$ from~$C$. The merge
reduces the stack height by~1, so considering just the elements
of~$Z$, the righthand side of the inequality~(\ref{eq:Cinvariant})
decreases by at least $(2+\alpha)|Z|$. This decrease is
$\ge(2+\penalty10000\alpha)|Z| \ge |Y| +\penalty10000 |Z|$
by the assumption that $|Y| < \alpha |Z|$.
Thus the condition~(\ref{eq:Cinvariant}) is preserved in this case.
\item If a merge is triggerd by $|X| < \alpha |Y|$, there are several cases to
consider. Consider first the case where $|X| < |Z|$ so
we merge $X$ and~$Y$. In this case, the merge cost is
$|X| + |Y|$, so $C$ decreases by $|X| + |Y|$.
On the other hand, the righthand side of the
inequality~(\ref{eq:Cinvariant}) decreases by
$(2+\alpha)(|Y| + |Z|)$ since the elements of the runs
$Y$ and~$Z$ both have the stack height below them decrease by~1.
Since $|Z| < |X|$, the condition~(\ref{eq:Cinvariant}) is maintained.
\item Now suppose a merge is triggered by $|X| < \alpha |Y|$ and $|X|\ge|Z|$. In
this case we merge $Y$ and~$Z$ to form a run, denoted $YZ$, of length
$|Y| + |Z|$, and $C$~decreases by $|Y| + |Z|$. Now this step might
not preserve the condition~(\ref{eq:Cinvariant}). However,
after merging $Y$ and~$Z$, there must immediately be
another merge due to the fact that $X\le \alpha(|Y| + |Z|)$.
This next merge will either (a)~merge $W$ with~$X$ if $|W| < |Y| + |Z|$
or (b)~merge $X$ with $YZ$ if $|W|\ge |Y| + |Z|$.

Consider the case (b) where $X$ is merged with $YZ$. In this
case, $C$~has decreased by a total of $|X| + 2|Y| + 2|Z|$
during the two merges. At the same time, the stack heights
below the members of $Y$ and~$Z$ decreased by 1 and~2, respectively.
Therefore, the righthand side of~(\ref{eq:Cinvariant})
decreased by $(2+\alpha)(|Y|+2|Z|)$. Since $|X|\le \alpha|Y|$,
this is clearly greater than the decrease $|X| + 2|Y| + 2|Z|$
in~$C$. Thus the inequality~(\ref{eq:Cinvariant}) is
maintained in this case.

Now consider the case~(a) where $W$ and $X$ are merged.
In this case, $C$~decreases by $|W| + |X| + |Y| + |Z|$
from the two merges. Since $|W| < |Y| + |Z|$, this
means that $C$ decreases by less than $|X|+2|Y|+2|Z|$.
On the other hand, the reductions
in stack height means that the righthand side
of~(\ref{eq:Cinvariant}) decreases by
$(2+\alpha)(|X| + |Y|+2|Z|)$. This again implies that
the inequality~(\ref{eq:Cinvariant}) is maintained.
\end{enumerate}
\end{proof}

\subsection{Lower bound for 2-merge sort and \texorpdfstring{$\alpha$}{a}-merge sort}
\label{sec:alphaMergeLower}

\begin{theorem}\label{thm:alphamergeLower}
  Fix $\alpha>1$. The worst-case merge cost of $\alpha$-merge sort
  is $\ge (c_\alpha - o(1)) n \log n$.
\end{theorem}
\noindent
The corresponding theorem for $\alpha=2$ is:
\begin{theorem}\label{thm:twomergeLower}
  The worst-case merge cost of $2$-merge sort is
  $\ge (c_2  -  o(1)) n \log n$, where $c_2 = 3 / \log (27 / 4) \approx
  1.08897$.
\end{theorem}

The proof of Theorem~\ref{thm:alphamergeLower} also establishes
Theorem~\ref{thm:alphaLower}, as the same lower bound construction
works for both $\alpha$-stack sort and $\alpha$-merge sort.
The only difference is
that part~d.\ of Claim~\ref{clm:Ramerge} is used instead
of part~c.  In addition, the proof of Theorem~\ref{thm:alphamergeLower}
also establishes Theorem~\ref{thm:twomergeLower}; indeed, exactly the
same proof applies verbatim, just uniformly replacing ``$\alpha$'' with~``2''.

\begin{proof}[Proof of Theorem~\ref{thm:alphamergeLower}]
  Fix $\alpha > 1$. For $n \ge 1$, we define a sequence $\Ramerge(n)$ of run
  lengths that will establish the lower bound. Define $N_0$ to equal
  $3\cdot\lceil\alpha +  1\rceil$. For $n < N_0$, set $\Ramerge$ to be the
  sequence $\langle n \rangle$, containing a single run of length~$n$.
  For $n \ge N_0$, define $n^\ppprime = \lfloor \frac n{\alpha+1} \rfloor + 1$
  and $n^* = n - n^\ppprime$. Thus $n^\ppprime$ is the least integer greater than $n / (\alpha +  1)$. Similarly define
  $n^\pprime = \lfloor \frac {n^*}{\alpha + 1} \rfloor + 1$ and
  $n^\prime = n^* - n^\pprime$. These four values can be equivalently uniquely
  characterized as satisfying
  \begin{equation}\label{eq:npppnstar}
    \textstyle
    n^\ppprime = \frac 1{\alpha+1} n + \epsilon_1
    \qquad\qquad \hbox{and} \qquad\qquad\qquad
    n^* = \frac {\alpha}{\alpha+1} n - \epsilon_1
  \end{equation}
  \begin{equation}\label{eq:nppnp}
    \textstyle
    n^\pprime = \frac \alpha{(\alpha+1)^2} n
                 - \frac{1}{\alpha+1}\epsilon_1 + \epsilon_2
    \qquad \hbox{and} \qquad\qquad
    n^\prime = \frac {\alpha^2}{(\alpha+1)^2}n
                 - \frac \alpha{\alpha+1}\epsilon_1 - \epsilon_2
  \end{equation}
  for some $\epsilon_1, \epsilon_2 \in (0,1]$.
  The sequence $\Ramerge(n)$ of run
  lengths is inductively defined to be the concatenation
  of $\Ramerge(n^\prime)$, $\Ramerge(n^\pprime)$ and $\Ramerge(n^\ppprime)$.

  \begin{claim} \label{clm:alphemergeLowerNprimes}
    Let $n \ge N_0$.
    \begin{itemize}
      \setlength{\topsep}{0pt}
      \setlength{\itemsep}{0pt}
      \item[\rm a.] $n = n^\prime + n^\pprime + n^\ppprime$ and
        $n^* = n^\prime + n^\pprime$.
      \item[\rm b.] $\alpha (n^\ppprime - 3) \le n^*  < \alpha n^\ppprime$.
      \item[\rm c.] $\alpha (n^\pprime - 3) \le n^\prime < \alpha n^\pprime$.
      \item[\rm d.] $n^\ppprime \ge 3$.
      \item[\rm e.] $n^\prime \ge 1$ and $n^\pprime \ge 1$.
    \end{itemize}
  \end{claim}

  Part~a.\ of the claim is immediate from the definitions. Part~b.\ is immediate
  from the equalities~(\ref{eq:npppnstar}) since $0< \epsilon_1\le 1$ and
  $\alpha > 1$. Part~c.\ is similarly immediate from~(\ref{eq:nppnp}) since
  also $0 < \epsilon_2 \le 1$. Part~d.\ follows from (\ref{eq:npppnstar}) and
  $n \ge N_0 \ge 3 (\alpha + 1)$. Part~e. follows by (\ref{eq:nppnp}),
  $\alpha > 1$, and $n \ge N_0$.

  \begin{claim}\label{clm:Ramerge}
    Let $\Ramerge(n)$ be as defined above.
    \begin{itemize}
      \setlength{\topsep}{0pt}
      \setlength{\itemsep}{0pt}

      \item[\rm a.] The sums of the run lengths in $\Ramerge(n)$ is $n$.
      \item[\rm b.] If $n \ge N_0$, then the final run length in $\Ramerge(n)$
        is $\ge 3$.
      \item[\rm c.] Suppose that $\Ramerge(n)$ is the initial subsequence of a
        sequence~$\mathcal R^\prime$ of run lengths and that
        $\alpha$-merge sort is initially started with run lengths~$\mathcal
        R^\prime$ and (a)~with the stack~$\mathcal \QQ$ empty or (b)~with the
        top element of~$\mathcal \QQ$ a run of length $\ge \alpha (n - 3)$.
        Then $\alpha$-merge sort will start by processing exactly the runs
        whose lengths are those of $\Ramerge(n)$, merging them into single
        run which becomes the new top element of~$\mathcal \QQ$. This will be
        done without merging any runs that were initially in~$\mathcal \QQ$
        and without (yet) processing any of the remaining runs
        in~$\mathcal{R}^\prime$.
      \item[\rm d.] The property~c.\ also holds for $\alpha$-stack sort.
    \end{itemize}
  \end{claim}

  Part~a.\ is immediate from the definitions using induction on~$n$. Part~b.\
  is a consequence of Claim~\ref{clm:alphemergeLowerNprimes}(d.)\  and the fact
  that the final entry of $\Ramerge$ is a value $n^\ppprime<N_0$ for some~$n$.
  Part~c.\ is proved by induction on~$n$, similarly to the proof of
  Claim~\ref{clm:timsortLower}.  It is trivial for the base case $n<N_0$. For
  $n\ge N_0$, $\Ramerge(n)$~is the concatenation of
  $\Ramerge(n^\prime), \Ramerge(n^\pprime), \Ramerge(n^\ppprime)$. Applying the
  induction hypothesis to $\Ramerge(n^\prime)$ yields that these runs are
  initially merged into a single new run of length~$n^\prime$ at the top of the
  stack. Then applying the induction hypothesis to $\Ramerge(n^\pprime)$ shows
  that those runs are merged to become the top run on the stack. Since the last
  member of $\Ramerge(n^\pprime)$ is a run of length $\ge 3$, every intermediate
  member placed on the stack while merging the runs of $\Ramerge(n^\pprime)$ has
  length $\le n^\pprime - 3$. And, by
  Claim~\ref{clm:alphemergeLowerNprimes}(c.),
  these cannot cause a merge with the run of length~$n^\prime$ already
  in~$\mathcal \QQ$. Next, again by Claim~\ref{clm:alphemergeLowerNprimes}(c.),
  the top two members of the stack are merged to form a run of length~$n^* =
  n^\prime +  n^\pprime$.  Applying the induction hypothesis a third time, and
  arguing similarly with Claim~\ref{clm:alphemergeLowerNprimes}(b.), gives that
  the runs of $\Ramerge(n^\ppprime)$ are merged into a single run of
  length~$n^\ppprime$, and then merged with the run of length~$n^*$ to obtain a
  run of length~$n$.  This proves part~c.\ of Claim~\ref{clm:Ramerge}. Part~d.\
  is proved exactly like part~c.; the fact that $\alpha$-stack sort is only
  2-aware and never merges $X$ and~$Y$ makes the argument slightly easier in
  fact.  This completes the proof of Claim~\ref{clm:Ramerge}.

  \begin{claim}\label{clm:amergeRecur1}
    Let $c(x)$ equal the merge cost of $\alpha$-merge sort on an input
    sequence with run lengths given by $\Ramerge(n)$. Then $c(n) = 0$ for
    $n < N_0$. For $n\ge N_0$,
    \begin{eqnarray}\label{eq:camergeRecur}
      c(n) &=& c(n^\prime) + c(n^\pprime) + c(n^\ppprime)
                 + 2n^\prime + 2n^\pprime + n^\ppprime \\
           &=& c(n^\prime) + c(n^\pprime) + c(n^\ppprime)
                 + n + n^\prime + n^\pprime.
    \end{eqnarray}
    For $n\ge N_0$, $c(n)$ is strictly increasing as a function of~$n$.
  \end{claim}
  The first equality of Equation~(\ref{eq:camergeRecur}) is an immediate consequence of the
  proof of part~c.\ of Claim~\ref{clm:Ramerge};
  the second follows from $n = n^\prime + n^\pprime + n^\ppprime$. To see that $c(n)$ is
  increasing for $n\ge N_0$,
  let $(n +\penalty10000 1)^\prime, (n +\penalty10000 1)^\pprime, (n +\penalty10000 1)^\ppprime$
  indicate the three values such that
  $\Ramerge(n +  1)$ is the concatenation of
  $\Ramerge((n +  1)^\prime)$, $\Ramerge((n +  1)^\pprime)$ and
  $\Ramerge((n +  1)^\ppprime)$. Note that
  $(n +  1)^\prime \ge n^\prime$,
  $(n +  1)^\pprime \ge n^\pprime$, and
  $(n +  1)^\ppprime \ge n^\ppprime$.
  An easy proof by induction now shows that $c(n + 1) > c(n)$ for $n \ge N_0$,
  and Claim~\ref{clm:amergeRecur1} is proved.

  Let
  $\delta = \lceil 2(\alpha + 1)^2/(2 \alpha + 1)\rceil$.
  (For $1 < \alpha \le 2$, we have $\delta \le 4$.)
  We have $\delta \le N_0 - 1$
  for all $\alpha > 1$.
  For real $x\ge N_0$,
  define $b(x) = c(\lfloor x \rfloor  -  \delta)$.
  Since $c(n)$ is increasing, $b(x)$ is nondecreasing.

  \begin{claim}\label{clm:bamergeRecur}
      \begin{itemize}
          \setlength{\parsep}{0pt}
          \setlength{\itemsep}{0pt}
          \item[\rm a.]
            $\frac 1{\alpha + 1} n - \delta \le (n - \delta)^\ppprime$.
          \item[\rm b.]
            $\frac \alpha{(\alpha + 1)^2} n - \delta \le (n - \delta)^\pprime$.
          \item[\rm c.]
            $\frac {\alpha^2}{(\alpha + 1)^2} n - \delta \le
              (n - \delta)^\prime$.
          \item[\rm d.] If $x \ge N_0 + \delta$, then
                $b(x) \, \ge\,
                b({\textstyle \frac{\alpha^2}{(\alpha + 1)^2} x }) +
                b({\textstyle \frac\alpha{(\alpha + 1)^2} x }) +
                b({\textstyle \frac1{\alpha + 1} x }) +
                {\textstyle \frac{2 \alpha + 1}{\alpha + 1} }
                  (x  -  \delta - 1) - 1$.
      \end{itemize}
  \end{claim}
  For~a.,
  (\ref{eq:npppnstar}) implies that
  $(n - \delta)^\ppprime \ge \frac{n-\delta}{\alpha+1}$,
  so a.~follows from $-\delta \le -\delta/(\alpha +  1)$.
  This holds as $\delta > 0$ and $\alpha>1$. For~b.,
  (\ref{eq:nppnp})~implies that
  $(n - \delta)^\pprime \ge
  \frac{\alpha}{(\alpha + 1)^2}(n - \delta) - \frac{1}{\alpha + 1}$,
  so after simplification, b.~follows from
  $\delta \ge (\alpha + 1) / (\alpha^2 +  \alpha +  1)$;
  it is easy to verify that this holds by choice of~$\delta$.
  For~c., (\ref{eq:nppnp})~also implies that
  $(n - \delta)^\prime \ge \frac{\alpha^2}{(\alpha + 1)^2}(n - \delta) - 2$,
  so after simplification, c.~follows from
  $\delta \ge 2(\alpha +  1)^2 / (2\alpha +  1)$.

  To prove part~d., letting $n = \lfloor x \rfloor$
  and using parts a., b.\ and c., equations
  (\ref{eq:npppnstar}) and (\ref{eq:nppnp}),
  and the fact that $b(x)$ and $c(n)$ are nondecreasing, we have
  \begin{eqnarray*}
    b(x) &=& c( n - \delta ) \\
      &=& c( (n - \delta)^\prime ) +
         c( (n - \delta)^\pprime ) +
         c( (n - \delta)^\ppprime ) +
         (n -\delta) + (n - \delta)^\prime + (n - \delta)^\pprime \\
      &\ge& \textstyle
         c( \lfloor \frac{\alpha^2}{(\alpha+1)^2} n \rfloor - \delta ) +
         c( \lfloor \frac \alpha {(\alpha+1)^2} n \rfloor - \delta ) +
         c( \lfloor \frac 1 {\alpha+1} n \rfloor - \delta ) +
      \\ & & \quad\quad\quad +
         \textstyle
         \bigl( 1 + \frac{\alpha^2}{(\alpha+1)^2} +
                \frac {\alpha}{(\alpha+1)^2} \bigr) \cdot (n - \delta)
      \\ & & \quad\quad\quad
         \textstyle
        - \frac {\alpha}{\alpha+1} \epsilon_1 -  \epsilon_2
        - \frac 1{\alpha+1}\epsilon_1 +  \epsilon_2 \\
      &\ge& \textstyle
        b({\textstyle \frac{\alpha^2}{(\alpha+1)^2} x }) +
        b({\textstyle \frac\alpha{(\alpha+1)^2} x }) +
        b({\textstyle \frac1{\alpha+1} x }) +
        \frac{ 2 \alpha+1 }{\alpha+1} (x - \delta - 1) - 1.
  \end{eqnarray*}
  Claim~\ref{clm:bamergeRecur}(d.) gives us the basic recurrence needed
  to lower bound $b(x)$ and hence $c(n)$.

  \begin{claim}\label{clm:bamergeLowerBd}
    For all $x \ge \delta + 1$,
    \begin{equation}\label{eq:bamergeLower}
      b(x)~\ge~ c_\alpha\cdot x \log x -Bx +A,
    \end{equation}
    where $A = \frac {2\alpha + 1}{2\alpha + 2}(\delta +  1) + \frac12$ and
    $B = \frac A {\delta + 1} +
    c_\alpha \log (\max\{
      N_0 + \delta +  1,
      \left\lceil \frac{(\delta + 1)(\alpha + 1)^2}{\alpha} \right\rceil
    \})$.
  \end{claim}

  The claim is proved by induction, namely we prove by
  induction on~$n$ that (\ref{eq:bamergeLower})
  holds for all $x < n$. The base case is for
  $x < n = \max\{ N_0 +  \delta +  1,
                  \left\lceil \frac{(\delta + 1)(\alpha + 1)^2}{\alpha} \right\rceil
               \}$.
  In this case $b(x)$ is non-negative, and the righthand side
  of~(\ref{eq:bamergeLower}) is $\le 0$ by choice of~$B$.
  Thus (\ref{eq:bamergeLower}) holds trivially.

  For the induction step, we may assume $n-1\le x < n$ and have
  \[
  \textstyle
  \delta + 1 \le \frac\alpha{(\alpha+1)^2} x < \frac{\alpha^2}{(\alpha + 1)^2} x
      < \frac1{\alpha + 1} x < n -1.
  \]
  The first of these inequalities follows from $x \ge \frac{(\delta + 1)(\alpha + 1)^2}{\alpha}$;
  the remaining inequalities follow from $1<\alpha<2$ and $x < n$ and $n\ge N_0 \ge 6$.
  Therefore, the induction hypothesis implies that the bound~(\ref{eq:bamergeLower})
  holds for
  $b(\frac{\alpha^2}{(\alpha+1)^2} x )$,
  $b(\frac\alpha{(\alpha+1)^2} x )$,
  and $b(\frac1{\alpha+1} x)$. So by Claim~\ref{clm:bamergeRecur}(d.),
  \begin{eqnarray*}
  b(x) &\ge& \textstyle
       c_\alpha \frac{\alpha^2}{(\alpha+1)^2}
           x \log \frac {\alpha^2 x}{(\alpha+1)^2}
           - B \frac {\alpha^2}{(\alpha+1)^2} x
           + \frac {2\alpha+1}{2\alpha+2}(\delta +  1) + \frac12 \\
       && \quad \textstyle
       + c_\alpha \frac{\alpha}{(\alpha+1)^2}
           x \log \frac {\alpha x}{(\alpha+1)^2}
           - B \frac {\alpha}{(\alpha+1)^2} x
           + \frac {2\alpha+1}{2\alpha+2}(\delta +  1) + \frac12 \\
       && \quad \textstyle
       + c_\alpha \frac1{\alpha+1}
           x \log \frac{x}{\alpha+1}
           - B \frac{1}{\alpha+1} x
           + \frac {2\alpha+1}{2\alpha+2}(\delta +  1) + \frac12 \\
       && \quad \textstyle
           + \frac{2 \alpha + 1}{\alpha+1} x
           - \frac{2 \alpha + 1}{\alpha+1} (\delta +  1) - 1 \\
   &=& \textstyle
       c_\alpha x \log x - B x + A \\
   && \quad \textstyle
        + c_\alpha x \left[
              \frac{\alpha^2}{(\alpha+1)^2} \log\frac{\alpha^2}{(\alpha+1)^2}
              + \frac{\alpha}{(\alpha+1)^2} \log\frac{\alpha}{(\alpha+1)^2}
              + \frac1{\alpha+1} \log\frac1{\alpha+1} \right]
              + \frac{2 \alpha + 1}{\alpha+1} x .
  \end{eqnarray*}
  The quantity in square brackets is equal to
  \begin{eqnarray*}
  \lefteqn{ \textstyle
        \left( \frac{2 \alpha^2}{(\alpha+1)^2}
               + \frac{\alpha}{(\alpha+1)^2} \right)\log \alpha
        - \left( \frac{2 \alpha^2}{(\alpha+1)^2}
               + \frac{2 \alpha}{(\alpha+1)^2}
               + \frac1{\alpha+1} \right) \log(\alpha +  1)} \\
  &=& \textstyle
        \frac {\alpha(2\alpha+1)}{(\alpha+1)^2} \log \alpha
        - \frac {2\alpha^2+3\alpha+1}{(\alpha+1)^2} \log(\alpha +  1)
        \hspace*{1in} % Stupid centering trick!
        \\
  &=& \textstyle
        \frac {\alpha(2\alpha+1)}{(\alpha+1)^2} \log \alpha
        - \frac {2\alpha+1}{\alpha+1} \log(\alpha +  1) \\
  &=& \textstyle
        \frac {\alpha \log \alpha - (\alpha +  1) \log(\alpha +  1) } {\alpha+1}
        \cdot \frac{2 \alpha + 1}{\alpha+1}.
  \end{eqnarray*}
  Since
  $c_\alpha = (\alpha +  1) /
    ((\alpha +  1) \log(\alpha +  1) - \alpha \log \alpha)$,
  we get that $b(x) \ge c_\alpha x \log x - B x + A$.
  This completes the induction step and proves Claim~\ref{clm:bamergeLowerBd}.

  Since $A$ and $B$ are constants, this gives
  $b(x) \ge (c_\alpha - o(1)) x \log x$.
  Therefore $c(n) = b(n +  \delta) \ge (c_\alpha - o(1)) n \log n$.
  This completes the proofs of Theorems \ref{thm:alphaLower},
  \ref{thm:alphamergeLower} and~\ref{thm:twomergeLower}.
\end{proof}

\subsection{Upper bound for 2-merge sort and \texorpdfstring{$\alpha$}{a}-merge sort --- preliminaries}
\label{sec:mergeUpperPrelim}

We next prove upper bounds on the worst-case runtime
of $2$-merge sort and $\alpha$-merge sort for $\varphi<\alpha < 2$.
The upper bounds will have the form
$n \cdot ( d_\alpha + c_\alpha \log n)$, with no hidden or missing constants.
$c_\alpha$~was already defined in~(\ref{eq:cAlphaDef}). For $\alpha =2$, $c_\alpha \approx 1.08897$
and the constant~$d_\alpha$ is
\begin{equation}\label{eq:dtwoDef}
  d_2 ~=~ 6 - c_2\cdot( 3 \log 6 -2 \log 4)
      ~=~ 6 - c_2\cdot( (3 \log 3) - 1) ~\approx~ 1.91104.
\end{equation}
For $\varphi<\alpha<2$, first define
\begin{equation}\label{eq:kalphaDef}
  k_0(\alpha) ~=~ \min \{
    {
      \textstyle \ell \in N ~:~
      \frac{\alpha^2 - \alpha - 1}{\alpha - 1} \ge \frac{1}{\alpha^\ell}
    }
  \}.
\end{equation}
Note $k_0(\alpha)\ge 1$. Then set, for $\varphi<\alpha<2$,
\begin{equation}\label{eq:dalphaDef}
  d_\alpha ~=~
  \frac{2^{k_0(\alpha)+1} \cdot \max\{ (k_0(\alpha) +  1), 3 \} \cdot
    (2\alpha - 1)}{\alpha - 1} + 1 .
\end{equation}
Our proof for $\alpha = 2$ is substantially simpler than the
proof for general $\alpha$: it also gives the better constant~$d_2$.
The limits $\lim_{\alpha\rightarrow \varphi^+} k(\alpha)$
and $\lim_{\alpha\rightarrow \varphi^+} d_\alpha$ are both equal
to~$\infty$; we suspect this is not optimal.\footnote{%
Already the term $2^{k_0(\alpha)+1}$ is not optimal as the
proof of Theorem~\ref{thm:alphamergeUpper} shows that the base~2 could be
replaced by $\sqrt{\alpha +  1}$; we conjecture however, that
in fact it is not necessary for the limit of~$d_\alpha$ to be
infinite.} However, by Theorems \ref{thm:alphamergeLower} and~\ref{thm:twomergeLower}, the constant~$c_\alpha$ is optimal.

\begin{theorem}\label{thm:alphamergeUpper}
  Let $\varphi < \alpha < 2$. The merge cost of $\alpha$-merge sort on
  inputs of length~$n$ composed of $m$ runs is
  $\le n \cdot (d_\alpha + c_\alpha \log m)$.
\end{theorem}

\noindent
The corresponding upper bound for $\alpha=2$ is:

\begin{theorem}\label{thm:twomergeUpper}
  The merge cost of $2$-merge sort on inputs of length~$n$ composed of $m$
  runs is $\le n \cdot (d_2 + \penalty10000 c_2\log m) \approx
    n \cdot (1.91104 + 1.08897 \log m)$.
\end{theorem}

Proving Theorems \ref{thm:alphamergeUpper} and \ref{thm:twomergeUpper}
requires handling three situations:
First, the algorithm may have top stack element~$Z$ which is
not too much larger than~$Y$ (so $|Z| \le \alpha |Y|$): in this
case either $Y$ and~$Z$ are merged or $Z$ is small
compared to~$Y$ and no merge occurs. This first case
will be handled by case~(A) of the proofs. Second,
the top stack element~$Z$ may be much larger than~$Y$
(so $|Z| > \alpha |Y|$): in this case, the algorithm will
repeatedly merge $X$ and~$Y$ until $|Z| \le |X|$. This is the
most complicated case of the argument, and is handled
by cases (C) and (D) of the proofs. (Case~(D)
is not needed when $\alpha = 2$.)
In the third case, the original runs in~$\mathcal R$
have been exhausted and the final loop on lines
\ref{algline:amergefinalloopA}-\ref{algline:amergefinalloopB}
repeatedly merges $Y$ and~$Z$. The third case is handled by
case~(B) of the proofs.

The next four technical lemmas are key for the proofs.
Lemma~\ref{lem:alphamerge-A} is used for case~(A) of
the proofs; the constant~$c_\alpha$ is exactly what
is needed to make this hold. Lemma~\ref{lem:alphamerge-B}
is used for case~(B) of the proofs.
Lemma~\ref{lem:alphamerge-Ctwo}
is used for case~(C),
and Lemma~\ref{lem:alphamerge-CD} is used
in case~(D) when $\alpha < 2$.

\begin{lemma}\label{lem:alphamerge-A}
Let $\alpha>1$.
Let $A,B,a,b$ be positive integers such that
$A \le \alpha B$ and $B \le \alpha A$.  Then
\begin{equation}\label{eq:alphamergebd1}
A \cdot c_\alpha \log a + B \cdot c_\alpha\log b + A + B
~\le~
(A  +   B) \cdot c_\alpha \log(a  +   b).
\end{equation}
\end{lemma}

\begin{lemma}\label{lem:alphamerge-B}
  Let $\alpha > 1$. Let $A, B, a, b$ be positive integers such that
  $(\alpha - 1)B \le A$.  Then
  \begin{equation}\label{eq:alphamergebd2}
    A \cdot c_\alpha \log a + B \cdot (1 + c_\alpha \log b) + A + B
    ~\le~
    (A + B) \cdot (1 + c_\alpha \log (a  +   b)).
  \end{equation}
\end{lemma}

\begin{lemma}\label{lem:alphamerge-Ctwo}
  Let $\varphi<\alpha\le 2$ and $A$, $B$, $a$, $b$ be positive integers.
  \begin{itemize}
    \item[\rm (a)] (For $\alpha=2$.) If $a\ge 2$ and $A \le 2B$, then
      \[
        A \cdot (d_2 + c_2 \log (a - 1)) + A + B
          \le (A  +   B) \cdot (d_2 - 1 + c_2 \log (a +  b)) .
      \]
    \item[\rm (b)] If $\varphi<\alpha<2$ and
      $A \le \frac{\alpha}{\alpha - 1} \cdot B$, then
      \[
        A \cdot (d_\alpha + c_\alpha \log a) + A + B
          \le (A + B) \cdot (d_\alpha - 1 + c_\alpha \log (a + b)) .
      \]
  \end{itemize}
\end{lemma}

\begin{lemma}\label{lem:alphamerge-CD}
  Let $\varphi < \alpha < 2$. Let $A$, $B$, $C$, $a$, $b$, and $k$ be positive integers such that $k\le k_0(\alpha)+1$ and
  $\frac{2^k (2\alpha - 1)}{\alpha - 1} C \ge A + B + C$. Then
  \begin{multline}\label{eq:alphamerge-CD}
      A \cdot (d_\alpha + c_\alpha \log a) +
      B \cdot (d_\alpha + c_\alpha \log b)) +
      k \cdot C + 2 B + A
      ~\le~\\
      A \cdot(d_\alpha - 1 + c_\alpha \log a) +
      (B + C) \cdot(d_\alpha - 1 + c_\alpha \log (b  +   1)).
  \end{multline}
\end{lemma}

\begin{proof}[Proof of Lemma~\ref{lem:alphamerge-A}]
  The inequality~(\ref{eq:alphamergebd1}) is equivalent to
  \[
    A \cdot (c_\alpha \log a - c_\alpha \log(a  +   b) + 1)
    ~\le~
    B \cdot (c_\alpha \log(a + b) - c_\alpha \log b - 1)
  \]
  and hence to
  \[
    A \cdot \Bigl(1 + c_\alpha \log \frac{a}{a + b} \Bigr)
    ~\le~
    B \cdot \Bigl(-1 - c_\alpha \log \frac{b}{a + b} \Bigr).
  \]
  Setting $t = b / (a  +   b)$, this is the same as
  \begin{equation}\label{eq:alphamergebd1_1}
    A \cdot(1 + c_\alpha \log (1  -  t))
    ~\le~
    B \cdot(-1 - c_\alpha \log t).
  \end{equation}
  Let $t_0 = 1 - 2^{-1 / c_\alpha}$. Since $c_\alpha>1$, we have $t_0 < 1/2$,
  so $t_0 < 1  -  t_0$. The lefthand side of~(\ref{eq:alphamergebd1_1}) is
  positive iff $t < t_0$. Likewise, the righthand side is positive
  iff $t < 1 - t_0$. Thus (\ref{eq:alphamergebd1_1}) certainly
  holds when $t_0 \le t \le 1 - t_0$ where the lefthand side is
  $\le 0$ and the righthand side is $\ge 0$.

  Suppose $0<t<t_0$, so $1 + c_\alpha \log (1  -  t)$
  and $-1 - c_\alpha \log t$ are both positive.
  Since $A\le \alpha B$, to prove~(\ref{eq:alphamergebd1_1})
  it will suffice to prove
  $\alpha (1 + c_\alpha \log(1  -  t)) \le -1 - c _\alpha\log t$,
  or equivalently that
  $-1 - \penalty10000 \alpha \ge c_\alpha \log( t (1  -  t)^\alpha )$. The derivative
  of $\log( t (1  -  t)^\alpha )$ is
  $(1- (1 + \alpha)t)/(t(1 - t))$; so $\log( t (1  -  t)^\alpha )$
  is maximized at $t = 1/(1  +   \alpha)$ with value
  $\alpha \log \alpha - (\alpha + 1)\log(\alpha + 1)$. Thus
  the desired inequality holds by the definition of~$c_\alpha$, so
  (\ref{eq:alphamergebd1_1}) holds in this case.

  Now suppose $1 - t_0 < t < 1$, so $1 + c_\alpha \log (1  -  t)$
  and $-1 - c_\alpha \log t$ are both negative. Since $B\le\alpha A$,
  it suffices to prove
  $1 + c_\alpha \log(1  -  t) \le \alpha (-1 - c_\alpha \log t)$
  or equivalently that
  $-1 - \alpha \ge c_\alpha \log( t^\alpha (1  -  t))$.
  This is identical to the situation of the previous paragraph, but
  with $t$ replaced by $1  -  t$, so
  (\ref{eq:alphamergebd1_1}) holds in this case also.
\end{proof}

\begin{proof}[Proof of Lemma~\ref{lem:alphamerge-B}]
  The inequality (\ref{eq:alphamergebd2}) is equivalent to
  \[
    B \cdot (1 + c_\alpha \log b - c_\alpha \log(a  +   b))
    ~\le~
    A \cdot (c_\alpha \log(a  +   b) - c_\alpha \log a).
  \]
  Since $\log(a  +   b) - \log a > 0$ and $(\alpha  -  1)B \le A$,
  it suffices to prove
  \[
    1 + c_\alpha \log b - c_\alpha \log(a  +   b)
      ~\le~
      (\alpha - 1) \cdot (c_\alpha \log(a  +   b) - c_\alpha \log a).
  \]
  This is equivalent to
  \[
    c_\alpha\log b + (\alpha  -  1)\cdot c_\alpha\log(a) - \alpha\cdot c_\alpha \log(a +  b)
      ~\le~ -1.
  \]
  Letting $t = b / (a +  b)$, we must show
  $-1 \ge c_\alpha\log(t (1  -  t)^{\alpha - 1})$.
  Similarly to the previous proof, taking the first derivative shows that
  the righthand side is maximized with $t = 1/\alpha$, so it will suffice
  to show that
  $-1 \ge c_\alpha \log( (\alpha  -  1)^{\alpha-1}/\alpha^\alpha )$,
  i.e., that
  $c_\alpha\cdot(\alpha \log \alpha - (\alpha  -  1)\log(\alpha  -  1)) \ge 1$.
  Numerical examination shows that this is true for $\alpha>1.29$.
\end{proof}

\begin{proof}[Proof of Lemma~\ref{lem:alphamerge-Ctwo}]
  We assume w.l.o.g.\ that $b = 1$.
  The inequality of part~(a) is equivalent to
  \[
      A \cdot (2 + c_2 \log (a - 1) - c_2 \log (a +  1))
      ~\le~
      B \cdot (d_2 - 2 + c_2 \log (a  +   1)).
  \]
  Since the righthand side is positive and $A \le 2 B$,
  we need to prove
  \[
    2 \cdot(2 + c_2 \log(a - 1) - c_2 \log(a +  1))
    ~\le~
    d_2 - 2 + c_2 \log (a +  1).
  \]
  This is easily seen to be the same as
  $6 - d_2 \le
      c_2 \log ((a  +   1)^3/(a - 1)^2)$.
  With $a>1$ an integer, the quantity
  $(a  +   1)^3/(a - 1)^2$ is minimized when $a=5$.
  Thus, we must show that
  \[
    d_2 ~\ge~ 6 - c_2 \log( 6^3/4^2 ) ~=~ 6 - c_2(3 \log 6 - 2 \log 4).
  \]
  In fact, $d_2$ was defined so that equality holds.
  Thus (a) holds.

  Arguing similarly for part~(b), we must show
  \[
      \frac{2\alpha}{\alpha-1}+2-d_\alpha
      ~\le~ c_2 \log ((a  +   1)^{\alpha/(\alpha-1)}/a).
  \]
  This holds trivially, as $d_\alpha\ge 12$
  for $\alpha<2$ so the lefthand side is negative and
  the righthand side is positive.
\end{proof}

\begin{proof}[Proof of Lemma~\ref{lem:alphamerge-CD}]
    The inequality~(\ref{eq:alphamerge-CD}) is equivalent to
    \[
      2 A + B \cdot (3 + c_\alpha \log b - c_\alpha \log (b  +   1)) + k \cdot C
      ~\le~
      C \cdot (d_\alpha - 1 + c_\alpha \log (b  +   1)).
    \]
    Since $\frac{2^k (2\alpha - 1)}{\alpha - 1} C \ge  A + B + C$, it is enough
    to prove
    \[
      \max(k, 3) \cdot \frac{2^k (2\alpha  -  1)}{\alpha  -  1} C
      ~\le~
      (d_\alpha  -  1) \cdot C .
    \]
    This is equivalent to
    \[
      d_\alpha ~\ge~
      \frac{2^k \cdot \max\{k, 3\} \cdot (2\alpha  -  1)}{\alpha - 1} + 1.
    \]
    This holds by the definition of $d_\alpha$,
    since $k \le k_0(\alpha) +  1$.
\end{proof}


The proofs of Theorems \ref{thm:alphamergeUpper} and~\ref{thm:twomergeUpper}
use two functions $G_\alpha$ and~$H_\alpha$ to bound the
merge cost of runs stored on the stack.
\begin{definition}
  For $\alpha=2$, define
  \begin{eqnarray*}
    G_2(n,m) &=& n \cdot (d_2 - 1 + c_2 \log m) \\[1ex]
    H_2(n,m) &=&
    \left\{
      \begin{array}{ll}
        n \cdot (d_2 + c_2 \log (m - 1)) \quad & \hbox{if $m\ge 2$} \\
        0 & \hbox{if $m=1$.}
      \end{array}
    \right.
  \end{eqnarray*}
  For $\alpha < 2$,
  \begin{eqnarray*}
      G_\alpha(n,m) &=& n \cdot (d_\alpha - 1 + c_\alpha \log m) \\[1ex]
      H_\alpha(n,m) &=& n \cdot (d_\alpha     + c_\alpha \log m).
  \end{eqnarray*}
\end{definition}
Recall that $m_X$ is the number of original runs
merged to form a run~$X$.
For the proof of Theorem~\ref{thm:twomergeUpper} in the next section,
upper bounding the merge cost of $2$-merge sort,
the idea is that for most runs~$X$ on the stack~$\mathcal \QQ$,
the merge cost of~$X$ will be bounded by $G_2(|X|, m_X)$.
However, many of the runs formed by merges in cases (B) and~(C)
will instead have merge cost bounded by $H_2(|X|,m_X)$.

A similar intuition applies to the proof of Theorem~\ref{thm:alphamergeUpper}
for $\varphi<\alpha<2$, in Section~\ref{sec:alphaMergeUpperPf}.
However, the situation is more complicated as
that proof will bound the total merge cost
instead of individual merge costs~$w_{\QQ_i}$.

The next lemma is the crucial property of $G_2$ and~$G_\alpha$
that is needed for both Theorems
\ref{thm:twomergeUpper} and~\ref{thm:alphamergeUpper}.
The lemma is used to bound the merge costs incurred when merging
two runs which differ in size by at most a factor~$\alpha$.
The constant $c_\alpha$ is exactly what is needed to make this lemma hold.
\begin{lemma}\label{lem:Gbound}
  Suppose $n_1, n_2, m_1, m_2$ are positive
  integers, and $\varphi < \alpha \le 2$. Also suppose $n_1 \le \alpha n_2$ and
  $n_2 \le \alpha n_1$. Then,
  \[
    G_\alpha(n_1, m_1) + G_\alpha(n_2, m_2) + n_1 + n_2
    ~\le~
    G_\alpha(n_1  +   n_2, m_1  +   m_2).
  \]
\end{lemma}
\begin{proof}
  The inequality expresses that
  \[
    n_1 \cdot (d_\alpha - 1 + c_\alpha \log m_1)
     + n_2 \cdot (d_\alpha - 1 + c_\alpha \log m_2) + n_1 + n_2
    ~\le~
    (n_1 + n_2) \cdot (d_\alpha - 1 + c_\alpha \log (m_1  +   m_2)).
  \]
  This is an immediate consequence of Lemma~\ref{lem:alphamerge-A} with $A$,
  $B$, $a$, $b$ replaced with $n_1$, $n_2$, $m_1$, $m_2$.
\end{proof}

\subsection{Upper bound proof for 2-merge sort}
\label{sec:twoMergeUpperPf}
This section gives the proof of Theorem~\ref{thm:twomergeUpper}.
Lemma~\ref{lem:GHtwobounds} states some properties of $G_2$ and $H_2$ which
follow from Lemmas \ref{lem:alphamerge-B} and~\ref{lem:alphamerge-Ctwo}(a).

\begin{lemma}\label{lem:GHtwobounds}
  Suppose $n_1, n_2, m_1, m_2$ are positive integers.
  \begin{description}
    \item[\rm (a)] If $n_2 \le n_1$, then
      \[
        G_2(n_1, m_1) + H_2(n_2, m_2) + n_1 + n_2
        ~\le~
        H_2(n_1  +   n_2, m_1  +   m_2).
      \]
    \item[\rm (b)]
      If $n_1 \le 2 n_2$, then
      \[
        H_2(n_1,m_1) + n_1 + n_2 ~\le~ G_2(n_1 +  n_2, m_1 +  m_2) .
      \]
  \end{description}
\end{lemma}

\begin{proof}
  If $m_2 \ge 2$, part~(a) states that
  \[
    n_1 \cdot (d_2 - 1 + c_2 \log m_1)
     + n_2 \cdot (d_2 + c_2 \log (m_2 - 1)) + n_1 + n_2
    ~\le~
    (n_1  +   n_2) \cdot (d_2 + c_2 \log (m_2  +   m_2  -  1)).
  \]
  This is an immediate consequence of Lemma~\ref{lem:alphamerge-B}.
  If $m_2 = 1$, then part~(a) states
  \[
    n_1 \cdot (d_2 - 1 + c_2 \log m_1) + n_1 + n_2
    ~\le~
    (n_1 + n_2) \cdot (d_2 + c_2 \log m_1).
  \]
  This holds since $d_2 \ge 1$ and $n_2 > 0$ and $m_1 \ge 1$.

  When $m_1 \ge 2$, part~(b) states that
  \[
    n_1 \cdot (d_2 + c_2 \log (m_1 - 1)) + n_1 + n_2
    ~\le~
    (n_1 + n_2) \cdot (d_2 - 1 + c_2 \log (m_1  +   m_2));
  \]
  this is exactly Lemma~\ref{lem:alphamerge-Ctwo}(a).
  When $m_1 = 1$, (b)~states
  $n_1 +  n_2 \le (n_1 +  n_2)(d_2  -  1  +   c_2\log(m_2 +  1))$,
  and this is trivial since $c_2 + d_2 \ge 2$ and $m_2 \ge 1$.
\end{proof}

We next prove Theorem~\ref{thm:twomergeUpper}. We use the convention that
the $2$-merge sort algorithm maintains a stack~$\mathcal \QQ$ containing runs $\QQ_1$,
$\QQ_2$, \dots, $\QQ_\ell$. The last four runs are denoted $W$, $X$, $Y$, $Z$. Each
$\QQ_i$ is a run of $|\QQ_i|$ many elements. Recall that $m_{\QQ_i}$ and
$w_{\QQ_i}$ denote the number of original runs that were merged to form $\QQ_i$
and the merge cost of~$\QQ_i$ (respectively). If $\QQ_i$ is an original run, then
$m_{\QQ_i} = 1$ and $w_{\QQ_i} = 0$. If $m_{\QQ_i}=2$, then $\QQ_i$ was formed by a
single merge, so $w_{\QQ_i} = |\QQ_i|$. To avoid handling the special cases for
$\ell \le 2$, we adopt the convention that there is a virtual initial run $\QQ_0$
with infinite length, so $|\QQ_0| = \infty$.

\begin{lemma}\label{lem:GHtwobound}
  Suppose $\QQ_i$ is a run in~$\mathcal \QQ$ and that
  $w_{\QQ_i} \le G_2(|\QQ_i|, m_{\QQ_i})$. Then $w_{\QQ_i} \le H_2(|\QQ_i|, m_{\QQ_i})$.
\end{lemma}

\begin{proof}
  If $m_{\QQ_i}=1$, the lemma
  holds since $w_X = 0$. If $m_{\QQ_i} = 2$, then it holds since
  $w_{\QQ_i} = |\QQ_i|$. If $m_{\QQ_i} > 2$, then it holds since
  $c_2 \log(m_{\QQ_i} / (m_{\QQ_i} - 1)) < 1$ and hence
  $G_2(|\QQ_i|, m_{\QQ_i}) < H_2(|\QQ_i|, m_{\QQ_i})$.
\end{proof}

\begin{proof}[Proof of Theorem~\ref{thm:twomergeUpper}]
  We describe the $2$-merge algorithm by using
  three invariants (A), (B), (C) for the stack;
  and analyzing what action is taken
  in each situation.
  Initially, the stack contains a single original $\QQ_1$,
  so $\ell=1$ and $m_{\QQ_1} = 1$ and $w_{\QQ_1}=0$, and case~(A)
  applies.

  \smallskip

  \noindent
  {\bf (A): Normal mode.} The stack satisfies
  \begin{itemize}
  \setlength{\itemsep}{0pt}
  \item[(A-1)] $|\QQ_i| \ge 2 \cdot |\QQ_{i + 1}|$ for all $i < \ell  -  1$.
  This includes $|X|\ge 2|Y|$ if $\ell\ge 2$.
  \item[(A-2)] $2 \cdot |Y| \ge |Z|$; i.e.\ $2 |\QQ_{\ell - 1}| \ge |\QQ_\ell|$.
  \item[(A-3)] $w_{\QQ_i} \le G_2(|\QQ_i|, m_{\QQ_i})$ for all $i \le \ell$.
  \end{itemize}
  \noindent
  If $\ell \ge 2$, (A-1) and (A-2) imply $|X|\ge |Z|$, i.e.\
  $|\QQ_{\ell - 2}| \ge |\QQ_\ell|$.
  The $\alpha$-merge algorithm does one of the following:

  \begin{itemize}
  \item If $2 |Z| \le |Y|$ and there are no more original
  runs to load, then it goes to case~(B). We claim the four
  conditions of (B) hold (see below). The condition (B-2)
  holds by $|Y| \ge 2|Z|$, and
  (B-1) and (B-3) hold by (A-1) and (A-3).  Condition (B-4)
  holds by (A-3) and Lemma~\ref{lem:GHtwobound}.
  \item If $2 |Z| \le |Y|$ and there is another original
  run to load, then the algorithm loads the next run as $\QQ_{\ell + 1}$.
  \begin{itemize}
  \item[$\circ$] If $|\QQ_{\ell + 1}| \le 2 |\QQ_\ell|$,
      then we claim that case~(A) still holds with
      $\ell$ incremented by one. In particular, (A-1) will hold
      since $|Y| \ge 2 |Z|$ is the same as
      $2 |\QQ_\ell| \le |\QQ_{\ell - 1}|$.
      Condition (A-2)
      will hold by the assumed bound on $|\QQ_{\ell + 1}|$.
      Condition (A-3) will still hold since $|\QQ_{\ell + 1}|$ is an
      original run so $m_{\QQ_{\ell + 1}}=1$ and
      $w_{\QQ_{\ell + 1}}=0$.
  \item[$\circ$] Otherwise $|\QQ_{\ell + 1}| > 2 |\QQ_\ell|$, and
      we claim that case~(C) below holds with
      $\ell$ incremented by one. (C-1) and (C-4) will hold by (A-1) and (A-3).
      For (C-2), we need
      $|\QQ_{\ell - 1}| \ge |\QQ_\ell|$;
      i.e.\ $|Y| \ge |Z|$: this follows trivially
          from $|Y| \ge 2 |Z|$.
      (C-5) holds by (A-3) and Lemma~\ref{lem:GHtwobound}.
      (C-3) holds since $2 |\QQ_\ell| < |\QQ_{\ell + 1}|$.
      (C-6) holds since $\QQ_{\ell + 1}$ is an original run.
  \end{itemize}
  \item If $2|Z| > |Y|$, then the algorithm merges the two
  runs $Y$ and~$Z$. We claim the resulting stack satisfies condition~(A)
  with $\ell$ decremented by one. (A-1) clearly will still hold.
  For (A-2) to still hold,
  we need $2 |X| \ge |Y| +  |Z|$: this
  follows from $2 |Y| \le |X|$ and $|Z|\le |X|$.
  (A-3) will clearly still hold for all $i<\ell - 1$.
  For $i = \ell - 1$, since merging $Y$ and $Z$ added $|Y| +  |Z|$
  to the merge cost, (A-3) implies that
  the new top stack element will
  have merge cost at most
  \[
  G_2(|Y|,m_Y) + G_2(|Z|,m_Z) + |Y| + |Z|.
  \]
  By (A-2) and Lemma~\ref{lem:Gbound},
  this is $\le G_2(|Y| +  |Z|, m_Y +  m_Z)$,
  so (A-3) holds.
  \end{itemize}

  \noindent
  {\bf (B): Wrapup mode, lines 15-18 of
  Algorithm~\ref{alg:alphaMergeSort}.}
  There are no more original runs to process. The
  entire input has been combined into the
  runs $\QQ_1, \ldots, \QQ_\ell$ and they satisfy:
  \begin{itemize}
  \setlength{\itemsep}{0pt}
  \item[(B-1)] $|\QQ_i| \ge 2\cdot |\QQ_{i + 1}|$ for all $i < \ell - 1$.
  This includes $|X|\ge 2|Y|$ if $\ell\ge 2$.
  \item[(B-2)] $|Y| \ge |Z|$;
         i.e., $|\QQ_{\ell - 1}| \ge |\QQ_\ell|$.
  \item[(B-3)] $w_{\QQ_i} \le G_2(|\QQ_i|,m_{\QQ_i})$ for all $i \le \ell - 1$,
  \item[(B-4)] $w_Z \le H_2(|Z|, m_Z)$.
  \end{itemize}
  If $\ell=1$, the run $Z = \QQ_1$ contains the entire input in sorted order
  and the algorithm terminates. The total merge cost is $\le H_2(|Z|, m_Z)$.
  This is $< n \cdot (d_2 + c_2 \log m )$
  as needed for Theorem~\ref{thm:twomergeUpper}.

  Otherwise $\ell>1$,
  and $Y$ and $Z$ are merged.\footnote{
      Note that in this case, if $\ell\ge 2$, $|Z| < |X|$,
       since (B-1) and (B-2) imply that
      $|X|\ge 2 |Y| \ge 2|Z| > |Z|$.
      This is the reason why Algorithm~\ref{alg:twoMergeSort} does not check
      for the condition $|X| < |Z|$ in lines
      \ref{algline:2mergefinalloopA}-\ref{algline:2mergefinalloopB} (unlike what
      is done on line~\ref{algline:2mergeXZ}).}
  We claim the resulting stack of
  runs satisfies case~(B),
  now with $\ell$ decremented by one. It is obvious that
  (B-1) and (B-3) still hold. (B-4) will still hold since by
  (B-3) and (B-4) the merge cost
  of the run formed by merging $Y$ and~$Z$ is at most
  $|Y| + |Z| + G_2(|Y|,m_Y) + H_2(|Z|,m_Z)$, and this is
  $\le H_2(|Y| +  |Z|,m_Y +  m_Z)$ by Lemma~\ref{lem:GHtwobounds}(a).
  To show (B-2) still holds, we must
  show that $|X| \ge |Y| +  |Z|$. To prove this,
  note that $\frac 1 2 |X| \ge |Y|$ by (B-1); thus from
  (B-2), also $\frac 1  2|X| \ge |Z|$.
  Hence $|X| \ge |Y| +  |Z|$.

  \smallskip

  \noindent
  {\bf (C): Encountered long run $Z$.}
  When case~(C) is first entered,
  the final run~$|Z|$ is long relative to~$|Y|$.
  The algorithm will repeatedly merge $X$ and~$Y$
  until $|Z|\le |X|$, at which point it merges $Y$ and~$Z$ and returns
  to case~(A). (The
  merge of $Y$ and $Z$ must eventually occur by the convention
  that $\QQ_0$ has infinite length.)
  The following conditions hold with $\ell\ge 2$:
  \begin{itemize}
    \setlength{\itemsep}{0pt}
    \item[(C-1)] $|\QQ_i| \ge 2 |\QQ_{i + 1}|$ for all $i < \ell - 2$.
      If $\ell \ge 3$, this includes $|W| \ge 2|X|$.
    \item[(C-2)] $|X| \ge |Y|$;
      i.e., $|\QQ_{\ell - 2}| \ge |\QQ_{\ell - 1}|$.
    \item[(C-3)] $|Y| < 2 |Z|$;
      i.e., $|\QQ_{\ell - 1}| < 2 |\QQ_\ell|$.
    \item[(C-4)] $w_{\QQ_i} \le G_2(|\QQ_i|, m_{\QQ_i})$ for all
      $i \le \ell - 2$.
    \item[(C-5)] $w_Y \le H_2(|Y|, m_Y)$.
    \item[(C-6)] $m_Z = 1$ and $w_Z = 0$,
      because $Z$ is an original run and has not undergone a merge.
  \end{itemize}
  By (C-3), the test on line~\ref{algline:2mergeXYZ} of Algorithm~\ref{alg:twoMergeSort}
  will now trigger a merge, either of $Y$ and~$Z$ or of $X$ and~$Y$,
  depending on the relative sizes of $X$ and~$Z$.
  We handle separately the cases $|Z| > |X|$ and $|Z| \le |X|$.
  \begin{itemize}
    \item Suppose $|Z|>|X|$. Then $\ell\ge 3$ and the algorithm
      merges $X$ and~$Y$. We claim that case~(C) still holds,
      now with $\ell$ decremented by~1. It is obvious that
      (C-1), (C-4) and (C-6) still hold.
      (C-5) will still hold, since by (C-4) and (C-5),
      the merge cost of the run obtained by
      merging $X$ and $Y$ is at most
      $|X| + |Y|+G_2(|X|,m_X) + H_2(|Y|,m_Y)$, and this is
      $\le H_2(|X| + |Y|,m_X +  m_Y)$ by Lemma~\ref{lem:GHtwobounds}(a)
      since $|X|\ge |Y|$.
      To see that (C-2) still holds, we argue exactly
      as in case~(B) to
      show that $|W| \ge |X| +  |Y|$. To prove this,
      note that $\frac 1 2 |W| \ge |X|$ by (C-1); thus from
      (C-2), $\frac 1 2 W \ge |Y|$. Hence $|W| \ge |X| +  |Y|$.
      To establish that (C-3) still holds, we must
      prove that $|X| +  |Y| <  2|Z|$.
      By the assumption that $|Z|>|X|$, this follows from
      $|Y| \le |X|$, which holds by (C-2).

    \item Otherwise, $|Z| \le |X|$ and $Y$ and $Z$ are merged. We claim that
      now case~(A) will hold.
      (A-1) will hold by (C-1).
      To show (A-2) will hold, we need $|Y| + |Z| \le 2|X|$:
      this holds by (C-2) and $|Z| \le |X|$.
      (A-3) will hold for $i < \ell - 1$ by (C-4).
      For $i=\ell - 1$, the merge cost $w_\YZ$ of the run obtained by
      merging $Y$ and~$Z$ is $\le H_2(|Y|,m_Y) + |Y| + |Z|$
      by (C-5) and (C-6). By (C-3) and Lemma~\ref{lem:GHtwobounds}(b)
      this is $\le G_2(|Y| + |Z|, m_Y + m_Z)$.
      Hence (A-3) will hold with $i = \ell - 1$.
  \end{itemize}

  That completes the proof of Theorem~\ref{thm:twomergeUpper}.
\end{proof}

Examination of the above proof shows why
the $2$-merge Algorithm~\ref{alg:twoMergeSort}
does not need to test
the condition $|X| < 2|Y|$ on line~\ref{algline:2mergeXYZ};
in contrast to what the $\alpha$-merge
Algorithm~\ref{alg:alphaMergeSort} does. In cases
(A) and~(B), the test will fail by conditions (A-1) and (B-1).
In case~(C), condition (C-3) gives
$|Y| < 2|Z|$, so an additional test would be redundant.

\subsection{Upper bound proof for \texorpdfstring{$\alpha$}{a}-merge sort}
\label{sec:alphaMergeUpperPf}

This section gives the proof Theorem~\ref{thm:alphamergeUpper}.
The general outline of the proof is similar to that of
Theorem~\ref{thm:twomergeUpper}; however, we must handle
a new, and fairly difficult, case (D). It is also necessary to
bound the total merge cost $\sum_i w_{\QQ_i}$ of all the runs
in the stack~$\mathcal \QQ$, instead of bounding
each individual merge cost $w_{\QQ_i}$.
We first prove a lemma stating properties of $G_\alpha$
and $H_\alpha$ which follow from
Lemmas \ref{lem:alphamerge-B}, \ref{lem:alphamerge-Ctwo}(b)
and~\ref{lem:alphamerge-CD}.  Parts (a) and~(b) of the lemma generalize
Lemma~\ref{lem:GHtwobounds}.

\begin{lemma}\label{lem:GHalphabounds}
  Suppose $n_1, n_2, m_1, m_2$ are positive integers, and
  $\varphi < \alpha < 2$.
  \begin{description}
    \item[\rm (a)] If $(\alpha  -  1) n_2 \le n_1$, then
      \[
        G_\alpha(n_1, m_1) + H_\alpha(n_2, m_2) + n_1 + n_2
        ~\le~
        H_\alpha(n_1  +   n_2, m_1  +   m_2).
      \]
    \item[\rm (b)] If $n_1 \le \frac{\alpha}{\alpha-1} \cdot n_2$, then
      \[
        H_\alpha(n_1,m_1) + n_1 + n_2 ~\le~ G_\alpha(n_1 +  n_2, m_1 +  m_2).
      \]
    \item[\rm (c)] If $n_1 \le \frac{\alpha}{\alpha-1} n_2$, then
      \[
          H_\alpha(n_1, m_1) ~\le~ G_\alpha(n_1,m_1) + G_\alpha(n_2, 1).
      \]
    \item[\rm (d)] If $k\le k_0(\alpha) +  1$ and
      $\frac{2^k (2\alpha - 1)}{\alpha - 1} n_3 \ge n_1 + n_2 + n_3$, then
      \[
       H_\alpha(n_1, m_1) + H_\alpha(n_2, m_2) + k \cdot n_3 + n_1 + 2n_2
       ~\le~ G_\alpha(n_1  +   n_2  +   n_3, m_1  +   m_2  +   1) .
      \]
  \end{description}
\end{lemma}

\begin{proof}
  Part (a) states that
  \[
    n_1 \cdot (d_\alpha - 1 + c_\alpha \log m_1)
     + n_2 \cdot (d_\alpha + c_\alpha \log m_2) + n_1 + n_2
    ~\le~
    (n_1  +   n_2) \cdot (d_\alpha + c_\alpha \log (m_2  +   m_2)).
  \]
  This is an immediate consequence of Lemma~\ref{lem:alphamerge-B}.

  Part~(b) states that
  \[
    n_1 \cdot (d_\alpha + c_\alpha \log m_1))
     + n_1 + n_2
    ~\le~
    (n_1 + n_2) \cdot (d_\alpha - 1 + c_\alpha \log (m_1  +   m_2)).
  \]
  This is exactly Lemma~\ref{lem:alphamerge-Ctwo}(b).

  The inequality of part~(c) states
  \[
    n_1\cdot(d_\alpha + c_\alpha \log m_1) ~\le~
       n_1\cdot(d_\alpha - 1 + c_\alpha \log m_1)
       + n_2 \cdot (d_\alpha - 1 + 0 ).
  \]
  After cancelling common terms, this is the same as
  $n_1 \le n_2 \cdot (d_\alpha - 1)$.  To establish this,
  it suffices to show that $d_\alpha - 1 \ge \frac{\alpha}{\alpha-1}$.
  Since $k_0(\alpha)\ge 1$,
  we have $d_\alpha\ge \frac{12(2\alpha-1)}{\alpha-1}+1$.
  And, since $\alpha>1$, we have $12(2\alpha-1)>\alpha$.
  Therefore $d_\alpha - 1 > \frac{\alpha}{\alpha-1}$, and
  (c)~is proved.

  Part~(d) states that
  \begin{eqnarray}
    \nonumber
    \lefteqn{ n_1 \cdot (d_\alpha + c_\alpha \log m_1) +
        n_2 \cdot (d_\alpha + c_\alpha \log m_2) +
        k \cdot n_3 + n_1 + 2 n_2} \\
    \label{eq:GHalphabounds_d}
    &\le& (n_1 + n_2 + n_3) \cdot (d_\alpha - 1 + c_\alpha \log (m_1  +   m_2  +   1)).
    \hspace*{1in}
  \end{eqnarray}
  Lemma~\ref{lem:alphamerge-CD} implies that
  \begin{eqnarray*}
  \lefteqn{ n_1 \cdot (d_\alpha + c_\alpha \log m_1) +
      n_2 \cdot (d_\alpha + c_\alpha \log m_2) +
      k \cdot n_3 + n_1 + 2 n_2} \\
  &\le&
      n_1 \cdot (d_\alpha - 1 + c_\alpha \log m_1) +
      (n_2 + n_3) \cdot (d_\alpha -1 + c_\alpha \log (m_2  +   1)) .
  \end{eqnarray*}
  The desired inequality (\ref{eq:GHalphabounds_d}) follows easily.
\end{proof}

We now prove Theorem~\ref{thm:alphamergeUpper}.

\begin{proof}[Proof of Theorem~\ref{thm:alphamergeUpper}]
  We describe the $\alpha$-merge algorithm using four invariants
  (A), (B), (C), (D) for the stack; and analyze what action is taken
  in each situation. Initially, the stack contains a single original $\QQ_1$, so
  $\ell = 1$ and $m_{\QQ_1} = 1$ and $w_{\QQ_1}=0$, and case~(A) applies.

  \smallskip

  \noindent
  {\bf (A): Normal mode.} The stack satisfies
  \begin{itemize}
    \setlength{\itemsep}{0pt}
    \item[(A-1)]
      $|\QQ_i| \ge \alpha |\QQ_{i + 1}|$ for all $i < \ell  -  1$. This includes
      $|X| \ge \alpha|Y|$ if $\ell \ge 2$.
    \item[(A-2)] $\alpha |Y| \ge |Z|$; i.e.\ $\alpha |\QQ_{\ell - 1}| \ge |\QQ_\ell|$.
    \item[(A-3)] $\sum_{i = 1}^\ell w_{\QQ_i} ~\le~
      \sum_{i = 1}^\ell G_\alpha(|\QQ_i|, m_{\QQ_i})$.
  \end{itemize}

  If $\ell \ge 2$, (A-1) and (A-2) imply $|X|\ge |Z|$, i.e.\
  $|\QQ_{\ell - 2}| \ge |\QQ_\ell|$.
  The $\alpha$-merge algorithm does one of the following:
  \begin{itemize}
    \item If $\alpha |Z| \le |Y|$ and there are no more original runs to load,
      then it goes to case~(B). Condition (B-1) holds by (A-1). (B-3) holds by
      (A-3) since
      $G_\alpha(|\QQ_\ell|, m_{\QQ_\ell}) \le H_\alpha(|\QQ_\ell|, m_{\QQ_\ell})$.
      Condition (B-2) states that $(\alpha - 1) |Z| \le |Y|$ and this holds since
      $\alpha |Z| \le |Y|$.

    \item If $\alpha |Z| \le |Y|$ and there is another original run to load, then
      the algorithm loads the next run as $\QQ_{\ell + 1}$.
      \begin{itemize}
        \item[$\circ$] If $|\QQ_{\ell + 1}| \le \alpha |\QQ_\ell|$,
          then we claim that case~(A) still holds after $\ell$ is incremented
          by one. In particular, (A-1) and $\alpha |Z| \le |Y|$ imply that
          (A-1) will still hold since $\alpha |Z| \le |Y|$ is the same as
          $\alpha |\QQ_\ell| \le |\QQ_{\ell - 1}|$. (A-2) still holds by the
          assumed bound on $|\QQ_{\ell + 1}|$. Condition (A-3) still holds since
          $|\QQ_{\ell + 1}|$ is an original run so $m_{\QQ_{\ell + 1}}=1$ and
          $w_{\QQ_{\ell + 1}} = 0$.
        \item[$\circ$] Otherwise $|\QQ_{\ell + 1}| > \alpha |\QQ_\ell|$, and
          we claim that case~(C) below holds with
          $\ell$ incremented by one.
          (C-1) holds by (A-1).
          For (C-2), we need $|\QQ_{\ell - 1}| \ge (\alpha  -  1)|\QQ_\ell|$, i.e.
          $|Y| \ge (\alpha  -  1)|Z|$: this follows trivially from
          $|Y| \ge \alpha |Z|$. (C-4) holds by (A-3), since
          $G_\alpha(|\QQ_\ell|,m_{\QQ_\ell}) \le H_\alpha(|\QQ_\ell|,m_{\QQ_\ell})$ and
          since $\QQ_{\ell + 1}$ is an original run so $m_{\QQ_{\ell + 1}} = 1$ and
          $w_{\QQ_{\ell + 1}} = 0$. To have (C-3) hold, we need
          $|Z| \le \frac{\alpha}{(\alpha - 1)}|\QQ_{\ell + 1}|$. This follows
          from the hypothesis $|\QQ_{\ell + 1}| > \alpha |Z|$ and $\alpha > 1$.
          Finally, (C-5) will hold since $\QQ_{\ell + 1}$ is an original run.
      \end{itemize}

    \item If $\alpha |Z| > |Y|$, then $\ell\ge 2$ . In this case, since
      $|Z|\le|X|$, the algorithm merges the two runs $Y$ and~$Z$. We claim the
      resulting stack satisfies case~(A) with $\ell$ decremented by one.
      It is obvious that (A-1) still holds. (A-3) still holds by
      Lemma~\ref{lem:Gbound}. For (A-2), we need $|Y| + |Z| \le \alpha|X|$;
      this follows from $|Y| \le \frac{1}{\alpha} |X|$ and $|Z| \le |X|$ and
      $1 + \frac{1}{\alpha} < \alpha$ as $\varphi<\alpha$.
  \end{itemize}

  \noindent
  {\bf (B): Wrapup mode, lines 15-18 of
  Algorithm~\ref{alg:alphaMergeSort}.}
  There are no more original runs to process. The
  entire input has been combined into the
  runs $\QQ_1, \ldots, \QQ_\ell$ and they satisfy:
  \begin{itemize}
  \item[(B-1)] $|\QQ_i| \ge \alpha |\QQ_{i + 1}|$ for all $i < \ell  -  1$.
     This includes $|X| \ge \alpha |Y|$ if $\ell\ge 2$.
  \item[(B-2)] $|Y| \ge (\alpha  -  1)|Z|$; i.e.,
      $|\QQ_{\ell - 1}| \ge (\alpha  -  1)|\QQ_\ell|$.
  \item[(B-3)]
      $\sum_{i = 1}^\ell w_{\QQ_i} ~\le~
           \sum_{i = 1}^{\ell - 1}  G_\alpha(|\QQ_i|, m_{\QQ_i}) +
           H_\alpha(|\QQ_\ell|, m_{\QQ_\ell})$.
  \end{itemize}
  If $\ell = 1$, the run $Z = \QQ_1$ contains the entire input in sorted order
  and the algorithm terminates. The total merge cost is
  $\le H_\alpha(|Z|, m_Z)$. This is $n \cdot (d_\alpha + c_\alpha \log m )$
  as needed for Theorem~\ref{thm:alphamergeUpper}.

  If $\ell > 1$, then $Y$ and $Z$ are merged.\footnote{
      Note that in this case, if $\ell\ge 2$, $|Z| < |X|$ since (B-1) and (B-2) imply that
      $|X|\ge \alpha |Y| \ge \alpha(\alpha - 1)|Z|$ and $\alpha^2-\alpha > 1$
      since $\varphi < \alpha$.
      This is the reason why Algorithm~\ref{alg:alphaMergeSort} does not check
      for the condition $|X| < |Z|$ in lines
      \ref{algline:amergefinalloopA}-\ref{algline:amergefinalloopB} (unlike what
      is done on line~\ref{algline:amergeXZ}).
  }
  We claim that the resulting stack of runs satisfies (B), now with $\ell$ decremented by one.
  It is obvious that (B-1) will still holds.
  (B-3) will still hold since merging $Y$ and~$Z$
  adds $|Y| + |Z|$ to the total merge cost and since
  \[
    G(|Y|, m_Y) + H(|Z|, m_Z) + |Y| + |Z|
    ~\le~
    H(|Y|  +   |Z|, m_Y  +   m_Z)
  \]
  by Lemma~\ref{lem:GHalphabounds}(a) since $|Y| \ge (\alpha - 1)|Z|$ by (B-2).
  To show (B-2) will still hold, we must show that
  $|X| \ge (\alpha  -  1)(|Y|  +   |Z|)$. To prove this, note that
  $\frac{1}{\alpha} |X| \ge |Y|$ by (B-1); thus from (B-2),
  $\frac{1}{\alpha(\alpha-1)}|X| \ge |Z|$. This gives
  $\bigl( \frac 1 \alpha + \frac 1 {\alpha(\alpha-1)} \bigr)|X| \ge |Y|  +   |Z|$;
  hence $|X| \ge (\alpha  -  1)(|Y|  +   |Z|)$.

  \smallskip

  \noindent
  {\bf (C): Encountered long run $Z$.}
  When case~(C) is first entered, the final run~$Z$ is long relative to~$Y$.
  The algorithm will repeatedly merge $X$ and~$Y$ as long as $|Z| < |X|$, staying
  in case~(C). Once $|Z|\le|X|$, as discussed below,
  there are several possibilities. First, it may be
  that case~(A) already applies. Otherwise, $Y$ and~$Z$ are merged, and the algorithm
  proceeds to either case~(A) or case~(D).

  Formally, the following conditions hold during case~(C)
  with $\ell\ge 2$:
  \begin{itemize}
    \setlength{\itemsep}{0pt}
    \item[(C-1)] $|\QQ_i| \ge \alpha |\QQ_{i + 1}|$ for all
      $i < \ell  -  2$. If $\ell\ge 4$, this includes $|W| \ge \alpha|X|$.
    \item[(C-2)] $|X| \ge (\alpha  -  1) |Y|$;
      i.e., $|\QQ_{\ell - 2}| \ge (\alpha  -  1)|\QQ_{\ell - 1}|$.
    \item[(C-3)] $|Y| \le \frac \alpha{(\alpha - 1)} |Z|$;
      i.e., $|\QQ_{\ell  -  1}| \le \frac{\alpha}{(\alpha - 1)} |\QQ_\ell|$.
    \item[(C-4)] $\sum_{i = 1}^\ell w_{\QQ_i} \le
      \sum_{i = 1}^{\ell - 2} G_\alpha(|\QQ_i|, m_{\QQ_i}) + H_\alpha(|Y|, m_Y)$.
    \item[(C-5)] $Z$ is an original run, so $m_Z=1$ and $w_Z = 0$.
  \end{itemize}

  It is possible that no merge is needed, namely if $|X| \ge \alpha |Y|$ and
  $|Y| \ge \alpha |Z|$. In this case we claim that case (A) holds. Indeed,
  (A-1) will hold by (C-1) and since $|X| \ge \alpha |Y|$. Condition (A-2)
  holds by $|Y| \ge \alpha |Z|$. Condition (A-3) follows from (C-4) and the
  fact that, using (C-3), Lemma~\ref{lem:GHalphabounds}(c) gives the inequality
  $H_\alpha(|Y|, m_Y) \le G_\alpha(|Y|, m_Y) +  G_\alpha(|Z|, 1)$.\footnote{
    It is this step which requires us to bound the total merge cost $\sum_i
    w_{\QQ_i}$ instead of the individual merge costs $w_{\QQ_i}$. Specifically,
    $G_\alpha(|Y|, m_Y)$ may not be an upper bound for $w_Y$.
  }

  Otherwise, a merge occurs. The cases $|Z| > |X|$ and $|Z|\le|X|$
  are handled separately:

  \begin{itemize}
    \item Suppose $|Z| > |X|$. We have $\ell\ge 3$ by the convention that
      $|\QQ_0| = \infty$, and the algorithm
      merges $X$ and~$Y$. We claim that case~(C) still holds, now
      with $\ell$ decremented by~1.
      It is obvious that (C-1) and (C-5) will still hold.
      (C-4) will still hold since merging $X$ and~$Y$
      adds $|X| + |Y|$ to the total merge cost, and since
      \[
        G_\alpha(|X|, m_X) + H_\alpha(|Y|, m_Y) + |X| + |Y|
          ~\le~ H_\alpha(|X| + |Y|, m_X + m_Y)
      \]
      by Lemma~\ref{lem:GHalphabounds}(a) since $|X| \ge (\alpha - 1) |Y|$
      by (C-3).

      To see that (C-2) still holds, we argue exactly as in case~(B): We must
      show that $|W| \ge (\alpha - 1)(|X| + |Y|)$. To prove this, note that $\frac{1}{\alpha} |W| \ge |X|$ by (C-1); thus from (C-2),
      $\frac{1}{\alpha(\alpha-1)} W \ge |Y|$. This gives
      $\left(
          \frac{1}{\alpha} + \frac{1}{\alpha(\alpha - 1)}
      \right) |W| \ge |X| + |Y|$;
      hence $|W| \ge (\alpha  -  1)(|X|  +   |Y|)$.

      To establish that (C-3) still holds, we must
      prove that $|X| + |Y| \le \frac{\alpha}{\alpha - 1}|Z|$. Since
      $|Z| > |X|$, it suffices to show $|X| + |Y| \le
      \frac{\alpha}{\alpha - 1}|X|$, or equivalently
      $|Y| \le \frac{1}{\alpha - 1}|X|$. This holds by (C-2).


    \item Otherwise $|Z| \le |X|$, so $\ell \ge 2$,
      and $Y$ and $Z$ are merged. The analysis
      splits into two cases, depending on whether $|Y| + |Z| \le \alpha |X|$
      holds.

      First, suppose $|Y| + |Z| \le \alpha |X|$. Then we
      claim that, after the merge of $Y$ and~$Z$, case~(A) holds.
      Indeed, (A-1) will hold by (C-1). (A-2) will hold by
      $|Y| + |Z| \le \alpha |X|$. Condition (A-3) will hold by (C-4) and (C-5)
      and the fact that, using (C-3), Lemma~\ref{lem:GHalphabounds}(b) gives
      the inequality
      $H_\alpha(|Y|, m_Y) + |Y| + |Z|\le G_\alpha(|Y| +  |Z|, m_Y +  1)$.

      Second, suppose $|Y| + |Z| > \alpha |X|$ and thus $\ell \ge 3$.
      We claim that, after the merge of $Y$ and~$Z$, case (D) holds.
      (D-1) holds by (C-1).
      (D-2) holds with $k = 0$ so $Z_1$ is the empty run, and
      with $Z_2$ and~$Z_3$ equal to the just-merged runs $Y$ and~$Z$
      (respectively).
      (D-3) holds since $k=0$ and $|X| \ge |Z|$.
      (D-4) holds by (C-2) and the choice of $Z_1$ and~$Z_2$.
      (D-5) holds by (C-3).
      (D-6) holds since is the same as the assumption
      that $|Y| + |Z| > \alpha |X|$.
      (D-7) holds since, by (C-3),
          $|Y|  +   |Z| \le
          \Bigl(\frac{\alpha}{\alpha - 1}  +   1\Bigr) |Z| =
          \frac{2\alpha - 1}{\alpha - 1} |Z|$.
      Finally, we claim that (D-8) holds by (C-4).
      To see this, note
      that with $k = 0$, the quantity ``$(k  +   1) Z_3 + Z_2$'' of (D-8) is
      equal to the cost $|Y| + |Z|$ of merging $Y$ and~$Z$,
      and that the quantities ``$m_Z  -  1$'' and ``$|Z_1| + |Z_2|$''
      of (D-8) are the same as our $m_Y$ and $|Y|$.
  \end{itemize}

  \noindent
  {\bf (D): Wrapping up handling a long run.}
  In case~(D), the original long run,
  which was earlier called ``$Z$'' during case~(C), is now
  called ``$Z_3$'' and has been merged with runs at the
  top of the stack to form the current top stack element $Z$.
  This $Z$ is equal to the merge of three runs $Z_1$, $Z_2$ and~$Z_3$.
  The runs $Z_2$ and~$Z_3$ are the two runs $Y$ and~$Z$ which
  were merged when leaving case~(C) to enter case~(D). The run
  $Z_1$ is equal to the merge of $k$ many runs $U_k, \ldots,U_1$
  which were just below the top of the stack when leaving case~(C)
  to enter case~(D). Initially $k=0$, so $Z_1$ is empty.

  The runs $Z_2$ and~$Z_3$ do not change while the algorithm is
  in case~(D).
  Since $Z_3$ was an original run, $m_{Z_3}=1$. In other words,
  $m_Z - 1 = m_{Z_1} +  m_{Z_2}$.

  There are two possibilities for how the merge algorithm proceeds in
  case~(D). In the simpler case, it merges Y and Z, and either
  goes to case~(A) or stays in case~(D).
  If it stays in case~(D), $k$~is incremented by~1.
  In the more complicated case, it merges X and Y, then merges the
  resulting run with Z, and then again either goes to (A) or stays in (D).
  In this case, if it stays in~(D), $k$ is incremented by~2.
  Thus $k$ is equal to the number of merges that have been performed.
  We will show that $k<k_0(\alpha)$ must always hold.

  Formally, there is an integer $k < k_0(\alpha)$
  and there exists runs $Z_1$, $Z_2$ and
  $Z_3$ (possibly $Z_1$ is empty) such that $\ell\ge 2$ and the following hold:
  \begin{itemize}
      \setlength{\itemsep}{0pt}
      \item[(D-1)] $|\QQ_i| \ge \alpha |\QQ_{i + 1}|$ for all
          $i < \ell  -  1$. This includes $X\ge \alpha Y$.
      \item[(D-2)] $Z$ is equal to the merge of three runs $Z_1,Z_2,Z_3$.
      \item[(D-3)] $|Y| \ge \alpha^k |Z_3|$.
      \item[(D-4)] $|Y|\ge (\alpha - 1)(|Z_1| + |Z_2|)$.
      \item[(D-5)] $|Z_3| \ge \frac{\alpha - 1}{\alpha} |Z_2|$.
      \item[(D-6)] $\alpha |Y| < |Z|$.
      \item[(D-7)] $|Z| \le  \frac{2^k (2\alpha - 1)}{\alpha - 1} |Z_3|$.
      \item[(D-8)] $\sum_{i = 1}^\ell w_{\QQ_i} ~\le~
          \sum_{i = 1}^{\ell - 1} G_\alpha(|\QQ_i|, m_{\QQ_i}) + (k  +   1) |Z_3| + |Z_2| +
          H_\alpha(|Z_1| +  |Z_2|, m_Z  -  1)$.
  \end{itemize}
  We claim that conditions (D-3), (D-4) and (D-6) imply that $k<k_0(\alpha)$.
  To prove this, suppose $k\ge k_0(\alpha)$. From the definition
  of $k_0$, this implies that $\alpha \ge \frac 1 {\alpha^k} + \frac 1{\alpha-1}$.
  (D-3) gives $\frac1{\alpha^k} |Y| \ge |Z_3|$;
  (D-4) gives $\frac1{\alpha-1} |Y| \ge |Z_1| + |Z_2|$.
  With (D-6) and $|Z| = |Z_1| +  |Z_2| +  |Z_3|$, these imply
  \[
  |Z| ~>~ \alpha|Y|
       ~\ge~ \Bigr(\frac1{\alpha^k} + \frac1{\alpha-1}\Bigl ) |Y|
       ~\ge~ |Z_1| + |Z_2| + |Z_3| ~=~ |Z| ,
  \]
  which is a contradiction.

  By (D-6) and the test in line~\ref{algline:amergeXYZ} of
  Algorithm~\ref{alg:alphaMergeSort}, the algorithm must perform a merge,
  either of $X$ and~$Y$ or of $Y$ and~$Z$, depending on the relative sizes of
  $X$ and~$Z$. The cases of $|Z| \le |X|$ and $|Z| > |X|$ are handled
  separately.
  \begin{itemize}
    \item Suppose $|Z| \le |X|$.  Therefore, the algorithm merges $Y$ and~$Z$.

      In addition, suppose that $\alpha |X| \ge |Y| + |Z|$. We claim that this
      implies case~(A) holds after the merge. Indeed, (D-1) implies that
      condition (A-1) holds. The assumption $\alpha |X| \ge |Y| + |Z|$ gives
      that (A-2) will hold. For (A-3), we argue by applying
      Lemma~\ref{lem:GHalphabounds}(d) with $n_1 = |Y|$ and $n_2 = |Z_1| +
      |Z_2|$ and $n_3 = |Z_3|$ and with $k +  2$ in place of~$k$.
      For this, we need $k +  2 \le k_0(\alpha) +  1$ as was already proved and
      also need
      \[
        \frac{2^{k+2} (2\alpha  -  1)}{\alpha - 1} |Z_3|
        ~\ge~ |Y|+ |Z|.
      \]
      This is true by (D-7) since $|Y| + |Z| < (1 + 1/\alpha)|Z|$ by (D-6)
      and since $1 + 1 / \alpha < 4$. We also have $m_Z  -  1 = m_{Z_1}  +   m_{Z_2} > 0$.
      Thus, Lemma~\ref{lem:GHalphabounds}(d) implies
      \[
        H_\alpha(|Y|, m_Y) + H_\alpha(|Z_1| + |Z_2|, m_Z - 1) +
        (k + 2) \cdot |Z_3| + |Y| + 2(|Z_1| + |Z_2|)
        ~\le~
        G_\alpha(|Y|  +   |Z|, m_Y +  m_Z).
      \]
      Since $\alpha < 2$, $G_\alpha(|Y|, m_Y) \le H_\alpha(|Y|, m_Y)$.
      Therefore, since $|Z| = |Z_1|  +   |Z_2|  +   |Z_3|$,
      \[
        G_\alpha(|Y|,m_Y) + (k  +   1) |Z_3| + |Z_2| +
            H_\alpha(|Z_1|  +   |Z_2|, m_Z  -  1) + |Y| +|Z|
        ~\le~
        G_\alpha(|Y| + |Z|, m_Y + m_Z).
      \]
      Since the cost of merging $Y$ and~$Z$ is equal to $|Y| + |Z|$, this
      inequality plus the bound (D-8) implies that (A-3) will hold after the
      merge.

      Alternately, suppose $\alpha|X| < |Y| + |Z|$. In this case,
      $Y$ and $Z$ are merged:
      $k$~will be incremented by~1 and $Y$~will become part of~$Z_1$.
       We claim that case~(D) still holds after the merge.
      Clearly (D-1) still holds.
      (D-2) still holds with $Z_1$ now including~$Y$. (That is,
      $Y$ becomes~$U_{k+1}$.) (D-1) and (D-3) imply
      $|X| \ge \alpha |Y| \ge \alpha^{k + 1} |Z_3|$,
      so (D-3) will still hold.
      (D-4) implies
      $\frac{|Y|}{\alpha - 1} \ge |Z_1| + |Z_2|$.
      Therefore, (D-1) gives
      \[
        \frac{|X|}{\alpha - 1} ~\ge~ \frac{\alpha |Y|}{\alpha - 1}
          ~=~ |Y| + \frac{|Y|}{\alpha - 1} ~\ge~ |Y| + |Z_1| + |Z_2|.
      \]
      Thus (D-4) will still hold after the merge (since, after the merge,
      $Y$ becomes part of~$Z_1$).
      The hypothesis $\alpha|X| < |Y| + |Z|$ implies (D-6) will still hold.
      By (D-6) we have $|Z| > \alpha |Y| > |Y|$, so (D-7) gives
      \[
      |Z| + |Y| ~\le~ 2 |Z| ~\le~ \frac{2^{k + 1} (2\alpha  -  1)}{\alpha - 1} |Z_3|.
      \]
      Thus (D-7) will still hold. Finally,
      by (D-4), we may apply Lemma~\ref{lem:GHalphabounds}(a)
      with $n_1=|Y|$ and $n_2=|Z_1| +  |Z_2|$ to obtain
      \[
        G_\alpha(|Y|,m_Y) + H_\alpha(|Z_1| +  |Z_2|,m_Z - 1)
         + |Y| + |Z_1| + |Z_2| + |Z_3|
         ~\le~ H_\alpha(|Y| + |Z_1| +  |Z_2|,m_Y+m_Z - 1) + |Z_3|.
      \]
      Since the cost of merging $Y$ and $Z$ is
      $|Y| + |Z_1| + |Z_2| + |Z_3|$ and since $k$ is incremented
      by~1 after the merge, this implies that (D-8) will still
      hold after the merge.

    \item Now suppose $|Z| > |X|$, so $\ell\ge 3$.
      In this case, algorithm merges $X$ and $Y$; the result
      becomes the second run on the stack, which we denote $(XY)$.
      We claim that immediately after this, the
      algorithm merges the combination $(XY)$
      of $X$ and~$Y$ with the run $Z$.
      Indeed, since $\varphi<\alpha$, we have $\alpha > 1+ 1/\alpha$
      and therefore by (D-6) and by the assumed bound on $|X|$
      \[
          \alpha |Z| ~>~ \frac{1}{\alpha} |Z| + |Z| ~>~ |Y| + |X|.
      \]
      Thus, the test on line~\ref{algline:amergeXYZ} of
      Algorithm~\ref{alg:alphaMergeSort} triggers a second
      merge operation.  Furthermore, since $\varphi<\alpha$,
      we have $1 > \frac{1}{\alpha^2} + \frac{1}{\alpha^2 (\alpha - 1)}$
      and thus, using (D-1), (D-3) and (D-4),
      \[
          |W| ~>~ \frac{1}{\alpha^{k + 2}} |W| +
                  \frac{1}{\alpha^2 (\alpha  -  1)} |W|
              ~\ge~ \frac{1}{\alpha^k} |Y| +
                  \frac{1}{\alpha - 1} |Y|
              ~\ge~ |Z_1| + |Z_2| + |Z_3|= |Z|.
      \]
      With $|W| > |Z|$, the second merge acts to merge $(XY)$ and~$Z$
      instead of $W$ and $(XY)$. After the second merge,
      the top three runs $X,Y,Z$ on the stack
      have been merged, with an additional merge cost
      of $2|X| +  2|Y| +  |Z|$.  As we argue next, the algorithm
      now either transitions to case~(A)
      or stays in case~(D), depending whether
      $\alpha |W| \ge |X| + |Y| + |Z|$ holds.

      First, suppose that $\alpha |W| \ge |X| + |Y| + |Z|$.
      We claim that this implies case~(A) holds after the two merges.
      Indeed, (D-1) implies that condition (A-1) will hold.
      The assumption $\alpha |W| \ge |X| + |Y| + |Z|$ gives
      that (A-2) will hold.
      For (A-3), we argue as follows. We apply
      Lemma~\ref{lem:GHalphabounds}(d) with $n_1 = |X|  +   |Y|$
      and $n_2 = |Z_1|  +   |Z_2|$ and $n_3 = |Z_3|$
      and with $k+2$ in place of~$k$.
      For this, we need $k +  2 \le k_0(\alpha)+1$ as was already proved
      and also need
      \begin{equation}\label{eq:ZthreeXYZ}
          \frac{2^{k + 2} (2\alpha  -  1)}{\alpha - 1} |Z_3|
          ~\ge~ |X| + |Y| + |Z|.
      \end{equation}
      To prove~(\ref{eq:ZthreeXYZ}), first note that
      that $|X| + |Y| +|Z| < (2 + 1 / \alpha)|Z|$ by
      (D-6) and the assumption that $|Z| > |X|$;
      then (\ref{eq:ZthreeXYZ}) follows from (D-7)
      and the fact that $2+1/\alpha<4$.
      We have $m_Z  -  1 = m_{Z_1}  +   m_{Z_2} > 0$.
      Thus, Lemma~\ref{lem:GHalphabounds}(d) implies
      \begin{eqnarray}
        \nonumber
        \lefteqn{H_\alpha(|X|  +   |Y|, m_X  +   m_Y) + H_\alpha(|Z_1|  +   |Z_2|, m_Z  -  1) +
                          (k  +   2) \cdot |Z_3|
                        + |X| + |Y| + 2(|Z_1| + |Z_2|)} \\
        \label{eq:Dtwo1}
        &\le& G_\alpha(|X|  +   |Y|  +   |Z|, m_X  +   m_Y  +   m_Z).
              \hspace*{3.3in}
      \end{eqnarray}
      By (D-1), we have $(\alpha  -  1) |Y| < \alpha |Y| \le |X|$.
      Thus Lemma~\ref{lem:GHalphabounds}(a), with $n_1 = |X|$ and
      $n_2 = |Y|$ and $m_1=m_X$ and $m_2 = m_Y$ gives
      \begin{equation}\label{eq:Dtwo2}
        G_\alpha(|X|, m_X) + H_\alpha(|Y|, m_Y) + |X| + |Y|
         ~\le~ H_\alpha(|X|  +   |Y|, m_X  +   m_Y).
      \end{equation}
      Since $\alpha<2$, $G_\alpha(|Y|, m_Y) < H_\alpha(|Y|, m_Y)$.
      So, using $|Z| = |Z_1| +  |Z_2| +  |Z_3|$,
      (\ref{eq:Dtwo1}) and~(\ref{eq:Dtwo2}) imply
      \begin{eqnarray*}
        \lefteqn{G_\alpha(|X|, m_X) +
              G_\alpha(|Y|, m_Y) +
              (k  +   1) |Z_3| + |Z_2| +
              H_\alpha(|Z_1|  +   |Z_2|, m_Z  -  1) + 2|X|  +   2|Y|  +  |Z|} \\
        &\le& G_\alpha(|X|  +   |Y|  +   |Z|, m_X  +   m_Y  +   m_Z).
                \hspace*{3.3in}
      \end{eqnarray*}
      Since the cost of the two merges combining $X$, $Y$, and~$Z$
      was $2|X|  +   2|Y|  +  |Z|$,
      the last inequality and (D-8) imply
      that (A-3) will hold after the two merges.

      Alternately, suppose $\alpha |W| < |X| + |Y| + |Z|$. This implies
      $\ell \ge 4$. We claim that in this
      case~(D) still holds after the two merges combining
      $X$, $Y$ and~$Z$, and with $k$ incremented by~2.
      Certainly, (D-1) still holds.
      (D-2) is also still true:
      $Z_2$ and~$Z_3$ are unchanged, and $Z_1$ will
      include $X$ and~$Y$ (as $U_{k+2}$ and $U_{k+1}$).
      (D-1) and (D-3) imply
      \[
        |W| ~\ge~ \alpha |X| ~\ge~ \alpha^2 |Y| ~\ge~ \alpha^{k + 2} |Z_3|,
      \]
      so (D-3) will still hold.
      (D-4) implies $\frac{|Y|}{\alpha - 1} \ge |Z_1|  +   |Z_2|$.
      Hence, (D-1) and (D-4) give
      \begin{eqnarray*}
        \frac{|W|}{\alpha - 1}
          &\ge& \frac{\alpha |X|}{\alpha - 1}
          ~=~ \frac{|X|}{\alpha - 1} + |X|
          ~\ge~ \frac{\alpha |Y|}{\alpha - 1} + |X|
          ~=~ |X| + |Y| + \frac{|Y|}{\alpha - 1} \\
        &\ge& (|X| + |Y| + |Z_1|) + |Z_2|;
      \end{eqnarray*}
      i.e., (D-4) will still hold.
      (D-5) is obviously still true.
      $\alpha|W| < |X|  +   |Y|  +   |Z|$ implies (D-6)
      will still hold.
      The already proved equation~(\ref{eq:ZthreeXYZ}) implies
      that (D-7) will still hold.
      Finally, we need to establish (D-8).

      By (D-4), $|Y| \ge (\alpha  -  1) (|Z_1|  +   |Z_2|)$;
      and thus by (D-1) and (D-3),
      $|X| \ge \alpha|Y| \ge (\alpha - 1)(|Y| +  |Z_1| +  |Z_2|)$
      Therefore, we can apply Lemma~\ref{lem:GHalphabounds}(a) twice,
      first with $n_1 = |Y|$ and $n_2 = |Z_1| +  |Z_2|$
      and then with $n_1 = |X|$ and $n_3 = |Y| +  |Z_1| +  |Z_2|$,
      to obtain
      \begin{eqnarray*}
      \lefteqn{G_\alpha(|X|, m_X) + G_\alpha(|Y|, m_y) +
                 H_\alpha(|Z_1|  +   |Z_2|, m_Z  -  1) + 2|X| + 2|Y| + |Z|} \\
      &<& G_\alpha(|X|, m_X) + G_\alpha(|Y|, m_y) +
                      H_\alpha(|Z_1|  +   |Z_2|, m_Z  -  1) + |X| + 2|Y| + 2|Z| \\
      &\le& G_\alpha(|X|, m_X) + H_\alpha(|Y|  +   |Z_1|  +   |Z_2|, m_Y  +   m_Z  -  1) +
                  |X| + |Y| + |Z| + |Z_3| \\
      &\le& H_\alpha(|X|  +   |Y|  +   |Z_1|  +  |Z_2|, m_X  +   m_Y  +   m_Z  -  1) + 2|Z_3|
      \end{eqnarray*}
      where the first inequality uses the assumption $|X| < |Z|$
      and the other two inequalities use
      Lemma~\ref{lem:GHalphabounds}(a) and $|Z| = |Z_1| +  |Z_2| +  |Z_3|$.
      From this, it is easily seen that (D-8) will still hold after the
      two merges combining $X$, $Y$ and~$Z$: this is because the
      additional merge cost is $2|X|  +   2|Y|  +   |Z|$ and
      since $k$ will be incremented by~2.
  \end{itemize}
  This completes the proof of Theorem~\ref{thm:alphamergeUpper}.
\end{proof}

\section{Experimental results}\label{sec:experiments}
This section reports some computer experiments comparing the $\alpha$-stack
sorts, the $\alpha$-merge sorts, Timsort, Shivers sort, adaptive Shivers sort
and powersort. The test
sequences use the following model. We only measure merge costs, so the inputs to
the sorts are sequences of run lengths (not arrays to be sorted).  Let $\mu$ be
a distribution over integers. A sequence of $m$ run lengths is chosen by
choosing each of the $m$ lengths independently according to the
distribution~$\mu$. We consider two types of distributions for~$\mu$:
\begin{enumerate}
  \item The uniform distribution over numbers between $1$ and $100$,
  \item A mixture of the uniform distribution over integers between $1$ and
    $100$ and the uniform distribution over integers between $10000$ and
    $100000$, with mixture weights $0.95$ and $0.05$. This distribution was
    specially tailored to work better with $3$-aware algorithms while still
    being formulated in a general way that avoids favoring any
    particular algorithm.
\end{enumerate}
We also experimented with power law distributions.  However,
they gave very similar
results to the uniform distributions so we do not report these results
here.

In Figures
\ref{figure:alpha-stack-sort-plot}
and~Figure~\ref{figure:alpha-merge-sort-plot}, we estimate the ``best'' $\alpha$
values for the $\alpha$-merge and $\alpha$-stack sorts under the uniform
distribution. The experiments show that the best value for $\alpha$ for both
types of algorithms is around the golden ratio, or even slightly lower.  For $\alpha$
at or below $\varphi$, the results start to show oscillation,
albeit within a small range.  We discuss
oscillation for other sorts below, but we do not know why the results $\alpha$-merge sort
oscillate for small values of~$\alpha$.

\begin{figure}
    \centering
    \input{alphaplussorts-uniform-1-100.tex}
    \caption{Comparison between $\alpha$-merge sorts for different $\alpha$ on
    uniform distribution over integers between 1 and 100.
    For all our figures, the $x$-axis
    shows the ``number
    of runs'', namely the number of presorted subsequences (ascending or
    descending) in the input sequence.  The $y$-axis shows the ``normalized
    merge cost'', namely the ratio of the total merge cost and $n\log m$.
    The data points reported in
    Figures \ref{figure:alpha-stack-sort-plot}-\ref{figure:allsortsMixed},
    are the average of 100 random trials.}
    \label{figure:alpha-stack-sort-plot}
\end{figure}


\begin{figure}
    \centering
    \input{alphasorts-uniform-1-100.tex}
    \caption{Comparison between $\alpha$-stack sorts for different $\alpha$ on
    uniform distribution over integers between 1 and 100.}
    \label{figure:alpha-merge-sort-plot}
\end{figure}

\begin{figure}
    \centering
    \input{allsorts-uniform-1-100.tex}
    \caption{Comparison between sorting algorithms using the
    uniform distribution over integers between 1 and 100.}
    \label{figure:allsortsUniform}
\end{figure}

\begin{figure}
    \centering
    \input{allsorts-compose-095-uniform-1-100-uniform-10000-100000}
    \caption{Comparison between sorting algorithms using a
            mixture of the uniform distribution over integers
            between 1 and 100 and the uniform distribution over integers between
            10000 and 100000, with mixture weights 0.95 and 0.05}
    \label{figure:allsortsMixed}
\end{figure}

\begin{figure}
\centering
\input{von_neumann_uniform.tex}
\caption{The performance of the von Neumann merge sort (nonadaptive
merge sort) on the uniform distribution of run lengths
between 1 and 100. The random run lengths affect the total
input length~$n$, but the von Neumann sort ignores the runs in the initial runs in
the input sequence, and greedily builds binary trees of merges.
Unlike the other graphs, the data points on this graph are the result
of averaging 10 tests.}
\label{figure:vonNeumannUnif}
\end{figure}


Next we compared all the stable merge sorts discussed in
the paper, plus the adaptive Shivers sort of Jug\'e~\cite{Juge:AdaptiveShivers} and
the powersort of Munro-Wild~\cite{MunroWild:MergeSort}.  Figure~\ref{figure:allsortsUniform} reports on comparisons
using the uniform distribution.  It shows
that $1.62$-merge sort performs slightly better
than $1.62$-stack sort; and they perform better than Timsort,
$2$-stack sort and $2$-merge sort. The Shivers sort and the adaptive
Shivers sort performed
comparably to the $1.62$-stack and $1.62$-merge sorts, but exhibited
a great deal of oscillation in performance. This is discussed below,
but this is
presumably due to the Shivers and adaptive Shivers sorts rounding to powers of two.
The powersort algorithm performed the best in this test. The powersort knows
the total length~$n$ of the input to be sorted; this allows it to make
decisions on merges that approximate better a binary merge tree
over $n$~inputs.

Figure~\ref{figure:allsortsMixed} considers the mixed distribution. This is the
situation that is more meaningful for our stable merge sorts. As expected,
$1.62$-stack sort, $2$-stack sort, and Shivers sort performed the least well, approximately
15\% or 20\% worse than the other algorithms.
The other algorithms' performances were closer to each other. The Timsort
and $2$-merge sort were
marginally worse than the remaining three algorithms.  The
$1.62$-merge sort and the adaptive Shivers performed comparably, with
the adaptive Shivers sort exhibiting oscillatory behavior.
The powersort performed the best, presumably because its use of
the value~$n$ allowed it to make merge decisions that were
globally optimal.

The figures show that in many cases the merge cost oscillates periodically with the
number of runs; this was true in many other test cases not reported here. This seems
to be due in part to the fact that some of the sorts with this behavior (notably Shivers sort and
adaptive Shivers sort) base their merge decisions using powers of two, and do not take into
account the number $n$ of elements being sorted. In fact, the same behavior
is exhibited by the simple von Neumann sort, namely the non-adaptive sort that greedily
builds binary merge trees.  This is seen in Figure~\ref{figure:vonNeumannUnif}
using the uniform distribution. In this figure, for each value of~$m$,
the input sequences were chosen under the uniform distribution; these
have a fixed number, $m$, of runs, but different total input lengths~$n$.
That is, for fixed $m$,
the input length $n$ is a random variable;he von Neumann sort
ignores runs, and its merge cost depends only on~$n$.
The oscillations in the von Neumann and
Shivers sorts are clearly very similar, so we expect they have similar causes.
Under the mixed distribution (not shown), the oscillation with the von Neumann
sort was not present, presumably to the larger relative variance of~$n$. It is still present
for the Shivers and adaptive Shivers sort in the mixed distribution, presumably because
of their sensitivity to longer runs.


\section{Conclusion and open questions}\label{sec:conclude}
Theorem~\ref{thm:alphamergeUpper} analyzed
$\alpha$-merge sort only for $\alpha > \varphi$.
This leaves several open questions:
\begin{question}
    For $\alpha \le \varphi$, does $\alpha$-merge sort run in time
    $c_{\alpha} (1  +   \omega_m(1)) n \log m$?
\end{question}
\noindent
It is likely that when $\alpha<\varphi$, $\alpha$-merge sort
could be improved by making it 4-aware, or more generally,
as $\alpha\rightarrow 1$, making it $k$-aware
for even larger $k$'s.
\begin{question}
Is it necessary that the constants $d_\alpha \rightarrow \infty$
as $\alpha$ approaches~$\varphi$?
\end{question}

An \textit{augmented Shivers sort} can defined by
replacing the inner while loop on
lines \ref{algline:shiversYZ}-\ref{algline:ShiversInnerLoopB}
of the Shivers sort
with the code:

\noindent
\hspace*{0.5in}\hbox{
    \vbox{
        \begin{algorithmic}
            \While{$2^{\lfloor{\log |Y|}\rfloor} \le |Z|$}
              \If {$|Z|\le|X|$}
                \State{Merge $Y$ and $Z$}
              \Else
                \State{Merge $X$ and $Y$}
              \EndIf
            \EndWhile
        \end{algorithmic}
    }
}
\noindent
The idea is to incorporate the $3$-aware features of $2$-merge sort
into the Shivers sort method. The hope is that this might give an
improved worst-case upper bound:
\begin{question}
    Does the augmented Shivers sort run in time $O(n \log m)$?
    Does it run in time $(1 +  o_m(1)) n \log m$?
\end{question}
\noindent
The notation $o_m(1)$ is intended to denote a value that
tends to zero as $m\rightarrow\infty$.

Jug\'e's adaptive Shivers sort~\cite{Juge:AdaptiveShivers} is similar
to the above augmented Shivers sort, and it does have run time
$(1 +  o_m(1)) n \log m$. In fact it is asymptotically optimal
in a rather strong sense: It
has run time asymptotically equal to the entropy-based lower
bounds of Barbay and Navarro~\cite{BarbayNavarro:AdaptiveSorting}.
This also answered the open Question~4 in~\cite{BussKnop:StableMergeSortSODA}
about the existence of asymptotically optimal $k$-aware algorithms.
The powersort algorithm of Munro and Wild~\cite{MunroWild:MergeSort}
has similar asymptotic run time bounds.


Our experiments indicate that $1.62$-merge sort, the adaptive
Shivers sort, and powersort are all strong contenders for
the ``best'' adaptive merge sort in practice.
The powersort was the best in our experiments by a
small margin, but it uses a more complicated algorithm
to decide when to merge, and this will increase the running time.
(It potentially will use $O(\log n)$ time
per merge decision, although this can be improved on.) The adaptive
Shivers sort has better theoretical worst-case upper bounds than
$1.62$-merge sort, but this probably only arises in rare cases and it
is a small difference in any event.
The adaptive Shivers sort also exhibited some oscillation.
Both of these algorithms are easy to implement;
neither has
a clear edge in our experiments.

Our experiments used random distributions that probably
do not do a very good job of modeling real-world data.
Is it possible to create better models for real-world data?
Finally, it might be beneficial to run experiments with actual real-world data,
e.g., by modifying deployed sorting algorithms to calculate statistics based
on run lengths that arise in actual practice.

\bibliographystyle{siam}
\bibliography{logic}

\end{document}

