{
  "content": "Ideas\n\n- Scope: We aim for the coarse entropy-adaptivity bound for Python\u2019s patched TimSort (with the extra collapse rule r_2 + r_3 \u2265 r_4): comparisons = O(n + n H), H = \u2211_i (\u2113_i/n) log_2(n/\u2113_i). This suffices to show adaptation to the entropy; sharp constant 3/2 can follow later.\n- Strategy recap: comparisons \u2264 merge cost + O(n). Decompose into starting vs. ending sequences. Prove (i) starting sequences contribute O(n) (with an explicit constant \u03b3), (ii) ending sequences are paid by a token scheme because each element suffers only O(1 + log(n/\u2113)) height decreases (height bound), so total \u2264 O(n + nH).\n\nSmall lemmas with proofs (to add to proofs.md)\n\n1) Immediate continuation after #4/#5 (s-token safety)\n- Claim. In the Python-patched TimSort, a merge by #4 or #5 can never end an ending sequence; the very next step is another merge.\n- Proof. Suppose #4 triggers on top three lengths r_1, r_2, r_3 with r_1 + r_2 \u2265 r_3 and merges R_1 and R_2, producing top r_1\u2032 = r_1 + r_2, r_2\u2032 = r_3. Then rule #3 (r_1\u2032 \u2265 r_2\u2032) immediately holds, so another merge must follow. Similarly, suppose #5 triggers on top four lengths r_1, r_2, r_3, r_4 with r_2 + r_3 \u2265 r_4 and merges R_1 and R_2, producing top three r_1\u2032 = r_1 + r_2, r_2\u2032 = r_3, r_3\u2032 = r_4. Then r_1\u2032 + r_2\u2032 = (r_1 + r_2) + r_3 \u2265 r_2 + r_3 \u2265 r_4 = r_3\u2032, so #4 holds, and another merge follows. \u220e\n- Why useful here. This alone ensures the s-token nonnegativity: any s-token spent in #4/#5 is immediately re-credited on the subsequent merge when the height decreases.\n\n2) Starting sequences: explicit constant\n- Claim. If a starting sequence begins by pushing a run of length r and performs k\u22121 merges #2 (merging R_2,\u2026,R_k), then its merge cost C satisfies C \u2264 \u03b3 r with \u03b3 = 2 \u2211_{j\u22651} j\u00b72^{\u2212j/2} = 2\u00b7q/(1\u2212q)^2 and q = 2^{\u22121/2} \u2248 0.7071, so \u03b3 \u2248 16.4924.\n- Proof. Let the pre-push stack be S, and after the push and k\u22121 merges #2 we merge runs R_1,\u2026,R_k (excluding trivial k=1). Then C \u2264 \u2211_{i=1}^k (k+1\u2212i) r_i. Since #2 applied k\u22121 times and then stopped, r > r_k. The invariant yields r_k \u2265 2^{(k\u22121\u2212i)/2} r_i for all i, so C/r \u2264 \u2211_{i=1}^k (k+1\u2212i) 2^{(i+1\u2212k)/2} = 2 \u2211_{j=1}^k j 2^{\u2212j/2} < 2 \u2211_{j\u22651} j 2^{\u2212j/2} = 2\u00b7q/(1\u2212q)^2. \u220e\n- Why useful here. Summing over pushes (\u2211 r = n) gives total starting-sequence cost \u2264 \u03b3 n = O(n).\n\n3) Height bound after a starting sequence\n- Claim. Let r be the length of the just-pushed run; when its starting sequence ends, the stack height h \u2264 4 + 2 log_2(n/r).\n- Proof. Below the top two runs, none has been merged during the starting sequence, so by the growth corollary r_3 \u2264 2^{2\u2212h/2} r_h \u2264 2^{2\u2212h/2} n. Since #2 is no longer applicable, r \u2264 r_3, hence r \u2264 2^{2\u2212h/2} n and h \u2264 4 + 2 log_2(n/r). \u220e\n- Why useful here. Each element accrues credits only when its height decreases during ending sequences; this bounds the per-element budget by O(1 + log(n/r)).\n\n4) Ending sequences: token accounting and summation\n- Token scheme. Credit 2 c-tokens and 1 s-token to an element when its run is pushed, and whenever its height decreases due to a merge in an ending sequence. Spend as follows: #2: 1 c from each element of R_1 and R_2; #3: 2 c from each element of R_1; #4/#5: 1 c from each element of R_1 and 1 s from each element of R_2.\n- Nonnegativity. For #2/#3/#4/#5, all charged elements\u2019 heights decrease, so the fresh 2c credit covers c-spends. By Lemma 1, #4/#5 are immediately followed by another merge; on that merge, elements of the new top run (including former R_2) decrease height again and regain an s-token. Thus s-balances never go negative.\n- Summation to O(n + nH). By Lemma 3, each element in a run of length r incurs at most O(1 + log(n/r)) height decreases in ending sequences, thus receives O(1 + log(n/r)) tokens. Summing over all runs gives O(\u2211 r (1 + log(n/r))) = O(n + nH).\n\n5) Run detection overhead (comparisons)\n- Claim. The greedy run decomposition scan uses \u2264 2(n\u22121) comparisons; hence O(n).\n- Proof sketch. In a one-pass scan, each time we begin a new run we compare A[i] and A[i+1] to fix the direction; within a run, each adjacent pair is compared at most once until the monotonicity breaks. Each index participates in at most a constant number of such adjacency checks; overall O(n). \u220e\n\nExamples (sanity checks)\n\n- After #4: r_1=5, r_2=6, r_3=10: #4 merges (5,6) \u2192 (11,10), and #3 immediately applies (11\u226510); s-token spent at #4 is regained at the next step.\n- Highly skewed runs: one run of size n/2, the rest tiny. Height bound for the big run is O(1), so its elements pay O(1) tokens; the total ending cost is dominated by smaller runs and sums to O(n + nH).\n\nObstacles\n\n- The target with leading constant 1 (\u2264 nH + O(n)) is false for TimSort in general; Buss\u2013Knop exhibit inputs with merge cost \u2265 (3/2) n log n \u2212 O(n). Our coarse bound O(n + nH) is correct and sufficient for this task.\n- The s-token argument hinges on the corrected #5 rule; the Java unpatched variant breaks the guarantee (immediate continuation may fail), so our proof is for the Python-patched TimSort.\n\nNext steps\n\n- Move Lemmas 1\u20135 and their proofs into proofs.md, with the explicit constant \u03b3 in Lemma 2 and the h-bound. Conclude comparisons \u2264 O(n + nH) because merging a,b uses \u2264 a+b\u22121 comparisons, so comparisons \u2264 merge cost + O(n).\n- Then add the sharp (3/2)\u00b7nH + O(n) bound via the potential \u03a6(r)=(3/2) r log_2 r: balanced-merge lemma (sizes within \u03c6\u00b2), forbidding short patterns (#2#2, XX#2), pairing unbalanced merges, and grouping with O(r) slack per ending sequence; total slack O(n)."
}