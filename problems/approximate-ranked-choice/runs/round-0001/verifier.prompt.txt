=== SYSTEM ===
You are a strict mathematical verifier & research manager.

You are being run in a loop with provers. Your tasks:
- Audit their ideas for correctness and rigor.
- Curate files (notes.md and output.md).
- Suggest the next steps.

Inputs: 
- task statement
- outputs of provers
- notes.md (summary so far)
- output.md (rigorously proven results so far)

Guidelines:
- notes.md: should be a readable running log of interesting work. Prefer appending. Replace only if old notes are clearly misleading. Remember -- notes should be self-contained and should make sense even without seeing outputs of past provers. 
- output.md:  Think twice before you add something to this file. Everything here should be rigorous, almost publication-ready. It should contain only fully proven theorems that are clearly an interesting step towards the solution (or the solution itself). The beginning of the file is a clear list of interesting theorems you can prove, together with a discussion about why it is relevant. From the statement of the theorem it has to be clear that this constitutes nontrivial progress towards solution. The rest of the file contains a fully rigorous list of the proofs of the theorems. Keep only proven theorems with clear proofs. If a statement in that file is flawed / unprecise and you cannot fix it on the spot, remove it from output.md and instead add it to notes.md with explanation.
- Be careful with the booleans `new_notes_append` and `new_outputs_append`: default is True (append). Use False only when deliberately rewriting everything.

Tasks:
1) Audit rigor: identify unjustified steps, false statements, missing lemmas; produce minimal counterexamples.
2) Triage value: separate genuine progress from noise.
3) Guide next steps: propose what provers should try next.

**Return strictly JSON**:
{
  "feedback_md": "Detailed critique of prover outputs (≥200 words). Include concrete next-step suggestions. The next round of provers may not see last round outputs; ensure all valuable info is stored in notes.",
  "new_notes_md": "Markdown (KaTeX allowed). Usually appended to notes.",
  "new_notes_append": "True|False",
  "new_outputs_md": "Markdown (KaTeX allowed). Usually appended to output.md.",
  "new_outputs_append": "True|False",
  "verdict": "success|partial success|nothing so far"
}

**Verdict definitions**:
- **success**: Something that one could publish, even if it is not the full solution (e.g., significant theorem, key lemma, novel technique, partial solution with clear path forward)
- **partial success**: Something worthy enough to write into output.md (e.g., correct but preliminary results, useful technical lemmas, validated approaches that advance understanding)
- **nothing so far**: No meaningful mathematical progress or only exploratory work


=== USER ===
=== Task (txt) ===

# Problem Statement

We consider the problem of estimating the winner in an election run under **Ranked Choice Voting** (also known as *Instant-Runoff Voting*).  

Suppose there are \(K\) candidates. Each voter submits a complete ranking of these candidates. The voting rule proceeds in rounds: in each round, every ballot is assigned to the top-ranked candidate who is still in contention. The candidate with the smallest number of votes is eliminated (with ties broken according to a fixed deterministic rule), and this candidate is crossed out from all ballots. The process repeats until only one candidate remains, who is declared the winner.

Our goal is not to compute the exact winner from the full set of ballots, but rather to estimate a *robust* winner from a random sample of ballots. To make this precise, we introduce the notion of an **ε-winner**. A candidate \(c\) is called an ε-winner of an election profile if it is possible to add at most an ε-fraction of additional ballots to the election so that \(c\) becomes the winner under Ranked Choice Voting. In other words, \(c\) is “close” to being the true winner, in the sense that a small perturbation of the electorate would make \(c\) win. There may be more ε-winners. The real winner is ε-winner for any ε. 

The sampling model is as follows. We assume there is a large population of voters, each ballot drawn independently from the empirical distribution of this population. The algorithm does not see all the ballots; instead, it may sample ballots at random, with replacement. The task of the algorithm is: given parameters \(\varepsilon\) and \(\delta\), and access to at most \(m\) random ballots, output a candidate \(\hat{c}\) such that with probability at least \(1-\delta\), the candidate \(\hat{c}\) is an ε-winner of the true election.

The central question is: **what is the sample complexity of this problem?**  
That is, how many samples \(m\) are sufficient (and necessary) in order for an algorithm to reliably identify an ε-winner with probability at least \(1-\delta\)? We seek both upper bounds, by designing sampling algorithms and analyzing their guarantees, and lower bounds, by proving that no algorithm can succeed with significantly fewer than a certain number of samples. 



=== Notes ===

# Notes




=== Current Output ===

# Output

No output yet.



=== Prover 01 Output ===

# Ideas

- Define, for any subset of remaining candidates S ⊆ [K] and any a ∈ S, the top-within-S tally:
  t_a(S) := Pr_{b∼P}[a is the top-ranked among S on ballot b]. These are linear functionals of the population distribution P over S_K; for each fixed (a,S), t_a(S) is the expectation of an indicator.
- Adding up to ε mass of additional ballots corresponds to adding a nonnegative measure y on S_K with total mass ≤ ε. Under addition, the (unnormalized) round-t tallies update by s'_a(S) = t_a(S) + ∑_r y(r) 1[top_S(r)=a]. The IRV eliminations depend only on comparisons among these s'_a(S) for the relevant S’s.
- Two complementary approaches to upper bounds:
  1) Conservative “certifier” based only on adding c-first ballots. It yields a simple sufficient condition for c to be made winner by ε additions, reducing to checking all S containing c.
  2) Exact optimization (for a fixed elimination order) via a linear program whose coefficients are the t_a(S). This characterizes the minimal ε needed for c to win, and suggests a sample-based estimator with uniform concentration guarantees over the family of functionals t_a(S).

# Clean lemmas and why useful

Lemma 1 (LP for a fixed elimination order). Fix an elimination order π of [K] with π(K)=c and define S_1=[K], S_{j+1}=S_j\{π(j)}. Let variables y(r)≥0 for r∈S_K and objective ε* = ∑_r y(r). Consider constraints, for each round j and each a∈S_j\{π(j)}:
  t_{π(j)}(S_j) + ∑_r y(r) 1[top_{S_j}(r)=π(j)] ≤ t_a(S_j) + ∑_r y(r) 1[top_{S_j}(r)=a].
Then the minimal ε needed to realize elimination order π with winner c equals the optimum ε* of this LP.
- Why useful: Reduces the combinatorial IRV dynamics to linear inequalities in additive perturbations; enables sensitivity analysis and sample-based estimation through concentration for the t_a(S).
- Proof sketch: The inequalities enforce that, at each round j, π(j) is (weakly) the minimum tally in S_j under the augmented profile; summing new ballots y(r) respects additivity across rounds because the top-within-S indicators are deterministic functions of r. Feasibility is necessary and sufficient to realize π under the deterministic tie-breaking rule. Minimizing ∑ y(r) is exactly minimizing added mass.

Lemma 2 (Uniform convergence for all t_a(S)). The family F = {f_{a,S}(r) := 1[top_S(r)=a] : a∈S⊆[K]} has cardinality at most K·2^{K-1}. Given m i.i.d. ballots, with probability at least 1-δ,
  sup_{a,S} |\hat t_a(S) - t_a(S)| ≤ τ
whenever m ≥ C (log(K·2^{K-1}) + log(2/δ)) / τ^2 for a universal constant C.
- Why useful: Ensures we can estimate all top-within-S tallies uniformly to accuracy τ with O((K + log(1/δ))/τ^2) dependence up to the 2^K factor from the number of subsets.
- Proof: Hoeffding + union bound over |F| ≤ K·2^{K-1} indicators.

Lemma 3 (Conservative certificate via c-first boosters). Define for c∈[K]
  M_c := max_{S⊆[K]: c∈S} [ min_{a∈S} t_a(S) − t_c(S) ]_+.
If ε ≥ M_c, then adding ε mass of ballots that rank c first (arbitrary completion below) guarantees that c is an IRV winner under the augmented profile, irrespective of tie-breaking among non-c candidates.
- Why useful: Provides a checkable sufficient condition that depends only on the collection {t_a(S)} and avoids searching elimination orders.
- Proof: Adding c-first ballots increases c’s top-within-S tally by ε for every S containing c, and does not decrease any other candidate’s tallies. If ε ≥ M_c then for every S⊇{c}, we have t'_c(S) ≥ min_{a∈S} t'_a(S), so c is never uniquely the lowest in any subset encountered along any elimination path; hence c cannot be eliminated, and thus is the final winner.

Lemma 4 (Estimation error for M_c). If sup_{a,S} |\hat t_a(S)−t_a(S)| ≤ τ then |\hat M_c − M_c| ≤ 2τ.
- Why useful: Gives a simple plug-in estimator \hat M_c with tight additive error control, leading to a clean sample complexity for a correct certificate.
- Proof: For any fixed S, the quantity min_{a∈S} t_a(S) − t_c(S) changes by at most 2τ under τ-perturbations; taking max over S preserves this bound.

# Algorithmic upper bound (certifier)

- Input: ε, δ, access to m random ballots.
- Procedure:
  1) Estimate all \hat t_a(S) from the sample.
  2) Compute \hat M_c for each c.
  3) Output any c with \hat M_c ≤ ε − 2τ, where τ is chosen for the concentration bound.
- Guarantee: Choose m ≥ C (log(K·2^{K-1}) + log(2/δ)) / τ^2 and set τ = ε/4. With probability ≥ 1−δ, every output c is an ε-winner (by Lemma 3 and Lemma 4). Hence
  m = O((K·2^{K-1} + log(1/δ)) / ε^2)
  samples suffice for a sound certifier.
- Note: This is a sufficient-condition algorithm; it may abstain if no c satisfies the threshold, even if an ε-winner exists. It never produces a false positive with the stated probability.

# Exact characterization via LP and a refined plan

- For each candidate c, define
  g_c(P) := min_{π: π(K)=c} OptVal_LP(π; {t_a(S)}),
  the true minimal extra mass needed to make c a winner.
- Sample-based estimator: Replace t_a(S) by \hat t_a(S) in the LP constraints and compute \hat g_c. Output a candidate with \hat g_c ≤ ε − margin.
- Stability note: In our LP the only estimated quantities are the constants t_a(S); the coefficients of y(r) are exact indicator patterns. Thus the LP optimum is Lipschitz in the vector d of baseline deficits d_{j,a} := t_a(S_j)−t_{π(j)}(S_j). By standard LP sensitivity (duality), the change in optimum is bounded by the ℓ_1-norm of an optimal dual solution times the ℓ_∞ perturbation size. We conjecture a bound of the form |\hat g_c − g_c| ≤ C_K τ with C_K polynomial in K (likely O(K)), whenever sup_{a,S} |\hat t_a(S)−t_a(S)| ≤ τ. This would yield a sample complexity m = O((K·2^{K-1}+log(1/δ)) / (ε/C_K)^2).
- Gap: We have not yet derived an explicit, audited constant C_K from the dual; this is a priority for the next round.

# Special cases and examples

- K=2 (majority): IRV reduces to plurality with two candidates. For c, g_c(P) = [1/2 − p_c]_+ where p_c is the share of ballots ranking c over the other. Sample complexity Θ((log(1/δ))/ε^2) is both necessary and sufficient.
- K=3 toy profile: Suppose t_A({A,B,C})=0.34, t_B=0.33, t_C=0.33. Also t_A({A,B})=0.49, t_B=0.51; t_A({A,C})=0.52, t_C=0.48; t_B({B,C})=0.55, t_C=0.45. Compute M_A:
  • S={A,B,C}: min=0.33, deficit 0.33−0.34 = 0 (no deficit).
  • S={A,B}: min=0.49, deficit 0.49−0.49=0.
  • S={A,C}: min=0.48, deficit 0.48−0.52=0.
  Thus M_A=0; certifier declares A a 0-winner (indeed A already wins via IRV: eliminate C first, then A beats B pairwise). This illustrates the certificate can be tight.

# Lower bounds (initial)

- Baseline: Any algorithm must use m = Ω((log(1/δ))/ε^2) samples, even for K=2, by standard coin-flip mean estimation.
- Dependence on K (heuristic): We conjecture an Ω(K/ε^2) lower bound in the worst case by constructing a packing of profiles where, for each of Θ(K) subsets S_j containing a fixed c, the gap min_{a∈S_j} t_a(S_j) − t_c(S_j) is either +Θ(ε) or −Θ(ε), and these choices are close to statistically independent under the sampling distribution. Distinguishing whether M_c ≤ 0 versus M_c ≥ Θ(ε) would then require estimating Θ(K) such gaps to accuracy Θ(ε), costing Ω(K/ε^2) samples by a direct-sum argument. Gap: A full construction over permutations ensuring consistency of the family {t_a(S)} remains to be written.

# Obstacles and gaps

- Exponential factor 2^{K}: The certifier’s union bound over all S is crude. It is open whether one can control M_c using only O(poly(K)) statistics (e.g., along a carefully chosen chain of subsets) while preserving correctness.
- LP sensitivity constant C_K: We need an explicit bound (preferably O(K)) derived from the dual to turn \hat g_c into a sound estimator with polynomial-in-K sample complexity.
- Computational complexity: Enumerating all π is factorial; our focus here is sample complexity, but a separation oracle for the dual (or column generation over S) might avoid enumerating all S/π and also reduce the effective number of estimated functionals needed for certification.

# Next steps

- Prove a formal sensitivity bound for the LP optimum in Lemma 1 with respect to perturbations of {t_a(S)}, aiming for |\hat g_c − g_c| ≤ O(K)·τ.
- Investigate whether M_c can be exactly expressed as the optimum of a simpler min-max problem over subsets S (potentially reducing the number of required S), or whether there is an equivalent chain condition that only needs O(K) subsets per c.
- Develop and verify a lower bound construction in the K≫1 regime to establish Ω(poly(K)/ε^2) sample complexity, even for certifiers.
- Explore a two-tier algorithm: run the certifier; if it abstains, fall back to solving the LP for a small set of promising elimination orders inferred from the sample, and quantify the additional estimation needed.

# Gaps relative to output.md

- No output.md exists yet; the claims above (Lemma 1–4 and the certifier upper bound) are new and should be checked and, if correct, curated into output.md. We flagged the LP sensitivity as a gap to be filled before curating an LP-based upper bound with polynomial-in-K sample complexity.


=== Prover 02 Output ===

Ideas
- Define, for a distribution P over full rankings and a candidate c, the tallies after eliminating a set S ⊆ C \ {c} as T_x(S) = P[the top-ranked among C \ S is x]. These are linear functionals of P.
- If we add an α-fraction of ballots all ranking c first, then T_c(S) increases by α for all S, while T_j(S) for j ≠ c are unchanged. Hence the minimal α that guarantees c is never last at any elimination stage is
  f_c(P) := max_{S ⊆ C\{c}} ( min_{j ∉ S ∪ {c}} T_j(S) − T_c(S) )_+.
  Why useful: If f_c(P) ≤ ε then c is an ε-winner (sufficient certificate); conversely, any addition strategy cannot do better for the “never-last” condition by allocating mass away from c-first ballots, since that weakly increases min_{j≠c} T_j(S).
- Necessary condition via pairwise margins: For any j ≠ c, considering S = C \ {c, j} (so R = {c, j}) gives T_j(S) − T_c(S) = P[j ≻ c] − P[c ≻ j]. Therefore f_c(P) ≥ max_j (P[j ≻ c] − P[c ≻ j])_+.

Special case K = 3 (exact formula)
- Let candidates be {a, b, c}. Then the only relevant S for c are ∅, {a}, {b}. Thus
  f_c(P) = max{ (min{T_a(∅), T_b(∅)} − T_c(∅))_+, (P[a ≻ c] − P[c ≻ a])_+, (P[b ≻ c] − P[c ≻ b])_+ }.
  Here T_x(∅) are first-place shares. Proof sketch: T_x({y}) with x ∈ {b, c} equals head-to-head shares between x and the remaining opponent, because R = {x, c} or {x, b}; for S = ∅ we compare first-round tallies. There are no other S.
- Usefulness: For K = 3, identifying an ε-winner reduces to estimating three quantities (two pairwise margins w.r.t. c and the first-place deficit against the smaller of {a, b}).

Sample-complexity implications
- Upper bound for K = 3: With m = O((log(1/δ))/ε^2) samples, we can estimate first-place shares and pairwise margins within ±ε/4 (Hoeffding + union bound over a constant number of events). If min_c f̂_c ≤ ε/2, then with probability ≥ 1 − δ, the true f_c ≤ ε and the returned c is an ε-winner.
- General K (existence, exponential in K): Estimating all T_x(S) uniformly within ±τ over all pairs (S, x ∉ S) requires m = O(((K 2^{K}) + log(1/δ))/τ^2) samples by a union bound. Then computing f̂_c exactly from these estimates yields a correct certificate whenever min_c f̂_c ≤ ε − 2τ.
- General lower bound: Even with K = 2, the problem subsumes estimating a Bernoulli mean within ε to decide if a candidate is an ε-winner, implying m = Ω((1/ε^2) log(1/δ)). Therefore the dependence on ε and δ is necessary.
- K-dependence lower bound (sketch): Distinguishing among ≈ K hypotheses where exactly one candidate is an ε-winner (constructed by small perturbations of P concentrating probability mass among disjoint top patterns) suggests m = Ω((log K)/ε^2) via Fano’s inequality. Details to be worked out.

Examples
- K = 3 toy distribution: Suppose first-place shares are (a, b, c) = (0.36, 0.34, 0.30), and pairwise margins are P[a ≻ c] − P[c ≻ a] = 0.07, P[b ≻ c] − P[c ≻ b] = 0.01. Then f_c(P) = max{0.04, 0.07, 0.01} = 0.07; c is an ε-winner iff ε ≥ 0.07.
- K = 4 phenomenon: One can construct P where f_c(P) is driven by a 3-way residual set R of size 3 (not captured by first-place or pairwise deficits). E.g., c has small first-place share and loses many “top among {c, j, k}” contests due to a strong third candidate siphoning top positions when both j and k remain. This indicates that for K ≥ 4, relying only on pairwise margins and first-round shares is insufficient for certification.

Obstacles
- Exact computation/certification of f_c(P) for general K requires knowing T_x(S) for exponentially many S. This leads to exponential dependence on K in naïve sampling algorithms.
- Deriving tight necessary and sufficient conditions in terms of only polynomially many statistics (e.g., pairwise margins, a bounded collection of multiway-top events) remains open.

Next steps
- Prove rigorously the K = 3 formula above (write out the short formal proof) and add it to output.md.
- Seek structural reductions for K ≥ 4: is sup over S attained at sets where |C \ S| ≤ 3? If yes, certification reduces to O(K^3) multiway-top probabilities; otherwise produce a counterexample.
- Develop a polynomial-time, polynomial-sample upper bound by proposing a relax-and-certify scheme: consider only all R of size ≤ r (e.g., r = 3 or 4), compute f_c^r = max over such R of (min_{j∈R\{c}} T_j(R) − T_c(R))_+. If f_c^r ≤ ε we certify; analyze how conservative this is and whether, under mild distributional assumptions (e.g., smoothness of rank distributions), r = 3 suffices.
- Strengthen lower bounds in K via explicit Fano constructions tying ε-winner identity to a log K-bit index encoded in small perturbations of top-of-R events.

Gaps in output.md
- There is currently no output. We can add the precise K = 3 characterization and the general sufficient certificate f_c(P) ≤ ε with associated sample-complexity bounds, plus the Ω((1/ε^2) log(1/δ)) lower bound.