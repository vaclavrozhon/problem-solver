{
  "content": "Ideas\n- Close the LP sensitivity gap for general K: prove a clean, order-independent bound sup_{\u03bb\u2208\u039b} ||\u03bb||_1 \u2264 K\u22121 for the dual of the fixed-order augmentation LP. This yields an explicit Lipschitz constant for the optimum value v(b) with respect to per-constraint tally errors, giving polynomial-in-K sample complexity for certifying any fixed elimination order.\n- For K = 3, sharpen the Lipschitz constant for the exact two-type formula to L = 3 (improves on the coarse L = 6), tightening the sampling margin.\n- Turn the sensitivity bound into a general sampling theorem: with m = \u0398(((K\u22121)^2 (K log K + log(1/\u03b4)))/\u03b5^2) samples, we can, w.h.p., verify whether min over all elimination orders ending in c has augmentation cost \u2264 \u03b5; hence we can certify that c is an \u03b5-winner. This is polynomial in K.\n\nRigorous results and proofs\n\nLemma A (Dual norm bound for the fixed-order LP).\nFix K and an elimination order \u03c0 of the candidates. For S_j := {\u03c0(j),\u2026,\u03c0(K)} and slack \u03b3 > 0, consider the LP whose variables y(r) \u2265 0 (r ranges over full rankings) minimize ||y||_1 subject to, for each j \u2208 {1,\u2026,K\u22121} and each a \u2208 S_j \\ {\u03c0(j)}:\n  (t_a(S_j) \u2212 t_{\u03c0(j)}(S_j)) + \u2211_r y(r) (1[top_{S_j}(r) = a] \u2212 1[top_{S_j}(r) = \u03c0(j)]) \u2265 \u03b3.\nLet b_{j,a} := \u03b3 \u2212 (t_a(S_j) \u2212 t_{\u03c0(j)}(S_j)). The dual is\n  maximize \u27e8b, \u03bb\u27e9 subject to A^T \u03bb \u2264 1 and \u03bb \u2265 0,\nwhere A_{(j,a), r} = 1[top_{S_j}(r) = a] \u2212 1[top_{S_j}(r) = \u03c0(j)]. Define R_K := sup_{\u03bb feasible} ||\u03bb||_1. Then R_K \u2264 K \u2212 1.\n\nProof.\nWrite pos_\u03c0(x) for the position of x in \u03c0. For each x \u2208 C, consider the ranking r_x that places x first (and breaks ties arbitrarily below). For any j < pos_\u03c0(x), we have top_{S_j}(r_x) = x \u2260 \u03c0(j), and for j \u2265 pos_\u03c0(x), either x \u2209 S_j or j = pos_\u03c0(x) (for which there is no variable \u03bb_{j,x} since x = \u03c0(j)). Therefore, the dual constraint A^T \u03bb \u2264 1 evaluated at r_x becomes \u2211_{j < pos_\u03c0(x)} \u03bb_{j,x} \u2264 1. Summing these K inequalities over all x yields \u2211_x \u2211_{j < pos_\u03c0(x)} \u03bb_{j,x} \u2264 K \u2212 1, but the left-hand side equals ||\u03bb||_1 because \u03bb_{j,a} is defined only for pairs (j,a) with a \u2208 S_j \\ {\u03c0(j)}, equivalently j < pos_\u03c0(a). Hence ||\u03bb||_1 \u2264 K \u2212 1. \u220e\n\nCorollary B (Lipschitz sensitivity for the fixed-order LP optimum).\nLet v(b) be the optimal primal value for the fixed-order LP with RHS vector b. For any perturbation \u0394, we have\n  |v(b + \u0394) \u2212 v(b)| \u2264 (K \u2212 1) ||\u0394||_\u221e.\n\nProof.\nBy LP duality, v(b) = max_{\u03bb \u2208 \u039b} \u27e8b, \u03bb\u27e9. Then for any \u0394, |v(b + \u0394) \u2212 v(b)| = |max_{\u03bb} \u27e8b + \u0394, \u03bb\u27e9 \u2212 max_{\u03bb} \u27e8b, \u03bb\u27e9| \u2264 max_{\u03bb \u2208 \u039b} |\u27e8\u0394, \u03bb\u27e9| \u2264 (sup_{\u03bb \u2208 \u039b} ||\u03bb||_1) ||\u0394||_\u221e \u2264 (K \u2212 1) ||\u0394||_\u221e by Lemma A. \u220e\n\nCorollary C (Sampling for a fixed elimination order \u03c0).\nFix \u03c0 and \u03b3 > 0. Let \u03c4 > 0. Estimate all tallies t_x(S_j) appearing in the LP for this \u03c0 by empirical frequencies from m i.i.d. ballots. There are at most T_\u03c0 := \u2211_{j=1}^{K\u22121} |S_j| = (K \u2212 1)(K + 2)/2 such tallies. Hoeffding and a union bound yield that with probability \u2265 1 \u2212 \u03b4, we have sup error \u2264 \u03c4 provided m \u2265 c (log(2 T_\u03c0/\u03b4))/\u03c4^2 for a universal constant c. The induced RHS perturbation is ||\u0394||_\u221e \u2264 2\u03c4. Therefore, with probability \u2265 1 \u2212 \u03b4,\n  |\\hat v_\u03c0 \u2212 v_\u03c0| \u2264 2 (K \u2212 1) \u03c4.\n\nRemark: This is a per-order, polynomial-in-K guarantee. It requires no exponential union bound over all subsets; only the O(K^2) tallies used by this \u03c0 enter.\n\nTheorem D (General K: polynomial-sample \u03b5-winner certification via per-order LPs).\nFix \u03b5, \u03b4 \u2208 (0,1) and \u03b3 \u2208 (0, \u03b5/4). For each candidate c, consider all elimination orders \u03c0 that end with c. For each such \u03c0, form the LP above (with slack \u03b3) and compute the plug-in optimum \\hat v_\u03c0 from m i.i.d. ballots. Output any candidate \u0109 for which min over \u03c0 ending with \u0109 of \\hat v_\u03c0 \u2264 \u03b5 \u2212 2 (K \u2212 1) \u03c4, where \u03c4 is a target accuracy parameter. If\n  m \u2265 c \u00b7 (log(2 (K \u2212 1)! \u00b7 T_\u03c0 / \u03b4)) / \u03c4^2  with  T_\u03c0 = (K \u2212 1)(K + 2)/2,\nthen with probability at least 1 \u2212 \u03b4 the output \u0109 (if any) is an \u03b5-winner of the true election. Choosing \u03c4 = \u03b5 / (4 (K \u2212 1)) gives\n  m = \u0398(((K \u2212 1)^2 (K log K + log(1/\u03b4)))/\u03b5^2).\n\nProof.\nThere are at most (K \u2212 1)! orders ending with a fixed c. For a fixed \u03c0, by Corollary C, Pr[|\\hat v_\u03c0 \u2212 v_\u03c0| > 2 (K \u2212 1) \u03c4] \u2264 2 T_\u03c0 e^{\u22122 m \u03c4^2}. Union bound over the (K \u2212 1)! relevant orders and all c multiplies this failure probability by at most K \u00b7 (K \u2212 1)!, which adjusts the log by O(K log K). On the complement event, for every \u03c0 and c we have |\\hat v_\u03c0 \u2212 v_\u03c0| \u2264 2 (K \u2212 1) \u03c4. Consequently, if min_\u03c0 \\hat v_\u03c0 \u2264 \u03b5 \u2212 2 (K \u2212 1) \u03c4, then min_\u03c0 v_\u03c0 \u2264 \u03b5, i.e., there exists an elimination order \u03c0 making c win with total added mass \u2264 \u03b5. The choice \u03b3 < \u03b5/4 ensures strict margins so that tie-breaking is irrelevant. \u220e\n\nNote: This is a sample-complexity result; enumerating all orders is computationally expensive but does not affect m. One can also reduce computation by sample-splitting: use a small training subsample to select a handful of promising orders and a disjoint validation subsample with the above bound to certify one of them.\n\nK = 3 refinements\n\nLemma E (Tight Lipschitz constant L = 3 for \u03b5^*(\u03b3; c) when K = 3).\nRecall \u03b5_a^*(\u03b3) = B_\u03b3 + max{A_\u03b3, B_\u03b3 + D_\u03b3}, with A_\u03b3 = [p_a \u2212 p_c + \u03b3]_+, B_\u03b3 = [p_a \u2212 p_b + \u03b3]_+, and D_\u03b3 = t_b({b,c}) \u2212 t_c({b,c}) + \u03b3. Each of A_\u03b3, B_\u03b3, D_\u03b3 is 1-Lipschitz in the sup norm over the five inputs (p_a,p_b,p_c,t_c({b,c}),t_c({a,c})), and max preserves the larger Lipschitz constant of its arguments. Hence Lip(max{A_\u03b3, B_\u03b3 + D_\u03b3}) \u2264 max{1, 1 + 1} = 2 and Lip(B_\u03b3) \u2264 1, so Lip(\u03b5_a^*(\u03b3)) \u2264 3. The same holds for \u03b5_b^*(\u03b3). Therefore Lip(\u03b5^*(\u03b3; c)) = Lip(min{\u03b5_a^*(\u03b3), \u03b5_b^*(\u03b3)}) \u2264 max{3, 3} = 3.\n\nCorollary F (Sharper K = 3 sampling margin).\nWith m \u2265 c (log(5/\u03b4))/\u03c4^2, the plug-in \\widehat{\u03b5}^*(\u03b3; c) satisfies |\\widehat{\u03b5}^*(\u03b3; c) \u2212 \u03b5^*(\u03b3; c)| \u2264 3 \u03c4 with probability \u2265 1 \u2212 \u03b4. Thus, the acceptance threshold can use margin 3\u03c4 instead of the coarse 6\u03c4.\n\nExamples and sanity checks\n- Counterexample revisited (K = 3): a = 0.38, b = 0.35, c = 0.27, and t_c({b,c}) \u2212 t_b({b,c}) = 0.10. With \u03b3 \u2192 0, we have A_0 = 0.11, B_0 = 0.03, D_0 = \u22120.10. Hence \u03b5_a^*(0) = 0.03 + max{0.11, \u22120.07} = 0.14 and \u03b5_b^*(0) computed symmetrically is \u2265 0.20 (since c loses to a by margin 0.20). So \u03b5^*(c) = 0.14, strictly below the c-first certificate 0.20; this matches the earlier construction using 0.11 of c-first and 0.03 of b-first ballots.\n\nObstacles and caveats\n- The general K algorithm above certifies existence of a favorable elimination order but is computationally heavy (enumerating (K \u2212 1)! orders per target c). This does not impact sample complexity but motivates algorithmic heuristics (e.g., prune orders via coarse certificates, sample-splitting selection, or greedy search).\n- While Lemma A settles the key sensitivity constant, deriving a smaller constant or tighter order-dependent constants (e.g., depending on the shape of \u03c0) could further reduce margins.\n- Strong K-dependent lower bounds (e.g., \u03a9((K log K)/\u03b5^2)) for \u03b5-winner identification remain open; constructing hard families that force knowing which elimination order helps is nontrivial.\n\nNext steps\n- Incorporate Lemma A, Corollary B, and Theorem D into notes.md/proofs.md, as they resolve the previously marked-open sensitivity step and give a clean polynomial-in-K sample bound for the per-order LP approach.\n- Add the L = 3 refinement (Lemma E) and Corollary F to the K = 3 section of output.md.\n- Empirical/algorithmic: explore order-pruning strategies that preserve correctness (no false acceptance) while cutting computation, e.g., filter by the c-first certificate and by pairwise margins to select a small set of promising orders before LP evaluation.\n- Lower bounds: attempt a packing over elimination orders by embedding index information into small perturbations of per-round tallies so that only one order yields augmentation \u2264 \u03b5, targeting a Fano-style \u03a9((log((K \u2212 1)!))/\u03b5^2) lower bound.\n"
}