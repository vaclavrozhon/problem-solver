
--- Page 1 ---
arXiv:2307.13685v1  [cs.DS]  25 Jul 2023
Noisy k-means++ Revisited
Christoph Grunau
ETH Zurich
cgrunau@inf.ethz.ch
Ahmet Alper Özüdo˘gru
ETH Zurich
oahmet@student.ethz.ch
Václav Rozhoň
ETH Zurich
rozhonv@ethz.ch
July 26, 2023
Abstract
The k-means++ algorithm by Arthur and Vassilvitskii [SODA 2007] is a classical and time-tested
algorithm for the k-means problem. While being very practical, the algorithm also has good theoretical
guarantees: its solution is O(log k)-approximate, in expectation.
In a recent work, Bhattacharya, Eube, Roglin, and Schmidt [ESA 2020] considered the following
question: does the algorithm retain its guarantees if we allow for a slight adversarial noise in the sampling
probability distributions used by the algorithm? This is motivated e.g. by the fact that computations with
real numbers in k-means++ implementations are inexact. Surprisingly, the analysis under this scenario
gets substantially more diﬃcult and the authors were able to prove only a weaker approximation guarantee
of O(log2 k). In this paper, we close the gap by providing a tight, O(log k)-approximate guarantee for
the k-means++ algorithm with noise.
1
Introduction
The k-means problem is a classical problem in computer science: given a point set X ⊆Rd consisting of
n points and a parameter k, we are asked to return a set of k clusters with corresponding cluster centers
C ⊆Rd so as to minimize the sum of the squared distances of points of X with respect to their closest cluster
center in C. Formally, we are asked to minimize the function ϕ(X, C) deﬁned by ϕ(x, C) = minc∈C ||x −c||2
for a single point x and as ϕ(X, C) = P
x∈X ϕ(x, C) for a set of points.
There exists some ﬁxed constant c > 1 such that it is NP-hard to ﬁnd a c-approximate solution to
the k-means objective [ADHP09, ACKS15]. On the other hand, a substantial amount of work has been
devoted to ﬁnding polynomial time algorithms with a good approximation guarantee, with the currently
best approximation ratio being 5.912 [CAEMN22]. On the practical side, the celebrated clustering algorithm
k-means++ by Arthur and Vassilvitskii [AV07] is one of the classical algorithms for the k-means problem.
Due to its simplicity, it is widely used in practice, for example in the well-known Python Scikit-learn library
[PVG+11]. It is also very appealing from the theoretical perspective, as it returns a solution that is O(log k)-
approximate, in expectation.
The k-means++ algorithm (Algorithm 1 with ε = 0) is indeed very simple: we sample C ⊆X in k steps.
The ﬁrst center is taken as a uniformly random point of X. To get each subsequent center, we always ﬁrst
compute the current costs ϕ(x, Ci) for each x ∈X; then we sample each point of X as the next center with
probability proportional to ϕ(x, Ci).
In [BERS20], the authors made an intriguing observation: the classical analysis of the algorithm by Arthur
and Vassilvitski [AV07] fails to work if we allow small errors in the sampling probabilities. That is, consider
Algorithm 1: this is the k-means++ algorithm, however, with an additional small positive parameter ε. In
every step, before we sample, we allow an adversary to perturb the sampling distribution such that the
multiplicative change of each probability is within 1 ± ε of its original value.
Does the noisy k-means++ algorithm retain the original guarantees? This question is natural since in
every implementation, there are small numerical errors associated with the distance computations made by
Algorithm 1. It would be shocking if these errors could substantially aﬀect the quality of the algorithm’s
output! From a more theoretical perspective, the authors of [BERS20] considered this problem as a ﬁrst
1

--- Page 2 ---
Algorithm 1 (1 + ε)-noisy k-means++
Input: X, k, 0 ≤ε < 1/2
1: Sample x ∈X w.p. in

(1 −ε) · 1
n, (1 + ε) · 1
n

, set C1 = {x}.
2: for i ←0, 1, . . ., k −1 do
3:
Sample x ∈X w.p. in
h
(1 −ε) · ϕ(x,Ci)
ϕ(X,Ci), (1 + ε) · ϕ(x,Ci)
ϕ(X,Ci)
i
and set Ci+1 = Ci ∪{x}.
return C := Ck
step towards understanding other questions related to the k-means++ algorithm, in particular the analysis
of the greedy variant of k-means++, a related algorithm later analyzed in [GÖRT22].
Going back to noisy k-means++, the authors of [BERS20] proved that Algorithm 1 remains O(log2 k)-
approximate even for small constant ε (think e.g. ε = 0.01). In this paper, we improve their analysis to
recover the tight O(log k)-approximation guarantee. That is, we show that the adversarial noise worsens the
approximation guarantee by at most a constant multiplicative factor.
Theorem 1.1. Algorithm 1 is O(log k)-approximate, in expectation.
Remark 1.2. It would be interesting to see an analysis of the approximation ratio of Algorithm 1 that would
be within a 1 + O(ε)-factor of the classical k-means++ analysis from [AV07], or a counterexample showing
this is not possible. In our analysis, we lose a very large constant factor even for very small ε.
Related Work: There is a lot of work related to the k-means++ algorithm, both improving the algorithm
or its analysis [LS19, CGPR20, ADK09, Wei16, MRS20, BERS20, GÖRT22] and adapting it to other setups
[BMV+12, BLHK16b, Roz20, MRS20, BLHK16a, BLK17, BVX19, GR20].
Acknowledgements: We would like to thank Mohsen Ghaﬀari for many helpful comments.
2
Reduction to a Sampling Game
To analyze Algorithm 1, the authors of [BERS20] follow the proof of [AV07] (more precisely, they follow the
proof from [Das19]) and show that most arguments of that proof, in fact, work even in the adversarial noise
scenario. The part of the proof that does not generalize from ε = 0 to ε > 0 can be distilled into a simple
sampling process that we analyze in this paper. We next describe this process and state its relation to the
analysis of noisy k-means++ (cf. the discussion on page 15 of [BERS20]).
Deﬁnition 2.1 ((1 + ε)-adversarial sampling process). Let 0 < ε < 1/2. We deﬁne the (1 + ε)-adversarial
sampling process as follows. At the beginning, there is a set E0 of k elements where each element e ∈E0
has some nonnegative weight w0(e). The process has k rounds where in each round, we form the new set
Ei+1 from Ei as follows:
1. We deﬁne the distribution Di over Ei where the probability of selecting e ∈Ei is deﬁned as wi(e)/ P
e∈Ei wi(e).
Next, an adversary chooses an arbitrary distribution Dε
i over Ei that satisﬁes for any e ∈Ei that
(1 −ε)PDi(e) ≤PDε
i (e) ≤(1 + ε)PDi(e).
(1)
We sample an element ei+1 ∈Ei according to Dε
i and set Ei+1 = Ei \ {ei+1}.
2. Next, an adversary chooses a new weight function wi+1(e) for every element e ∈Ei+1 as an arbitrary
function that satisﬁes
0 ≤wi+1(e) ≤wi(e).
We will be interested in the expected average weight of an element after some number of steps in this
process, that is, we need to understand the value of E
h P
e∈Ei wi(e)
k−i
i
for 0 ≤i < k. If ε = 0, one can prove
that
E
P
e∈Ei wi(e)
k −i

≤
P
e∈Ei−1 wi−1(e)
k −(i −1)
(2)
2

--- Page 3 ---
where the randomness is over the sampling in the i-th step (we always regard the adversary as ﬁxed in
advance). Why is Eq. (2) true? The inequality would clearly hold with equality if the distribution Di were
a uniform one and there was no adversary; we in fact give larger sampling probabilities to heavier elements
in Di and, moreover, the adversary can lower the weights arbitrarily after we sample, but both of these
operations can make the left-hand side of Eq. (2) only smaller.
However, this monotonic behavior is no longer true for ε > 0. The question that needs to be analyzed as
a part of the analysis of noisy k-means++ is whether the adversarial choices can make the average size of
an element drift so that in the end the left-hand side of Eq. (2) is substantially larger than P
e∈E0 w0(e)/k.
More precisely, we will need to bound the following quantity that we call the adversarial advantage.
Deﬁnition 2.2 (Adversarial advantage). We say that the adversarial advantage is at most some function
f if the following conclusion holds: Consider a (1 + ε)-adversarial sampling process on k elements for any
0 < ε < 1
2, any starting set E0, and any adversary. For any 0 ≤i < k, we have
E
P
e∈Ei wi(e)
k −i

≤f(k) ·
P
e∈E0 w0(e)
k
.
(3)
Although we require the inequality Eq. (3) to hold for all i, note that for all 0 ≤i ≤(1−δ)k we can choose
f(k) = 1/δ in Eq. (3) and it will be satisﬁed for those values of i simply because P
e∈Ei wi(e) ≤P
e∈E0 w0(e)
is true deterministically. Thus, intuitively, i = k −1 is the hardest case.
In [BERS20], the authors proved that if we adapt the analysis of k-means++ to the noisy k-means++,
it only picks up the multiplicative factor of f(k). That is, analyzing the (1 + ε)-adversarial sampling process
is enough to get an upper bound for noisy k-means++. The following theorem is proven in [BERS20] (it is
proven only for f(k) = O(log k), but it directly generalizes to any f(k)).
Theorem 2.3 (Theorem 2 in [BERS20]). For any 0 < ε < 1/2, (1 + ε)-noisy k-means++ is O(f(k) · log k)-
approximate, in expectation.
In Lemma 10 of [BERS20], the authors prove that f(k) = O(log k). The reason for this is that if an
element e ∈E0 is Θ(log k) times larger than the average size of an element of E0, it will be sampled in the
ﬁrst k/2 steps of the process with probability 1 −1/kO(1). Thus, the contribution of elements Ω(log k) larger
than the average to the left-hand side of Eq. (3) is negligible even for i = k −1. Hence, f(k) = O(log k).
Lemma 2.4 (Lemma 10 in [BERS20]). The adversarial advantage is at most O(log k).
Our technical contribution is to show that the adversarial advantage is bounded by O(1).
Lemma 2.5. The adversarial advantage is at most O(1).
Theorem 1.1 then follows from Theorem 2.3 and Lemma 2.5.
3
Analysis of the Sampling Process
This section is devoted to the proof of Lemma 2.5. We view the adversary as a function ﬁxed at the beginning
of the argument. We start by normalizing the starting weights w0 so that the average at the beginning is one,
i.e., from now on we assume that (P
e∈E0 w0(e))/k = 1. For every E ⊆Ei, we deﬁne wi(E) = P
e∈E wi(e)
and similarly PDε
i (E) = P
e∈E PDε
i (e). In every step i, we consider the partition Ei = Bi ⊔Mi ⊔Si where
e ∈Ei is in
1. the big set Bi iﬀwi(e) ≥80,
2. the medium set Mi iﬀ2 < wi(e) < 80 and
3. the small set Si iﬀwi(e) ≤2.
The main idea of the analysis is to show that wi(Bi) = O(|Si|), and thus wi(Ei)
k−i
=
O(|Si|)
|Si|+|Mi|+|Bi| = O(1),
with probability 1 −e−Ω(|Si|). This turns out (see the proof of Lemma 2.5) that this is suﬃcient to show
that the adversarial advantage is O(1), i.e., that E
h
wi(Ei)
k−i
i
= O(1).
3

--- Page 4 ---
Roughly speaking, we call an iteration with ℓsmall elements bad, if the total weight of the big elements
is greater than 4ℓ, which intuitively means the average drifted way above 1. In general we use the number
of the small elements as our main way to refer to the iterations. Then in Lemma 3.2 we denote with ℓmax
the number of small elements at the ﬁrst bad iteration. Using that the previous iterations were good, and
wi2ℓ(Bi2ℓ) ≤8ℓfor the bad iterations (Deﬁnition 3.1), we provide an upper bound on the average element
size for the following iterations. Even though this bound is depending on the number of the small elements
ℓ, we show in Lemma 3.3 that an iteration is bad with probability at most e−ℓ
40 , which is enough to show
the constant average in expectation.
The following deﬁnition is crucial for our analysis.
Deﬁnition 3.1. For every ℓ∈{1, 2, . . ., |S0|}, we deﬁne iℓas the smallest i for which |Si| = ℓ. We refer to
a given ℓ∈{1, 2, . . ., ⌊|S0|/2⌋} as bad if both wi2ℓ(Bi2ℓ) ≤8ℓand wiℓ(Biℓ) > 4ℓand otherwise we refer to ℓ
as good.
Note that iℓis well-deﬁned in the sense that there has to exist at least one i with |Si| = ℓfor every
ℓ∈{1, 2, . . ., |S0|}. This follows from |Si+1| ≥|Si| −1 for every i ∈{1, 2, . . ., k −1} and |Sk−1| ≤1.
Lemma 3.2. Let ℓmax be deﬁned as the largest ℓ∈{1, 2, . . ., ⌊|S0|/2⌋} such that ℓis bad, if there exists
such an ℓ, and otherwise let ℓmax = 1. Then, for every i ∈{0, 1, . . ., k −1}, we have wi(Ei)
k−i
≤90ℓmax.
Proof. We ﬁrst prove by induction that wi(Bi) ≤max(4|Si|, 8ℓmax) for every i ∈{0, 1, . . ., k −1}. As our
base case, we consider any i with |Si| ≥|S0|/2. Using that the average weight is 1 at the beginning, we
get |S0| ≥k/2 by Markov’s inequality and therefore wi(Bi) ≤k ≤2|S0| ≤4|Si|. For our induction step,
consider some arbitrary i with |Si| < |S0|/2. Let ℓ:= |Si|. First, we consider the case that ℓmax ≥ℓ. In
particular, this implies |Si−1| ≤|Si| + 1 ≤ℓ+ 1 ≤ℓmax + 1 and therefore we get by induction that
wi(Bi) ≤wi−1(Bi−1) ≤max(4|Si−1|, 8ℓmax) ≤max(4(ℓmax + 1), 8ℓmax) ≤8ℓmax.
Thus, it suﬃces to consider the case that ℓ> ℓmax, which in particular implies that ℓis good. We have i2ℓ<
iℓ≤i (since ℓ≤|S0|/2 ≤i) and therefore we can assume by induction that wi2ℓ(Bi2ℓ) ≤max(4(2ℓ), 8ℓmax) =
8ℓ. As ℓis good, this implies that wiℓ(Biℓ) ≤4ℓand therefore wi(Bi) ≤wiℓ(Biℓ) ≤4ℓ= 4|Si|. This ﬁnishes
the induction and thus we indeed have wi(Bi) ≤max(4|Si|, 8ℓmax) for every i ∈{0, 1, . . ., k −1}. Therefore,
wi(Ei)
k −i ≤
wi(Ei)
|Si| + |Mi| + |Bi| ≤
wi(Bi)
max(|Si|, 1) + 80(|Si| + |Mi|)
|Si| + |Mi|
≤max(4, 8ℓmax) + 80 ≤90ℓmax.
Lemma 3.3. Let ℓ∈{1, 2, . . ., ⌊|S0|/2⌋}. Then, ℓis bad with probability at most e−ℓ
40 .
For the proof of Lemma 3.3, we need the following Chernoﬀ-bound variant.
Lemma 3.4 (Chernoﬀbound). Let X1, . . . , Xℓbe independent Bernoulli-distributed random variables, each
equal to one with probability p. Then,
P
 ℓ
X
i=1
Xi < pℓ
2
!
≤e−pℓ/8.
Proof of Lemma 3.3. Throughout the proof, we assume that wi2ℓ(Bi2ℓ) ≤8ℓ. In particular,
|Bi2ℓ| ≤wi2ℓ(Bi2ℓ)
80
≤ℓ
10.
Below, we will deﬁne for every j ∈{1, 2, . . ., ℓ} an indicator variable Xj in such a way that
1. E[Xj|X1, X2, . . . , Xj−1] ≥1
5 for every j ∈{1, 2, . . ., ℓ} and
2. if X := Pℓ
j=1 Xj ≥
ℓ
10, then wiℓ(Biℓ) ≤4ℓ.
4

--- Page 5 ---
The ﬁrst property implies that X stochastically dominates a random variable X′ which is the sum of ℓ
independent Bernoulli-distributed random variables, each equal to one with probability 1/5. Thus, using
Lemma 3.4, we get
P

X < ℓ
10

≤P

X′ < ℓ
10

≤e−ℓ
40 .
Thus, we can now use the second property to deduce that ℓis bad with probability at most e−ℓ
40 . It thus
remains to deﬁne the random variables and show that they indeed satisfy the two properties. To that end,
ﬁx some j ∈{1, 2, . . ., ℓ}. We deﬁne i′
j as the smallest i ∈{i2ℓ, i2ℓ+ 1, . . . , iℓ−1} with |Si| = 2ℓ−j + 1
and ei+1 /∈Mi. Note that there exists at least one such i as there exists some i with |Si| = 2ℓ−j + 1 and
|Si+1| = 2ℓ−j, and for this i it holds that ei+1 ∈Si and therefore ei+1 /∈Mi. Note that it furthermore holds
that i′
1 < i′
2 < . . . < i′
ℓ. We set Xj = 1 if wi′
j(Bi′
j) ≤4ℓor ei′
j+1 ∈Bi′
j and otherwise we set Xj = 0. We
start by showing that the second property holds by proving the contrapositive. To that end, assume that
wiℓ(Biℓ) > 4ℓ. In particular, we have for every j that wi′
j (Bi′
j) > 4ℓ. Thus, if Xj = 1, we get ei′
j+1 ∈Bi′
j
and therefore |Bi′
j+1| ≤|Bi′
j| −1. As |Bi2ℓ| <
ℓ
10, we therefore get that X <
ℓ
10, as needed.
It remains to show the ﬁrst property. To that end, consider any i and assume we have already sampled
e1, . . . , ei in an arbitrary manner such that |Si| ≤2ℓand wi(Bi) ≥4ℓ. Then, conditioned on ei+1 /∈Mi, we
get ei+1 ∈Bi with probability at least
Dε
i (Bi)
Dε
i (Bi) + Dε
i (Si) ≥
(1 −ε)wi(Bi)
(1 −ε)wi(Bi) + (1 + ε)wi(Si) ≥
0.5 · 4ℓ
0.5 · 4ℓ+ 1.5 · 2 · 2ℓ≥1
5.
In particular, this directly implies E[Xj|X1, X2, . . . , Xj−1] ≥1
5 for every j ∈{1, 2, . . ., ℓ}.
Finally, we are ready to prove Lemma 2.5 by combining Lemmas 3.2 and 3.3.
Proof of Lemma 2.5. Fix some i ∈{0, 1, . . ., k −1}. Let ℓmax be deﬁned as in Lemma 3.2. Lemma 3.2 gives
that for every ℓwith Pr[ℓmax = ℓ] > 0, we have
E
P
e∈Ei wi(e)
k −i
|ℓmax = ℓ

≤90ℓ.
Moreover, for ℓ> 1 , we can use Lemma 3.3 to deduce that P[ℓmax = ℓ] ≤P[ℓis bad] ≤e−ℓ
40 . Therefore,
E
P
e∈Ei wi(e)
k −i

≤
∞
X
ℓ=1
90ℓ· e−ℓ−1
40 = O(1).
References
[ACKS15]
Pranjal Awasthi, Moses Charikar, Ravishankar Krishnaswamy, and Ali Kemal Sinop.
The
hardness of approximation of euclidean k-means. arXiv preprint arXiv:1502.03316, 2015.
[ADHP09]
Daniel Aloise, Amit Deshpande, Pierre Hansen, and Preyas Popat. Np-hardness of euclidean
sum-of-squares clustering. Machine learning, 75(2):245–248, 2009.
[ADK09]
Ankit Aggarwal, Amit Deshpande, and Ravi Kannan. Adaptive sampling for k-means clus-
tering. In Approximation, Randomization, and Combinatorial Optimization. Algorithms and
Techniques, pages 15–28. Springer, 2009.
[AV07]
David Arthur and Sergei Vassilvitskii. k-means++: The advantages of careful seeding. In
Proceedings of the eighteenth annual ACM-SIAM symposium on Discrete algorithms, pages
1027–1035. Society for Industrial and Applied Mathematics, 2007.
5

--- Page 6 ---
[BERS20]
Anup Bhattacharya, Jan Eube, Heiko Röglin, and Melanie Schmidt. Noisy, greedy and not so
greedy k-means++. In 28th Annual European Symposium on Algorithms (ESA 2020). Schloss
Dagstuhl-Leibniz-Zentrum für Informatik, 2020.
[BLHK16a]
Olivier Bachem, Mario Lucic, Hamed Hassani, and Andreas Krause. Fast and provably good
seedings for k-means. In Advances in neural information processing systems, pages 55–63, 2016.
[BLHK16b]
Olivier Bachem, Mario Lucic, S Hamed Hassani, and Andreas Krause.
Approximate k-
means++ in sublinear time. In Thirtieth AAAI Conference on Artiﬁcial Intelligence, 2016.
[BLK17]
Olivier Bachem, Mario Lucic, and Andreas Krause. Distributed and provably good seedings for
k-means in constant rounds. In Proceedings of the 34th International Conference on Machine
Learning-Volume 70, pages 292–300. JMLR. org, 2017.
[BMV+12]
Bahman Bahmani, Benjamin Moseley, Andrea Vattani, Ravi Kumar, and Sergei Vassilvitskii.
Scalable k-means++. Proceedings of the VLDB Endowment, 5(7):622–633, 2012.
[BVX19]
Aditya Bhaskara, Sharvaree Vadgama, and Hong Xu. Greedy sampling for approximate clus-
tering in the presence of outliers. Advances in Neural Information Processing Systems, 32,
2019.
[CAEMN22] Vincent Cohen-Addad, Hossein Esfandiari, Vahab Mirrokni, and Shyam Narayanan. Improved
approximations for euclidean k-means and k-median, via nested quasi-independent sets, 2022.
[CGPR20]
Davin Choo, Christoph Grunau, Julian Portmann, and Václav Rozhon. k-means++: few more
steps yield constant approximation. In International Conference on Machine Learning, pages
1909–1917. PMLR, 2020.
[Das19]
Sanjoy Dasgupta. Lecture 3 – algorithms for k-means clustering, 2013. accessed May 8th, 2019.
[GÖRT22]
Christoph Grunau, Ahmet Alper Özüdo˘gru, Václav Rozhoň, and Jakub Tětek. A nearly tight
analysis of greedy k-means++. arXiv preprint arXiv:2207.07949, 2022.
[GR20]
Christoph Grunau and Václav Rozhoň. Adapting k-means algorithms for outliers, 2020.
[LS19]
Silvio Lattanzi and Christian Sohler.
A better k-means++ algorithm via local search.
In
International Conference on Machine Learning, pages 3662–3671, 2019.
[MRS20]
Konstantin Makarychev, Aravind Reddy, and Liren Shan. Improved guarantees for k-means++
and k-means++ parallel. Advances in Neural Information Processing Systems, 33:16142–16152,
2020.
[PVG+11]
F. Pedregosa, G. Varoquaux, A. Gramfort, V. Michel, B. Thirion, O. Grisel, M. Blondel,
P. Prettenhofer, R. Weiss, V. Dubourg, J. Vanderplas, A. Passos, D. Cournapeau, M. Brucher,
M. Perrot, and E. Duchesnay. Scikit-learn: Machine learning in Python. Journal of Machine
Learning Research, 12:2825–2830, 2011.
[Roz20]
Václav Rozhoň. Simple and sharp analysis of k-means——. In International Conference on
Machine Learning, pages 8266–8275. PMLR, 2020.
[Wei16]
Dennis Wei. A constant-factor bi-criteria approximation guarantee for k-means++. In Advances
in Neural Information Processing Systems, pages 604–612, 2016.
6
