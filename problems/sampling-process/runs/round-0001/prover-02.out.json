{
  "progress_md": "# Reading and scope\n- Goal: understand asymptotic order of g(k,2) in the two-point adversarial sampling process; explore partial success under the \u201clighter-item adversary\u201d (delete the lighter of two i.i.d. size\u2011biased samples, with fixed weights), and the geometric layered inputs.\n- Ambiguity check: In task.tex, g(k,2) is stated without an expectation. In G\u00d6RT (Fact B.4), discussion uses sampling; all known comparisons (\u201cwith constant probability\u201d) suggest the intended guarantee is in expectation. I proceed with expected values; if pathwise bounds are required, the trivial O(k) upper bound is essentially best possible (rare but possible sequences can remove all light items first).\n\n# Lighter\u2011item adversary: exact one\u2011step formulas\nFix a multiset of positive weights v = (v_1,\u2026,v_n) at some step, with W = \u2211 v_i, n = k\u2212i.\n- Let the size\u2011biased law be P(Z = v_j) = v_j/W. For two i.i.d. draws Z_1, Z_2, the rule deletes min(Z_1,Z_2).\n- Sort v nondecreasingly: v_1 \u2264 \u2026 \u2264 v_n. Let tail sums S_j = v_j + \u2026 + v_n.\n\nClaim 1 (deletion probability and expected drop).\n- Probability item j is deleted in this step is\n  \n  P_del(j) = 2 (v_j/W) (S_j/W) \u2212 (v_j/W)^2 = v_j (2 S_j \u2212 v_j)/W^2.\n  \n  Proof sketch: P(delete j) = P(X=j, Y\u2265v_j) + P(Y=j, X\u2265v_j) \u2212 P(X=Y=j) with X,Y i.i.d.\n- Expected weight removed in the step is\n  \n  E_drop = E[min(Z_1,Z_2)] = (1/W^2) \u2211_{j=1}^n v_j^2 (2 S_j \u2212 v_j).\n  \n  This representation is useful and testable (e.g., by coding or small hand\u2011checks).\n\nImmediate corollaries:\n- Upper bound: E_drop \u2264 2 \u2211 v_j^2 / W = 2 E[Z]. (Use S_j \u2264 W.)\n- Lower bound: E_drop \u2265 \u2211 v_j^3 / W^2. (Use S_j \u2265 v_j.) This is weak but universal.\n\n# Hazard of a single heavy outlier\nConsider one heavy v_max = M and all other weights summing to R; let p = M/(M+R).\n\nLemma 2 (heavy hazard). In any step with two\u2011sample deletion, the heavy is deleted with probability exactly p^2 (the event both draws are the heavy). Hence while the heavy survives, its deletion time T satisfies\n- P(T > t) \u2264 (1 \u2212 p^2)^t and E[T] \u2264 1/p^2.\nMoreover, while the heavy survives and other items are removed, p is nondecreasing (weights fixed), so these bounds are conservative across time. This gives a quick (and checkable) explanation why truly massive outliers cannot be postponed for long under the lighter\u2011item rule.\n\nImplication: In the extreme instance (M \u224d k, others weight 1), the expected number of deletions before the heavy disappears is O(1); during O(1) deletions, the factor (n/(n\u2212t)) multiplying average is 1+O(1/k), so the expected average cannot blow up by more than a constant in this phase.\n\n# Two\u2011value distributions: exact minimization of E[min]/AVG\nA clean tractable family: m light items of weight a and (n\u2212m) heavy items of weight b = r a (r \u2265 1). Let p_h be the size\u2011biased mass of the heavy side: p_h = (n\u2212m) b / W and p_\u2113 = 1 \u2212 p_h.\n- E[min(Z_1,Z_2)] = a + p_h^2 (b \u2212 a). (Since min is a unless both draws are heavy.)\n- AVG = W/n = (m a + (n\u2212m) b)/n.\nFor n=2 (one a and one b), with r = b/a we obtain\n- E[min]/AVG = 2 (r^3 + 2 r + 1)/(1 + r)^3.\nDifferentiating shows the minimum over r\u22651 occurs at r* = (4 + 2\u221a7)/6 \u2248 1.54858, with value\n- c_2 := min_r E[min]/AVG \u2248 0.943931\u2026\nFor general m, n but still two weights, optimizing over the size\u2011biased share p and r yields the same qualitative picture: the worst ratio E[min]/AVG is attained in a two\u2011item configuration (this can be checked for the two\u2011group model by expressing in (p,r) and observing the minimal achieved at boundary m=1, n\u2212m=1), yielding the same constant c_2.\n\nConjecture 1 (Schur\u2011convex minimizer). Among all weight vectors of length n with fixed W, the quantity E[min(Z_1,Z_2)]/AVG is minimized by a two\u2011point support (one a and one b). If true, then universally\n- E[min(Z_1,Z_2)] \u2265 c_2 \u22c5 (W/n) with c_2 \u2248 0.9439.\nThis is amenable to a proof via majorization/Schur\u2011convexity since the functional \u2211_{i,j} v_i v_j min(v_i,v_j) is symmetric cubic; I have not completed this, but it is a crisp target to check (first for n=3 by hand).\n\nConsequence (conditional): With W\u2032 = W \u2212 E_drop and n\u2032 = n \u2212 1,\n- E[AVG\u2032] = E[W\u2032/n\u2032] \u2264 (W \u2212 c_2 W/n)/(n\u22121) = AVG \u22c5 (1 \u2212 c_2/n)/(1 \u2212 1/n).\nTelescoping over n = k,k\u22121,\u2026 gives\n- E[AVG_i] \u2264 AVG_0 \u22c5 exp(\u2211_{t=n+1}^{k} [(1\u2212c_2)/t + O(1/t^2)]) = O(1) \u22c5 AVG_0 \u22c5 k^{1\u2212c_2}.\nThus, under the lighter\u2011item adversary with fixed weights,\n- g(k,2) \u2264 O(k^{1\u2212c_2}) with 1\u2212c_2 \u2248 0.0561, i.e., a subpolynomial exponent. This is a concrete, nontrivial improvement over O(k), and it reduces the problem to proving Conjecture 1.\n\nRemark: Even without Conjecture 1, a weaker universal bound E[min] \u2265 c \u22c5 (W/n) for some explicit c>0 would imply g(k,2) \u2264 O(k^{1\u2212c}). The two\u2011item calculation gives c\u22650.9439 in that restricted family; proving any uniform c>0.8 appears tractable and would already yield g(k,2) \u2264 k^{0.2}.\n\n# General adversary (not restricted to lighter)\nTwo knobs make the adversary stronger than the lighter rule: (i) choosing which of two sampled items to delete, and (ii) shrinking weights. Even with fixed weights, the adversary can choose in each step whether to use \u2113_i=1 (safe from p_max^2 hazards) or \u2113_i=2 (to accelerate removal of light). This suggests the potential for a lower bound strictly larger than a constant. A promising direction: adapt \u201cpostponing a heavy\u201d strategies by switching to \u2113=1 whenever p_max becomes too large, and using \u2113=2 otherwise to skim light weight. A layered instance should then realize a growth at least \u03a9(log k). I will formalize this in the next round (I have a blueprint combining the hazard control with a coarse potential for W).\n\n# Geometric layered inputs under lighter\u2011item adversary (heuristics)\nInstance: \u03b1^i items at weight 2^{\u2212i}, i=0,\u2026,I, total \u2248k.\n- The size\u2011biased mass at layer i is \u221d \u03b1^i \u22c5 2^{\u2212i} = (\u03b1/2)^i. Thus if \u03b1<2, most sampling mass is on small i; the lighter\u2011item deletion then removes primarily those layers, keeping W in check. If \u03b1>2, mass shifts to deep layers; nevertheless, the \u201cboth draws in deep layers\u201d event becomes frequent and removes deep elements themselves. Heuristic computation (using the explicit two\u2011group formulas layer\u2011by\u2011layer) suggests no blow\u2011up beyond a constant factor for fixed \u03b1 (I will turn this into a precise bound; the relevant recurrence is E_drop \u2248 \u2211 p_i^2 2^{\u2212i} + (1 \u2212 \u2211 p_i^2) min\u2011layer weight, with p_i \u221d (\u03b1/2)^i).\n\n# Why the noisy\u2011k\u2011means++ constant\u2011average proof is relevant (and different)\nThe O(1) adversarial\u2011advantage of [Noisy k\u2011means++ revisited, 2023] relies on per\u2011draw (1\u00b1\u03b5) perturbations and an arbitrary but monotone decrease of weights. The crucial combinatorial lemma (bounding the total number of bad iterations when \u201cbig mass\u201d dominates) depends on the fact that only one draw is made per step. In our \u2113=2 model, the quadratic hazard p_max^2 interacts adversarially with the choice \u2113_i\u2208{1,2}; still, their \u201cbig/medium/small\u201d partition and the integral identity E[min]=\u222b S(t)^2/W^2 may be transplantable to produce an O(1) bound in the lighter case. I will try to adapt their Lemma 3.3\u2011style argument with S(t) in place of the distorted probabilities.\n\n# Concrete, checkable items produced this round\n1) Exact deletion probability and expected drop (Claim 1): formulas ready to verify numerically on toy inputs.\n2) Heavy hazard lemma (Lemma 2): simple, rigorous bound E[T] \u2264 1/p^2, with qualitative corollary that massive outliers cannot persist.\n3) Two\u2011value family minimization: closed form for E[min]/AVG, minimum at r*\u22481.5486 with constant c_2\u22480.9439; derivation is explicit and checkable.\n4) Conditional bound: g(k,2) \u2264 O(k^{1\u2212c_2}) under lighter\u2011item adversary if Conjecture 1 holds; even weaker constants would still give subpolynomial growth. This is a clear, testable milestone.\n\n# Obstacles / gaps\n- The key gap is Conjecture 1 (minimizer is two\u2011point support). I have not yet proved Schur\u2011convexity of L(v):=\u2211_{i,j} v_i v_j min(v_i,v_j) under fixed W and n. This is the crux for a universal c.\n- For the full adversary (choosing which of the two to delete and optionally shrinking weights), even establishing \u03a9(log k) lower bounds needs a formal strategy (switching \u2113_i), which I have only sketched.\n- Layered instances: I outlined the heuristic; a clean potential\u2011based proof is pending.\n\n# Next steps\n- Prove Conjecture 1 for n=3 explicitly, then attempt induction via Karamata/majorization to show that spreading mass (in the sense of majorization) increases E[min]/AVG. If successful, this certifies c_2.\n- Fallback: obtain a universal explicit constant c\u22653/4 such that E[min] \u2265 c \u22c5 W/n, via a simpler inequality (e.g., Jensen\u2011type bounds on \u222b S(t)^2). This would already give g(k,2) \u2264 k^{1/4} in the lighter model.\n- General adversary lower bound: formalize a strategy that alternates between \u2113=2 (to skim light) and \u2113=1 (to avoid p_max^2 deletions), and construct a layered instance achieving E[AVG] \u2265 \u03a9(log k) \u22c5 AVG_0.\n- Layered inputs under lighter adversary: write the exact mean\u2011field recurrence for layer counts using Claim 1 and bound E[AVG_i]/AVG_0 for any fixed \u03b1.\n\n# Sanity checks / toy examples\n- Two items weights (1,2): E[min]=13/9, AVG=3/2, ratio\u22480.9629 < 1 (average increases slightly in one step), consistent with formulas.\n- Heavy\u2011outlier (M=k, k\u22121 ones): initial p=1/2 \u21d2 heavy eliminated each step with prob 1/4; expected survival \u2264 4 steps; numeric evolution shows E[AVG_i] stays within a small constant factor.\n\n# References used\n- G\u00d6RT 2022 Appendix B.2 (definition of the \u2113\u2011point adversarial sampling process) for context.\n- Grunau\u2013\u00d6z\u00fcdo\u011fru\u2013Rozho\u0148 2023 (Noisy k\u2011means++ revisited) for the \u222b S^2 identity style and \u201cbig/medium/small\u201d partition idea.\n"
}