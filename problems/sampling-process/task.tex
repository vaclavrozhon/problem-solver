% \newcommand{\R}{\mathbb{R}}
% \newcommand{\E}{\mathbb{E}}
% \newcommand{\OPT}{\mathrm{OPT}}
% \newcommand{\ALG}{\mathrm{ALG}}
\newcommand{\AVG}{\mathrm{AVG}}

\section*{Model and task: the \emph{two-point adversarial sampling process}}

\paragraph{State.}
Consider the following process. At round $i\in\{0,1,\dots,k\}$ the process maintains a multiset $E_i$ of $k-i$ elements. Every element $e\in E_i$ has a nonnegative weight $w_i(e)$. Let $W_i=\sum_{e\in E_i} w_i(e)$ and define the average weight
\[
\AVG_i \;=\; \frac{W_i}{k-i}\,.
\]

\paragraph{Sampling rule (two-point case).}
Given $E_i$ and weights $w_i$, define the size-biased distribution
\[
\mathsf{D}_i(e)\;=\;\frac{w_i(e)}{W_i}\,.
\]
An adversary chooses $\ell_i\in\{1,2\}$. Then we draw $\ell_i$ points $e^{(1)}_i,\ldots,e^{(\ell_i)}_i$ \emph{independently} from $\mathsf{D}_i$ (sampling with replacement). The adversary selects one of these sampled points, call it $e_i\in\{e^{(1)}_i,\ldots,e^{(\ell_i)}_i\}$, and we delete it: $E_{i+1}=E_i\setminus\{e_i\}$. Finally the adversary chooses new weights $w_{i+1}(e)\in[0,w_i(e)]$ for each remaining $e\in E_{i+1}$. % This is exactly the $\ell$-point adversarial sampling process of Definition~B.3 specialized to $\ell=2$.  :contentReference[oaicite:0]{index=0}

\medskip
This is the $\ell$-point adversarial sampling process of Grunau–Özüdoğru–Rozhoň–Tětek (\emph{GÖRT}) specialized to $\ell=2$; see Appendix~B, Def.~B.3 for the general formulation. % :contentReference[oaicite:1]{index=1}

\paragraph{Quantity of interest.}
Let $g(k,2)$ be the smallest function such that for every adversary, every initial instance $(E_0,w_0)$, and every $0\le i<k$,
\[
\AVG_i \;\le\; g(k,2)\cdot \AVG_0\,.
\]
This is the $g(k,\ell)$ of GÖRT specialized to $\ell=2$ (Fact~B.4). % :contentReference[oaicite:2]{index=2}

\section*{The task}
Determine the asymptotic order of $g(k,2)$ as a function of $k$. In particular, is $g(k,2)=O(1)$ (that is, bounded by a universal constant independent of $k$)? Give the best possible matching upper and lower bounds.

\section*{Context and prior results}

\paragraph{What is known for general $\ell$.}
GÖRT observe (Appendix~B.2) that for $\ell=1$ the average never increases, hence $g(k,1)=1$, while for $\ell>1$ one always has $\Omega(\ell)\le g(k,\ell)\le O(k)$; the lower bound is witnessed by a simple instance where a single very heavy element can be postponed with constant probability when $\ell=\Omega(k)$. The precise growth is open.

\paragraph{Related but different ‘noisy’ model.}
A closely related model allows the adversary to distort sampling probabilities by a $(1\pm\varepsilon)$ multiplicative factor before each draw. Bhattacharya–Eube–Röglin–Schmidt showed an $O(\log k)$ blow-up of the average in that model, yielding an $O(\log^2 k)$ guarantee for the algorithm; GÖRT later improved the analysis to an $O(1)$ average, which recovers the tight $O(\log k)$ bound. See also the direct constant‑average proof in the follow‑up “Noisy k‑means++ revisited,” Lemmas 3.2–3.4 leading to Lemma 2.5. 

\section*{Partial results that would already be interesting}

Consider the following two simplifications. Even analyzing the process under both of them is considered a success. 

\paragraph{1.\ The \emph{lighter-item adversary}.}
Restrict the adversary to the myopic rule “always choose $\ell = 2$, always delete the lighter of the two sampled items” (break ties arbitrarily), and set $w_{i+1}\equiv w_i$ (since decreasing weights only helps \emph{us}, this is the hardest case for upper bounds on $\AVG_i$).

\paragraph{2.\ A concrete geometric input family.}
Consider instances composed of \emph{weight layers}: for each $i\in\{0,1,\dots\}$ there are $\alpha^i$ items of weight $2^{-i}$, for a parameter $\alpha>0$. (Truncate to the largest $i$ so that the total item count is $k$.) Analyze $\AVG_i/\AVG_0$ under the lighter‑item adversary for every fixed $\alpha$.

