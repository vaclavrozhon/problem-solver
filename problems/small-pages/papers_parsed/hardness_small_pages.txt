
--- Page 1 ---
Algorithmica manuscript No.
(will be inserted by the editor)
General Caching Is Hard: Even with Small Pages
Luk√°¬≤ Folwarczn√Ω ¬∑ Ji¬∞√≠ Sgall
August 1, 2016
Abstract Caching (also known as paging) is a classical problem concerning
page replacement policies in two-level memory systems. General caching is the
variant with pages of dierent sizes and fault costs. The strong NP-hardness
of its two important cases, the fault model (each page has unit fault cost) and
the bit model (each page has the same fault cost as size) has been established,
but under the assumption that there are pages as large as half of the cache
size. We prove that this already holds when page sizes are bounded by a small
constant: The bit and fault models are strongly NP-complete even when page
sizes are limited to {1, 2, 3}.
Considering only the decision versions of the problems, general caching is
equivalent to the unsplittable ow on a path problem and therefore our results
also improve the hardness results about this problem.
Keywords General caching ¬∑ Small pages ¬∑ NP-hardness ¬∑ Unsplittable Flow
on a Path
1 Introduction
Caching (also known as uniform caching or paging) is a classical problem in
the area of online algorithms and has been extensively studied since 1960s. It
models a two-level memory system: There is the fast memory of size C (the
cache) and a slow but large main memory where all data reside. The problem
instance comprises a sequence of requests, each demanding a page from the
Partially supported by the Center of Excellence  ITI, project P202/12/G061 of GA ¬ÉR (J.
Sgall) and by the project 14-10003S of GA ¬ÉR (L. Folwarczn√Ω).
Computer Science Institute of Charles University,
Faculty of Mathematics and Physics,
Malostransk√© n√°m. 25, CZ-11800 Praha 1,
Prague, Czech Republic.
E-mail: {folwar,sgall}@iuuk.m.cuni.cz

--- Page 2 ---
2
Luk√°¬≤ Folwarczn√Ω, Ji¬∞√≠ Sgall
main memory. No cost is incurred if the requested page is present in the cache
(a cache hit). If the requested page is not present in the cache (a cache fault),
the page must be loaded at the fault cost of one; some page must be evicted to
make space for the new one when there are already C pages in the cache. The
natural objective is to evict pages in such a way that the total fault cost is
minimized. For a reference on classical results, see Borodin and El-Yaniv [11].
In 1990s, with the advent of World Wide Web, a generalized variant called
le caching or simply general caching was studied [19,21]. In this setting, each
page p has its size(p) and cost(p). It costs cost(p) to load this page into the
cache and the page occupies size(p) units of memory there. Uniform caching
is the special case satisfying size(p) = cost(p) = 1 for every page p. Other
important cases of this general model are
 the cost model (weighted caching): size(p) = 1 for every page p;
 the bit model: cost(p) = size(p) for every page p;
 the fault model: cost(p) = 1 for every page p.
Caching, as described so far, requires the service to load the requested page
when a fault occurs, which is known as caching under the forced policy. Al-
lowing the service to pay the fault cost without actually loading the requested
page to the cache gives another useful and studied variant of caching, caching
under the optional policy.
Previous work. In this article, we consider the problem of nding the optimal
service in the oine version of caching when the whole request sequence is
known in advance. Uniform caching is solvable in polynomial time with a natu-
ral algorithm known as Belady's rule [9]. Caching in the cost model is a special
case of the k-server problem and is also solvable in polynomial time [13]. In
late 1990s, the questions about the complexity status of general caching were
raised. The situation was summed up by Albers et al. [3]: The hardness re-
sults for caching problems are very inconclusive. The NP-hardness result for
the Bit model uses a reduction from partition, which has pseudopolynomial
algorithms. Thus a similar algorithm may well exist for the Bit model. We do
not know whether computing the optimum in the Fault model is NP-hard.
There was no improvement until a breakthrough in 2010 when Chrobak et
al. [15] showed that general caching is strongly NP-hard, already in the case
of the fault model as well as in the case of the bit model. General caching is
usually studied under the assumption that the largest page size is very small
in comparison with the total cache size, as is for example the case of the
aforementioned article by Albers et al. [3]. Instances of caching with pages
larger than half of the cache size (so called obstacles) are required in the proof
given by Chrobak et al. Therefore, this hardness result is in fact still quite
inconclusive.
Independently, a weak NP-hardness of the fault model was proven by Dar-
mann et al. [16]. A substantially simpler proof of the strong NP-hardness of
general caching was given by Bonsma et al. [10]; only pages of sizes in {1, 2, 3}
are needed in the proof, but they have many dierent costs: This means that

--- Page 3 ---
General Caching Is Hard: Even with Small Pages
3
the costs are far from the fault model and in fact they are also far from the
bit model.
Contribution. We give a novel proof of strong NP-hardness for general
caching which gives the rst hardness result restricted to small pages in the
fault and bit models:
Theorem 1.1 General caching is strongly NP-hard even in the case when the
page sizes are limited to {1, 2, 3}, for both the fault model and the bit model,
and under each of the forced and optional policies.
We also give a separate self-contained version of the proof of the result for
general costs (and sizes {1, 2, 3}). This version is rather simple, in particular
signicantly simpler than the one given by Chrobak et al. [15] and at the same
time it uses only two dierent costs, which is a simpler structure compared to
Bonsma et al. [10]. At the same time, the simplied version illustrates some of
the main ideas of our reduction. The reductions for the result in the fault and
bit models are signicantly more involved and require a non-trivial potential-
function-like argument.
Open problems. We prove that general caching with page sizes {1, 2, 3} is
strongly NP-hard while general caching with unit page sizes is easily polyno-
mially solvable. Closing the remaining gap will denitely contribute to a better
understanding of the problem:
Question 1.2 Is general caching also (strongly) NP-hard when page sizes are
limited to {1, 2}? Can caching with page sizes {1, 2} be solved in polynomial
time, at least in the bit or fault model?
In a broader perspective, the complexity status of general caching is still far
from being well understood as the best known result on approximation is a 4-
approximation due to Bar-Noy et al. [7] and there is no result on the hardness
of approximation. Therefore, better understanding of the complexity of general
caching remains an important challenge.
Question 1.3 Is there an algorithm for general caching with an approxima-
tion ratio better than 4, or even a PTAS? Is general caching APX-hard?
Related work. In the decision version, oine general caching is equivalent
(together with interval scheduling/packing, resource allocation and other prob-
lems) to the unsplittable ow on a path problem (UFPP). This equivalence does
not translate to the approximation version of the problem, as the ow corre-
sponds not to the cost of caching, but rather to the complementary quantity
of the total saving of the cost compared to the trivial service that faults on
each request. An important parameter in the world of UFPP is task density,
in the language of caching it is the fault cost divided by the page size. An
approximation scheme with quasi-polynomial time complexity when the ra-
tio of the maximum density to the minimum density is quasi-polynomial was
given by Bansal et al. [6]. The rst non-trivial instance of UFPP for which
a PTAS was invented by Batra et al. [8] is the one when the task densities are

--- Page 4 ---
4
Luk√°¬≤ Folwarczn√Ω, Ji¬∞√≠ Sgall
in a constant range. The bit model of caching is equivalent to the case when
all task densities are equal to one. The best known approximation for UFPP
is (2 + Œµ)-approximation due to Anagnostopoulos et al. [4]. It is also known
that UFPP is not APX-hard, unless NP ‚äÜDTIME(2polylog(n)) [6].
We wish to conclude the overview of results with recent advances in ran-
domized online caching (competitive analysis against oblivious adversary is
considered). Classical results state that the lower bound on the competi-
tive ratio for uniform caching is HC (the C-th harmonic number) [17] and
HC-competitive algorithms were designed [20,1]; it has also been known that
for general caching an HC-competitive algorithm is impossible [14]. Recently,
Brodal et al. [12] developed a substantially faster HC-competitive algorithm
for uniform caching. Based on new methods for rounding of linear programs,
Bansal et al. [5] recently invented an O(log2 C)-competitive algorithm for gen-
eral caching and Adamaszek et al. [2] improved the algorithm to be O(log C)-
competitive.
Outline. The main part of our work  a polynomial-time reduction from
independent set to caching in the fault model under the optional policy with
page sizes restricted to {1, 2, 3}  is explained in Section 2 and its validity
is proven in Section 3. In Section 4, we show how to modify the reduction so
that it works for the bit model as well. In Section 5, we show how to obtain
the hardness results also for the forced policy. Finally, we give a self-contained
presentation of the simple proof of strong NP-hardness for general costs (in
fact, only two dierent and polynomial costs are needed) in Appendix A.
An extended abstract of this paper appeared in Proceedings of the 26th
International Symposium on Algorithms and Computation (ISAAC 2015) [18].
2 Reduction
The decision problem IndependentSet is well-known to be NP-complete.
By 3Caching(forced) and 3Caching(optional) we denote the decision
versions of caching under each policy with page sizes restricted to {1, 2, 3}.
Problem:
IndependentSet
Instance:
A graph G and a number K.
Question:
Is there an independent set of cardinality K in G?
Problem:
3Caching(policy)
Instance:
A universe of pages, a sequence of page requests, numbers
C and L. For each page p it holds size(p) ‚àà{1, 2, 3}.
Question:
Is there a service under the policy policy of the request
sequence using the cache of size C with a total fault cost
of at most L?
We dene 3Caching(fault,policy) to be the problem 3Caching(policy)
with the additional requirement that page costs adhere to the fault model.
The problem 3Caching(bit,policy) is dened analogously.

--- Page 5 ---
General Caching Is Hard: Even with Small Pages
5
In this section, we describe a polynomial-time reduction from Indepen-
dentSet to 3Caching(fault,optional). Informally, a set of pages of size
two and three is associated with each edge and a page of size one is associ-
ated with each vertex. Each vertex-page is requested only twice while there
are many requests on pages associated with edges. The request sequence is de-
signed in such a way that the number of vertex-pages that are cached between
the two requests in the optimal service is equal to the the size of the maximum
independent set.
We now show the request sequence of caching corresponding to the graph
given in IndependentSet with a parameter H. In the next section, we prove
that it is possible to set a proper value of H and a proper fault cost limit L
such that the reduction becomes a valid polynomial-time reduction.
Reduction 2.1 Let G = (V, E) be the instance of IndependentSet. The
graph G has n vertices and m edges and there is an arbitrary xed order of
edges e1, . . . , em. Let H be a parameter bounded by a polynomial function of
n.
The corresponding instance IG of 3Caching(fault,optional) is an in-
stance with the cache size C = 2mH + 1 and the total of 6mH + n pages.
The structure of the pages and the requests sequence is described below.
Pages. For each vertex v, we have a vertex-page pv of size one. For each edge e,
there are 6H edge-pages associated with it that are divided into H groups.
The ith group consists of six pages ¬Øae
i, Œ±e
i , ae
i, be
i, Œ≤e
i ,¬Øbe
i where pages Œ±e
i and Œ≤e
i
have size three and the remaining four pages have size two.
For a xed edge e, let ¬Øae-pages be all pages ¬Øae
i for i = 1, . . . , H. Let also
¬Øa-pages be all ¬Øae-pages for e = e1, . . . , em. The remaining collections of pages
(Œ±e-pages, Œ±-pages, . . . ) are dened in a similar fashion.
Request sequence. The request sequence of IG is organized in phases and
blocks. There is one phase for each vertex v ‚ààV , we call such a phase the
v-phase. There are exactly two requests on each vertex-page pv, one just before
the beginning of the v-phase and one just after the end of the v-phase; these
requests do not belong to any phase. The order of phases is arbitrary. In each v-
phase, there are 2H adjacent blocks associated with every edge e incident with
v; the blocks for dierent incident edges are ordered arbitrarily. In addition,
there is one initial block I before all phases and one nal block F after all
phases. Altogether, there are d = 4mH + 2 blocks.
Let e = {u, v} be an edge, let us assume that the u-phase precedes the
v-phase. The blocks associated with e in the u-phase are denoted by Be
1,1,
Be
1,2, . . . , Be
i,1, Be
i,2, . . . , Be
H,1, Be
H,2, in this order, and the blocks in the v-
phase are denoted by Be
1,3, Be
1,4, . . . , Be
i,3, Be
i,4, . . . , Be
H,3, Be
H,4, in this order.
An example is given in Fig. 1.
Even though each block is associated with some xed edge, it contains one
or more requests to the associated pages for every edge e. In each block, we
process the edges in the order e1, . . . , em that was xed above. Pages associated
with the edge e are requested in two rounds. In each round, we process groups

--- Page 6 ---
6
Luk√°¬≤ Folwarczn√Ω, Ji¬∞√≠ Sgall
Be1
1,1Be1
1,2Be1
2,1Be1
2,2
u-phase
I
Be2
1,1Be2
1,2Be2
2,1Be2
2,2
v-phase
Be1
1,3Be1
1,4Be1
2,3Be1
2,4
w-phase
Be2
1,3Be2
1,4Be2
2,3Be2
2,4
F
pv
pu
pw
Fig. 1 An example of phases, blocks and requests on vertex-pages for a graph with three
vertices u, v, w and two edges e1 = {u, w}, e2 = {v, w} when H = 2
Table 1 Requests associated with an edge e
Block
First round
‚Ä¢
Second round
before Be
i,1
¬Øae
i
‚Ä¢
Be
i,1
¬Øae
i , Œ±e
i
‚Ä¢
be
i
Be
i,2
Œ±e
i , ae
i
‚Ä¢
be
i
between Be
i,2 and Be
i,3
ae
i
‚Ä¢
be
i
Be
i,3
ae
i
‚Ä¢
be
i , Œ≤e
i
Be
i,4
ae
i
‚Ä¢
Œ≤e
i , ¬Øbe
i
after Be
i,4
‚Ä¢
¬Øbe
i
1, . . . , H in this order. When processing the ith group of the edge e, we request
one or more pages of this group, depending on the block we are in. Table 1
determines which pages are requested. Note that the number of requests to
pages of size two is between mH and 2mH per block; in addition, each block
except for the initial and nal ones contains exactly one request to a page of
size three.
Reduction 2.1 is now complete. An example of requests on edge-pages as-
sociated with one edge e is depicted in Fig. 2. Note that the order of the pages
associated with e is the same in all blocks; more precisely, in each block the
requests on the pages associated with e form a subsequence of
¬Øae
1 Œ±e
1 ae
1 . . . ¬Øae
i Œ±e
i ae
i . . . ¬Øae
H Œ±e
H ae
H be
1 Œ≤e
1 ¬Øbe
1 . . . be
i Œ≤e
i ¬Øbe
i . . . be
H Œ≤e
H ¬Øbe
H.
(1)
Preliminaries for the proof. Instead of minimizing the service cost, we
maximize the total saving compared to the service which does not use the
cache at all. This is clearly equivalent when considering the decision versions
of the problems.
Without loss of generality, we assume that any page is brought into the
cache only immediately before some request to that page and removed from the
cache only after some (possibly dierent) request to that page; furthermore,
the cache is empty at the beginning and at the end. That is, a page may be
in the cache only between two consecutive requests to this page, and either it
is in the cache for the whole interval or not at all.
At each time, there exists at most one page of size three that can possibly
be cached. This follows since each block contains at most one request to a
page of size three and each page of size three is requested only twice in two

--- Page 7 ---
General Caching Is Hard: Even with Small Pages
7
¬Øae
1
Be
1,1 Be
1,2
Be
1,3 Be
1,4
Be
2,1 Be
2,2 Be
3,1 Be
3,2
Be
2,3 Be
2,4 Be
3,3 Be
3,4
Œ±e
1
ae
1
¬Øae
2
Œ±e
2
ae
2
¬Øae
3
Œ±e
3
ae
3
be
1
Œ≤e
1
¬Øbe
1
be
2
Œ≤e
2
¬Øbe
2
be
3
Œ≤e
3
¬Øbe
3
Fig. 2 Requests on all pages associated with the edge e when H = 3. Each column rep-
resents some block(s). The labeled columns represent the blocks in the heading, the rst
column represents every block before Be
1,1, the middle column represents every block be-
tween Be
3,2 and Be
1,3, and the last column represents every block after Be
3,4. The requests
in one column are ordered from top to bottom.
consecutive blocks. Since the cache size C = 2m + 1 is odd, this implies that
a service of edge-pages is valid if and only if at each time, at most mH edge-
pages are in the cache. It is convenient to think of the cache as of mH slots
for edge-pages.
As each vertex-page is requested twice, the saving on the n vertex-pages is
at most n. Furthermore, a vertex-page can be cached if and only if during the
phase it never happens that at the same time all slots for edge-pages are full
and a page of size three is cached.
Let SB denote the set of all edge-pages cached at the beginning of the
block B and let Se
B be the set of pages in SB associated with the edge e.
We use sB = |SB| and se
B = |Se
B| for the sizes of the sets. Each edge-page is
requested only in a contiguous segment of blocks, once in each block. It follows
that an edge-page can possibly be in SB only if it is requested both in B and
in the previous block. The total saving on edge-pages is then equal to P
B sB
where the sum is over all blocks. In particular, the maximal possible saving
on the edge-pages is (d ‚àí1)mH, using the fact that SI is empty. We shall

--- Page 8 ---
8
Luk√°¬≤ Folwarczn√Ω, Ji¬∞√≠ Sgall
show that the maximum saving is (d ‚àí1)mH + K where K is the size of the
maximum independent set in G.
Almost-fault model. To understand the reduction, we consider what hap-
pens if we relax the requirements of the fault model and set the cost of each
vertex-page to 1/(n + 1) instead of 1 as required by the fault model.
In this scenario, the total saving on vertex-pages is n/(n + 1) < 1 which
is less than a saving achieved by any single edge-page. Therefore, edge-pages
must be served optimally in the optimal service of the whole request sequence.
In this case, the reduction works already for H = 1. This leads to a quite
short proof of the strong NP-hardness for general caching and we give this
proof in Appendix A. Here, we show just the main ideas that are important
also for the design of our caching instance in the fault and bit models.
We rst prove that for each edge e and each block B Ã∏= I we have se
B = 1
(see Appendix A for details). Using this we show below that for each edge e,
at least one of the pages Œ±e
1 and Œ≤e
1 is cached between its two requests. This
implies that the set of all vertices v such that pv is cached between its two
requests is independent.
For a contradiction, let us assume that for some edge e, neither of the pages
Œ±e
1 and Œ≤e
1 is cached between its two requests. Because pages Œ±e
1 and Œ≤e
1 are
forbidden, there is be
1 in SBe
1,2 and ae
1 in SBe
1,4. Thus somewhere in the two
blocks Be
1,2 and Be
1,3, we must switch from caching be
1 to caching ae
1. However,
this is impossible, because the order of requests implies that we would have
to cache both be
1 and ae
1 at some moment, or none of them would be in Be
1,3
(see Fig. 3). However, there is no place in the cache for such an operation, as
se‚Ä≤
B = 1 for every e‚Ä≤ and B Ã∏= I and in each block, the requests associated with
any e‚Ä≤ are not interleaved with those associated to e.
¬Øae
1
Be
1,1 Be
1,2
Be
1,3 Be
1,4
Œ±e
1
ae
1
be
1
Œ≤e
1
¬Øbe
1
Fig. 3 Pages associated with one edge when H = 1
In the fault model, the corresponding claim se
B = H does not hold. Instead,
we prove that the value of se
B cannot change much during the service and when
we use H large enough, we still get a working reduction.

--- Page 9 ---
General Caching Is Hard: Even with Small Pages
9
3 Proof of Correctness
In this section, we show that the reduction described in the previous section is
indeed a reduction from IndependentSet set to 3Caching(fault,optional).
We prove that there is an independent set of cardinality K in G if and only
if there is a service of the caching instance IG with the total saving at least
(d ‚àí1)mH + K. First the easy direction, which holds for any value of the
parameter H.
Lemma 3.1 Let G be a graph and IG the corresponding caching instance from
Reduction 2.1. Suppose that there is an independent set W of cardinality K
in G. Then there exists a service of IG with the total saving at least (d ‚àí
1)mH + K.
Proof For any edge e, denote e = {u, v} so that the u-phase precedes the v-
phase. If u ‚ààW, we keep all ¬Øae-pages, be-pages, Œ≤e-pages and ¬Øbe-pages in the
cache from the rst to the last request on each page, but we do not cache ae-
pages and Œ±e-pages at any time. Otherwise, we cache all ¬Øae-pages, Œ±e-pages,
ae-pages and ¬Øbe-pages, but do not cache be-pages and Œ≤e-pages at any time.
Fig. 4 shows these two cases for the rst group of pages. In both cases, at each
time at most one page associated with each group of each edge is in the cache
and the saving on those pages is (d ‚àí1)mH. We know that the pages t in
the cache because of the observations made in Section 2.
For any v ‚ààW, we cache pv between its two requests. To check that this is
a valid service, observe that if v ‚ààW, then during the corresponding phase no
page of size three is cached. Thus, the page pv always ts in the cache together
with at most mH pages of size two.
‚äì‚äî
¬Øae
1
Be
1,1 Be
1,2
Be
1,3 Be
1,4
Œ±e
1
ae
1
be
1
Œ≤e
1
¬Øbe
1
¬Øae
1
Be
1,1 Be
1,2
Be
1,3 Be
1,4
Œ±e
1
ae
1
be
1
Œ≤e
1
¬Øbe
1
Fig. 4 The two ways of caching in Lemma 3.1
We prove the converse in a sequence of lemmata. In Section 4, we will
show how to reuse the proof for the bit model. To be able to do that, we list
explicitly all the assumptions about the caching instance that are used in the
following proofs.

--- Page 10 ---
10
Luk√°¬≤ Folwarczn√Ω, Ji¬∞√≠ Sgall
Properties 3.2 Let TG be an instance of general caching corresponding to
a graph G = (V, E) with n vertices, m edges e1, . . . , em, the same cache size and
the same universe of pages as in Reduction 2.1. The request sequence is again
split into phases, one phase for each vertex. Each phase is again partitioned
into blocks, there is one initial block I before all phases and one nal block F
after all phases. There is the total of d blocks.
The instance TG is required to fulll the following list of properties:
(a) Each vertex page pv is requested exactly twice, right before the v-phase
and right after the v-phase.
(b) The total saving achieved on edge-pages is equal to P sB (summing over
all blocks).
(c) For each edge e, there are exactly H pages associated with e requested in
I, all the ¬Øae-pages, and exactly H pages associated with e requested in F,
all the ¬Øbe-pages.
(d) In each block, pages associated with e1 are requested rst, then pages
associated with e2 are requested and so on up to em.
(e) For each block B and each edge e, all requests on ae-pages and ¬Øbe-pages
in B precede all requests on ¬Øae-pages and be-pages in B.
(f) Let e = {u, v} be an edge and p an Œ±e-page or Œ≤e-page. Let B be the rst
block and B the last block where p is requested. Then B and B are either
both in the u-phase or both in the v-phase. Furthermore, no other page of
size three is requested in B, B, or any block between them.
Lemma 3.3 The instance from Reduction 2.1 satises Properties 3.2.
Proof All properties (a), (b), (c), (d), (f) follow directly from Reduction 2.1
and the subsequent observations. To prove (e), recall that the pages associated
with an edge e requested in a particular block always follow the ordering (1).
We need to verify that when the page ae
i is requested, no page ¬Øae
j for j ‚â§i is
requested and that when the page ¬Øbe
i is requested, no ¬Øae-page and no page be
j
for j ‚â§i is requested. This can be seen easily when we explicitly write down
the request sequences for each kind of block, see Table 2.
‚äì‚äî
For the following claims, let TG be an instance fullling Properties 3.2. We
x a service of TG with the total saving at least (d ‚àí1)mH.
Let B be the set of all blocks and B the set of all blocks except for the
initial and nal one. For a block B, we denote the block immediately following
it by B‚Ä≤.
We dene two useful values characterizing the service for the block B:
Œ¥B = mH ‚àísB (the number of free slots for edge-pages at the start of the
service of the block) and Œ≥e
B = |se
B‚Ä≤ ‚àíse
B| (the change of the number of slots
occupied by pages associated with e after requests from this block are served).
The rst easy lemma says that only a small number of blocks can start
with some free slots in the cache.

--- Page 11 ---
General Caching Is Hard: Even with Small Pages
11
Table 2 Request sequences on all pages associated with an edge e
Block
First round ‚Ä¢ Second round
before Be
1,1
¬Øae
1 . . . ¬Øae
H ‚Ä¢
Be
i,1
ae
1 . . . ae
i‚àí1 ¬Øae
i Œ±e
i ¬Øae
i+1¬Øae
i+2 . . . ¬Øae
H ‚Ä¢ be
1 . . . be
i
Be
i,2
ae
1 . . . ae
i‚àí1 Œ±e
i ae
i ¬Øae
i+1¬Øae
i+2 . . . ¬Øae
H ‚Ä¢ be
1 . . . be
i
between Be
H,2 and Be
1,3
ae
1 . . . ae
H ‚Ä¢ be
1 . . . be
H
Be
i,3
ae
i . . . ae
H ‚Ä¢ ¬Øbe
1 . . .¬Øbe
i‚àí1 be
i Œ≤e
i be
i+1be
i+2 . . . be
H
Be
i,4
ae
i . . . ae
H ‚Ä¢ ¬Øbe
1 . . .¬Øbe
i‚àí1 Œ≤e
i ¬Øbe
i be
i+1be
i+2 . . . be
H
after Be
H,4
‚Ä¢ ¬Øbe
1 . . .¬Øbe
H
Lemma 3.4 We have (summing over all blocks except for the initial one)
X
B‚ààB\{I}
Œ¥B ‚â§n.
Proof The total saving on vertex-pages is at most n due to the property (a).
Using the property (b), sI = 0, and the denition of Œ¥B, the total saving on
edge-pages is equal to
X
B‚ààB\{I}
sB = (d ‚àí1)mH ‚àí
X
B‚ààB\{I}
Œ¥B.
The total saving is assumed to be at least (d ‚àí1)mH, thus summing the
savings form vertex- and edge-pages we obtain
n + (d ‚àí1)mH ‚àí
X
B‚ààB\{I}
Œ¥B ‚â•(d ‚àí1)mH.
The claim of the lemma now follows.
‚äì‚äî
The second lemma states that the number of slots occupied by pages asso-
ciated with a given edge does not change much during the whole service.
Lemma 3.5 For each edge e ‚ààE,
X
B‚ààB
Œ≥e
B ‚â§6n.
Proof Let us use the notation S‚â§k
B
= Se1
B ‚à™¬∑ ¬∑ ¬∑ ‚à™Sek
B and s‚â§k
B
=
S‚â§k
B
. First,
we shall prove for each k ‚â§m
X
B‚ààB
s‚â§k
B‚Ä≤ ‚àís‚â§k
B
 ‚â§3n.
(2)

--- Page 12 ---
12
Luk√°¬≤ Folwarczn√Ω, Ji¬∞√≠ Sgall
Let P denote the set of all blocks B from B satisfying s‚â§k
B‚Ä≤ ‚àís‚â§k
B
‚â•0 and
let N denote the set of all the remaining blocks from B.
As a consequence of the property (c), we get s‚â§k
I‚Ä≤
‚àà[kH ‚àíŒ¥I‚Ä≤, kH] and
s‚â§k
F
‚àà[kH ‚àíŒ¥F , kH]. So we obtain the inequality
s‚â§k
F
‚àís‚â§k
I‚Ä≤ ‚â•‚àíŒ¥F .
(3)
We claim s‚â§k
B‚Ä≤ ‚àís‚â§k
B
‚â§Œ¥B for each B ‚ààB. We assume for a contra-
diction s‚â§k
B‚Ä≤ ‚àís‚â§k
B
> Œ¥B for some block B. We use the property (d). Then
after processing the edge ek in B, the number of edge-pages in the cache is
(sB ‚àís‚â§k
B ) + s‚â§k
B‚Ä≤ > sB + Œ¥B = mH. But more than mH edge-pages in the
cache means a contradiction.
The summation over all blocks from P and Lemma 3.4 give us the rst
bound
X
B‚ààP

s‚â§k
B‚Ä≤ ‚àís‚â§k
B

‚â§
X
B‚ààP
Œ¥B ‚â§n.
(4)
Using the fact P Àô‚à™N = B and (3), we have
X
B‚ààP

s‚â§k
B‚Ä≤ ‚àís‚â§k
B

+
X
B‚ààN

s‚â§k
B‚Ä≤ ‚àís‚â§k
B

= s‚â§k
F
‚àís‚â§k
I‚Ä≤ ‚â•‚àíŒ¥F ;
together with (4) and Œ¥F ‚â§n which follows by Lemma 3.4, we obtain the
second bound
‚àí
X
B‚ààN

s‚â§k
B‚Ä≤ ‚àís‚â§k
B

‚â§
X
B‚ààP

s‚â§k
B‚Ä≤ ‚àís‚â§k
B

+ Œ¥F ‚â§2n.
(5)
Combining the bounds (4) and (5), we prove (2)
X
B‚ààB
s‚â§k
B‚Ä≤ ‚àís‚â§k
B
 =
X
B‚ààP

s‚â§k
B‚Ä≤ ‚àís‚â§k
B

‚àí
X
B‚ààN

s‚â§k
B‚Ä≤ ‚àís‚â§k
B

‚â§n + 2n = 3n.
For the edge e1, the claim of this lemma is weaker than (2) because Œ≥e1
B =
|se1
B‚Ä≤ ‚àíse1
B |. Proving our lemma for ek when k > 1 is just a matter of using (2)
for k ‚àí1 and k together with the formula |x ‚àíy| ‚â§|x| + |y|:
X
B‚ààB
Œ≥ek
B ‚â§
X
B‚ààB
s‚â§k
B‚Ä≤ ‚àís‚â§k
B
+
X
B‚ààB
s‚â§k‚àí1
B‚Ä≤
‚àís‚â§k‚àí1
B
 ‚â§3n+3n = 6n
‚äì‚äî
For the rest of the proof, we set H = 6mn + 3n + 1. This enables us to
show that the xed service must cache some of the pages of size three.
Lemma 3.6 For each edge e ‚ààE, there is a block B such that some Œ±e-page
or Œ≤e-page is in SB and Œ¥B = 0.

--- Page 13 ---
General Caching Is Hard: Even with Small Pages
13
Proof Fix an edge e = ek. For each block B, we dene
ŒµB = number of Œ±e-pages and Œ≤e-pages in SB.
Observe that due to the property (f), ŒµB is always one or zero. We use
a potential function
Œ¶B = number of ae-pages and ¬Øbe-pages in SB.
Because there are only ¬Øa-pages in the initial block and only ¬Øb-pages in the
nal block (property (c)), we know
Œ¶I‚Ä≤ = 0
and
Œ¶F ‚â•H ‚àíŒ¥F .
(6)
Now we bound the increase of the potential function as
Œ¶B‚Ä≤ ‚àíŒ¶B ‚â§Œ¥B +
k‚àí1
X
‚Ñì=1
Œ≥e‚Ñì
B + ŒµB.
(7)
To justify this bound, we x a block B and look at the cache state after
requests on edges e1, . . . , ek‚àí1 are processed. How many free slots there can be
in the cache? There are initial Œ¥B free slots in the beginning of the block B,
and the number of free slots can be further increased when the number of
pages in the cache associated with e1, . . . , ek‚àí1 decreases. This increase can
be naturally bounded by Pk‚àí1
‚Ñì=1 Œ≥e‚Ñì
B . Therefore, the number of free slots in the
cache is at most Œ¥B + Pk‚àí1
‚Ñì=1 Œ≥e‚Ñì
B .
Because of the property (e), the number of cached ae-pages and ¬Øbe-pages
can only increase by using the free cache space or caching new pages instead
of Œ±e-pages and Œ≤e-pages. We already bounded the number of free slots and
ŒµB is a natural bound for the increase gained on Œ±e-pages and Œ≤e-pages. Thus,
the bound (7) is correct.
Summing (7) over all B ‚ààB, we have
Œ¶F ‚àíŒ¶I‚Ä≤ =
X
B‚ààB
(Œ¶B‚Ä≤ ‚àíŒ¶B) ‚â§
X
B‚ààB

Œ¥B +
k‚àí1
X
‚Ñì=1
Œ≥e‚Ñì
B + ŒµB

which we combine with (6) into
H ‚àíŒ¥F ‚â§
X
B‚ààB

Œ¥B +
k‚àí1
X
‚Ñì=1
Œ≥e‚Ñì
B + ŒµB

,
and use Lemmata 3.4 and 3.5 to bound P ŒµB as
X
B‚ààB
ŒµB ‚â•H ‚àíŒ¥F ‚àí
X
B‚ààB

Œ¥B +
k‚àí1
X
‚Ñì=1
Œ≥e‚Ñì
B

‚â•H ‚àín ‚àín ‚àí(k ‚àí1)6n
‚â•H ‚àí6mn ‚àí2n = n + 1.

--- Page 14 ---
14
Luk√°¬≤ Folwarczn√Ω, Ji¬∞√≠ Sgall
As there is at most one page of size three requested in each block (prop-
erty (f)), the inequality P ŒµB ‚â•n + 1 implies that there are at least n + 1
blocks where an Œ±e-page or a Œ≤e-page is cached. At most n blocks have Œ¥B
non-zero (Lemma 3.4); we are done.
‚äì‚äî
We are ready to complete the proof of the harder direction.
Lemma 3.7 Suppose that there exists a service of TG with the total saving
of at least (d ‚àí1)mH + K. Then the graph G has an independent set W of
cardinality K.
Proof Let W be a set of vertices such that the corresponding page pv is cached
between its two requests. There are at least K vertices in W because the
maximal saving on edge-pages is (d ‚àí1)mH.
Consider an arbitrary edge e = {u, v}. Due to Lemma 3.6, there exists
a block B such that Œ¥B = 0 and some Œ±e-page or Œ≤e-page is cached in the
beginning of the block. Thus the cache of size 2mH + 1 is full, as it contains
mH edge-pages including one page of size three. This block B is either in the
u-phase or in the v-phase, because of the statement of the property (f). This
means that at least one of the two pages pu and pv is not cached between its
two requests, because the cache is full. As a consequence, the set W is indeed
independent.
‚äì‚äî
The value of H was set to 6mn + 3n + 1, therefore Reduction 2.1 is in-
deed polynomial. Lemmata 3.1, 3.3 and 3.7 together imply that there is an
independent set of cardinality K in G if and only if there is a service of the
instance IG with the total saving at least (d ‚àí1)mH + K. We showed that
the problem 3Caching(fault,optional) is indeed strongly NP-hard.
4 Bit Model
In this section, we show how to modify the proof for the fault model from the
previous sections so that it works as a proof for the bit model as well.
Reduction 4.1 Let G be a graph and IG the corresponding instance of the
problem 3Caching(fault,optional) from Reduction 2.1. Then the modied
instance eIG is an instance of 3Caching(bit,optional) with the same cache
size and the same set of pages with the same sizes.
The structure of phases and requests on vertex-pages is also preserved.
The blocks from IG are also used, but between each pair of consecutive blocks
there are ve new blocks inserted. Let B and B‚Ä≤ be two consecutive blocks.
Between B and B‚Ä≤ we insert ve new blocks B(1), . . . , B(5) with the following
requests
 B(1): do not request anything;
 B(2): request all pages of size two that are requested both in B and B‚Ä≤;
 B(3): request a page (there is either one or none) of size three that is
requested both in B and B‚Ä≤;

--- Page 15 ---
General Caching Is Hard: Even with Small Pages
15
 B(4): request all pages of size two that are requested both in B and B‚Ä≤;
 B(5): do not request anything.
See Fig. 5 for an example. In each new block, the order of chosen requests is
the same as in B (which is the same as in B‚Ä≤, as both follow the same ordering
of edges (1)). The new instance has the total of Àúd = d + 5(d ‚àí1) = 6d ‚àí5
blocks. This time we prove that the maximal total saving is ( Àúd ‚àí1)mH + K
where K is the cardinality of the maximum independent set in G.
a
B(1) B(2)
Œ≤
B
B(3) B(4) B(5) B‚Ä≤
a
B‚Ä≤
Œ≤
B
Fig. 5 The modication of the instance for a page a of size two and a page Œ≤ of size three
Lemma 4.2 Suppose that the graph G has an independent set W of cardinal-
ity K. Then there exists a service of the modied instance eIG with the total
saving at least ( Àúd ‚àí1)mH + K.
Proof We consider the service of the original instance IG described in the proof
of Lemma 3.1 and modify it so it becomes a service of the modied instance.
In the new service, vertex-pages are served the same way as in the original
service. The saving on vertex-pages is thus again K.
For each pair of consecutive blocks B and B‚Ä≤, each page kept in the cache
between B and B‚Ä≤ in the original service is kept in the cache in the new service
for the whole time between B and B‚Ä≤ (it spans over seven blocks now). For
a page of size two, a saving two is achieved three times. For a page of size three,
a saving three is incurred twice. On each page in the new service we save six
instead of one. Therefore, the total saving on edge-pages is 6(d ‚àí1)mH =
( Àúd ‚àí1)mH.
The total saving is ( Àúd ‚àí1)mH + K.
‚äì‚äî
Lemma 4.3 Suppose that there exists a service of the modied instance eIG
with the total saving at least ( Àúd ‚àí1)mH + K. Then the graph G has an inde-
pendent set W of cardinality K.
Proof This lemma is the same as Lemma 3.7. We just need to verify that the
modied instance fullls Properties 3.2.
To prove that the property (b) is preserved, we observe that each two
consecutive requests on a page of size two are separated by exactly one block
where the page is not requested. Consequently, when there is a saving two
on a request on the page of size two, we assign one unit of the saving to the
block where the saving was achieved and one unit of the saving to the previous
block. Similarly, each pair of consecutive requests on a page of size three is
separated by exactly two blocks where the page is not requested. When there

--- Page 16 ---
16
Luk√°¬≤ Folwarczn√Ω, Ji¬∞√≠ Sgall
is a saving three on a request on the page of size three, we assign one unit
of the saving to the block where the saving was achieved and one unit of the
saving to each of the two previous blocks. As a consequence, the total saving
gained on edge-pages may indeed be computed as P sB.
The property (a) is preserved because the requests on vertex-pages are the
same in both instances. The property (c) is preserved because the initial and
nal blocks are the same in both instances.
Each sequence of requests in a block of the modied instance eIG is either
the same or a subsequence of the sequence in a block in the original instance.
Therefore, the properties (d), (e) and (f) are preserved as well.
‚äì‚äî
Lemmata 4.2 and 4.3 imply that we have a valid polynomial-time reduction
and so the problem 3Caching(bit,optional) is strongly NP-hard.
5 Forced Policy
Theorem 5.1 Both the problem 3Caching(fault,forced) and the problem
3Caching(bit,forced) are strongly NP-hard.
Proof For both the fault model and the bit model, we show a polynomial-time
reduction from caching under optional policy to the corresponding variant of
caching with under forced policy. Let us have an instance of caching under the
optional policy with the cache size C and the request sequence œÅ = r1 . . . rn;
let M be the maximal size of a page in œÅ (in our previous reductions, M = 3).
We create an instance of caching under the forced policy. The cache size is
C‚Ä≤ = C + M. The request sequence is œÅ‚Ä≤ = r1q1r2q2 . . . rnqn where q1, . . . , qn
are requests to n dierent pages that do not appear in œÅ and have size M. The
costs of the new pages are one in the fault model and M in the bit model.
We claim that there is a service of the optional instance with saving S if
and only if there is a service of the forced instance with saving S.
‚áíWe serve the requests on original pages the same way as in the optional
instance. The cache is larger by M which is the size of the largest page. Thus,
pages that were not loaded into the cache because of the optional policy t in
there; we can load them and immediately evict them. New pages t into the
cache as well and we also load them and immediately evict them. This way we
have the same saving as in the optional instance.
‚áêWe construct a service for the optional instance: For each i, when serving
ri we consider the evictions done when serving ri and qi of the forced instance.
If a page requested before ri is evicted, we evict it as well. If a page requested
by ri is evicted, we do not cache it at all. Because the page requested by qi
has size M, the original pages occupy at most C slots in the cache after qi is
served. This way we obtain a service of the optional instance with the same
saving.
Using the strong NP-hardness of the problems 3Caching(fault,optional)
and 3Caching(bit,optional) proven in Sections 2 and 3 and the observation
that the reduction preserves the maximal size of a page, we obtain the strong

--- Page 17 ---
General Caching Is Hard: Even with Small Pages
17
NP-hardness of 3Caching(fault,forced) and 3Caching(bit,forced).
‚äì‚äî
Acknowledgments. We are grateful to the anonymous reviewers, in partic-
ular for bringing the articles [10,16] to our attention and for suggestions that
helped us to improve the presentation.
References
1. Achlioptas, D., Chrobak, M., Noga, J.: Competitive analysis of randomized paging al-
gorithms.
Theoretical Computer Science 234(1-2), 203218 (2000).
DOI 10.1016/
S0304-3975(98)00116-9.
URL http://dx.doi.org/10.1016/S0304-3975(98)00116-9.
A preliminary version appeared at ESA 1996.
2. Adamaszek, A., Czumaj, A., Englert, M., R√§cke, H.: An O(log k)-competitive algorithm
for generalized caching. In: Proceedings of the 23rd Annual ACM-SIAM Symposium
on Discrete Algorithms (SODA), pp. 16811689 (2012). DOI 10.1137/1.9781611973099.
URL http://dl.acm.org/citation.cfm?id=2095116.2095249
3. Albers, S., Arora, S., Khanna, S.: Page replacement for general caching problems. In:
Proceedings of the 10th Annual ACM-SIAM Symposium on Discrete Algorithms, pp.
3140 (1999). URL http://dl.acm.org/citation.cfm?id=314500.314528
4. Anagnostopoulos, A., Grandoni, F., Leonardi, S., Wiese, A.: A mazing 2+Œµ approxima-
tion for unsplittable ow on a path. In: C. Chekuri (ed.) Proceedings of the Twenty-Fifth
Annual ACM-SIAM Symposium on Discrete Algorithms, SODA 2014, Portland, Ore-
gon, USA, January 5-7, 2014, pp. 2641. SIAM (2014). DOI 10.1137/1.9781611973402.3.
URL http://dx.doi.org/10.1137/1.9781611973402.3
5. Bansal, N., Buchbinder, N., Naor, J.: Randomized competitive algorithms for gener-
alized caching.
SIAM Journal on Computing 41(2), 391414 (2012).
DOI 10.1137/
090779000. URL http://dx.doi.org/10.1137/090779000. A preliminary version ap-
peared at STOC 2008.
6. Bansal, N., Chakrabarti, A., Epstein, A., Schieber, B.: A quasi-PTAS for unsplittable
ow on line graphs.
In: J.M. Kleinberg (ed.) Proceedings of the 38th Annual ACM
Symposium on Theory of Computing, pp. 721729. ACM (2006). DOI 10.1145/1132516.
1132617. URL http://doi.acm.org/10.1145/1132516.1132617
7. Bar-Noy, A., Bar-Yehuda, R., Freund, A., Naor, J., Schieber, B.: A unied approach to
approximating resource allocation and scheduling. Journal of the ACM 48(5), 1069
1090 (2001). DOI 10.1145/502102.502107. URL http://doi.acm.org/10.1145/502102.
502107. A preliminary version appeared at STOC 2000.
8. Batra, J., Garg, N., Kumar, A., M√∂mke, T., Wiese, A.: New approximation schemes for
unsplittable ow on a path. In: P. Indyk (ed.) Proceedings of the 26th Annual ACM-
SIAM Symposium on Discrete Algorithms, pp. 4758. SIAM (2015). DOI 10.1137/1.
9781611973730.5. URL http://dx.doi.org/10.1137/1.9781611973730.5
9. Belady, L.A.: A study of replacement algorithms for a virtual-storage computer. IBM
Systems Journal 5(2), 78101 (1966). DOI 10.1147/sj.52.0078. URL http://dx.doi.
org/10.1147/sj.52.0078
10. Bonsma, P.S., Schulz, J., Wiese, A.: A constant-factor approximation algorithm for
unsplittable ow on paths. SIAM Journal on Computing 43(2), 767799 (2014). DOI
10.1137/120868360. URL http://dx.doi.org/10.1137/120868360
11. Borodin, A., El-Yaniv, R.: Online computation and competitive analysis. Cambridge
University Press (1998)
12. Brodal, G.S., Moruz, G., Negoescu, A.: Onlinemin: A fast strongly competitive ran-
domized paging algorithm. Theory of Computing Systems 56(1), 2240 (2015). DOI
10.1007/s00224-012-9427-y.
URL http://dx.doi.org/10.1007/s00224-012-9427-y.
A preliminary version appeared at WAOA 2011.

--- Page 18 ---
18
Luk√°¬≤ Folwarczn√Ω, Ji¬∞√≠ Sgall
13. Chrobak, M., Karlo, H.J., Payne, T.H., Vishwanathan, S.: New results on server
problems.
SIAM Journal on Discrete Mathematics 4(2), 172181 (1991).
DOI
10.1137/0404017. URL http://dx.doi.org/10.1137/0404017. A preliminary version
appeared at SODA 1990.
14. Chrobak, M., Larmore, L.L., Lund, C., Reingold, N.: A better lower bound on the
competitive ratio of the randomized 2-server problem. Information Processing Letters
63(2), 7983 (1997). DOI 10.1016/S0020-0190(97)00099-9. URL http://dx.doi.org/
10.1016/S0020-0190(97)00099-9
15. Chrobak, M., Woeginger, G.J., Makino, K., Xu, H.: Caching is hard  Even in the fault
model. Algorithmica 63(4), 781794 (2012). DOI 10.1007/s00453-011-9502-9. URL
http://dx.doi.org/10.1007/s00453-011-9502-9. A preliminary version appeared at
ESA 2010.
16. Darmann, A., Pferschy, U., Schauer, J.: Resource allocation with time intervals. The-
oretical Computer Science 411(49), 42174234 (2010). DOI 10.1016/j.tcs.2010.08.028.
URL http://dx.doi.org/10.1016/j.tcs.2010.08.028
17. Fiat, A., Karp, R.M., Luby, M., McGeoch, L.A., Sleator, D.D., Young, N.E.: Compet-
itive paging algorithms. Journal of Algorithms 12(4), 685699 (1991). DOI 10.1016/
0196-6774(91)90041-V.
URL http://dx.doi.org/10.1016/0196-6774(91)90041-V.
A preliminary version appeared in 1988.
18. Folwarczn√Ω, L., Sgall, J.: General caching is hard: Even with small pages.
In: K.M.
Elbassioni, K. Makino (eds.) Algorithms and Computation - 26th International Sym-
posium, ISAAC 2015, Nagoya, Japan, December 9-11, 2015, Proceedings, Lecture
Notes in Computer Science, vol. 9472, pp. 116126. Springer (2015).
DOI 10.1007/
978-3-662-48971-0_11. URL http://dx.doi.org/10.1007/978-3-662-48971-0_11
19. Irani, S.: Page replacement with multi-size pages and applications to web caching. Al-
gorithmica 33(3), 384409 (2002). DOI 10.1007/s00453-001-0125-4. URL http://dx.
doi.org/10.1007/s00453-001-0125-4. A preliminary version appeared at STOC 1997.
20. McGeoch, L.A., Sleator, D.D.: A strongly competitive randomized paging algorithm.
Algorithmica 6(6), 816825 (1991). DOI 10.1007/BF01759073. URL http://dx.doi.
org/10.1007/BF01759073. A preliminary version appeared in 1989.
21. Young, N.E.: On-line le caching.
Algorithmica 33(3), 371383 (2002).
DOI
10.1007/s00453-001-0124-5.
URL http://dx.doi.org/10.1007/s00453-001-0124-5.
A preliminary version appeared at SODA 1998.

--- Page 19 ---
General Caching Is Hard: Even with Small Pages
19
A The Simple Proof
In this appendix, we present a simple variant of the proof for the almost-fault model with
two distinct costs. This completes the sketch of the proof presented at the end of Section 2.
We present it with a complete description of the simplied reduction, so that it can be read
independently of the rest of the paper. This appendix can therefore serve as a short proof
of the hardness of general caching.
Theorem A.1 General caching is strongly NP-hard, even in the case when page sizes are
limited to {1, 2, 3} and there are only two distinct fault costs.
We prove the theorem for the optional policy. It is easy to obtain the theorem also for
the forced policy the same way as in the proof of Theorem 5.1.
The Reduction
The reduction described here will be equivalent to Reduction 2.1 with H = 1 and the fault
cost of each vertex-page set to 1/(n + 1).
Suppose we have a graph G = (V, E) with n nodes and m edges. We construct an
instance of general caching whose optimal solution encodes a maximum independent set in
G. Fix an arbitrary numbering of edges e1, . . . , em.
The cache size is C = 2m + 1. For each vertex v, we have a vertex-page pv with size one
and cost 1/(n + 1). For each edge e, we have six associated edge-pages ae, ¬Øae, Œ±e, be,¬Øbe, Œ≤e;
all have cost one, pages Œ±e, Œ≤e have size three and the remaining pages have size two.
The request sequence is organized in phases and blocks. There is one phase for each
vertex. In each phase, there are two adjacent blocks associated with every edge e incident
with v; the incident edges are processed in an arbitrary order. In addition, there is one
initial block I before all phases and one nal block F after all phases. Altogether, there are
d = 4m + 2 blocks. There are four blocks associated with each edge e; denote them Be
1, Be
2,
Be
3, Be
4, in the order as they appear in the request sequence.
For each v ‚ààV , the associated page pv is requested exactly twice, right before the
beginning of the v-phase and right after the end of the v-phase; these requests do not belong
to any phase. An example of the structure of phases and blocks is given in Fig. 6.
Be1
1
Be1
2
u-phase
I
pv
pu
pw
Be2
1
Be2
2
v-phase
Be1
3
Be1
4
Be2
3
Be2
4
w-phase
F
Fig. 6 An example of phases, blocks and requests on vertex-pages for a graph with three
vertices u, v, w and two edges e1 = {u, w}, e2 = {v, w} when H = 2
Even though each block is associated with some xed edge, it contains one or more re-
quests to the associated pages for every edge e. In each block, we process the edges e1, . . . , em
in this order. For each edge e, we make one or more requests to the associated pages according
to Table 3.
Figure 7 shows an example of the requests on edge-pages associated with one particular
edge.
Proof of Correctness
Instead of minimizing the service cost, we maximize the saving compared to the service
which does not use the cache at all. This is clearly equivalent when considering the decision
version of the problem.

--- Page 20 ---
20
Luk√°¬≤ Folwarczn√Ω, Ji¬∞√≠ Sgall
Table 3 Requests associated with an edge e
Block
Requested pages
before Be
1
¬Øae
Be
1
¬Øae, Œ±e, be
Be
2
Œ±e, ae, be
between Be
2 and Be
3
ae, be
Be
3
ae, be, Œ≤e
Be
4
ae, Œ≤e, ¬Øbe
after Be
4
¬Øbe
¬Øae
Be
1
Be
2
Be
3
Be
4
Œ±e
ae
be
Œ≤e
¬Øbe
Fig. 7 Requests on all pages associated with the edge e. Each column represents some
block(s). The four labeled columns represent the blocks in the heading, the rst column
represents every block before Be
1, the middle column represents every block between Be
3 and
Be
4, and the last column represents every block after Be
4. The requests in one column are
ordered from top to bottom.
Without loss of generality, we assume that any page is brought into the cache only
immediately before a request to that page and removed from the cache only immediately
after a request to that page; furthermore, at the beginning and at the end the cache is
empty. I.e., a page may be in the cache only between two consecutive requests to this page,
and either it is in the cache for the whole interval or not at all.
Each page of size three is requested only twice in two consecutive blocks, and these
blocks are distinct for all pages of size three. Thus, a service of edge-pages is valid if and
only if at each time, at most m edge-pages are in the cache. It is thus convenient to think
of the cache as of m slots for edge-pages.
Each vertex-page is requested twice. Thus, the saving on the n vertex-pages is at
most n/(n + 1) < 1. Since all edge-pages have cost one, the optimal service must serve
them optimally. Furthermore, a vertex-page can be cached if and only if during the phase
it never happens that at the same time all slots for edge-pages are full and a page of size
three is cached.
Let SB denote the set of all edge-pages cached at the beginning of the block B and
let sB = |SB|. Now observe that each edge-page is requested only in a contiguous segment
of blocks, once in each block. It follows that the total saving on edge-pages is equal to
P
B sB where the sum is over all blocks. In particular, the maximal possible saving on the
edge-pages is (d ‚àí1)m, using the fact that SI is empty.
We prove that there is a service with the total saving at least (d ‚àí1)m + K/(n + 1) if
and only if there is an independent set of size K in G. First the easy direction.

--- Page 21 ---
General Caching Is Hard: Even with Small Pages
21
¬Øae
Be
1
Be
2
Be
3
Be
4
Œ±e
ae
be
Œ≤e
¬Øbe
¬Øae
Be
1
Be
2
Be
3
Be
4
Œ±e
ae
be
Œ≤e
¬Øbe
Fig. 8 The two ways of caching in Lemma A.2
Lemma A.2 Suppose that G has an independent set W of size K. Then there exists a
service with the total saving (d ‚àí1)m + K/(n + 1).
Proof For any e, denote e = {u, v} so that u precedes v in the ordering of phases. If u ‚ààW,
we keep ¬Øae, be,¬Øbe and Œ≤e in the cache from the rst to the last request on each page, and
we do not cache ae and Œ±e at any time. Otherwise we cache ¬Øbe, ae, ¬Øae and Œ±e, and do not
cache be and Œ≤e at any time. In both cases, at each time at most one page associated with
e is in the cache and the saving on those pages is (d ‚àí1)m. See Fig. 8 for an illustration.
For any v ‚ààW, we cache pv between its two requests. To check that this is a valid
service, observe that if v ‚ààW, then during the corresponding phase no page of size three
is cached. Thus the page pv always ts in the cache together with at most m pages of size
two.
‚äì‚äî
Now we prove the converse in a sequence of claims. Fix a valid service with saving at
least (d ‚àí1)m. For a block B, let B‚Ä≤ denote the following block.
Claim A.3 For any block B, with the exception of B = I, we have sB = m.
Proof For each B Ã∏= I we have sB ‚â§m. Because sI = 0, the total saving on edge-pages is
P
B sB ‚â§(d ‚àí1)m. We need an equality.
‚äì‚äî
We now prove that each edge occupies exactly one slot during the service.
Claim A.4 For any block B Ã∏= I and for any e, SB contains exactly one page associated
with e.
Proof Let us use the notation S‚â§k
B
= Se1
B ‚à™¬∑ ¬∑ ¬∑‚à™Sek
B and s‚â§k
B
=
S‚â§k
B
. First, we shall prove
for each k ‚â§m
s‚â§k
B
= k.
(8)
This is true for B = F, as only the m edge-pages ¬Øbe can be cached there, and by
the previous claim all of them are indeed cached. Similarly for B = I‚Ä≤ (i.e., immediately
following the initial block).
If (8) is not true, then for some k and B Ã∏‚àà{I, F} we have s‚â§k
B
< s‚â§k
B‚Ä≤ . Then after
processing the edge ek in the block B we have in the cache all the pages in (SB \S‚â§k
B )‚à™S‚â§k
B‚Ä≤ .
Their number is (m ‚àís‚â§k
B ) + s‚â§k
B‚Ä≤ > m, a contradiction.
The statement of the claim is an immediate consequence of (8).
‚äì‚äî
Claim A.5 For any edge e, at least one of the pages Œ±e and Œ≤e is cached between its two
requests.

--- Page 22 ---
22
Luk√°¬≤ Folwarczn√Ω, Ji¬∞√≠ Sgall
Proof Assume that none of the two pages is cached. It follows from the previous claim that
be ‚ààSBe
2 , as at this point Œ±e and be are the only pages associated with e that can be cached.
Similarly, ae ‚ààSBe
4 .
It follows that there exists a block B between Be
1 and Be
4 such that SB contains the page
be and SB‚Ä≤ contains the page ae. However, in B, the page ae is requested before the page be.
Thus at the point between the two requests, the cache contains two pages associated with e,
plus one page associated with every other edge, the total of m+1 pages, a contradiction.
‚äì‚äî
Now we are ready to complete this direction.
Lemma A.6 Suppose that there exists a valid service with the total saving (d ‚àí1)m +
K/(n + 1). Then G has an independent set W of size K.
Proof Let W be the set of all v such that pv is cached between its two requests. The total
saving implies that |W| = K.
Now we claim that W is independent. Suppose not, let e = uv be an edge with u, v ‚ààW.
Then pu and pv are cached in the corresponding phases. Thus neither Œ±e nor Œ≤e can be
cached, since together with other m ‚àí1 requests of size 2 associated with the remaining
edges, the cache size needed would be 2m + 2. However, this contradicts the last claim.
‚äì‚äî
Lemmata A.2 and A.6 together show that we constructed a valid polynomial-time re-
duction from the problem of independent set to general caching. Therefore, Theorem A.1 is
proven.
