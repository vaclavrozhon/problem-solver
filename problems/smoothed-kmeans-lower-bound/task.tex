%\newcommand{\X}{X}
%\newcommand{\R}{\mathbb{R}}
%\newcommand{\cost}{\phi}
%\newcommand{\OPT}{\mathrm{OPT}}
%\newcommand{\ALG}{\mathrm{ALG}}
%\newcommand{\E}{\mathbb{E}}

\section*{Algorithm, model, and task}

\paragraph{Objective.}
Fix a dataset $\X=\{x_1,\dots,x_n\}\subseteq\R^d$ and the standard $k$-means cost
\begin{equation*}
\cost(C) \;=\; \sum_{x\in \X} \min_{c\in C} \|x-c\|_2^2
\qquad\text{for a center set } C\subseteq\R^d,\ |C|=k.
\end{equation*}
Let $\OPT(k)=\min_{|C|=k}\cost(C)$ and let $\ALG(k)$ denote the cost of the clustering returned by the $k$-means++ seeding.

\paragraph{$k$-means++ seeding (Arthur--Vassilvitskii).}
Pick the first center uniformly from $\X$. Then, iteratively, sample the next center $c$ from $\X$ with probability proportional to $D(x)^2$, where $D(x)$ is the distance from $x$ to its closest previously chosen center. Stop after $k$ centers are chosen; assign each $x$ to its nearest chosen center.

\paragraph{Smoothed random-$k$ experiment.}
Given an integer $k_0\ge 1$, draw $k$ uniformly at random from $\{k_0,k_0{+}1,\dots,2k_0{-}1\}$, run $k$-means++ with this $k$ on $\X$, and set the random variable
\begin{equation*}
\alpha \;=\; \frac{\ALG(k)}{\OPT(k)} \;\ge\; 1.
\end{equation*}

\section*{The task}
Prove the strongest possible asymptotic lower bound on
\begin{equation*}
\E_{k\sim U([k_0,2k_0)),\,\text{$k$-means++}}\!\!\Big[\; \tfrac{\ALG(k)}{\OPT(k)} \;\Big],
\end{equation*}
as a function of $k_0$. 

Even a relatively small improvement over $O(1)$, e.g., $\Omega(\log\log k)$ is considered as a success. 

\section*{Context and prior results}

For standard $k$-means++ with $k$ centers, $\E[\ALG(k)]\le O(\log k)\cdot \OPT(k)$ and there are instances with $\E[\ALG(k)]\ge \Omega(\log k)\cdot \OPT(k)$.

It is unknown whether their lower bound construction can be successfully adapted to the setup of this task. 