
--- Page 1 ---
arXiv:2010.14487v1  [cs.LG]  27 Oct 2020
Improved Guarantees for k-means++ and k-means++ Parallel
Konstantin Makarychev, Aravind Reddy, and Liren Shan
Department of Computer Science
Northwestern University
Evanston, IL, USA
Abstract
In this paper, we study k-means++ and k-means∥, the two most popular algorithms for the
classic k-means clustering problem. We provide novel analyses and show improved approxima-
tion and bi-criteria approximation guarantees for k-means++ and k-means∥. Our results give a
better theoretical justiﬁcation for why these algorithms perform extremely well in practice. We
also propose a new variant of k-means∥algorithm (Exponential Race k-means++) that has the
same approximation guarantees as k-means++.
1
Introduction
k-means clustering is one of the most commonly encountered unsupervised learning problems. Given
a set of n data points in Euclidean space, our goal is to partition them into k clusters (each
characterized by a center), such that the sum of squares of distances of data points to their nearest
centers is minimized. The most popular heuristic for solving this problem is Lloyd’s algorithm
(Lloyd, 1982), often referred to simply as “the k-means algorithm".
Lloyd’s algorithm uses iterative improvements to ﬁnd a locally optimal k-means clustering. The
performance of Lloyd’s algorithm crucially depends on the quality of the initial clustering, which
is deﬁned by the initial set of centers, called a seed. Arthur and Vassilvitskii (2007) and Ostrovsky,
Rabani, Schulman, and Swamy (2006) developed an elegant randomized seeding algorithm, known
as the k-means++ algorithm. It works by choosing the ﬁrst center uniformly at random from the
data set and then choosing the subsequent k−1 centers by randomly sampling a single point in each
round with the sampling probability of every point proportional to its current cost. That is, the
probability of choosing any data point x is proportional to the squared distance to its closest already
chosen center. This squared distance is often denoted by D2(x). Arthur and Vassilvitskii (2007)
proved that the expected cost of the initial clustering obtained by k-means++ is at most 8 (ln k + 2)
times the cost of the optimal clustering i.e., k-means++ gives an 8 (ln k + 2)-approximation for the
k-means problem. They also provided a family of k-means instances for which the approximation
factor of k-means++ is 2 ln k and thus showed that their analysis of k-means++ is almost tight.
Due to its speed, simplicity, and good empirical performance, k-means++ is the most widely
used algorithm for k-means clustering. It is employed by such machine learning libraries as Apache
The conference version of this paper will appear in the proceedings of the 34th Conference on Neural Information
Processing Systems (NeurIPS 2020). Author order is alphabetical.
1

--- Page 2 ---
Spark MLlib, Google BigQuery, IBM SPSS, Intel DAAL, and Microsoft ML.NET. In addition
to k-means++, these libraries implement a scalable variant of k-means++ called k-means∥(read
“k-means parallel”) designed by Bahmani, Moseley, Vattani, Kumar, and Vassilvitskii (2012). Some-
what surprisingly, k-means∥not only works better in parallel than k-means++ but also slightly
outperforms k-means++ in practice in the single machine setting (see Bahmani et al. (2012) and
Figure 1 below). However, theoretical guarantees for k-means∥are substantially weaker than for
k-means++.
The k-means∥algorithm makes T passes over the data set (usually T = 5). In every round,
it independently draws approximately ℓ= Θ(k) random centers according to the D2 distribution.
After each round it recomputes the distances to the closest chosen centers and updates D2(x) for
all x in the data set. Thus, after T rounds, k-means∥chooses approximately Tℓcenters. It then
selects k centers among Tℓcenters using k-means++.
Our contributions.
In this paper, we improve the theoretical guarantees for k-means++,
k-means∥, and Bi-Criteria k-means++ (which we deﬁne below).
First, we show that the expected cost of the solution output by k-means++ is at most 5(ln k+2)
times the optimal solution’s cost. This improves upon the bound of 8(ln k+2) shown by Arthur and
Vassilvitskii (2007) and directly improves the approximation factors for several algorithms which
use k-means++ as a subroutine like Local Search k-means++ (Lattanzi and Sohler, 2019). To
obtain this result, we give a reﬁned analysis of the expected cost of covered clusters (see Lemma
3.2 in Arthur and Vassilvitskii (2007) and Lemma 4.1 in this paper). We also show that our new
bound on the expected cost of covered clusters is tight (see Lemma C.1).
Then, we address the question of why the observed performance of k-means∥is better than the
performance of k-means++. There are two possible explanations for this fact. (1) This may be the
case because k-means∥picks k centers in two stages. At the ﬁrst stage, it samples ℓT ≥k centers.
At the second stage, it prunes centers and chooses k centers among ℓT centers using k-means++.
(2) This may also be the case because k-means∥updates the distribution function D2(x) once
in every round. That is, it recomputes D2(x) once for every ℓchosen centers, while k-means++
recomputes D2(x) every time it chooses a center. In this paper, we empirically demonstrate that
the ﬁrst explanation is correct. First, we noticed that k-means∥for ℓ· T = k is almost identical
with k-means++ (see Appendix A). Second, we compare k-means∥with another algorithm which
we call Bi-Criteria k-means++ with Pruning. This algorithm also works in two stages: At the
Bi-Criteria k-means++ stage, it chooses k + ∆centers in the data set using k-means++. Then, at
the Pruning stage, it picks k centers among the k +∆centers selected at the ﬁrst stage again using
k-means++. Our experiments on the standard data sets BioTest from KDD-Cup 2004 (Elber, 2004)
and COVTYPE from the UCI ML repository (Dua and Graﬀ, 2017) show that the performance
of k-means∥and Bi-Criteria k-means++ with Pruning are essentially identical (see Figures 1 and
Appendix A).
These results lead to another interesting question: How good are k-means++ and k-means∥
algorithms that sample k+∆instead of k centers? The idea of oversampling using k-means++ was
studied earlier in the literature under the name of bi-criteria approximation. Aggarwal, Deshpande,
and Kannan (2009) showed that with constant probability, sampling k + ∆centers by k-means++
provides a constant-factor approximation if ∆≥βk for some constant β > 0. Wei (2016) improved
on this result by showing an expected approximation ratio of 8(1 + 1.618k/∆). Note that for bi-
criteria algorithms we compare the expected cost of the clustering with k +∆centers they produce
and the cost of the optimal clustering with exactly k centers.
2

--- Page 3 ---
10
20
30
40
50
3
4
5
6
#centers
cost
(a) BioTest
k-means++
BiCriteria k-means++ w/Pruning
k-means∥
10
20
30
40
50
10
15
20
25
30
#centers
cost
(b) COVTYPE
k-means++
BiCriteria k-means++ w/Pruning
k-means∥
Figure 1: Performance of k-means++, k-means∥, and Bi-Criteria k-means++ with pruning on the
BioTest and COVTYPE datasets. For k = 10, 15, · · · , 50, we ran these algorithms for 50 iterations
and took their average. We normalized the clustering costs by dividing them by cost1000(X).
In this paper, we show that the expected bi-criteria approximation ratio for k-means++ with
∆additional centers is at most the minimum of two bounds:
(A) 5

2 + 1
2e + ln 2k
∆

for 1 ≤∆≤2k; and (B) 5

1 +
k
e (∆−1)

for ∆≥1
Both bounds are better than the bound by Wei (2016). The improvement is especially noticeable
for small values of ∆. More speciﬁcally, when the number of additional centers is ∆= k/ log k, our
approximation guarantee is O(log log k) while Wei (2016) gives an O(log k) approximation.
We believe that our results for small values of ∆provide an additional explanation for why
k-means++ works so well in practice. Consider a data scientist who wants to cluster a data set
X with k∗true clusters (i.e. k∗latent groups). Since she does not know the actual value of k∗,
she uses the elbow method (Boehmke and Greenwell, 2019) or some other heuristic to ﬁnd k. Our
results indicate that if she chooses slightly more number of clusters (for instance, 1.05k∗), then she
will get a constant bi-criteria approximation to the optimal clustering.
We also note that our bounds on the approximation factor smoothly transition from the regular
(∆= 0) to bi-criteria (∆> 0) regime. We complement our analysis with an almost matching lower
bound of Θ(log(k/∆)) on the approximation factor of k-means for ∆≤k (see Appendix C).
We then analyze Bi-Criteria k-means∥algorithm, the variant of k-means∥that does not prune
centers at the second stage.
In their original paper, Bahmani, Moseley, Vattani, Kumar, and
Vassilvitskii (2012) showed that the expected cost of the solution for k-means∥with T rounds and
oversampling parameter ℓis at most:
16
1 −αOPTk(X) +
1 + α
2
T
OPT1(X),
where α = exp

−

1 −e−ℓ/(2k)
; OPTk(X) is the cost of the optimal k-means clustering of X;
OPT1(X) is the cost of the optimal clustering of X with 1 center (see Section 2 for details). We
note that OPT1(X) ≫OPTk(X).
For ℓ= k, this result gives a bound of ≈49 OPTk(X) +
0.83T OPT1(X).
Bachem, Lucic, and Krause (2017) improved the approximation guarantee for
3

--- Page 4 ---
ℓ≥k to
26OPTk(X) + 2
 k
eℓ
T
OPT1(X).
In this work, we improve this bound for ℓ≥k and also obtain a better bound for ℓ< k. For ℓ≥k,
we show that the cost of k-means∥without pruning is at most
8OPTk(X) + 2
 k
eℓ
T
OPT1(X).
For ℓ< k, we give a bound of
5
1 −e−ℓ
k
OPTk(X) + 2

e−ℓ
k
T
OPT1(X)
Finally, we give a new parallel variant of the k-means++ algorithm, which we call Exponential
Race k-means++ (k-means++ER). This algorithm is similar to k-means∥. In each round, it also
selects ℓcandidate centers in parallel (some of which may be dropped later) making one pass over
the data set. However, after T rounds, it returns exactly k centers. The probability distribution
of these centers is identical to the distribution of centers output by k-means++. The expected
number of rounds is bounded as follows:
O
k
ℓ+ log OPT1(X)
OPTk(X)

.
This algorithm oﬀers a unifying view on k-means++ and k-means∥. We describe it in Section 7.
Other related work. Dasgupta (2008) and Aloise, Deshpande, Hansen, and Popat (2009) showed
that k-means problem is NP-hard. Awasthi, Charikar, Krishnaswamy, and Sinop (2015) proved that
it is also NP-hard to approximate k-means objective within a factor of (1 + ε) for some constant
ε > 0 (see also Lee, Schmidt, and Wright (2017)). We also mention that k-means was studied not
only for Euclidean spaces but also for arbitrary metric spaces.
There are several known constant factor approximation algorithms for the k-means problem.
Kanungo, Mount, Netanyahu, Piatko, Silverman, and Wu (2004) gave a 9 + ε approximation local
search algorithm. Ahmadian, Norouzi-Fard, Svensson, and Ward (2019) proposed a primal-dual
algorithm with an approximation factor of 6.357.
This is the best known approximation for k-
means. Makarychev, Makarychev, Sviridenko, and Ward (2016) gave constant-factor bi-criteria
approximation algorithms based on linear programming and local search. Note that although these
algorithms run in polynomial time, they do not scale well to massive data sets.
Lattanzi and
Sohler (2019) provided a constant factor approximation by combining the local search idea with
the k-means++ algorithm. Choo, Grunau, Portmann, and Rozhoň (2020) further improved upon
this result by reducing the number of local search steps needed from O(k log log k) to O(k).
Independently and concurrently to our work, Rozhoň (2020) gave an interesting analysis for
k-means∥by viewing it as a balls into bins problem and showed that O(log n/ log log n) rounds
suﬃce to give a constant approximation with high probability.
Acknowledgments. We would like to thank all the reviewers for their helpful comments. Kon-
stantin Makarychev, Aravind Reddy, and Liren Shan were supported in part by NSF grants CCF-
1955351 and HDR TRIPODS CCF-1934931. Aravind Reddy was also supported in part by NSF
CCF-1637585.
4

--- Page 5 ---
2
Preliminaries
Given a set of points X = {x1, x2, · · · , xn} ⊆Rd and an integer k ≥1, the k-means clustering
problem is to ﬁnd a set C of k centers in Rd to minimize
cost(X, C) :=
X
x∈X
min
c∈C ∥x −c∥2.
For any integer i ≥1, let us deﬁne OPTi(X) := min|C|=i cost (X, C) . Thus, OPTk(X) refers to the
cost of the optimal solution for the k-means problem. Let C∗denote a set of optimal centers. We
use {Pi}k
i=1 to denote the clusters induced by the center set C∗.
For any Y ⊆X, the cost of Y with center set C, denoted by cost (Y, C) = P
x∈Y minc∈C∥x−c∥2.
The optimal cost for subset Y with i centers is OPTi(Y). Let µ = P
x∈Y x/ |Y| be the centroid of
the cluster Y. Then, we have the following closed form expression for the optimal cost of Y with
one center (see Appendix B for proof),
OPT1(Y) =
X
x∈Y
∥x −µ∥2 =
P
(x,y)∈Y×Y∥x −y∥2
2 |Y|
.
(1)
k-means++ seeding: The k-means++ algorithm samples the ﬁrst center uniformly at random
from the given points and then samples k −1 centers sequentially from the given points with
probability of each point being sampled proportional to its cost i.e. cost(x, C)/cost(X, C).
Algorithm 1 k-means++ seeding
1: Sample a point c uniformly at random from X and set C1 = {c}.
2: for t = 2 to k do
3:
Sample x ∈X w.p. cost(x, Ct)/cost(X, Ct).
4:
Ct = Ct−1 ∪{x}.
5: end for
6: Return Ck
k-means∥and k-means∥Pois seeding: In the k-means∥algorithm, the ﬁrst center is chosen
uniformly at random from X. But after that, at each round, the algorithm samples each point
independently with probability min {ℓ· cost(x, C)/cost(X, C), 1} where ℓis the oversampling pa-
rameter chosen by the user and it usually lies between 0.1k and 10k. The algorithm runs for T
rounds (where T is also a parameter chosen by the user) and samples around ℓT points, which is
usually strictly larger than k. This oversampled set is then weighted using the original data set X
and a weighted version of k-means++ is run on this set to get the ﬁnal k-centers. We only focus
on the stage in which we get the oversampled set because the guarantees for the second stage come
directly from k-means++.
For the sake of analysis, we also consider a diﬀerent implementation of k-means∥, which we
call k-means∥Pois
(Algorithm 3).
This algorithm diﬀers from k-means∥in that each point is
sampled independently with probability 1 −exp(−ℓ· cost(x, C)/cost(X, C)) rather than min{ℓ·
cost(x, C)/cost(X, C), 1}. In practice, there is essentially no diﬀerence between k-means∥and k-
means∥Pois, since ℓ· cost(x, C)/cost(X, C) is a very small number for all x and thus the sampling
probabilities for k-means∥and k-means∥Pois are almost equal.
5

--- Page 6 ---
Algorithm 2 k-means∥seeding
1: Sample a point c uniformly from X and
set C1 = {c}
2: for t = 1 to T do
3:
Sample each point x into C′ indepen-
dently w.p. min{1, λt(x)} where
λt(x) = ℓ· cost(x, Ct)/cost(X, Ct)
4:
Let Ct+1 = Ct ∪C′.
5: end for
Algorithm 3 k-means∥Pois seeding
1: Sample a point c uniformly from X and
set C1 = {c}
2: for t = 1 to T do
3:
Sample each point x into C′ indepen-
dently w.p. 1 −e−λt(x) where
λt(x) = ℓ· cost(x, Ct)/cost(X, Ct)
4:
Let Ct+1 = Ct ∪C′.
5: end for
In the rest of the paper, we focus only on the seeding step of k-means++, k-means∥, and k-
means∥Pois and ignore Lloyd’s iterations as the approximation guarantees for these algorithms come
entirely from the seeding step.
3
General framework
In this section, we describe a general framework we use to analyze k-means++ and k-means∥Pois.
Consider k-means++ or k-means∥Pois algorithm.
Let Ct be the set of centers chosen by this
algorithm after step t. For the sake of analysis, we assume that Ct is an ordered set or list of
centers, and the order of centers in Ct is the same as the order in which our algorithm chooses these
centers. We explain how to order centers in k-means∥Pois algorithm in Section 6. We denote by T
the stopping time of the algorithm. Observe that after step t of the algorithm, the probabilities of
choosing a new center in k-means++ or a batch of new centers in k-means∥Pois are deﬁned by the
current costs of points in X which, in turn, are completely determined by the current set of centers
Ct. Thus, the states of the algorithm form a Markov chain.
In our analysis, we ﬁx the optimal clustering P = {P1, . . . , Pk} (if this clustering is not unique,
we pick an arbitrary optimal clustering). The optimal cost of each cluster Pi is OPT1(Pi) and the
optimal cost of the entire clustering is OPTk(X) = Pk
i=1 OPT1(Pi).
Following the notation in Arthur and Vassilvitskii (2007), we say that a cluster Pi is hit or
covered by a set of centers C if C ∩Pi ̸= ∅; otherwise, we say that Pi is not hit or uncovered. We
split the cost of each cluster Pi into two components which we call the covered and uncovered costs
of Pi. For a given set of centers C,
The covered or hit cost of Pi,
H(Pi, C) :=
(
cost(Pi, C),
if Pi is covered by C
0,
otherwise.
The uncovered cost of Pi,
U(Pi, C) :=
(
0,
if Pi is covered by C
cost(Pi, C),
otherwise.
Let H(X, C) = Pk
i=1 H(Pi, C) and U(X, C) = Pk
i=1 U(Pi, C). Then,
cost(X, C) = H(X, C) + U(X, C).
For the sake of brevity, we deﬁne costt(Y) := cost(Y, Ct) for any Y ⊆X, Ht(Pi) := H(Pi, Ct), and
Ut(Pi) := U(Pi, Ct). In Section 4, we show that for any t, we have E[Ht(X)] ≤5OPTk(X), which is
an improvement over the bound of 8OPTk(X) given by Arthur and Vassilvitskii (2007). Then, in
6

--- Page 7 ---
Sections 5 and 6, we analyze the expected uncovered cost U(X, CT ) for k-means++ and k-means∥
algorithms.
Consider a center c in C. We say that c is a miss if another center c′ covers the same cluster
in P as c, and c′ appears before c in the ordered set C. We denote the number of misses in C by
M(C) and the the number of clusters in P not covered by centers in C by K(C).
Observe that the stochastic processes Ut(Pi) with discrete time t are non-increasing since the
algorithm never removes centers from the set Ct and therefore the distance from any point x to
Ct never increases. Similarly, the processes Ht(Pi) are non-increasing after the step ti when Pi is
covered ﬁrst time. In this paper, we sometimes use a proxy eHt(Pi) for Ht(Pi), which we deﬁne
as follows. If Pi is covered by Ct, then eHt(Pi) = Hti(Pi), where ti ≤t is the ﬁrst time when Pi
is covered by Ct. If Pi is not covered by Ct, then eHt(Pi) = 5OPT1(Pi). It is easy to see that
Ht(Pi) ≤eHt′(Pi) for all t ≤t′. In Section 4, we also show that eHt(Pi) is a supermartingale i.e.,
E[ eHt′(Pi) | Ct] ≤eHt(Pi) for all t ≤t′.
4
Bound on the cost of covered clusters
In this section, we improve the bound by Arthur and Vassilvitskii (2007) on the expected cost
of a covered cluster in k-means++. Our bound also works for k-means∥Pois algorithm. Pick an
arbitrary cluster Pi in the optimal solution P = {P1, . . . , Pk} and consider an arbitrary state
Ct = {c1, . . . , cN} of the k-means++ or k-means∥Pois algorithm. Let Dt+1 be the set of new centers
the algorithm adds to Ct at step t (for k-means++, Dt+1 contains only one center). Suppose now
that centers in Dt+1 cover Pi i.e. Dt+1 ∩Pi ̸= ∅. We show that the expected cost of cluster Pi
after step (t + 1) conditioned on the event {Dt+1 ∩Pi ̸= ∅} and the current state of the algorithm
Ct is upper bounded by 5OPT1(Pi) i.e.
E [cost(Pi, Ct+1) | Ct, {Dt+1 ∩Pi ̸= ∅}] ≤5OPT1(Pi).
(2)
We now prove the main lemma.
Lemma 4.1. Consider an arbitrary set of centers C = {c1, . . . , cN} ⊆Rd and an arbitrary set
P ⊆X. Pick a random point c in P with probability Pr(c = x) = cost(x, C)/cost(P, C). Let
C′ = C ∪{c}. Then, Ec [cost(P, C′)] ≤5OPT1(P).
Remarks: Lemma 3.2 in the paper by Arthur and Vassilvitskii (2007) gives a bound of 8OPT1(P).
We also show in Appendix C that our bound is tight (see Lemma C.1).
Proof. The cost of any point y after picking center c equals the squared distance from y to the
set of centers C′ = C ∪{c}, which in turn equals min{cost(y, C), ∥y −c∥2}.
Thus, if a point
x ∈P is chosen as a center, then the cost of point y equals min{cost(y, C), ∥x −y∥2}.
Since
Pr(c = x) = cost(x, C)/cost(P, C), we have
Ec
cost(P, C′)
 =
X
x∈P
y∈P
cost(x, C)
cost(P, C) · min{cost(y, C), ∥x −y∥2}.
We write the right hand side in a symmetric form with respect to x and y. To this end, we deﬁne
function f as follows:
f(x, y) = cost(x, C) · min
n
∥x −y∥2, cost(y, C)
o
+ cost(y, C) · min
n
∥x −y∥2, cost(x, C)
o
.
7

--- Page 8 ---
Note that f(x, y) = f(y, x). Then,
Ec
cost(P, C′)
 =
1
2cost(P, C)
X
(x,y)∈P ×P
f(x, y).
We now give an upper bound on f(x, y) and then use this bound to ﬁnish the proof of Lemma 4.1.
Lemma 4.2. For any x, y ∈P, we have f(x, y) ≤5 min {cost(x, C), cost(y, C)} ∥x −y∥2.
Proof. Since f(x, y) is a symmetric function with respect to x and y, we may assume without loss of
generality that cost(x, C) ≤cost(y, C). Then, we need to show that f(x, y) ≤5cost(x, C)∥x −y∥2.
Consider three cases.
Case 1: If cost(x, C) ≤cost(y, C) ≤∥x −y∥2, then
f(x, y) = 2cost(x, C)cost(y, C) ≤2cost(x, C)∥x −y∥2.
Case 2: If cost(x, C) ≤∥x −y∥2 ≤cost(y, C), then
f(x, y) = cost(x, C)∥x −y∥2 + cost(y, C)cost(x, C).
By the triangle inequality, we have
cost(y, C) ≤
q
cost(x, C) + ∥x −y∥
2
≤4∥x −y∥2.
Thus, f(x, y) ≤5cost(x, C)∥x −y∥2.
Case 3: If ∥x −y∥2 ≤cost(x, C) ≤cost(y, C), then
f(x, y) = (cost(x, C) + cost(y, C)) ∥x −y∥2.
By the triangle inequality,
cost(y, C) ≤
q
cost(x, C) + ∥x −y∥
2
≤4cost(x, C).
Thus, we have f(x, y) ≤5cost(x, C)∥x −y∥2.
In all cases, the desired inequality holds. This concludes the proof of Lemma 4.2.
We use Lemma 4.2 to bound the expected cost of P. Let φ∗be a vector in RP with φ∗
x =
cost(x, C) for any x ∈P. Then, f(x, y) ≤5 min
n
φ∗
x, φ∗
y
o
∥x −y∥2. Since cost(P, C) = P
z∈P φ∗
z, we
have
Ec
cost(P, C′)
 ≤
5 P
(x,y)∈P ×P min
n
φ∗
x, φ∗
y
o
∥x −y∥2
2 P
z∈P φ∗z
|
{z
}
5F (φ∗)
.
For arbitrary vector φ ∈RP
≥0, deﬁne the following function:
F(φ) =
P
(x,y)∈P ×P min {φx, φy} ∥x −y∥2
2 P
z∈P φz
.
(3)
8

--- Page 9 ---
We have Ec [cost(P, C′)] ≤5F(φ∗). Thus, to ﬁnish the proof of Lemma 4.1, it suﬃces to show that
F(φ) ≤OPT1(P) for every φ ≥0 and particularly for φ = φ∗. By Lemma 4.3 (which we state and
prove below), function F(φ) is maximized when φ ∈{0, 1}P . Let φ∗∗be a maximizer of F(φ) in
{0, 1}P and P ′ = {x ∈P : φ∗∗
x = 1}. Observe that
F(φ∗∗) =
P
(x,y)∈P ′×P ′∥x −y∥2
2|P ′|
= OPT1(P ′).
Here we used the closed form expression (1) for the optimal cost of cluster P ′. Since P ′ ⊂P, we
have OPT1(P ′) ≤OPT1(P). Thus, F(φ∗) ≤F(φ∗∗) ≤OPT1(P).
Lemma 4.3. There exists a maximizer φ∗∗of F(φ) in the region {φ ≥0} such that φ ∈{0, 1}P .
Proof. Let m = |P| be the size of the cluster P and Π be the set of all bisections or permutations
π : {1, . . . , m} →P. Partition the set {φ ≥0} into m! regions (“cones over order polytopes”):
{φ : φ ≥0} = ∪π∈ΠOπ,
where Oπ = {φ : 0 ≤φπ(1) ≤φπ(2) ≤· · · ≤φπ(m)}. We show that for every π ∈Π, there exists a
maximizer φ∗∗of F(φ) in the region Oπ, such that φ∗∗∈{0, 1}P . Therefore, there exists a global
maximizer φ∗∗that belongs {0, 1}P
Fix a π ∈Π. Denote by V the hyperplane {φ : P
x∈P φx = 1}. Observe that F is a scale
invariant function i.e., F(φ) = F(λφ) for every λ > 0. Thus, for every φ ∈Oπ, there exists a
φ′ ∈Oπ ∩V (namely, φ′ = φ/(P
x∈P φx)) such that F(φ′) = F(φ). Hence, max{F(φ) : φ ∈Oπ} =
max{F(φ) : φ ∈Oπ ∩V }. Note that for φ ∈V , the denominator of (3) equals 2, and for φ ∈Oπ,
the numerator of (3) is a linear function of φ. Therefore, F(φ) is a linear function in the convex
set Oπ ∩V . Consequently, one of the maximizers of F must be an extreme point of Oπ ∩V .
The polytope Oπ ∩V is deﬁned by m inequalities and one equality. Thus, for every extreme
point φ of this polytope, all inequalities φπ(i) ≤φπ(i+1) but one must be tight. In other words, for
some j < m, we have
0 = φπ(1) = · · · = φπ(j) < φπ(j+1) = · · · = φπ(m).
(4)
Therefore, there exists a maximizer φ of F(φ) in Oπ ∩V satisfying (4) for some j. After rescaling
φ – multiplying all coordinates of φ by (m −j) – we obtain a vector φ∗∗whose ﬁrst j coordinates
φ∗∗
π(1), . . . , φ∗∗
π(j) are zeroes and the last m −j coordinates φ∗∗
π(j+1), . . . , φ∗∗
π(m) are ones. Thus, φ∗∗∈
{0, 1}P . Since F is rescaling invariant, F(φ∗∗) = F(φ). This concludes the proof.
Replacing the bound in Lemma 3.2 from the analysis of Arthur and Vassilvitskii (2007) by our
bound from Lemma 4.1 gives the following result (see also Lemma 5.6).
Theorem 4.4. The approximation factor of k-means++ is at most 5(ln k + 2).
We now state an important corollary of Lemma 4.1.
Corollary 4.5.
For every P ∈P, the process eHt(P) for k-means++ is a supermartingale i.e.,
E
h
eHt+1(X) | Ct
i
≤eHt(X).
9

--- Page 10 ---
Proof. The value of eHt(X) changes only if at step t, we cover a yet uncovered cluster P. In this
case, the value of eHt+1(P) changes by the new cost of P minus 5OPT(P). By Lemma 4.1 this
quantity is non-positive in expectation.
Since the process eHt(P) is a supermartingale, we have E[ eHt(P)] ≤eH0(P) = 5OPT1(P). Hence,
E[Ht(P)] ≤E[ eHt(P)] = 5OPT1(P). Thus, E[Ht(X)] ≤5OPTk(X). Since costt(X) = Ht(X) +
Ut(X) and we have a bound on the expectation of the covered cost, Ht(X), in the remaining
sections, we shall only analyze the uncovered cost Ut(X).
5
Bi-criteria approximation of k-means++
In this section, we give a bi-criteria approximation guarantee for k-means++.
Theorem 5.1. Let costk+∆(X) be the cost of the clustering with k + ∆centers sampled by the
k-means++ algorithm. Then, for ∆≥1, the expected cost E [costk+∆(X)] is upper bounded by
(below (a)+ denotes max(a, 0)).
min
n
2 + 1
2e +

ln 2k
∆
+
, 1 +
k
e (∆−1)
o
5OPTk(X).
Note that the above approximation guarantee is the minimum of two bounds: (1) 2+ 1
2e + ln 2k
∆
for 1 ≤∆≤2k; and (2) 1 +
k
e(∆−1) for ∆≥1. The second bound is stronger than the ﬁrst bound
when ∆/k ⪆0.085.
5.1
Proof overview of Theorem 5.1
We now present a high level overview of the proof and then give a formal proof. Our proof consists
of three steps.
First, we prove bound (2) on the expected cost of the clustering returned by k-means++ after
k + ∆rounds. We argue that the expected cost of the covered clusters is bounded by 5OPTk(X)
(see Section 3) and thus it is suﬃcient to bound the expected cost of uncovered clusters. Consider
an optimal cluster P ∈P. We need to estimate the probability that it is not covered after k + ∆
rounds. We upper bound this probability by the probability that the algorithm does not cover P
before it makes ∆misses (note: after k + ∆rounds k-means++ must make at least ∆misses).
In this overview, we make the following simplifying assumptions (which turn out to be satisﬁed
in the worst case for bi-criteria k-means++): Suppose that the uncovered cost of cluster P does not
decrease before it is covered and equals U(P) and, moreover, the total cost of all covered clusters
almost does not change and equals H(X) (this may be the case if one large cluster contributes
most of the covered cost, and that cluster is covered at the ﬁrst step of k-means++). Under these
assumptions, the probability that k-means++ chooses ∆centers in the already covered clusters
and does not choose a single center in P equals (H(X)/(U(P) + H(X)))∆. If k-means++ does
not choose a center in P, the uncovered cost of cluster P is U(P); otherwise, the uncovered cost of
cluster P is 0. Thus, the expected uncovered cost of P is (H(X)/(U(P)+H(X)))∆U(P). It is easy
to show that (H(X)/(U(P) + H(X)))∆U(P) ≤H(X)/(e(∆−1)). Thus, the expected uncovered
cost of all clusters is at most
k
(e(∆−1))E[H(X)] ≤
k
(e(∆−1))5OPTk(X).
10

--- Page 11 ---
Then, we use ideas from Arthur and Vassilvitskii (2007), Dasgupta (2013) to prove the following
statement: Let us count the cost of uncovered clusters only when the number of misses after k
rounds of k-means++ is greater than ∆/2. Then the expected cost of uncovered clusters is at most
O(log(k/∆)) · OPTk(X). That is, E[H(Uk(X) · 1{M(Ck) ≥∆/2}] ≤O(log(k/∆)) · OPTk(X).
Finally, we combine the previous two steps to get bound (1). We argue that if the number of
misses after k rounds of k-means++ is less than ∆/2, then almost all clusters are covered. Hence,
we can apply bound (2) to k′ ≤∆/2 uncovered clusters and ∆remaining rounds of k-means++
and get a 5(1+1/(2e)) approximation. If the number of misses is greater than ∆/2, then the result
from the previous step yields an O(log(k/∆)) approximation.
5.2
Analysis of k-means++
In this section, we analyze the bi-criteria k-means++ algorithm and prove Theorem 5.1. To this end,
we establish the ﬁrst and second bounds from Theorem 5.1 on the expected cost of the clustering
after k + ∆rounds of k-means. We will start with the second bound.
5.2.1
Bi-criteria bound for large ∆
Lemma 5.2. The following bi-criteria bound holds
E [costk+∆(X)] ≤5

1 +
k
e (∆−1)

OPTk(X).
Consider the discrete time Markov chain Ct associated with k-means++ algorithm (see Sec-
tion 3). Let P ∈P be an arbitrary cluster in the optimal solution. Partition all states of the
Markov chain into k + ∆disjoint groups M0, M1, · · · , Mk+∆−1 and H. Each set Mi contains all
states C with i misses that do not cover P: Mi = {C : M(C) = i, P ∩C = ∅} . The set H contains
all states C that cover P: H = {C : P ∩C ̸= ∅}.
We now deﬁne a new Markov chain Xt. To this end, we ﬁrst expand the set of states {C}. For
every state C of the process Ct, we create two additional “virtual” states Ca and Cb. Then, we let
X2t = Ct for every even step 2t, and
X2t+1 =
(
Ca
t ,
if Ct+1 ∈Mi
Cb
t ,
if Ct+1 ∈Mi+1 ∪H.
for every odd step 2t + 1. We stop Xt when Ct stops or when Ct hits the set H (i.e., Ct ∈H).
Loosely speaking, Xt follows Markov chain Ct but makes additional intermediate stops. When Ct
moves from one state in Mi to another state in Mi, X2t+1 stops in Ca
t ; and when Ct moves from
a state in Mi to a state in Mi+1 or H, X2t+1 stops in Cb
t .
Write transition probabilities for Xt:
P [X2t+1 = Ca | X2t = C] = U(X, C) −U(P, C)
cost(X, C)
,
P
h
X2t+1 = Cb | X2t = C
i
= U(P, C) + H(X, C)
cost(X, C)
,
and for all C ∈Mi and C′ = C ∪{x} ∈Mi,
P
X2t+2 = C′ | X2t+1 = Ca =
cost(x, C)
U(X, C) −U(P, C),
11

--- Page 12 ---
for all C ∈Mi and C′ = C ∪{x} ∈Mi+1 ∪H,
P
h
X2t+2 = C′ | X2t+1 = Cbi
=
cost(x, C)
U(P, C) + H(X, C).
Above, U(X, C) −U(P, C) is the cost of points in all uncovered clusters except for P. If we pick a
center from these clusters, we will necessarily cover a new cluster, and therefore X2t+2 will stay in
Mi. Similarly, U(P, C) + H(X, C) is the cost of all covered clusters plus the cost of P. If we pick
a center from these clusters, then X2t+2 will move to Mi+1 or H.
Deﬁne another Markov chain {Yt}. The transition probabilities of {Yt} are the same as the
transition probabilities of Xt except Y never visits states in H and therefore for C ∈Mi and
C′ = C ∪{x} ∈Mi+1, we have
P
h
Y2t+2 = C′ | Y2t+1 = Cbi
= cost(x, C)
H(X, C) .
We now prove a lemma that relates probabilities of visiting states by Xt and Yt.
Lemma 5.3.
For every t ≤k + ∆and states C′ ∈Mi, C′′ ∈M∆, we have
P [C′′ ∈{Xj} | X2t = C′]
P [C′′ ∈{Yj} | Y2t = C′] ≤
 
eH(X, C′′)
eH(X, C′′) + U(P, C′′)
!∆−i
where {C′′ ∈{Xj}} and {C′′ ∈{Yj}} denote the events X visits C′′ and Y visits C′′, respectively.
Proof. Consider the unique path p from C′ to C′′ in the state space of X (note that the transition
graphs for X and Y are directed trees). The probability of transitioning from C′ to C′′ for X and
Y equals the product of respective transition probabilities for every edge on the path. Recall that
transitions probabilities for X and Y are the same for all states but Cb, where C ∈∪jMj. The
number of such states on the path p is equal to the number transitions from Mj to Mj+1, since X
and Y can get from Mj to Mj+1 only through a state Cb on the boundary of Mj and Mj+1. The
number of transitions from Mj to Mj+1 equals ∆−i. For each state Cb on the path, the ratio of
transition probabilities from Cb to the next state C ∪{x} for Markov chains X and Y equals
H(X, C)
U(P, C) + H(X, C) ≤
eH(X, C′′)
U(P, C′′) + eH(X, C′′)
,
here we used that (a) U(P, C) ≥U(P, C′′) since Ut(P) is a non-increasing process; and (b)
H(P, C) ≤eH(P, C′′) since Ht(P) ≤eHt′(P) if t ≤t′ (see Section 3).
We now prove an analog of Corollary 4.5 for eH(X, Yj).
Lemma 5.4.
eH(X, Yt) is a supermartingale.
Proof. If Yj = C, then Yj+1 can only be in
n
Ca, Cbo
. Since eH(X, Ca) = eH(X, Cb) = eH(X, C), we
have E
h
eH(X, Yj+1) | Yj = C
i
= eH(X, Yj).
If Yj = Ca, then Yj+1 = C′ where the new center c should be in uncovered clusters with respect
to Ct.
E
H(P ′, Yj+1) | Yj = Ca, c ∈P ′ ≤5OPT1(P ′),
12

--- Page 13 ---
which implies
E
h
eH(P ′, Yj+1) | Yj = Ca, c ∈P ′i
≤eH(P ′, Yj).
Therefore, we have
E
h
eH(X, Yj+1) | Yj = Cai
≤eH(X, Yj).
If Yj = Cb, then for any possible state C′ of Yj+1, the new center should be in covered clusters with
respect to C. By deﬁnition, we must have eH(X, C′) = eH(X, C) = eH(X, Cb). Thus, it holds that
E
h
eH(X, Yj+1) | Yj = Cbi
= eH(X, Yj).
Combining all these cases, we get
n
eH(X, Yj)
o
is a supermartingale.
We now use Lemma 5.3 and Lemma 5.4 to bound the expected uncovered cost of P after k + ∆
rounds of k-means++.
Lemma 5.5.
For any cluster P ∈P and t ≤k + ∆, we have
E [Uk+∆(P) | Ct] ≤
eHt(X)
e(∆−M(Ct) −1).
Proof. Since k-means++ samples k + ∆centers and the total number of clusters in the optimal
solution P is k, k-means++ must make ∆misses. Hence, the process {Xt} which follows k-means++
must either visit a state in M≥∆or stop in H (recall that we stop process Xt if it reaches H).
If {Xt} stops in group H, then the cluster P is covered which means that Uk+∆(P) = 0. Let
∂M∆be the frontier of M∆i.e., the states that Xt visits ﬁrst when it reaches M∆(recall that
the transition graph of Xt is a tree). The expected cost E [Uk+∆(P) | Ct] is upper bounded by the
expected uncovered cost of P at time when Ct reaches ∂M∆. Thus,
E [Uk+∆(P) | Ct] ≤
X
C∈∂M∆
P [C ∈{Xj} | Ct] U(P, C).
Observe that by Lemma 5.3, for any C ∈∂M∆, we have
P [C ∈{Xj} | Ct] U(P, C) ≤P [C ∈{Yj} | Ct]
 
eH(X, C)
eH(X, C) + U(P, C)
!∆′
U(P, C).
Let f(x) = x(1/(1+ x))∆′. Then, f(x) is maximized at x = 1/(∆′ −1) and the maximum value
f(1/(∆′ −1)) = 1/(e(∆′ −1)). Therefore, for every C ∈∂M∆, we have
P[C ∈{Xj} | Ct]U(P, C) ≤P [C ∈{Yj} | Ct] f
 
U(P, C)
eH(X, C)
!
eH(X, C)
≤P [C ∈{Yj} | Ct]
eH(X, C)
e(∆′ −1).
Let τ = min {j : Yj ∈∂M∆} be the stopping time when Yj ﬁrst visits ∂M∆. We get
X
C∈∂M∆
P [C ∈{Yj} | Ct] eH(X, C) = E
h
eH(X, Yτ) | Ct
i
.
13

--- Page 14 ---
By Lemma 5.4, eH(X, Yj) is a supermartingale. Thus, by the optional stopping theorem,
E
h
eH(X, Yτ) | Ct
i
≤eH(X, Ct).
Therefore, we have
E [Uk+∆(P) | Ct] ≤
eHt(X)
e(∆−M(Ct) −1),
This concludes the proof.
We now add up bounds from Lemma 5.5 with t = 0 for all clusters P ∈P and obtain Lemma 5.2.
5.3
Bi-criteria bound for small ∆
In this section, we give another bi-criteria approximation guarantee for k-means++.
Lemma 5.6. Let costk+∆(X) be the cost of the the clustering resulting from sampling k+∆centers
according to the k-means++ algorithm (for ∆∈{1, . . . , 2k}). Then,
E [costk+∆(X)] ≤5

2 + 1
2e + ln 2k
∆

OPTk(X).
Proof. Consider k-means++ clustering algorithm and the corresponding random process Ct. Fix
a κ ∈{1, . . . , k}. Let τ be the ﬁrst iteration1 (stopping time) when K(Cτ) ≤κ if K(Ck) ≤κ; and
τ = k, otherwise. We refer the reader to Section 3 for deﬁnitions of M(Ct), Ut(X) = U(X, Ct), and
K(Ct).
We separately analyze the cost of uncovered clusters after the ﬁrst τ steps and the last k′ −τ
steps, where k′ = k + ∆is the total number of centers chosen by k-means++.
The ﬁrst step of our proof follows the analysis of k-means++ by Dasgupta (2013), and by
Arthur and Vassilvitskii (2007). Deﬁne a potential function Ψ (see Dasgupta 2013):
Ψt := M(Ct)U(X, Ct)
K(Ct)
.
If K(Ct) = 0, then M(Ct) and U(X, Ct) must be 0 and we let Ψt = 0
We use the following result by Dasgupta (2013) to estimate E[Ψτ(X)] in Lemma 5.8.
Lemma 5.7 (Dasgupta (2013)). For any 0 ≤t ≤k, we have
E [Ψt+1 −Ψt | Ct] ≤H(X, Ct)
K(Ct) .
Lemma 5.8. Then, the following bound holds:
E[Ψτ(X)] ≤5

1 + ln

k
κ + 1

OPTk(X).
1Recall, that K(Ct) is a non-increasing stochastic process with K(C0) = k.
14

--- Page 15 ---
Proof. Note that Ψ1 = 0 as M(C1) = 0. Thus,
E[Ψτ] ≤
τ−1
X
t=1
E
Ψt+1 −Ψt
 ≤E
h τ−1
X
t=1
H(X, Ct)
K(Ct)
i
.
Using the inequality H(X, Ct) ≤eHk(X) (see Section 3), we get:
E[Ψτ] ≤E
h τ−1
X
t=1
eHk(X)
K(Ct)
i
≤E
h
eHk(X) ·
τ−1
X
t=1
1
K(Ct)
i
.
Observe that K(C1), . . . , K(Cτ−1) is a non-increasing sequence in which two consecutive terms are
either equal or K(Ci+1) = K(Ci) −1. Moreover, K(C1) = k and K(Cτ−1) > κ. Therefore, by
Lemma 5.9 (see below), for every realization C0, C1, . . . , Cτ, we have:
τ−1
X
t=1
1
K(Ct) ≤1 + log k/(κ+1).
Thus,
E[Ψτ] ≤(1 + log k/(κ+1))E[ eHk(X)] ≤5(1 + log k/(κ+1)) OPTk(X).
This concludes the proof.
Let κ = ⌊(∆−1)/2⌋. By Lemma 5.8, we have
E
hM(Cτ)Uτ(X)
K(Cτ)
i
≤5

1 + ln 2k
∆

OPTk(X).
Since Ut(X) is a non-increasing stochastic process, we have E[Uk+∆(X)] ≤E[Uτ(X)]. Thus,
E
hM(Cτ)
K(Cτ) · Uk+∆(X)
i
≤5

1 + ln 2k
∆

OPTk(X).
Our goal is to bound E[Uk′(X)]. Write,
E[Uk′(X)] = E
hM(Cτ)
K(Cτ) · Uk′(X)
i
+ E
hK(Cτ) −M(Cτ)
K(Cτ)
· Uk′(X)
i
.
The ﬁrst term on the right hand side is upper bounded by 5
 1+ ln 2k
∆
OPTk(X). We now estimate
the second term, which we denote by (∗).
Note that K(Ct) −M(Ct) = k −t, since the number of uncovered clusters after t steps of
k-means++ equals the number of misses plus the number of steps remaining. Particularly, if τ = k,
we have K(Cτ) −M(Cτ) = K(Ck) −M(Ck) = 0. Consequently, if τ = k, then the second term (∗)
equals 0. Thus, we only need to consider the case, when τ < k. Note that in this case K(Cτ) = κ.
By Lemma 5.2 (applied to all uncovered clusters), we have
E[Uk′(X) | Cτ, τ] ≤
K(Cτ)
e(∆′ −1)
eHτ(X),
where ∆′ = ∆−M(Cτ).
15

--- Page 16 ---
Thus,
E
hK(Cτ) −M(Cτ)
K(Cτ)
· Uk′(X) | Cτ, τ
i
≤K(Cτ) −M(Cτ)
K(Cτ)
·
K(Cτ)
e(∆′ −1) · eHτ(X) = (∗∗).
Plugging in K(Cτ) = κ and the expression for ∆′ (see above), and using that κ ≤(∆−1)/2, we
get
(∗∗) =
κ −M(Cτ)
e(∆−M(Cτ) −1). · eHτ(X) ≤1
2e
eHτ(X).
Finally, taking the expectation over all Cτ, we obtain the bound
E
hK(Cτ) −M(Cτ)
K(Cτ)
· Uk′(X)
i
≤5OPT1(X)
2e
.
Thus, E[Uk′(X)] ≤5(1 + 1/2e + ln 2k/∆)OPTk(X). Therefore,
E[costk′(X)] = E[Hk′(X)] + Uk′(X) ≤5
 2 + 1
2e + ln 2k
∆
 OPTk(X).
We now prove Lemma 5.9.
Lemma 5.9.
For any t ≤k integers a1 ≥a2 ≥· · · ≥at such that a1 = k, at > κ and ai −ai+1 ∈
{0, 1} for all 1 ≤i < t, the following inequality holds
t
X
i=1
1
ai
≤1 + log

k
κ + 1

.
Proof. It is easy to see that the sum is maximized when t = k, and the sequence a1, . . . , ak is as
follows:
1
k,
1
k −1, . . . ,
1
κ + 2
|
{z
}
(k−(κ+1)) terms
,
1
κ + 1, . . . ,
1
κ + 1
|
{z
}
(κ+1) terms
.
The sum of the ﬁrst (k −(κ + 1)) terms is upper bounded by
Z 1/k
1/(κ+1)
1
x dx = ln
k
κ + 1.
The sum of the last (κ + 1) terms is 1.
6
Analysis of k-means∥
In this section, we give a sketch of analysis for the k-means∥algorithm. Speciﬁcally, we show upper
bounds on the expected cost of the solution after T rounds.
16

--- Page 17 ---
Theorem 6.1. The expected cost of the clustering returned by k-means∥algorithm after T rounds
are upper bounded as follows:
for ℓ< k,
E [costT+1(X)] ≤

e−ℓ
k
T
E [cost1(X)] + 5OPTk(X)
1 −e−ℓ
k
;
for ℓ≥k,
E [costT+1(X)] ≤
 k
eℓ
T
E [cost1(X)] + 5OPTk(X)
1 −k/eℓ
.
Remark: For the second bound (ℓ≥k), the additive term 5OPTk(X)/(1 −k/(eℓ)) ≤8OPTk(X).
The probability that a point is sampled by k-means∥is strictly greater than the probability
that it is sampled by k-means∥Pois since 1 −e−λ < λ for all λ > 0. Thus, for every round, we can
couple k-means∥Pois and k-means∥so that each point sampled by k-means∥Pois is also sampled by
k-means∥. Thus, the expected cost returned by k-means∥is at most the expected cost returned
by k-means∥Pois. In the following analysis, we show an upper bound for the expected cost of the
solution returned by k-means∥Pois.
As a thought experiment, consider a modiﬁed k-means∥Pois algorithm. This algorithm is given
the set X, parameter k, and additionally the optimal solution P = {P1, . . . , Pk}. Although this
modiﬁed algorithm is useless in practice as we do not know the optimal solution in advance, it will
be helpful for our analysis.
In every round t, the modiﬁed algorithm ﬁrst draws independent Poisson random variables
Zt(Pi) ∼Pois(λt(Pi)) for every cluster i ∈{1, . . . , k} with rate λt(Pi) = P
x∈Pi λt(x). Then, for
each i ∈{1, . . . , k}, it samples Zt(Pi) points x ∈Pi with repetitions from Pi, picking every point
x with probability λt(x)/λt(Pi) and adds them to the set of centers Ct. We assume that points in
every set Ct are ordered in the same way as they were chosen by this algorithm.
We claim that the distribution of the output sets CT of this algorithm is exactly the same as
in the original k-means∥Pois algorithm. Therefore, we can analyze the modiﬁed algorithm instead
of k-means∥Pois, using the framework described in Sections 3.
Lemma 6.2.
The sets Ct in the original and modiﬁed k-means∥Pois algorithms are identically
distributed.
Proof. Consider |Pi| independent Poisson point processes Nx(a) with rates λt(x), where x ∈Pi
(here, we use variable a for time). Suppose we add a center x at step t of the algorithm if Nx(t) ≥1.
On the one hand, the probability that we choose x is equal to 1 −e−λt(x) which is exactly the
probability that k-means∥Pois picks x as a center at step t. On the other hand, the sum NPi =
P
x∈Pi Nx is a Poisson point process with rate λt(Pi). Thus, the total number of jumps in the
interval [0, 1] of processes Nx with x ∈Pi is distributed as Zt(Pi). Moreover, the probability that
Nx jumps at time a conditioned on the event that NPi jumps at time a is λt(x)/λt(Pi). Thus, for
every jump of NPi, we choose one random center x with probability λt(x)/λt(Pi).
Lemma 6.3.
For k-means∥algorithm with parameter ℓ, the following bounds hold:
for ℓ< k,
E [costt+1(X)] ≤e−ℓ
k · E [costt(X)] + 5OPTk(X);
for ℓ≥k,
E [costt+1(X)] ≤
 k
eℓ

· E [costt(X)] + 5OPTk(X).
Proof. Since the expected cost returned by k-means∥is at most the expected cost returned by
k-means∥Pois, we analyze the expected cost of the clustering after one step of k-means∥Pois.
17

--- Page 18 ---
If the algorithm covers cluster Pi at round t, then at the next round, its uncovered cost equals
0. The number of centers chosen in Pi is determined by the Poisson random variable Zt+1(Pi).
Hence, Pi is uncovered at round t + 1 only if Zt+1(Pi) = 0. Since Ut(Pi) is non-increasing in t and
Ut(Pi) ≤costt(Pi), we have
E [Ut+1(Pi) | Ct] ≤P [Zt+1(Pi) = 0] Ut(Pi) ≤exp

−ℓcostt(Pi)
costt(X)

costt(Pi).
Deﬁne two function: f(x) = e−x · x; and g(x) = f(x) for x ∈[0, 1] and g(x) = e−1 for x ∈[1, ∞).
Then,
E [Ut+1(X) | Ct] ≤
 
1
k
k
X
i=1
f
ℓcostt(Pi)
costt(X)
!
kcostt(X)
ℓ
.
Since g(x) ≤f(x), and g(x) is concave for x ≥0, we have
E [Ut+1(X) | Ct] ≤
 
1
k
k
X
i=1
g
ℓcostt(Pi)
costt(X)
!
kcostt(X)
ℓ
≤g
 ℓ
k
 kcostt(X)
ℓ
.
Here, we use that P
i costt(Pi) = costt(X).
Therefore, for ℓ≤k, we have
E [Ut+1(X) | Ct] ≤

e−ℓ
k

costt(X);
and for ℓ≥k, we have
E [Ut+1(X) | Ct] ≤
 k
eℓ

costt(X).
Similar to Corollary 4.5, the process eHt(P) for k-means∥Pois is also a supermartingale, which
implies E [Ht+1(X)] ≤5OPTk(X). This concludes the proof.
Proof of Theorem 6.1. Applying the bound from Lemma 6.3 for t times, we get the following results.
For ℓ≤k,
E [costt+1(X)] ≤

e−ℓ
k
t
E [cost1(X)] + 5OPTk(X)ηt,
where ηt = Pt
j=1

e−ℓ
k
j−1 <
1
1−e−ℓ
k . For ℓ≥k,
E [costt+1(X)] ≤
 k
eℓ
t
E [cost1(X)] + 5OPTk(X)ηt,
where ηt = Pt
j=1

k
eℓ
j−1 ≤
1
1−k
eℓ.
Corollary 6.4.
Consider a data set X with more than k distinct points. Let
T = ln E
 cost1(X)
OPTk(X))

and ℓ> k. Then, after T rounds of k-means∥, the expected cost of clustering E [costT (X)] is at
most 9OPTk(X).
18

--- Page 19 ---
7
Exponential Race k-means++ and Reservoir Sampling
In this section, we show how to implement k-means++ algorithm in parallel using R passes over the
data set. This implementation, which we refer to as k-means++ER (exponential race k-means++),
is very similar to k-means∥, but has stronger theoretical guarantees. Like k-means∥, in every round,
k-means++ER tentatively selects ℓcenters, in expectation. However, in the same round, it removes
some of the just selected centers (without making another pass over the data set). Consequently,
by the end of each iteration, the algorithm keeps at most k centers.
We can run k-means++ER till it samples exactly k centers; in which case, the distribution of k
sampled centers is identical to the distribution of the regular k-means++, and the expected number
of rounds or passes over the data set R is upper bounded by
O
k
ℓ+ log OPT1(X)
OPTk(X)

.
We note that R is never greater than k. We can also run this algorithm for at most R∗rounds.
Then, the expected cost of the clustering is at most
5(ln k + 2) OPTk(X) + 5R∗
 4k
eℓR∗
R∗
· OPT1(X).
7.1
Algorithm
In this section, we give a high level description of our k-means++ER algorithm. In Section 7.2, we
show how to eﬃciently implement k-means++ER using lazy updates and explain why our algorithm
makes R passes over the data set.
The algorithm simulates n continuous-time stochastic processes.
Each stochastic process is
associated with one of the points in the data set. We denote the process corresponding to x ∈X
by Pt(x). Stochastic process Pt(x) is a Poisson process with variable arrival rate λt(x).
The algorithm chooses the ﬁrst center c1 uniformly at random in X and sets the arrival rate
of each process Pt(x) to be λt(x) = cost(x, {c1}). Then, it waits till one of the Poisson processes
Pt(x) jumps. When process Pt(x) jumps, the algorithm adds the point x ∈X (corresponding to
that process) to the set of centers Ct and updates the arrival rates of all processes to be
λt(y) = cost(y, Ct)
for all y ∈X. Note that if y is a center, then the arrival rate λt(y) is 0.
The algorithm also maintains a round counter R. In the lazy version of this algorithm (which
we describe in the next section), the algorithm makes a pass over the data set and samples a new
batch of centers every time this counter is incremented. Additionally, at the end of each round, the
algorithm checks if it chose at least one center in that round, and in the unlikely event that it did
not, it selects one center with probability proportional to the costs of the points.
Initially, the algorithm sets R = 0, t0 = 0, and t1 = ℓ/cost(X, {c1}). Then, at each time point
ti (i ≥1), we increment R and compute
ti+1 = ti + ℓ/cost(X, Cti),
where Cti is the set of all centers selected before time ti. We refer to the time frame [ti−1, ti] for
i ≥1 as the i-th round. The algorithm stops when one of the following conditions holds true (1) the
19

--- Page 20 ---
number of sampled centers is k; or (2) the round counter R equals the prespeciﬁed threshold R∗,
which may be ﬁnite or inﬁnite.
Before analyzing this algorithm, we mention that every Poisson process Pt with a variable
arrival rate λt can be coupled with a Poisson process Qs with rate 1. To this end, we substitute
the variable
s(t) =
Z t
0
λτdτ,
and let
Pt ≡Qs(t).
Observe that the expected number of arrivals for process Qs in the inﬁnitesimal interval [s, s + ds]
is ds = λtdt which is exactly the same as for process Pt.
It is convenient to think about the variables s as “current position”, t as “current time”, and λt
as “current speed” of s. To generate process Pt(x), we can ﬁrst generate Poisson process Qs(x) with
arrival rate 1 and then move the position st(x) with speed λt(x). The process Pt(x) = Qst(x)(x) is
a Poisson process with variable arrival rate λt(x).
Theorem 7.1. I. If the number of rounds is not bounded (i.e., R∗= ∞), then the distribution of
centers returned by k-means++ER is identical to the distribution of centers returned by k-means++.
II. Moreover, the expected number of rounds R is upper bounded by
(1 + ok(1)) ·

⌈k
ℓ⌉+ log 2 OPT1(X)
OPTk(X)

,
and never exceeds k.
III. If the threshold R∗is given (R∗< ∞), then the cost of the solution after R∗rounds is upper
bounded by
5(ln k + 2) OPTk(X) + 2R∗
 4k
eℓR∗
R∗
· OPT1(X).
Proof of Part I. For the sake of analysis, we assume that after the algorithm outputs solution C, it
does not terminate, but instead continues to simulate Poisson processes Pt(x). It also continues to
update the set Ct (but, of course, not the solution) and the arrival rates λt(x) till the set Ct contains
k centers. Once |Ct| = k, the algorithm stops updating the set of centers Ct and arrival rates but
still simulates continuous-time processes Pt(x). Clearly, this additional phase of the algorithm does
not aﬀect the solution since it starts after the solution is already returned to the user.
We prove by induction on i that the ﬁrst i centers c1, . . . , ci have exactly the same joint distri-
bution as in k-means++. Indeed, the ﬁrst center c1 is drawn uniformly at random from the data
set X as in k-means++. Suppose centers c1, . . . , ci are already selected. Then, we choose the next
center ci+1 at the time of the next jump of one of the Poisson processes Pt(x). Observe that the
conditional probability that a particular process Pt(x) jumps given that one of the processes Pt(y)
(y ∈X) jumps is proportional to λt(x), which in turn equals the current cost(x, Ct) of point x.
Hence, the distribution of center ci+1 is the same as in k-means++. This completes the proof of
item I.
Proof of Part II. We now show items II and III. Deﬁne process
Pt(X) =
X
x∈X
Pt(x).
20

--- Page 21 ---
Its rate λt(X) equals P
x∈X λt(x). We couple this process with a Poisson Qs(X) with arrival rate
1 as discussed above. We want to estimate the number of centers chosen by the algorithm in the
ﬁrst R′ rounds. To this end, we count the number of jumps of the Poisson process Pt(X) (recall
that we add a new center to Ct whenever Pt(X) jumps unless |Ct| already contains k centers). The
number of jumps equals PtR′ which, in turn, equals QsR′ where sR′(X) is the position of s(X) at
time tR′:
sR′(X) =
Z tR′
0
λτ(X) dτ =
R′−1
X
i=0
Z ti+1
ti
λτ(X) dτ ≥
R′−1
X
i=0
(ti+1 −ti) · λti+1(X).
Here, we used that λt(X) is non-increasing, and thus, λti+1(X) ≤λτ(X) for all τ ∈[ti, ti+1]. We
now recall that (ti+1 −ti) = ℓ/cost(X, Cti) and λti+1(X) = cost(X, Cti+1). Hence,
sR′(X) ≥ℓ
R′−1
X
i=0
cost(X, Cti+1)
cost(X, Cti) .
By the inequality of arithmetic and geometric means, we have
sR′(X) ≥ℓ· R′
 R′−1
Y
i=0
cost(X, Cti+1)
cost(X, Cti)
!1/R′
= ℓ· R′
 
cost(X, CtR′ )
cost(X, Ct0)
!1/R′
(5)
= ℓ· R′
 
cost(X, CtR′ )
cost(X, {c1})
!1/R′
.
We now use this equation to prove items II and III. For item II, we let random variable R′ to
be
R′ = 2e⌈k/ℓ⌉+ log cost(X, {c1})
OPTk(X)
.
Note that R′ depends on the ﬁrst center c1 (which is chosen in the very beginning of the algorithm)
but not on the Poisson processes Pt(x).
Since, Ct always contains at most k centers, we have
cost(x, CtR′ ) ≥OPTk(X), and consequently
sR′(X) ≥ℓ· R′
 
OPTk(X)
cost(X, {c1})
!1/R′
> ℓ· 2e⌈k/ℓ⌉· 1/e ≥2k.
The expected number of jumps of the Poisson process Qs(X) in the interval [0, sR′(X)] equals
QsR(X)(X). Observe that
QsR(X)(X) ≥Q2k(X)
and Q2k(X) is a Poisson random variable with parameter 2k. By the Chernoﬀbound2, it makes
fewer than k jumps with exponentially small probability in k; namely, with probability at most
(e/2)−k. Thus, with probability at least 1 −(e/2)−k, the algorithm selects k centers in the ﬁrst R′
rounds. Moreover, if it does not happen in the ﬁrst R∗rounds, then it selects k centers by the end
of the second R′ rounds again with probability at least 1 −(e/2)−k and so on. Hence, the expected
2We use the bound Pr{P ≤k} ≤e−λ eλ/kk, where P is a Poisson random variable with parameter λ and k < λ.
See e.g., Theorem 5.4.2 in Mitzenmacher and Upfal (2017).
21

--- Page 22 ---
number of rounds till it selects k centers is (1 + ok(1))R′. Finally, observe that the expectation of
cost(X, {c1}) over the choice of the ﬁrst center equals 2 OPTk(X). Since log(·) is a convex function,
we have
E[R′] ≤2e⌈k/ℓ⌉+ log 2 OPT1(X)
OPTk(X) .
Therefore, we showed that the expected number of rounds is upper bounded by the right hand side
of the expression above times a multiplicative factor of (1+ok(1)). A slightly more careful analysis
gives a bound of
(1 + ok(1))
 
e⌈k/ℓ⌉+ log 2 OPT1(X)
OPTk(X)
!
.
This concludes the proof of item II.
Proof of Part III. We now prove item III. Denote T = tR∗. Consider the event
E =
algorithm samples k centers in the ﬁrst R∗rounds
	.
Let ¯E be the complimentary events to E. Then,
E
cost(X, CT )
 = E
cost(X, CT ) · 1(E)
 + E
cost(X, CT ) · 1( ¯E)
.
We now separately upper bound each of the terms on the right hand side. It is easy to upper bound
the ﬁrst term:
E[cost(X, CT ) · 1(E)] ≤5(ln k + 2) · OPTk(X),
because the distribution of centers returned by k-means++ER is identical to the distribution of
centers returned by k-means++. We now bound the second term. Denote by Dρ the event
Dρ =

cost(X, CT ) ≥
 ρk
ℓR∗
R∗
cost(X, {c1})

.
We prove the following claim.
Claim 7.2. The following inequality holds for every real number ρ ∈[1, ℓR∗/k] and any choice of
the ﬁrst center c1:
Pr
  ¯E and Dρ | c1
 ≤e−(ρ−1)kρk−1.
Proof. We use inequality (5) with R′ = R∗:
sR∗(X) ≥ℓ· R∗
 
cost(X, CT )
cost(X, {c1})
!1/R∗
.
It implies that sR∗(X) ≥ρk if event Dρ occurs. On the other hand if ¯E occurs, then the number
of centers chosen by the end of round R∗is less than k and, consequently, the number of jumps of
Pt(X) in the interval [0, T] is less than k:
PT (X) ≡QsR∗(X)(X) < k.
22

--- Page 23 ---
Hence, we can bound Pr( ¯E and Dρ | c1) as follows:
Pr( ¯E and Dρ) ≤Pr
 Dρ and QsR∗(X) < k | c1
 ≤
≤Pr
 Dρ and Qρk(X) < k | c1
 ≤Pr
 Qρk(X) < k | c1
.
Random variable Qρk(X) has the Poisson distribution with parameter ρk and is independent of c1.
By the Chernoﬀbound, the probability that Qρk(X) ≤k −1 is at most (as in Part II of the proof):
Pr
Qρk(X) ≤k −1
	 ≤e−ρk eρk
k −1
k−1
= e−(ρ−1)k−1ρk−1 ·

k
k −1
k−1
|
{z
}
≤e
≤e−(ρ−1)kρk−1.
This completes the proof of Claim 7.2.
Let
Z =
ℓR∗
k
R∗
· cost(X, CT )
cost(X, {c1}).
Then, by Claim 7.2,
Pr
  ¯E and Z ≥ρR∗| c1
 ≤e−(ρ−1)kρk−1.
(6)
Write,
E
1( ¯E) · Z | c1
 =
Z ∞
0
Pr
 1( ¯E) and Z ≥r | c1
dr ≤1 +
Z ∞
1
Pr
 1( ¯E) and Z ≥r | c1
 dr.
We now substitute r = ρR∗and then use (6):
E
Z · 1( ¯E) | c1
 ≤1 + R∗
Z ∞
1
Pr
  ¯E and Z ≥ρR∗| c1
 · ρR∗−1dρ
≤1 + R∗
Z ∞
1
e−(ρ−1)kρk+R∗−2dρ.
We note that R∗< k, since our algorithm chooses at least one center in each round. Thus,
by Lemma 7.3 (which we prove below), the integral on the right hand side is upper bounded by
eR∗/2 · (4/e)R∗. Hence,
E
Z · 1( ¯E) | c1
 ≤1 + R∗·
4
e
R∗−2
.
Multiplying both sides of the inequality by (k/ℓR∗)R∗·cost(X, {c1}) and taking the expectation over
c1, we get the desired inequality:
E
cost(X, CT ) · 1( ¯E)
 ≤

1 + R∗
4
e
R∗ k
ℓR∗
R∗
Ec1
cost(X, {c1}

=

1 + R∗4
e
R∗−2 k
ℓR∗
R∗
· 2 OPT1(X)
< 2R∗
 4k
eℓR∗
R∗
OPT1(X).
This ﬁnishes the proof of Theorem 7.1.
23

--- Page 24 ---
Lemma 7.3. For R∗< k, we have
Z ∞
1
e−(ρ−1)kρk+R∗−2dρ ≤e
2
4
e
R∗
.
Proof. Since e−(ρ−1)ρ ≤1 for all ρ ≥1, we have e−(ρ−1)kρk ≤e−(ρ−1)R∗ρR∗for any R∗< k. Thus,
we have
Z ∞
1
e−(ρ−1)kρk+R∗−2dρ ≤
Z ∞
1
e−(ρ−1)R∗ρ2R∗−3dρ = eR∗Z ∞
1
e−ρR∗ρ2R∗−3dρ
= eR∗Z ∞
1
(e−ρρ2)R∗ρ−3dρ.
Observe that e−ρρ2 ≤4/e2 for any ρ ≥1. Hence,
(e−ρρ2)R∗= (e−ρρ2)R∗−1 · e−ρρ2 ≤(4/e2)R∗−1e−ρρ2.
Thus,
Z ∞
1
e−(ρ−1)kρk+R∗−2dρ ≤4R∗−1 · eR∗
e2(R∗−1)
·
Z ∞
1
e−ρ
ρ
dρ = 4R∗−1
eR∗−2 · 1
4 =
4
e
R∗−2
.
7.2
Lazy implementation of k-means++ER
We now describe how we can eﬃciently implement the k-means++ER algorithm using a lazy reser-
voir sampling. We remind the reader that the time of the ﬁrst jump of a Poisson process with
parameter λ is distributed as the exponential distribution with parameter λ. Imagine for a mo-
ment, that the arrival rates of our Poisson processes were constant. Then, in order to select the
ﬁrst k jumps, we would generate independent exponential random variables with parameters λ(x)
for all x and choose k smallest values among them. This algorithm is known as the reservoir sam-
pling(see Efraimidis and Spirakis (2006)). To adapt this algorithm to our needs, we need to update
the arrival rates of the exponential random variables. Loosely speaking, we do so by generating
exponential random variables with rate 1 for Poisson processes Qs(x) which are described above
and then updating the speeds λt(x) of variables st(x). We now formally describe the algorithm.
In the beginning of every round i, we recompute costs of all points in the data set.
Then,
we draw an independent exponential random variable Sx with rate 1 for every point x, and let
St(x) = Sx . We set
τt(x) = St(x)
λt(x).
Think of St(x) as the distance st(x) needs to travel till process Qs(x) jumps; λt(x) is the speed
of point st(x); and τt(x) is the time left till Qs(x) = Pt(x) jumps if the speed λt does not change.
Among all points x ∈X, we select a tentative set of centers Z for this round. The set Z contains all
points x with ti−1 + τt(x) ≤ti. This is the set of all points for which their Poisson processes would
jump in the current round if their arrival rates remained the same till the end of the round. Since
the arrival rates can only decrease in our algorithm, we know for sure that for points x outside of
Z, the corresponding processes Pt(x) will not jump in this round. Thus, we can safely ignore those
points during the current round.
24

--- Page 25 ---
We also note that in the unlikely event that the initial set Z is empty, we choose x with the
smallest time τt(x) and add it to the set of centers Ct. (This is equivalent to choosing a point
with probability proportional to cost(x, Ct) by the memorylessness property of the exponential
distribution).
The steps we described above – updating costs cost(x, Ct), drawing exponential random vari-
ables Sx, and selecting points in the set Z – can be performed in parallel using one pass over the
data set. In the rest of the current round, our algorithm deals only with the set Z whose size in
expectation is at most ℓ(see below).
While the set Z is not empty we do the following. We choose x ∈Z with the smallest value of
τt(x). This x corresponds to the process that jumps ﬁrst. Then, we perform the following updates:
We add x to the set of centers Ct. We set the “current time” t to t = t′ + τt′(x), where t′ is the
time of the previous update. If x is the ﬁrst center selected in the current round, then we let t′ to
be the time when the round started (i.e., ti−1). We recompute the arrival rates (speeds) λt(x) for
each x in Z. Finally, we update the values of all τt(x) for x ∈Z using the formula
τt(x) = St(x) −λt′(x) · (t −t′)
λt(x)
,
here λt′(x) · (t −t′) is the distance variable st(x) moved from the position where it was at time t′;
St(x) −λt′(x) · (t −t′) is the remaining distance st(x) needs to travel till the process Qt(x) jumps;
and τt(x) is the remaining time till Pt(x) jumps if we do not update its arrival rate. After we
update τt(x), we prune the set Z. Speciﬁcally, we remove from set Z all points x with t+τt(x) > ti.
As before, we know for sure that if x is removed from Z, then the corresponding processes Pt(x)
will not jump in the current round.
This algorithm simulates the process we described in the previous section. The key observation
is that Poisson processes Pt(x) we associate with points x removed from Z cannot jump in this
round and thus can be safely removed from our consideration. We now show that the expected size
of the set Z is at most ℓ. In the next section, we analyze the running time of this algorithm.
Then we show that the expected size of the set Z in the beginning of each round i + 1 is at
most ℓ. Since every point x belongs to Z with probability
Pr{x ∈Z} = Pr

Sx
cost(x, Cti) ≤
ℓ
cost(X, Cti)

= Pr

Sx ≤ℓ· cost(x, Cti)
cost(X, Cti)

.
The right hand side is the probability that the Poisson process Qs(x) with rate 1 jumps in the inter-
val of length ℓ· cost(x, Cti)/cost(X, Cti) which is upper bounded by the expected number of jumps
of Qs(x) in this interval. The expected number of jumps exactly equals ℓ· cost(x, Cti)/cost(X, Cti).
Thus, the expected size of Z is upper bounded as
E|Z| =
X
z∈X
Pr{z ∈Z} ≤
X
z∈X
ℓ· cost(z, Cti)
cost(X, Cti) = ℓ.
7.3
Run time analysis
According to our analysis above, the number of new centers chosen at each round of k-means++ER
is at most the size of set Z, which is O(ℓ) with high probability. In the beginning of every round,
we need to update costs of all data points, which requires O(nℓd) time. In each round, we also
25

--- Page 26 ---
need to maintain the rates of all points in set Z, which needs O(ℓ2d) time. Thus, the total running
time for k-means++ER with R rounds is O(Rnℓd). We note that before running our algorithm, we
can reduce the dimension d of the space to O(log k) using the Johnson–Lindenstrauss transform
(see Johnson and Lindenstrauss (1984)). This will increase the approximation factor by a factor of
(1 + ε) but make the algorithm considerably faster (see Makarychev et al. (2019), Becchetti et al.
(2019), and Boutsidis et al. (2010)).
References
A. Aggarwal, A. Deshpande, and R. Kannan. Adaptive sampling for k-means clustering. In Approx-
imation, Randomization, and Combinatorial Optimization. Algorithms and Techniques, pages
15–28. Springer, 2009.
S. Ahmadian, A. Norouzi-Fard, O. Svensson, and J. Ward. Better guarantees for k-means and
euclidean k-median by primal-dual algorithms. SIAM Journal on Computing, pages FOCS17–97,
2019.
D. Aloise, A. Deshpande, P. Hansen, and P. Popat.
Np-hardness of euclidean sum-of-squares
clustering. Machine learning, 75(2):245–248, 2009.
D. Arthur and S. Vassilvitskii. k-means++: The advantages of careful seeding. In Proceedings of
the eighteenth annual ACM-SIAM symposium on Discrete algorithms, pages 1027–1035. Society
for Industrial and Applied Mathematics, 2007.
P. Awasthi, M. Charikar, R. Krishnaswamy, and A. K. Sinop. The hardness of approximation of
euclidean k-means. In 31st International Symposium on Computational Geometry (SoCG 2015).
Schloss Dagstuhl-Leibniz-Zentrum fuer Informatik, 2015.
O. Bachem, M. Lucic, and A. Krause. Distributed and provably good seedings for k-means in
constant rounds.
In Proceedings of the 34th International Conference on Machine Learning-
Volume 70, pages 292–300. JMLR. org, 2017.
B. Bahmani, B. Moseley, A. Vattani, R. Kumar, and S. Vassilvitskii. Scalable k-means++. Pro-
ceedings of the VLDB Endowment, 5(7):622–633, 2012.
L. Becchetti, M. Bury, V. Cohen-Addad, F. Grandoni, and C. Schwiegelshohn. Oblivious dimension
reduction for k-means: beyond subspaces and the johnson–lindenstrauss lemma. In Proceedings
of the 51st Annual ACM SIGACT Symposium on Theory of Computing, pages 1039–1050, 2019.
B. Boehmke and B. M. Greenwell. Hands-on machine learning with R. CRC Press, 2019.
C. Boutsidis, A. Zouzias, and P. Drineas. Random projections for k-means clustering. In Advances
in Neural Information Processing Systems, pages 298–306, 2010.
T. Brunsch and H. Röglin. A bad instance for k-means++. Theoretical Computer Science, 505:
19–26, 2013.
D. Choo, C. Grunau, J. Portmann, and V. Rozhoň. k-means++: few more steps yield constant
approximation. In Proceedings of the 37th International Conference on Machine Learning, pages
7849–7057. JMLR. org, 2020.
26

--- Page 27 ---
S. Dasgupta. The hardness of k-means clustering. Department of Computer Science and Engineer-
ing, University of California, San Diego, 2008.
S. Dasgupta.
UCSD CSE 291,
Lecture Notes:
Geometric
Algorithms,
2013.
URL:
https://cseweb.ucsd.edu/~dasgupta/291-geom/kmeans.pdf. Last visited on 2020/06/01.
D. Dua and C. Graﬀ. UCI ML repository, 2017. URL http://archive.ics.uci.edu/ml.
P. S. Efraimidis and P. G. Spirakis. Weighted random sampling with a reservoir. Information
Processing Letters, 97(5):181–185, 2006.
R. Elber. Kdd-Cup, 2004. URL http://osmot.cs.cornell.edu/kddcup/.
W. B. Johnson and J. Lindenstrauss. Extensions of lipschitz mappings into a hilbert space. Con-
temporary mathematics, 26(189-206):1, 1984.
T. Kanungo, D. M. Mount, N. S. Netanyahu, C. D. Piatko, R. Silverman, and A. Y. Wu.
A
local search approximation algorithm for k-means clustering. Computational Geometry, 28(2):
89 – 112, 2004.
ISSN 0925-7721.
doi: https://doi.org/10.1016/j.comgeo.2004.03.003.
URL
http://www.sciencedirect.com/science/article/pii/S0925772104000215.
S. Lattanzi and C. Sohler.
A better k-means++ algorithm via local search.
In International
Conference on Machine Learning, pages 3662–3671, 2019.
E. Lee, M. Schmidt, and J. Wright. Improved and simpliﬁed inapproximability for k-means. Infor-
mation Processing Letters, 120:40–43, 2017.
S. Lloyd. Least squares quantization in pcm. IEEE transactions on information theory, 28(2):
129–137, 1982.
K. Makarychev, Y. Makarychev, M. Sviridenko, and J. Ward. A bi-criteria approximation algorithm
for k-means. Approximation, Randomization, and Combinatorial Optimization. Algorithms and
Techniques, 2016.
K. Makarychev, Y. Makarychev, and I. Razenshteyn. Performance of johnson–lindenstrauss trans-
form for k-means and k-medians clustering. In Proceedings of the 51st Annual ACM SIGACT
Symposium on Theory of Computing, pages 1027–1038, 2019.
M. Mitzenmacher and E. Upfal. Probability and computing: Randomization and probabilistic tech-
niques in algorithms and data analysis. Cambridge university press, 2017.
R. Ostrovsky, Y. Rabani, L. Schulman, and C. Swamy. The eﬀectiveness of lloyd-type methods
for the k-means problem. In 2006 47th Annual IEEE Symposium on Foundations of Computer
Science (FOCS’06), 2006.
V. Rozhoň.
Simple and sharp analysis of k-means||.
In Proceedings of the 37th International
Conference on Machine Learning, pages 7828–7837. JMLR. org, 2020.
D. Wei. A constant-factor bi-criteria approximation guarantee for k-means++. In Advances in
Neural Information Processing Systems, pages 604–612, 2016.
27

--- Page 28 ---
Appendix
In this appendix, we present our experiments, give proofs omitted in the main part of the paper,
and provide complimentary lower bounds.
A
Experiments
In this section, we present plots that show that the performance of k-means∥and “k-means++
with oversampling and pruning” algorithms are very similar in practice. Below, we compare the
following algorithms on the datasets BioTest from KDD Cup 2004 (Elber, 2004) and COVTYPE
from the UCI ML repository (Dua and Graﬀ, 2017):
• Regular k-means++. The performance of this algorithm is shown with a solid black line on
the plots below.
• k-means∥without pruning. This algorithm samples k centers using k-means∥with T = 5
rounds and ℓ= k/T.
• k-means∥. This algorithm ﬁrst samples 5k centers using k-means∥and then subsamples k
centers using k-means++. The performance of this algorithm is shown with a dashed blue
line on the plots below.
• k-means++ with oversampling and pruning. This algorithm ﬁrst samples 5k centers using
k-means++ and then subsamples k centers using k-means++.
The performance of this
algorithm is shown with a thin red line on the plots below.
For each k = 5, 10, · · · , 200, we ran these algorithms for 50 iterations and took their average.
We normalized all costs by dividing them by the cost of k-means++ with k = 1000 centers.
0
50
100
150
200
2
4
6
8
10
#centers
cost
BioTest
k-means++
BiCriteria k-means++ w/Pruning
k-means∥
10
20
30
40
50
3
4
5
6
#centers
cost
BioTest
k-means++
BiCriteria k-means++ w/Pruning
k-means∥
28

--- Page 29 ---
0
50
100
150
200
0
20
40
#centers
cost
COVTYPE
k-means++
BiCriteria k-means++ w/Pruning
k-means∥
10
20
30
40
50
10
15
20
25
30
#centers
cost
COVTYPE
k-means++
BiCriteria k-means++ w/Pruning
k-means∥
0
50
100
150
200
2
4
6
8
10
#centers
cost
BioTest
k-means++
k-means∥without Pruning
10
20
30
40
50
4
6
8
10
#centers
cost
BioTest
k-means++
k-means∥without Prunning
0
50
100
150
200
0
20
40
60
#centers
cost
COVTYPE
k-means++
k-means∥without Prunning
10
20
30
40
50
10
20
30
40
50
60
#centers
cost
COVTYPE
k-means++
k-means∥without Pruning
29

--- Page 30 ---
B
Details for Preliminaries
For any set of points Y ⊂Rd, let µ = P
x∈Y x/ |Y| be the centroid of the cluster Y. Then, the
optimal cost of Y with one center,
OPT1(Y) =
X
x∈Y
∥x −µ∥2 =
P
(x,y)∈Y×Y∥x −y∥2
2 |Y|
.
This is a well known formula which is often used for analyzing of k-means algorithms. For com-
pleteness, we give a proof below.
Proof. Consider any point z ∈Rd, then we have:
cost(Y, {z}) =
X
x∈Y
∥x −z∥2 =
X
x∈Y
∥(x −µ) + (µ −z)∥2
=
X
x∈Y

∥x −µ∥2 + ∥µ −z∥2 + 2 ⟨x −µ, µ −z⟩

=
X
x∈Y
∥x −µ∥2 + |Y| · ∥µ −z∥2 + 2
* X
x∈Y
(x −µ), µ −z
+
=
X
x∈Y
∥x −µ∥2 + |Y| · ∥µ −z∥2.
Thus, the optimal choice of z to minimize cost(Y, {z}) is µ and OPT1(Y) = P
x∈Y∥x −µ∥2.
X
x∈Y
∥x −µ∥2 =
X
x∈Y
⟨x −µ, x −µ⟩=
X
x∈Y
⟨x, x −µ⟩
=
X
x∈Y
*
x, x −
X
y∈Y
y
|Y|
+
=
1
|Y|
X
(x,y)∈Y×Y
⟨x, x −y⟩
=
1
2 |Y|


X
(x,y)∈Y×Y
⟨x, x −y⟩+
X
(x,y)∈Y×Y
⟨y, y −x⟩


=
P
(x,y)∈Y×Y∥x −y∥2
2 |Y|
.
C
Lower bounds
C.1
Lower bound on the cost of covered clusters
We show the following lower bound on the expected cost of a covered cluster in k-means++. There-
fore, the 5-approximation in Lemma 4.1 is tight.
Theorem C.1. For any ε > 0, there exists an instance of k-means such that for a set P ∈X
and a set of centers C ∈Rd, if a new center c is sampled from P with probability Pr(c = x) =
cost(x, C)/cost(P, C), then
Ec [cost(P, C ∪{c})] ≥(5 −ε)OPT1(P).
30

--- Page 31 ---
Proof. Consider the following one dimensional example, where P contains t points at 0 and one
point at 1, and the closest center already chosen in C to P is at −1.
−1
0
t
1
1
The new center c will be chosen at 0 with probability
t
t+4, and at 1 with probability
4
t+4. Then,
the expected cost of P is
Ec [cost(P, C ∪{c})] = 1 ·
t
t + 4 + t ·
4
t + 4 =
5t
t + 4;
and the optimal cost of P is OPT1(P) ≤1. Thus, by choosing t ≥4(5 −ε)/ε, we have
Ec [cost(P, C ∪{c})] ≥(5 −ε)OPT1(P).
C.2
Lower bound on the bi-criteria approximation
In this section, we show that the bi-criteria approximation bound of O(ln k
∆) is tight up to constant
factor. Our proof follows the approach by Brunsch and Röglin (2013). We show the following
theorem.
Theorem C.2.
For every k > 1 and ∆≤k, there exists an instance X of k-means such that the
bi-criteria k-means++ algorithm with k + ∆centers returns a solution of cost greater than
1
8 log k
∆· OPTk(X)
with probability at least 1 −e−
√
k/2.
Remark: This implies that the expected cost of bi-criteria k-means with k + ∆centers is at
least
1 −e−
√
k/2
8
· log k
∆· OPTk(X).
Proof. For every k and ∆≥
√
k, we consider the following instance. The ﬁrst cluster is a scaled
version of the standard simplex with N ≫k vertices centered at the origin, which is called the heavy
cluster. The length of the edges in this simplex is 1/
√
N −1. Each of the remaining k −1 clusters
contains a single point on k −1 axes, which are called light clusters. These clusters are located at
distance √α from the center of the heavy cluster and
√
2α from each other, where α = ln(k/∆)
4∆
.
For the sake of analysis, let us run k-means++ till we cover all clusters. At the ﬁrst step, the
k-means++ algorithm almost certainly selects a center from the heavy cluster since N ≫k. Then,
at each step, the algorithm can select a center either from one of uncovered light clusters or from
the heavy cluster. In the former case, we say that the algorithm hits a light cluster, and in the latter
case we say that the algorithm misses a light cluster. Below, we show that with high probability
the algorithm makes at least 2∆misses before it covers all but ∆light clusters.
31

--- Page 32 ---
Lemma C.3. Let ∆≥
√
k. By the time the k-means++ algorithm covers all but ∆light clusters,
it makes greater than 2∆misses with probability at least 1 −e−
√
k/2.
Proof sketch. Let ε = 1/
√
N. Observe that k-means++ almost certainly covers all clusters in εN
steps (since N ≫k). So in the rest of this proof sketch, we assume that the number chosen centers
is at most εN and, consequently, at least (1 −ε)N points in the heavy cluster are not selected as
centers. Hence, the cost of the heavy cluster is at least 1 −ε.
Consider a step of the algorithm when exactly u light clusters remain uncovered. At this step,
the total cost of all light clusters is αu (we assume for simplicity that distance between the light
clusters and the closest chosen center in the heavy cluster is the same as the distance to the origin).
The cost of the heavy cluster is at least 1 −ε. The probability that the algorithm chooses a center
from the heavy cluster and thus misses a light cluster is at least (1 −ε)/(1 + αu).
Deﬁne random variables {Xu} as follows. Let Xu = 1 if the algorithm misses a cluster at least
once when the number of uncovered light clusters is u; and let Xu = 0, otherwise. Then, {Xu} are
independent Bernoulli random variables. For each u, we have P [Xu = 1] ≥(1 −ε)/(1 + αu).
Observe that the total number of misses is lower bounded by Pk−1
u=∆Xu. Then, we have
E
" k−1
X
u=∆
Xu
#
≥(1 −ε)
k−1
X
u=∆
1
1 + αu ≥(1 −ε)
Z k
∆
du
1 + αu
= (1 −ε)α−1 ln 1 + αk
1 + α∆
≥(1 −ε)α−1 ln k
∆= 4(1 −ε)∆.
Let µ = E
hPk−1
u=∆Xu
i
≥4(1 −ε)∆. By the Chernoﬀbound for Bernoulli random variables, we
have
P
"
k
X
u=∆
Xu ≤2∆
#
≤e−µ
 eµ
2∆
2∆
.
Since f(x) = e−x( ex
2∆)2∆is a monotone decreasing function for x ≥2∆, we have
P
"
k
X
u=∆
Xu ≤2∆
#
≤e−(2−4ε)∆· 22∆≤e−∆/2.
Hence, with probability as least 1 −e−
√
k/2, the number of misses is greater than 2∆.
For every k and ∆≥
√
k, consider the instance we constructed. By Lemma C.3, the algorithm
chooses more than k+∆centers to cover all but ∆light clusters with probability at least 1−e−
√
k/2.
Thus, at the time when the algorithm chose k + ∆centers, the number of uncovered light clusters
was greater than ∆. Hence, in the clustering with k + ∆centers sampled by k-means++, the total
cost is at least 1
4 ln (k/∆), while the cost of the optimal solution with k clusters is 1. For every k
and ∆<
√
k, the total cost is at least 1
4 ln(k/∆′) with ∆′ =
√
k extra centers, which concludes the
proof.
32
