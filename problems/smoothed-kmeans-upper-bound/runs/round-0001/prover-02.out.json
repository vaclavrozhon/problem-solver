{
  "progress_md": "Ideas and setup\n- Goal: Bound E_k,++[ALG(k)/OPT(k)] when k ~ Uniform{k0,...,2k0-1}.\n- Key tool: Bi-criteria guarantees for k-means++ (Makarychev\u2013Reddy\u2013Shan 2020, Theorem 5.1): for baseline parameter r and oversampling \u0394 \u2265 1, the expected cost with r+\u0394 centers satisfies\n  E[cost_{r+\u0394}(X)] \u2264 5 \u00b7 min{ 2 + 1/(2e) + ln(2r/\u0394), 1 + r/(e(\u0394\u22121)) } \u00b7 OPT_r(X).\n- Main observation: Running k-means++ for k centers can be viewed as running it for s centers and then oversampling by \u0394 = k \u2212 s additional centers. If OPT(s) \u2248 OPT(k), we can evaluate the k-step run using the bi-criteria bound against OPT(s) and then translate to OPT(k).\n\nA per-k oversampling-based bound (rigorous)\n- Definition (2-plateau anchor). For any k \u2265 1, define\n  s(k) := min{ s \u2208 {1,\u2026,k} : OPT(s) \u2264 2 \u00b7 OPT(k) } and \u0394(k) := k \u2212 s(k).\n  Thus s(k) is the earliest index whose OPT is within a factor 2 of OPT(k). Always s(k) \u2264 k and \u0394(k) \u2265 0.\n- Lemma 1 (per-k bound). For any k with \u0394(k) \u2265 1,\n  E_{++}[ALG(k)] \u2264 10 \u00b7 min{ 2 + 1/(2e) + ln(2 s(k)/\u0394(k)), 1 + s(k)/(e(\u0394(k)\u22121)) } \u00b7 OPT(k).\n  For \u0394(k) = 0, the standard bound E[ALG(k)] \u2264 5(ln k + 2) \u00b7 OPT(k) applies.\n  Proof sketch: Run k-means++ for k steps. View the first s(k) steps as producing s(k) centers, then \u0394(k) additional centers. By Makarychev\u2013Reddy\u2013Shan (MRS) Theorem 5.1 with baseline r = s(k) and oversampling \u0394 = \u0394(k),\n  E[cost after s(k)+\u0394(k) steps] \u2264 5 \u00b7 min{2 + 1/(2e) + ln(2 s(k)/\u0394(k)), 1 + s(k)/(e(\u0394(k)\u22121))} \u00b7 OPT_{s(k)}.\n  Since s(k)+\u0394(k) = k, this equals E[ALG(k)]. Also OPT_{s(k)} \u2264 2\u00b7OPT_k by definition of s(k). Multiply by 2 to get the stated bound. For \u0394(k) = 0 we fall back to the MRS refined Arthur\u2013Vassilvitskii bound 5(ln k + 2).\n- Why useful here: This cleanly converts the \u201ceffective oversampling\u201d intuition into a computable per-k bound in terms of \u0394(k), allowing averaging over k.\n\nAveraging on a genuine plateau\n- Consider a true 2-plateau I = [a, b] \u2286 [k0, 2k0), meaning OPT(k) \u2208 [OPT(a), 2\u00b7OPT(a)] for all k \u2208 I. Then s(k) = a for all k \u2208 I, and \u0394(k) = k \u2212 a.\n- Proposition 2 (plateau average). Let L = b \u2212 a + 1 \u2265 2. For uniform k over I and ignoring the negligible \u0394=0 endpoint, the average of the logarithmic term admits\n  (1/L) \u03a3_{k\u2208I, \u0394\u22651} ln(2 s(k)/\u0394(k)) = ln(2a) \u2212 (1/(L\u22121)) \u03a3_{m=1}^{L\u22121} ln m = ln(2a) \u2212 (ln((L\u22121)!))/(L\u22121).\n  Using Stirling, ln((L\u22121)!)/(L\u22121) = ln(L\u22121) \u2212 1 + o(1).\n  Therefore the plateau-average of the log term is \u2248 1 + ln(2a/(L\u22121)) up to o(1).\n- Corollary 3 (expected ratio on a plateau). On a 2-plateau I = [a,b] of length L \u2265 2, we obtain\n  E_{k\u2208I, ++}[ ALG(k)/OPT(k) ] \u2264 O( 1 + ln( a/L )_+ ),\n  where x_+ = max{x,0}. In particular:\n  - If L \u2265 a/c for a constant c, the average is O(1).\n  - If L \u2265 a / polylog(a), the average is O(log log a).\n  - For half of the k in I (those with \u0394 \u2265 L/2), the \u201clarge-\u0394\u201d bi-criteria branch yields the constant bound 10 \u00b7 (1 + a/(e(\u0394\u22121))) = O( a/(e L) + 1 ); thus, if L = \u03a9(a), we get a constant-factor approximation with probability \u2265 1/2 over k in I.\n- Why useful: This formalizes the \u201coversampling on a plateau\u201d effect and quantifies how long plateaus translate into improved average guarantees under random k.\n\nDecomposition across [k0, 2k0)\n- Partition [k0, 2k0) into maximal 2-plateaus I1,\u2026,Im with lengths L_j and starts a_j. Averaging Lemma 1 across k and applying Proposition 2 on each plateau yields\n  E_{k\u223cU}[ALG(k)/OPT(k)] \u2264 O(1/k0) \u00b7 \u03a3_j L_j \u00b7 ( 1 + ln( a_j / L_j )_+ ) + tail terms where \u0394(k)=0.\n- Consequences:\n  - If at least one plateau satisfies L \u2265 a/ polylog(a) with a \u2265 \u0398(k0), its contribution already gives an O(log log k0) bound (and even O(1) if L = \u0398(a)).\n  - If all plateaus are \u201cshort,\u201d i.e., L_j \u2264 a_j / h(a_j) for some growth function h(\u00b7), then their average contribution is upper-bounded by O(1/k0) \u03a3_j L_j \u00b7 (1 + ln h(a_j)). Since \u03a3_j L_j = k0, this becomes O( 1 + (1/k0) \u03a3_j L_j ln h(a_j) ). If h(a) grows subpolynomially (e.g., ln^c a), we get O(log log k0). If h is constant (all L_j \u226a a_j), this argument alone does not beat O(log k0).\n- Gap: To convert this into a universal O(log log k0) bound, we need a structural inequality relating the multiscale distribution of lengths L_j to starts a_j (or to the overall decrease OPT(k0)/OPT(2k0\u22121)). Such a universal inequality may not hold, so a worst-case instance might still force an \u03a9(log k0) average; see Obstacles below.\n\nStrengthened coverage lemma for \u201cheavy\u201d part (towards many-short-plateau regime)\n- Hypothesis (restated with precision). Fix k1,k2 with k = k1 + k2. Assume strong scale separation: OPT(k1) \u2265 k^C \u00b7 OPT(k) for large constant C. Run k-means++ for k1+(1+\u03b5)k2 steps. Then with probability \u2265 1 \u2212 k^{\u2212\u03a9(1)}, before any collision among the heavy k1 optimal clusters, k-means++ places one center in each heavy cluster; afterwards, bi-criteria analysis on the remaining k2 clusters yields total cost O( log(1/\u03b5) ) \u00b7 OPT(k).\n- Why plausible: When OPT(k1) dominates, uncovered heavy clusters contribute almost all D^2 mass at early steps, so the chance of sampling twice in the same heavy cluster before hitting all heavy clusters is small by the MRS potential analysis refined Lemma 4.1 (5-approx on newly covered clusters) and the standard coupon-collector-with-drifting-probabilities style argument. Once heavy clusters are hit, the residual instance has OPT equal to OPT(k) up to negligible additive terms; oversampling on that part by (1+\u03b5) gives O(log(1/\u03b5)).\n- Sketch of a proof plan:\n  1) Formalize \u201cheavy clusters\u201d as those in the optimal k1-partition; show that at any point t \u2264 k1, the conditional probability of hitting an uncovered heavy cluster at step t is \u2265 1 \u2212 k^{\u2212\u03a9(1)} by comparing U_t(heavy)/cost_t(X) using the scale separation and Lemma 4.1 (eHt supermartingale) to control covered mass growth.\n  2) Union bound over k1 steps to argue all heavy clusters get covered without collision w.h.p.\n  3) Conditioned on this, apply the bi-criteria bound to the remaining k2 clusters with (1+\u03b5) oversampling.\n- Why useful here: If the interval [k0,2k0) has many short plateaus at different scales, we hope to peel off \u201cheavy\u201d parts with strong separation repeatedly and apply the above lemma, leading to O(log log k0) in aggregate even when no single long plateau exists.\n- Status: This remains a conjectural lemma; the potential-based supermartingale control (MRS Cor. 4.5) provides a credible path to bounding collision probabilities, but a complete high-probability argument needs careful tracking of the drift of U_t(heavy) and H_t.\n\nExamples and sanity checks\n- Power-of-two plateaus. Suppose OPT(k) halves only when k doubles: OPT(k) \u2248 \u0398(1/\u230alog_2 k\u230b) on dyadic blocks. Then within [k0,2k0), there is a single long 2-plateau I with L \u2248 a, and the plateau average from Proposition 2 gives O(1) expected ratio.\n- Unhelpful case: \u201cmicroscopic\u201d drops. If OPT(k\u22121) \u2264 2\u00b7OPT(k) for all k (always true) and moreover OPT(k\u22121) \u2248 (1+o(1)) OPT(k) (tiny drops), then s(k) \u2248 k\u22121 so \u0394(k)=1 almost everywhere. Lemma 1 reduces to the refined fixed-k bound 5(ln k + 2), offering no improvement. It is unclear whether adversarial k-means instances can enforce \u0394(k)=1 for a 1\u2212o(1) fraction of k in [k0,2k0); constructing such a family would show that randomizing k cannot help in the worst case beyond O(log k0).\n\nObstacles and possible lower bounds\n- It seems plausible that there exist instances where \u0394(k) is small for most k in [k0,2k0), yielding an \u03a9(log k0) average. A candidate is to adapt Arthur\u2013Vassilvitskii\u2019s lower-bound construction (regular simplices per cluster with large inter-cluster spacing) to be robust for all k in a range, e.g., create \u0398(2k0) nearly orthogonal tiny-variance clusters at growing radii so that OPT(k) shrinks steadily with each additional center, preventing plateaus. This requires verifying that OPT(k\u22121)/OPT(k) \u2272 constant for most steps.\n- Action item: Attempt to formalize an instance family with OPT(k\u22121) \u2265 c \u00b7 OPT(k) for a fixed c < 2 across most k, and analyze the expectation of the k-means++ ratio under uniform k; if successful, this would give a matching \u03a9(log k0) lower bound in the random-k model.\n\nNext steps (concrete, checkable)\n1) Full proof of Lemma 1 with constants, including the \u0394=0 fallback, and present as a self-contained lemma to be curated into output.md.\n2) Write and verify the exact Stirling-based bound in Proposition 2: for a \u2265 2 and L \u2265 2, compute the plateau average explicitly and bound the o(1) terms to a clean inequality (e.g., \u2264 1 + ln(2a/(L\u22121)) + 2/(L\u22121)).\n3) Derive a corollary for the probability-0.01 guarantee: On a plateau with L \u2265 a/100, P_k[ALG(k)/OPT(k) \u2264 C] \u2265 0.5 for some absolute C (from the large-\u0394 branch with \u0394 \u2265 L/2); quantify C.\n4) Attempt a high-probability version of the heavy-coverage lemma under an explicit scale-separation threshold (e.g., OPT(k1)/OPT(k) \u2265 k^8); start by proving that the expected number of collisions among heavy clusters before all are hit is o(1).\n5) Explore a lower-bound instance robust to random k: Formalize a sequence of nested simplices and radii ensuring \u0394(k)=1 for \u2265 0.9 k0 choices of k in [k0,2k0). If achievable, this would indicate the random-k smoothing cannot universally beat O(log k0).\n\nNotes on use of references\n- Arthur\u2013Vassilvitskii (2007) provides the baseline O(ln k) bound and the structural induction we reused.\n- Makarychev\u2013Reddy\u2013Shan (2020) gives the refined 5-constant and, crucially, the bi-criteria bounds we leveraged in Lemma 1.\n\nGaps relative to a final result\n- The unconditional average bound over random k remains O(log k0) at present. The proposed improvements are conditional: either the presence of a long 2-plateau or a (conjectured) heavy-coverage lemma enabling aggregation across short plateaus. A universal O(log log k0) upper bound is not established.\n- No lower bound in the random-k model is yet proven; constructing one would clarify optimality.\n"
}