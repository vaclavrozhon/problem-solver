{"progress_md":"Ideas and setup\n- Goal: Bound E_k,++[ALG(k)/OPT(k)] when k ~ Uniform{k0,...,2k0-1}.\n- Key tool: Bi-criteria guarantees for k-means++ (Makarychev–Reddy–Shan 2020, Theorem 5.1): for baseline parameter r and oversampling Δ ≥ 1, the expected cost with r+Δ centers satisfies\n  E[cost_{r+Δ}(X)] ≤ 5 · min{ 2 + 1/(2e) + ln(2r/Δ), 1 + r/(e(Δ−1)) } · OPT_r(X).\n- Main observation: Running k-means++ for k centers can be viewed as running it for s centers and then oversampling by Δ = k − s additional centers. If OPT(s) ≈ OPT(k), we can evaluate the k-step run using the bi-criteria bound against OPT(s) and then translate to OPT(k).\n\nA per-k oversampling-based bound (rigorous)\n- Definition (2-plateau anchor). For any k ≥ 1, define\n  s(k) := min{ s ∈ {1,…,k} : OPT(s) ≤ 2 · OPT(k) } and Δ(k) := k − s(k).\n  Thus s(k) is the earliest index whose OPT is within a factor 2 of OPT(k). Always s(k) ≤ k and Δ(k) ≥ 0.\n- Lemma 1 (per-k bound). For any k with Δ(k) ≥ 1,\n  E_{++}[ALG(k)] ≤ 10 · min{ 2 + 1/(2e) + ln(2 s(k)/Δ(k)), 1 + s(k)/(e(Δ(k)−1)) } · OPT(k).\n  For Δ(k) = 0, the standard bound E[ALG(k)] ≤ 5(ln k + 2) · OPT(k) applies.\n  Proof sketch: Run k-means++ for k steps. View the first s(k) steps as producing s(k) centers, then Δ(k) additional centers. By Makarychev–Reddy–Shan (MRS) Theorem 5.1 with baseline r = s(k) and oversampling Δ = Δ(k),\n  E[cost after s(k)+Δ(k) steps] ≤ 5 · min{2 + 1/(2e) + ln(2 s(k)/Δ(k)), 1 + s(k)/(e(Δ(k)−1))} · OPT_{s(k)}.\n  Since s(k)+Δ(k) = k, this equals E[ALG(k)]. Also OPT_{s(k)} ≤ 2·OPT_k by definition of s(k). Multiply by 2 to get the stated bound. For Δ(k) = 0 we fall back to the MRS refined Arthur–Vassilvitskii bound 5(ln k + 2).\n- Why useful here: This cleanly converts the “effective oversampling” intuition into a computable per-k bound in terms of Δ(k), allowing averaging over k.\n\nAveraging on a genuine plateau\n- Consider a true 2-plateau I = [a, b] ⊆ [k0, 2k0), meaning OPT(k) ∈ [OPT(a), 2·OPT(a)] for all k ∈ I. Then s(k) = a for all k ∈ I, and Δ(k) = k − a.\n- Proposition 2 (plateau average). Let L = b − a + 1 ≥ 2. For uniform k over I and ignoring the negligible Δ=0 endpoint, the average of the logarithmic term admits\n  (1/L) Σ_{k∈I, Δ≥1} ln(2 s(k)/Δ(k)) = ln(2a) − (1/(L−1)) Σ_{m=1}^{L−1} ln m = ln(2a) − (ln((L−1)!))/(L−1).\n  Using Stirling, ln((L−1)!)/(L−1) = ln(L−1) − 1 + o(1).\n  Therefore the plateau-average of the log term is ≈ 1 + ln(2a/(L−1)) up to o(1).\n- Corollary 3 (expected ratio on a plateau). On a 2-plateau I = [a,b] of length L ≥ 2, we obtain\n  E_{k∈I, ++}[ ALG(k)/OPT(k) ] ≤ O( 1 + ln( a/L )_+ ),\n  where x_+ = max{x,0}. In particular:\n  - If L ≥ a/c for a constant c, the average is O(1).\n  - If L ≥ a / polylog(a), the average is O(log log a).\n  - For half of the k in I (those with Δ ≥ L/2), the “large-Δ” bi-criteria branch yields the constant bound 10 · (1 + a/(e(Δ−1))) = O( a/(e L) + 1 ); thus, if L = Ω(a), we get a constant-factor approximation with probability ≥ 1/2 over k in I.\n- Why useful: This formalizes the “oversampling on a plateau” effect and quantifies how long plateaus translate into improved average guarantees under random k.\n\nDecomposition across [k0, 2k0)\n- Partition [k0, 2k0) into maximal 2-plateaus I1,…,Im with lengths L_j and starts a_j. Averaging Lemma 1 across k and applying Proposition 2 on each plateau yields\n  E_{k∼U}[ALG(k)/OPT(k)] ≤ O(1/k0) · Σ_j L_j · ( 1 + ln( a_j / L_j )_+ ) + tail terms where Δ(k)=0.\n- Consequences:\n  - If at least one plateau satisfies L ≥ a/ polylog(a) with a ≥ Θ(k0), its contribution already gives an O(log log k0) bound (and even O(1) if L = Θ(a)).\n  - If all plateaus are “short,” i.e., L_j ≤ a_j / h(a_j) for some growth function h(·), then their average contribution is upper-bounded by O(1/k0) Σ_j L_j · (1 + ln h(a_j)). Since Σ_j L_j = k0, this becomes O( 1 + (1/k0) Σ_j L_j ln h(a_j) ). If h(a) grows subpolynomially (e.g., ln^c a), we get O(log log k0). If h is constant (all L_j ≪ a_j), this argument alone does not beat O(log k0).\n- Gap: To convert this into a universal O(log log k0) bound, we need a structural inequality relating the multiscale distribution of lengths L_j to starts a_j (or to the overall decrease OPT(k0)/OPT(2k0−1)). Such a universal inequality may not hold, so a worst-case instance might still force an Ω(log k0) average; see Obstacles below.\n\nStrengthened coverage lemma for “heavy” part (towards many-short-plateau regime)\n- Hypothesis (restated with precision). Fix k1,k2 with k = k1 + k2. Assume strong scale separation: OPT(k1) ≥ k^C · OPT(k) for large constant C. Run k-means++ for k1+(1+ε)k2 steps. Then with probability ≥ 1 − k^{−Ω(1)}, before any collision among the heavy k1 optimal clusters, k-means++ places one center in each heavy cluster; afterwards, bi-criteria analysis on the remaining k2 clusters yields total cost O( log(1/ε) ) · OPT(k).\n- Why plausible: When OPT(k1) dominates, uncovered heavy clusters contribute almost all D^2 mass at early steps, so the chance of sampling twice in the same heavy cluster before hitting all heavy clusters is small by the MRS potential analysis refined Lemma 4.1 (5-approx on newly covered clusters) and the standard coupon-collector-with-drifting-probabilities style argument. Once heavy clusters are hit, the residual instance has OPT equal to OPT(k) up to negligible additive terms; oversampling on that part by (1+ε) gives O(log(1/ε)).\n- Sketch of a proof plan:\n  1) Formalize “heavy clusters” as those in the optimal k1-partition; show that at any point t ≤ k1, the conditional probability of hitting an uncovered heavy cluster at step t is ≥ 1 − k^{−Ω(1)} by comparing U_t(heavy)/cost_t(X) using the scale separation and Lemma 4.1 (eHt supermartingale) to control covered mass growth.\n  2) Union bound over k1 steps to argue all heavy clusters get covered without collision w.h.p.\n  3) Conditioned on this, apply the bi-criteria bound to the remaining k2 clusters with (1+ε) oversampling.\n- Why useful here: If the interval [k0,2k0) has many short plateaus at different scales, we hope to peel off “heavy” parts with strong separation repeatedly and apply the above lemma, leading to O(log log k0) in aggregate even when no single long plateau exists.\n- Status: This remains a conjectural lemma; the potential-based supermartingale control (MRS Cor. 4.5) provides a credible path to bounding collision probabilities, but a complete high-probability argument needs careful tracking of the drift of U_t(heavy) and H_t.\n\nExamples and sanity checks\n- Power-of-two plateaus. Suppose OPT(k) halves only when k doubles: OPT(k) ≈ Θ(1/⌊log_2 k⌋) on dyadic blocks. Then within [k0,2k0), there is a single long 2-plateau I with L ≈ a, and the plateau average from Proposition 2 gives O(1) expected ratio.\n- Unhelpful case: “microscopic” drops. If OPT(k−1) ≤ 2·OPT(k) for all k (always true) and moreover OPT(k−1) ≈ (1+o(1)) OPT(k) (tiny drops), then s(k) ≈ k−1 so Δ(k)=1 almost everywhere. Lemma 1 reduces to the refined fixed-k bound 5(ln k + 2), offering no improvement. It is unclear whether adversarial k-means instances can enforce Δ(k)=1 for a 1−o(1) fraction of k in [k0,2k0); constructing such a family would show that randomizing k cannot help in the worst case beyond O(log k0).\n\nObstacles and possible lower bounds\n- It seems plausible that there exist instances where Δ(k) is small for most k in [k0,2k0), yielding an Ω(log k0) average. A candidate is to adapt Arthur–Vassilvitskii’s lower-bound construction (regular simplices per cluster with large inter-cluster spacing) to be robust for all k in a range, e.g., create Θ(2k0) nearly orthogonal tiny-variance clusters at growing radii so that OPT(k) shrinks steadily with each additional center, preventing plateaus. This requires verifying that OPT(k−1)/OPT(k) ≲ constant for most steps.\n- Action item: Attempt to formalize an instance family with OPT(k−1) ≥ c · OPT(k) for a fixed c < 2 across most k, and analyze the expectation of the k-means++ ratio under uniform k; if successful, this would give a matching Ω(log k0) lower bound in the random-k model.\n\nNext steps (concrete, checkable)\n1) Full proof of Lemma 1 with constants, including the Δ=0 fallback, and present as a self-contained lemma to be curated into output.md.\n2) Write and verify the exact Stirling-based bound in Proposition 2: for a ≥ 2 and L ≥ 2, compute the plateau average explicitly and bound the o(1) terms to a clean inequality (e.g., ≤ 1 + ln(2a/(L−1)) + 2/(L−1)).\n3) Derive a corollary for the probability-0.01 guarantee: On a plateau with L ≥ a/100, P_k[ALG(k)/OPT(k) ≤ C] ≥ 0.5 for some absolute C (from the large-Δ branch with Δ ≥ L/2); quantify C.\n4) Attempt a high-probability version of the heavy-coverage lemma under an explicit scale-separation threshold (e.g., OPT(k1)/OPT(k) ≥ k^8); start by proving that the expected number of collisions among heavy clusters before all are hit is o(1).\n5) Explore a lower-bound instance robust to random k: Formalize a sequence of nested simplices and radii ensuring Δ(k)=1 for ≥ 0.9 k0 choices of k in [k0,2k0). If achievable, this would indicate the random-k smoothing cannot universally beat O(log k0).\n\nNotes on use of references\n- Arthur–Vassilvitskii (2007) provides the baseline O(ln k) bound and the structural induction we reused.\n- Makarychev–Reddy–Shan (2020) gives the refined 5-constant and, crucially, the bi-criteria bounds we leveraged in Lemma 1.\n\nGaps relative to a final result\n- The unconditional average bound over random k remains O(log k0) at present. The proposed improvements are conditional: either the presence of a long 2-plateau or a (conjectured) heavy-coverage lemma enabling aggregation across short plateaus. A universal O(log log k0) upper bound is not established.\n- No lower bound in the random-k model is yet proven; constructing one would clarify optimality.\n"}