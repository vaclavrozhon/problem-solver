--- SYSTEM ---
You are a research mathematician. 

You are being run in a loop with a verifier that checks rigor. Your goal is to make **incremental, auditable progress**. You are not expected to solve the problem at once, but to generate new, testable ideas.

Inputs: 
- task statement 
- notes.md (summary of past progress; optional) 
- output.md (rigorously proven results curated so far; optional) 
- summary of past rounds (optional) 
- possibly some reference papers.

What counts as progress:
- Extract small lemmas/heuristics from literature and state them cleanly with one-line "why useful here".
- Explore small examples & try to break your own claims with toy counterexamples.
- Prove special cases or nontrivial bounds.
- If an approach fails, explain crisply why.
- Point out flaws in notes.md or output.md (but do not rewrite output.md yourself).

**Discipline.** 
- Read notes, outputs, summaries carefully before proposing new work. 
- Reference papers if relevant, but focus on *incremental, checkable steps*. 
- Do not output Markdown code fences, only raw JSON. 
- Length: at least ~200 words. 
- Organize your reasoning with short headings (Ideas, Examples, Obstacles, Next steps), make clear what your claims are and how they are supported. 
- Remember: the verifier curates notes and outputs, you only suggest.

**Return strictly JSON**:
{
  "progress_md": "Your progress notes for this round in Markdown (KaTeX allowed). Point out any gaps in output.md clearly. Do not modify output.md directly."
}


--- USER ---
Work on this problem context:

=== task.tex ===
\newcommand{\X}{X}
\newcommand{\R}{\mathbb{R}}
\newcommand{\cost}{\phi}
\newcommand{\OPT}{\mathrm{OPT}}
\newcommand{\ALG}{\mathrm{ALG}}
%\newcommand{\E}{\mathbb{E}}

\section*{Algorithm, model, and task}

\paragraph{Objective.}
Fix a dataset $\X=\{x_1,\dots,x_n\}\subseteq\R^d$ and the standard $k$-means cost
\begin{equation*}
\cost(C) \;=\; \sum_{x\in \X} \min_{c\in C} \|x-c\|_2^2
\qquad\text{for a center set } C\subseteq\R^d,\ |C|=k.
\end{equation*}
Let $\OPT(k)=\min_{|C|=k}\cost(C)$ and let $\ALG(k)$ denote the cost of the clustering returned by the $k$-means++ seeding.

\paragraph{$k$-means++ seeding (Arthur--Vassilvitskii).}
Pick the first center uniformly from $\X$. Then, iteratively, sample the next center $c$ from $\X$ with probability proportional to $D(x)^2$, where $D(x)$ is the distance from $x$ to its closest previously chosen center. Stop after $k$ centers are chosen; assign each $x$ to its nearest chosen center.

\paragraph{Smoothed random-$k$ experiment.}
Given an integer $k_0\ge 1$, draw $k$ uniformly at random from $\{k_0,k_0{+}1,\dots,2k_0{-}1\}$, run $k$-means++ with this $k$ on $\X$, and set the random variable
\begin{equation*}
\alpha \;=\; \frac{\ALG(k)}{\OPT(k)} \;\ge\; 1.
\end{equation*}

\section*{The task}
Prove the strongest possible asymptotic upper bound on
\begin{equation*}
\E_{k\sim U([k_0,2k_0)),\,\text{$k$-means++}}\!\!\Big[\; \tfrac{\ALG(k)}{\OPT(k)} \;\Big],
\end{equation*}
as a function of $k_0$. Alternatively, prove the strongest possible upper bound on the value that $\tfrac{\ALG(k)}{\OPT(k)}$ achieves with probability at least $0.01$. 

\section*{Context and prior results}

\paragraph{Baseline (fixed $k$).}
For standard $k$-means++ with $k$ centers, $\E[\ALG(k)]\le O(\log k)\cdot \OPT(k)$ and there are instances with $\E[\ALG(k)]\ge \Omega(\log k)\cdot \OPT(k)$.

\paragraph{Oversampling phenomenon.}
Let $k'=(1{+}\varepsilon)k$. Results in the spirit of Theorem~5.1 of an attached paper show that sampling $k'$ centers by $D^2$-sampling (and evaluating against $\OPT(k)$) yields an expected $O(\log (1/\varepsilon))$-approximation. Intuitively, a mild amount of \emph{oversampling} dramatically sharpens the guarantee.

\paragraph{Why randomizing $k$ might help.}
As $k$ increases, $\OPT(k)$ decreases. Group the values of $OPT(k)$ by $\lfloor \log_2 (OPT(k)) \rfloor$. If there is a long "plateau" where $OPT$ stays roughly the same, up to $2$-factor, $k$-means++ effectively oversamples relative to the number of ``active'' clusters present on that scale. This could lead to a better guarantee. 


\section*{Possible approaches and technical ideas}

\paragraph{Important note}
Take the following section only as an inspiration of what might work. Take it with a huge grain of salt. 

\paragraph{1.\ Plateau decomposition of $\OPT(k)$}
Let $k_0=k^{(0)}<k^{(1)}<\cdots<k^{(m)}\le 2k_0$ be the indices at which $\OPT(\cdot)$ drops by a constant factor; define plateaus $I_j=[k^{(j)},k^{(j+1)}\!)$ on which $\OPT(k)$ is constant.
\begin{itemize}
  \item \textbf{Long vs.\ short plateaus.} Call $I_j$ \emph{long} if $|I_j|\ge k_0/\mathrm{poly}\log k_0$, else \emph{short}.
  \item \textbf{Averaging benefit on long plateaus.} If $k$ is uniform over a long plateau and $\OPT$ does not change, then running $k$-means++ with parameter $k$ is akin to oversampling relative to the \emph{effective} cluster count on that scale. Using Theorem~5.1 from the reference with $\varepsilon \approx |I_j|/k$ suggests an $O(\log\log k_0)$ \emph{conditional} bound on that plateau.
  \item \textbf{Bounding the mass of short plateaus.} Either there are few short plateaus at each scale (so their contribution in the uniform average is small), or there are many in a row; in the latter case, we can try to invoke a strengthened oversampling statement (see next) to regain a $\log\log$ bound.
\end{itemize}

\paragraph{2.\ A coverage lemma for the ``heavy'' part}
\label{subsec:coverage}
Aim to prove a statement of the form:

\medskip
\noindent\emph{Hypothesis (strengthening of Thm.~5.1).}
Let $k=k_1{+}k_2$. Suppose $\OPT(k_1) \ge k^{10}\,\OPT(k)$ (a strong scale separation).
Run $k$-means++ for $k'=k_1 + (1+\epsilon)k_2 $ steps. Then, with probability $1 - 1/k^{O(1)}$, the first $k_1$ steps place one center in each of the $k_1$ ``heavy'' clusters before any collision, and overall the returned cost is $O(\log(1/\varepsilon))\cdot \OPT(k)$.

\medskip
\noindent\textbf{Why plausible.}
Under strong separation, the contribution to $D^2$ from uncovered heavy clusters dominates. By the standard $k$-means++ potential argument, the chance to hit an already covered heavy cluster in the early steps is small. 

=== progress.md ===
## Round 0001 — 2025-08-27T15:22:33.907638Z

Ideas and setup
- Goal: Bound E_k,++[ALG(k)/OPT(k)] when k ~ Uniform{k0,...,2k0-1}.
- Key tool: Bi-criteria guarantees for k-means++ (Makarychev–Reddy–Shan 2020, Theorem 5.1): for baseline parameter r and oversampling Δ ≥ 1, the expected cost with r+Δ centers satisfies
  E[cost_{r+Δ}(X)] ≤ 5 · min{ 2 + 1/(2e) + ln(2r/Δ), 1 + r/(e(Δ−1)) } · OPT_r(X).
- Main observation: Running k-means++ for k centers can be viewed as running it for s centers and then oversampling by Δ = k − s additional centers. If OPT(s) ≈ OPT(k), we can evaluate the k-step run using the bi-criteria bound against OPT(s) and then translate to OPT(k).

A per-k oversampling-based bound (rigorous)
- Definition (2-plateau anchor). For any k ≥ 1, define
  s(k) := min{ s ∈ {1,…,k} : OPT(s) ≤ 2 · OPT(k) } and Δ(k) := k − s(k).
  Thus s(k) is the earliest index whose OPT is within a factor 2 of OPT(k). Always s(k) ≤ k and Δ(k) ≥ 0.
- Lemma 1 (per-k bound). For any k with Δ(k) ≥ 1,
  E_{++}[ALG(k)] ≤ 10 · min{ 2 + 1/(2e) + ln(2 s(k)/Δ(k)), 1 + s(k)/(e(Δ(k)−1)) } · OPT(k).
  For Δ(k) = 0, the standard bound E[ALG(k)] ≤ 5(ln k + 2) · OPT(k) applies.
  Proof sketch: Run k-means++ for k steps. View the first s(k) steps as producing s(k) centers, then Δ(k) additional centers. By Makarychev–Reddy–Shan (MRS) Theorem 5.1 with baseline r = s(k) and oversampling Δ = Δ(k),
  E[cost after s(k)+Δ(k) steps] ≤ 5 · min{2 + 1/(2e) + ln(2 s(k)/Δ(k)), 1 + s(k)/(e(Δ(k)−1))} · OPT_{s(k)}.
  Since s(k)+Δ(k) = k, this equals E[ALG(k)]. Also OPT_{s(k)} ≤ 2·OPT_k by definition of s(k). Multiply by 2 to get the stated bound. For Δ(k) = 0 we fall back to the MRS refined Arthur–Vassilvitskii bound 5(ln k + 2).
- Why useful here: This cleanly converts the “effective oversampling” intuition into a computable per-k bound in terms of Δ(k), allowing averaging over k.

Averaging on a genuine plateau
- Consider a true 2-plateau I = [a, b] ⊆ [k0, 2k0), meaning OPT(k) ∈ [OPT(a), 2·OPT(a)] for all k ∈ I. Then s(k) = a for all k ∈ I, and Δ(k) = k − a.
- Proposition 2 (plateau average). Let L = b − a + 1 ≥ 2. For uniform k over I and ignoring the negligible Δ=0 endpoint, the average of the logarithmic term admits
  (1/L) Σ_{k∈I, Δ≥1} ln(2 s(k)/Δ(k)) = ln(2a) − (1/(L−1)) Σ_{m=1}^{L−1} ln m = ln(2a) − (ln((L−1)!))/(L−1).
  Using Stirling, ln((L−1)!)/(L−1) = ln(L−1) − 1 + o(1).
  Therefore the plateau-average of the log term is ≈ 1 + ln(2a/(L−1)) up to o(1).
- Corollary 3 (expected ratio on a plateau). On a 2-plateau I = [a,b] of length L ≥ 2, we obtain
  E_{k∈I, ++}[ ALG(k)/OPT(k) ] ≤ O( 1 + ln( a/L )_+ ),
  where x_+ = max{x,0}. In particular:
  - If L ≥ a/c for a constant c, the average is O(1).
  - If L ≥ a / polylog(a), the average is O(log log a).
  - For half of the k in I (those with Δ ≥ L/2), the “large-Δ” bi-criteria branch yields the constant bound 10 · (1 + a/(e(Δ−1))) = O( a/(e L) + 1 ); thus, if L = Ω(a), we get a constant-factor approximation with probability ≥ 1/2 over k in I.
- Why useful: This formalizes the “oversampling on a plateau” effect and quantifies how long plateaus translate into improved average guarantees under random k.

Decomposition across [k0, 2k0)
- Partition [k0, 2k0) into maximal 2-plateaus I1,…,Im with lengths L_j and starts a_j. Averaging Lemma 1 across k and applying Proposition 2 on each plateau yields
  E_{k∼U}[ALG(k)/OPT(k)] ≤ O(1/k0) · Σ_j L_j · ( 1 + ln( a_j / L_j )_+ ) + tail terms where Δ(k)=0.
- Consequences:
  - If at least one plateau satisfies L ≥ a/ polylog(a) with a ≥ Θ(k0), its contribution already gives an O(log log k0) bound (and even O(1) if L = Θ(a)).
  - If all plateaus are “short,” i.e., L_j ≤ a_j / h(a_j) for some growth function h(·), then their average contribution is upper-bounded by O(1/k0) Σ_j L_j · (1 + ln h(a_j)). Since Σ_j L_j = k0, this becomes O( 1 + (1/k0) Σ_j L_j ln h(a_j) ). If h(a) grows subpolynomially (e.g., ln^c a), we get O(log log k0). If h is constant (all L_j ≪ a_j), this argument alone does not beat O(log k0).
- Gap: To convert this into a universal O(log log k0) bound, we need a structural inequality relating the multiscale distribution of lengths L_j to starts a_j (or to the overall decrease OPT(k0)/OPT(2k0−1)). Such a universal inequality may not hold, so a worst-case instance might still force an Ω(log k0) average; see Obstacles below.

Strengthened coverage lemma for “heavy” part (towards many-short-plateau regime)
- Hypothesis (restated with precision). Fix k1,k2 with k = k1 + k2. Assume strong scale separation: OPT(k1) ≥ k^C · OPT(k) for large constant C. Run k-means++ for k1+(1+ε)k2 steps. Then with probability ≥ 1 − k^{−Ω(1)}, before any collision among the heavy k1 optimal clusters, k-means++ places one center in each heavy cluster; afterwards, bi-criteria analysis on the remaining k2 clusters yields total cost O( log(1/ε) ) · OPT(k).
- Why plausible: When OPT(k1) dominates, uncovered heavy clusters contribute almost all D^2 mass at early steps, so the chance of sampling twice in the same heavy cluster before hitting all heavy clusters is small by the MRS potential analysis refined Lemma 4.1 (5-approx on newly covered clusters) and the standard coupon-collector-with-drifting-probabilities style argument. Once heavy clusters are hit, the residual instance has OPT equal to OPT(k) up to negligible additive terms; oversampling on that part by (1+ε) gives O(log(1/ε)).
- Sketch of a proof plan:
  1) Formalize “heavy clusters” as those in the optimal k1-partition; show that at any point t ≤ k1, the conditional probability of hitting an uncovered heavy cluster at step t is ≥ 1 − k^{−Ω(1)} by comparing U_t(heavy)/cost_t(X) using the scale separation and Lemma 4.1 (eHt supermartingale) to control covered mass growth.
  2) Union bound over k1 steps to argue all heavy clusters get covered without collision w.h.p.
  3) Conditioned on this, apply the bi-criteria bound to the remaining k2 clusters with (1+ε) oversampling.
- Why useful here: If the interval [k0,2k0) has many short plateaus at different scales, we hope to peel off “heavy” parts with strong separation repeatedly and apply the above lemma, leading to O(log log k0) in aggregate even when no single long plateau exists.
- Status: This remains a conjectural lemma; the potential-based supermartingale control (MRS Cor. 4.5) provides a credible path to bounding collision probabilities, but a complete high-probability argument needs careful tracking of the drift of U_t(heavy) and H_t.

Examples and sanity checks
- Power-of-two plateaus. Suppose OPT(k) halves only when k doubles: OPT(k) ≈ Θ(1/⌊log_2 k⌋) on dyadic blocks. Then within [k0,2k0), there is a single long 2-plateau I with L ≈ a, and the plateau average from Proposition 2 gives O(1) expected ratio.
- Unhelpful case: “microscopic” drops. If OPT(k−1) ≤ 2·OPT(k) for all k (always true) and moreover OPT(k−1) ≈ (1+o(1)) OPT(k) (tiny drops), then s(k) ≈ k−1 so Δ(k)=1 almost everywhere. Lemma 1 reduces to the refined fixed-k bound 5(ln k + 2), offering no improvement. It is unclear whether adversarial k-means instances can enforce Δ(k)=1 for a 1−o(1) fraction of k in [k0,2k0); constructing such a family would show that randomizing k cannot help in the worst case beyond O(log k0).

Obstacles and possible lower bounds
- It seems plausible that there exist instances where Δ(k) is small for most k in [k0,2k0), yielding an Ω(log k0) average. A candidate is to adapt Arthur–Vassilvitskii’s lower-bound construction (regular simplices per cluster with large inter-cluster spacing) to be robust for all k in a range, e.g., create Θ(2k0) nearly orthogonal tiny-variance clusters at growing radii so that OPT(k) shrinks steadily with each additional center, preventing plateaus. This requires verifying that OPT(k−1)/OPT(k) ≲ constant for most steps.
- Action item: Attempt to formalize an instance family with OPT(k−1) ≥ c · OPT(k) for a fixed c < 2 across most k, and analyze the expectation of the k-means++ ratio under uniform k; if successful, this would give a matching Ω(log k0) lower bound in the random-k model.

Next steps (concrete, checkable)
1) Full proof of Lemma 1 with constants, including the Δ=0 fallback, and present as a self-contained lemma to be curated into output.md.
2) Write and verify the exact Stirling-based bound in Proposition 2: for a ≥ 2 and L ≥ 2, compute the plateau average explicitly and bound the o(1) terms to a clean inequality (e.g., ≤ 1 + ln(2a/(L−1)) + 2/(L−1)).
3) Derive a corollary for the probability-0.01 guarantee: On a plateau with L ≥ a/100, P_k[ALG(k)/OPT(k) ≤ C] ≥ 0.5 for some absolute C (from the large-Δ branch with Δ ≥ L/2); quantify C.
4) Attempt a high-probability version of the heavy-coverage lemma under an explicit scale-separation threshold (e.g., OPT(k1)/OPT(k) ≥ k^8); start by proving that the expected number of collisions among heavy clusters before all are hit is o(1).
5) Explore a lower-bound instance robust to random k: Formalize a sequence of nested simplices and radii ensuring Δ(k)=1 for ≥ 0.9 k0 choices of k in [k0,2k0). If achievable, this would indicate the random-k smoothing cannot universally beat O(log k0).

Notes on use of references
- Arthur–Vassilvitskii (2007) provides the baseline O(ln k) bound and the structural induction we reused.
- Makarychev–Reddy–Shan (2020) gives the refined 5-constant and, crucially, the bi-criteria bounds we leveraged in Lemma 1.

Gaps relative to a final result
- The unconditional average bound over random k remains O(log k0) at present. The proposed improvements are conditional: either the presence of a long 2-plateau or a (conjectured) heavy-coverage lemma enabling aggregation across short plateaus. A universal O(log log k0) upper bound is not established.
- No lower bound in the random-k model is yet proven; constructing one would clarify optimality.

## Round 0001 — 2025-08-27T15:22:45.941832Z

Ideas

- Goal: Upper bound E_{k∼U([k0,2k0)), k-means++}[ALG(k)/OPT(k)] as a function of k0. A natural route is to exploit oversampling-style guarantees (Makarychev–Reddy–Shan 2020, arXiv:2010.14487) when k “overshoots” the “effective” number of active clusters on a given scale. We formalize this via a dyadic (factor-2) plateau decomposition of OPT(·).
- Key input from 2010.14487: For k-means++ seeding with k+Δ centers, the expected bi-criteria approximation against OPT(k) is at most 5·min{2 + 1/(2e) + ln(2k/Δ), 1 + k/(e(Δ−1))}. This is especially strong when Δ is a nontrivial fraction of k (or when Δ is large enough so that ln(2k/Δ) = O(log log k)).
- Strategy: Partition [k0,2k0) into dyadic plateaus Ij = [k(j), k(j+1)) where k(0)=k0 and k(j+1) is the smallest k ≥ k(j) with OPT(k) ≤ OPT(k(j))/2. Then OPT(k) ∈ (OPT(k(j))/2, OPT(k(j))] for all k∈Ij. If k varies uniformly over Ij, running k-means++ with parameter k acts like oversampling by Δ = k−k(j) relative to base m:=k(j) while we are evaluated against OPT(k), which differs by at most a factor 2 from OPT(m). Averaging the bi-criteria bound across Δ on each plateau yields an O(1 + ln(2m/|Ij|)) contribution.

Formal lemmas (clean, checkable)

- Lemma 1 (Per-plateau averaged guarantee). Fix a dyadic plateau I=[m, m+L)⊆[k0,2k0), so that for all k∈I, OPT(k)∈(Θ/2,Θ] with Θ:=OPT(m). Then, for k uniform in I and over k-means++ randomness,
  E_k[ ALG(k)/OPT(k) ] ≤ 10·(C0 + E_{Δ∼U({0,…,L−1})}[ f_m(Δ) ])
  where f_m(Δ) = min{2 + 1/(2e) + ln(2m/max{1,Δ}), 1 + m/(e(max{1,Δ}−1))}, and C0 is an absolute constant accounting for Δ=0. In particular, for L≥2,
  E_k[ ALG(k)/OPT(k) ] ≤ 10·(C1 + ln(2m/L)) + O((ln m)/L).
  Why useful here: Converts the bi-criteria guarantee (for k+Δ versus OPT(k)) into a directly applicable, per-plateau bound on the smoothed-in-k objective.
  Sketch of proof: For Δ≥1, apply Makarychev–Reddy–Shan Lemma 5.6 with base m and Δ and note OPT(k)≥Θ/2. Average the term ln(2m/Δ) over Δ=1,…,L−1:
  (1/(L−1))∑_{Δ=1}^{L−1} ln(2m/Δ) = ln(2m) − (ln((L−1)!))/(L−1) = ln(2m/L) + 1 + o(1) by Stirling. The Δ=0 case contributes ≤O((ln m)/L). Multiplying by the factor 2 from OPT(k)∈(Θ/2,Θ] yields the 10 factor.

- Corollary 2 (Mixture across plateaus). Let {Ij} be the dyadic plateaus covering [k0,2k0), with lengths Lj and starts mj. Then
  E_{k∼U([k0,2k0))}[ ALG(k)/OPT(k) ] ≤ 10·(C1 + (1/k0)∑_j Lj·ln(2mj/Lj)) + o(1).
  Why useful: Reduces the global bound to a single explicit functional of the plateau-lengths {Lj} and starts {mj}. It cleanly isolates the role of “long vs short” plateaus.
  Proof idea: Average the Lemma 1 bound over each plateau, weighted by Lj/k0.

- Corollary 3 (Long-plateau gain). If some plateau I=[m,m+L) satisfies L ≥ m / polylog(k0), then
  E_{k∈I}[ ALG(k)/OPT(k) ] ≤ O(log log k0). Consequently, if a constant fraction of [k0,2k0) is covered by such long plateaus, the overall smoothed-in-k expectation is O(log log k0).
  Why useful: Quantifies the oversampling benefit under a natural structural condition on OPT(·). This formally explains the “why randomizing k might help” intuition.

Toy example to sanity-check Lemma 1

- Construct OPT plateaus as follows: Let m=k0. For k∈[k0, k0 + L), define a dataset X so that OPT(k) remains within a factor of 2 of OPT(k0). Take L = k0 / (log k0)^2. Then Lemma 1 gives average over k in that interval E[ALG/OPT] ≤ 10·(C1 + ln(2m/L)) ≈ O(ln((2k0)/(k0/(log k0)^2))) = O(ln((log k0)^2)) = O(log log k0). The Δ=0 contribution is suppressed by its 1/L probability.

Global worst-case discussion (what we can and cannot get yet)

- Using Corollary 2 with mj ≤ 2k0 and ∑_j Lj = k0 gives the trivial upper bound E ≤ O(1) + (10/k0)∑_j Lj ln(2k0/Lj) = O(ln(2k0)) − (10/k0)∑_j Lj ln Lj. Since x ln x is convex, the sum ∑ Lj ln Lj is minimized (and the bound maximized) when many Lj are tiny; in the extreme Lj=1 for all j, the term becomes O(ln k0). Thus, without structural assumptions on OPT(·), the plateau calculus alone cannot beat the O(log k0) worst-case baseline. This indicates that any unconditional improvement (e.g., to O(log log k0)) must invoke additional structure beyond dyadic plateau averaging.

Strengthened coverage statement (toward handling runs of short plateaus)

- Working Lemma (coverage of heavy part under scale separation; to be proved). Let k = k1 + k2 with k1 clusters (“heavy”) contributing a (1−η) fraction of OPT(k1), and suppose OPT(k1) ≥ k^{10}·OPT(k). Run k-means++ for k′ = k1 + (1+ε)k2 steps. Then, with probability ≥ 1−k^{-c}, the first k1 steps hit all heavy clusters without collision, and the final cost is O(log(1/ε))·OPT(k). Why useful here: If many consecutive short plateaus reflect the unlocking of finer “light” clusters while a small set remains heavy, this would effectively re-create the oversampling regime even when individual plateaus are short.
- Plan to prove: Adapt the Markov-chain/misses framework of Makarychev–Reddy–Shan (Sec. 5) to a setting where a constant-η mass sits on the heavy clusters until they are covered. Show that the probability of a miss among heavy clusters before covering them all is ≤ poly(η)/poly(k). Then apply their uncovered-cost decay bound (Lemma 5.5) after heavy coverage to control the tail.

Obstacles and open points

- Unconditional bound: The plateau-based mixture (Corollary 2) alone does not preclude worst-cases yielding Θ(log k0) average (e.g., many very short plateaus). We likely need a nontrivial structural inequality on the decrease rate of OPT(k) over k∈[k0,2k0) or a heavy-coverage argument as above.
- Δ=0 boundary: On each plateau, there is one k with Δ=0 where only the standard O(log m) bound applies. This is negligible for long plateaus but may matter when many plateaus are short.
- Lower bounds: It remains open whether one can construct a single fixed dataset X for which the smoothed-in-k expectation over [k0,2k0) is still Ω(log k0). Arthur–Vassilvitskii’s fixed-k lower bound adapts the instance to k and thus does not immediately imply a lower bound for the smoothed experiment.

Concrete next steps

1) Rigorous proof of Lemma 1 with explicit constants using Theorem 5.1 (bi-criteria bound) of Makarychev–Reddy–Shan (2020) and the dyadic plateau definition; include the precise handling of Δ=0 and the OPT(k) versus OPT(m) factor.
2) Derive Corollary 2 formally and simplify the functional ∑ Lj ln(2mj/Lj); explore whether mild regularity constraints on OPT (e.g., quasiconvexity of k·OPT(k) or bounded discrete derivative decay) imply that a constant fraction of mass lies on long plateaus, yielding an O(log log k0) global bound.
3) Begin the heavy-coverage lemma proof: (i) formalize a “scale separation” condition in terms of uncovered-cost mass; (ii) prove a coupon-collector-type bound for hitting all heavy clusters within the first k1 samples under D^2-weighting; (iii) plug into Makarychev–Reddy–Shan’s uncovered-cost supermartingale to bound the tail.
4) Try to craft candidate lower-bound datasets that could keep the smoothed-in-k expectation at Ω(log k0), or else gather evidence that such a construction is impossible, steering toward an unconditional improvement.

Notes on output.md

- No output.md was provided; thus, I cannot check curated results. The above lemmas are new suggestions relying on Theorem 5.1 of Makarychev–Reddy–Shan (2020) and can be made fully rigorous with short calculations as outlined.



=== summary.md ===

## round-0001 — model summary — 2025-08-27T15:48:20.397295Z
Both provers developed a dyadic “plateau” approach to bound E_{k∼U([k0,2k0))}[ALG(k)/OPT(k)] using the Makarychev–Reddy–Shan (2020) bi-criteria oversampling guarantee. They define 2-plateaus (OPT(k) within a factor 2) and show a per-plateau averaged bound: for a plateau starting at m of length L, the k-means++ average ratio is O(1 + ln(2m/L)) (constants from MRS). A per-k formulation via s(k):=min{s:OPT(s)≤2OPT(k)} and Δ(k)=k−s(k) gives E[ALG(k)] ≤ 10·min{2+1/(2e)+ln(2s/Δ), 1 + s/(e(Δ−1))}·OPT(k) for Δ≥1 (Δ=0 falls back to O(ln k)). Consequences: long plateaus (L ≥ m / polylog m) yield O(log log k0) averages; but many short plateaus can force only the trivial O(log k0) bound, so extra structure (or a “heavy-coverage” high‑probability lemma) is needed for an unconditional improvement. Files relied on: prover-01.out.json, prover-02.out.json. Refs: Makarychev–Reddy–Shan (2020); Arthur–Vassilvitskii (2007).



=== notes.md ===
Problem and notation
- X ⊂ R^d finite dataset; cost(C) = ∑_{x∈X} min_{c∈C} ||x−c||^2.
- OPT_k := min_{|C|=k} cost(C), nonincreasing in k.
- ALG_k := cost of k-means++ after k centers; E_++[·] denotes expectation over its randomness.
- We study E_{k∼U({k0,…,2k0−1})} E_++[ ALG_k / OPT_k ].

Key external input (Makarychev–Reddy–Shan 2020)
- There is an absolute constant C_bi (quoted as 5 in MRS) such that for any r≥1, Δ≥1,
  E_++[cost after r+Δ centers] ≤ C_bi · min{ 2 + 1/(2e) + ln(2r/Δ), 1 + r/(e(Δ−1)) } · OPT_r.
- For Δ=0 (fixed-k), there is an absolute constant C_fix (also ≲5 in MRS) with E_++[ALG_k] ≤ C_fix · (ln k + O(1)) · OPT_k.

Per-k oversampling bound (robust to s-misalignment)
- For any k, define s(k) := min{ s∈{1,…,k} : OPT_s ≤ 2·OPT_k }. Let Δ(k)=k−s(k).
- If Δ(k)≥1, then
  E_++[ALG_k] ≤ 2·C_bi · min{ 2 + 1/(2e) + ln(2 s(k)/Δ(k)), 1 + s(k)/(e(Δ(k)−1)) } · OPT_k.
- If Δ(k)=0, fall back to E_++[ALG_k] ≤ C_fix · (ln k + O(1)) · OPT_k.
Note: On any factor-2 plateau I=[m,m+L) (i.e., OPT_k∈(OPT_m/2,OPT_m] for k∈I), we have s(k)≤m and Δ(k)≥k−m; since the MRS bound is monotone in (s,Δ), replacing (s(k),Δ(k)) by (m,k−m) yields a valid upper bound.

Per-plateau averaging and Stirling control
- For a factor-2 plateau I=[m,m+L), L≥2, uniform k over I gives
  E_{k,++}[ALG_k/OPT_k] ≤ 2·C_bi · (2 + 1/(2e) + (1/(L−1))∑_{Δ=1}^{L−1} ln(2m/Δ)) + (C_fix(ln m + O(1)))/L.
- Using ln(n!) ≥ n ln n − n + 1 (n≥1),
  (1/(L−1))∑_{Δ=1}^{L−1} ln(2m/Δ) = ln(2m) − ln((L−1)!)/(L−1) ≤ 1 + ln(2m/(L−1)).
- Hence E_{k,++}[ALG_k/OPT_k] ≤ const + const·ln^+(2m/(L−1)) + O((ln m)/L), where ln^+(x)=max{ln x,0} and all const’s are absolute.

Decomposition over [k0,2k0)
- Partition [k0,2k0) into maximal factor-2 plateaus I_j=[m_j, m_j+L_j). For L_j≥2, the per-plateau average is bounded as above. For L_j=1, the single k in I_j contributes ≤ C_fix(ln m_j + O(1)). Averaging with weights L_j/k0 yields a global bound explicitly in terms of {(m_j,L_j)}; in the worst case many L_j=1 plateaus can force Θ(log k0).

Consequences and regimes
- Long plateaus: If some I=[m,m+L) satisfies L ≥ m/polylog(k0), then E_{k∈I,++}[ALG/OPT] = O(log log k0). If a constant fraction of [k0,2k0) is covered by such plateaus, the global average is O(log log k0).
- Worst case without structure: If many plateaus are short (e.g., L_j=1), the global average can be Θ(log k0), aligning with classic worst-case fixed-k behavior.

Open directions
- Structural conditions on OPT(·) implying many long plateaus (e.g., bounded discrete derivative decay, quasiconvexity/regularity of k·OPT_k) to get unconditional O(log log k0).
- Heavy-coverage lemma (conjectural): under strong scale separation OPT_{k1} ≫ OPT_k, show k-means++ hits all k1 heavy clusters w.h.p. before collisions, then apply bi-criteria on the remaining part.
- Lower bounds: Construct instances with Δ(k)=1 for a 1−o(1) fraction of k in [k0,2k0), giving an Ω(log k0) average and clarifying optimality of random-k smoothing.


=== output.md ===
Theorems curated in this file depend on the following standard notation and external input.
- For a finite dataset X⊂R^d, write OPT_k for the optimal k-means cost with k centers, and ALG_k for the cost produced by k-means++ after k centers; E_++[·] is expectation over the algorithm’s randomness.
- External input (Makarychev–Reddy–Shan, 2020): There exists an absolute constant C_bi such that for all integers r≥1 and Δ≥1,
  E_++[cost after r+Δ centers] ≤ C_bi · min{ 2 + 1/(2e) + ln(2r/Δ), 1 + r/(e(Δ−1)) } · OPT_r.
  There is also an absolute constant C_fix with E_++[ALG_k] ≤ C_fix·(ln k + O(1))·OPT_k for all k (the fixed-k bound). We treat C_bi and C_fix as absolute numerical constants.

Theorem 1 (Per-k oversampling bound via a 2-approximate anchor).
Fix k≥1 and define
  s(k) := min{ s∈{1,…,k} : OPT_s ≤ 2·OPT_k },    Δ(k) := k − s(k).
Then:
- If Δ(k) ≥ 1, we have
  E_++[ALG_k] ≤ 2·C_bi · min{ 2 + 1/(2e) + ln(2 s(k)/Δ(k)), 1 + s(k)/(e(Δ(k)−1)) } · OPT_k.
- If Δ(k) = 0, we have E_++[ALG_k] ≤ C_fix·(ln k + O(1))·OPT_k.
Proof.
For Δ(k)≥1, apply the MRS bi-criteria inequality with r = s(k) and Δ = k−s(k):
  E_++[ALG_k] = E_++[cost after s(k)+Δ(k) centers]
  ≤ C_bi · min{ 2 + 1/(2e) + ln(2 s(k)/Δ(k)), 1 + s(k)/(e(Δ(k)−1)) } · OPT_{s(k)}.
By definition of s(k), OPT_{s(k)} ≤ 2·OPT_k, giving the stated bound. For Δ(k)=0 the fixed-k bound yields the second claim. ∎

Theorem 2 (Averaging over a factor-2 plateau).
Let I = {m, m+1, …, m+L−1} with L≥2 and assume OPT_k ∈ (OPT_m/2, OPT_m] for all k∈I. Then for k uniform in I and over the k-means++ randomness,
  E_{k∈I} E_++[ ALG_k / OPT_k ] ≤ A0 + A1 · ln^+( 2m/(L−1) ) + (A2 · ln m)/L,
where ln^+(x) := max{ln x,0} and A0,A1,A2 are absolute constants depending only on C_bi,C_fix.
Proof.
Average the bound from Theorem 1 over k∈I. For Δ:=k−m≥1, we may upper bound s(k) by m and lower bound Δ(k) by Δ (since s(k)≤m and Δ(k)≥Δ on a factor-2 plateau), and use only the logarithmic branch of MRS to get
  E_++[ALG_{m+Δ}/OPT_{m+Δ}] ≤ 2·C_bi · ( 2 + 1/(2e) + ln(2m/Δ) ).
Averaging Δ from 1 to L−1 gives
  (1/(L−1))∑_{Δ=1}^{L−1} ln(2m/Δ) = ln(2m) − ln((L−1)!)/(L−1) ≤ 1 + ln(2m/(L−1)),
where we used ln(n!) ≥ n ln n − n + 1 for n=L−1≥1. Multiplying by (L−1)/L and adding the Δ=0 endpoint contribution C_fix(ln m + O(1))/L yields the claim with suitable absolute constants A0,A1,A2. Replacing ln by ln^+ only weakens the bound. ∎

Corollary 3 (Decomposition across [k0,2k0)).
Partition the integer interval [k0,2k0) into maximal factor-2 plateaus I_j = [m_j, m_j+L_j), j=1,…,J, so OPT_k ∈ (OPT_{m_j}/2, OPT_{m_j}] for k∈I_j, and ∑_j L_j = k0. Then, for k uniform on [k0,2k0),
  E_k E_++[ ALG_k / OPT_k ]
  ≤ (1/k0) ∑_{j : L_j≥2} L_j [ A0 + A1 · ln^+( 2 m_j/(L_j−1) ) ]
    + (1/k0) ∑_{j : L_j≥2} A2 · ln m_j
    + (1/k0) ∑_{j : L_j=1} C_fix · (ln m_j + O(1)).
In particular, if a constant fraction of [k0,2k0) is covered by plateaus satisfying L_j ≥ m_j / polylog(k0), then the contribution of those plateaus is O(log log k0), and the remaining fraction must be controlled separately; in the worst case with many L_j=1 plateaus the global average can be Θ(log k0).
Proof.
Average the bound of Theorem 2 over plateaus with weights L_j/k0, and handle L_j=1 plateaus by the fixed-k bound. ∎


=== 1.pdf ===

--- Page 1 ---
k-means++: The Advantages of Careful Seeding
David Arthur ∗
Sergei Vassilvitskii†
Abstract
The k-means method is a widely used clustering technique
that seeks to minimize the average squared distance between
points in the same cluster. Although it oﬀers no accuracy
guarantees, its simplicity and speed are very appealing in
practice. By augmenting k-means with a very simple, ran-
domized seeding technique, we obtain an algorithm that is
Θ(log k)-competitive with the optimal clustering.
Prelim-
inary experiments show that our augmentation improves
both the speed and the accuracy of k-means, often quite
dramatically.
1
Introduction
Clustering is one of the classic problems in machine
learning and computational geometry. In the popular
k-means formulation, one is given an integer k and a set
of n data points in Rd. The goal is to choose k centers
so as to minimize φ, the sum of the squared distances
between each point and its closest center.
Solving this problem exactly is NP-hard, even with
just two clusters [10], but twenty-ﬁve years ago, Lloyd
[20] proposed a local search solution that is still very
widely used today (see for example [1, 11, 15]). Indeed,
a recent survey of data mining techniques states that it
“is by far the most popular clustering algorithm used in
scientiﬁc and industrial applications” [5].
Usually referred to simply as k-means, Lloyd’s
algorithm begins with k arbitrary centers, typically
chosen uniformly at random from the data points. Each
point is then assigned to the nearest center, and each
center is recomputed as the center of mass of all points
assigned to it. These two steps (assignment and center
calculation) are repeated until the process stabilizes.
One can check that the total error φ is monotoni-
cally decreasing, which ensures that no clustering is re-
peated during the course of the algorithm. Since there
are at most kn possible clusterings, the process will al-
ways terminate. In practice, very few iterations are usu-
ally required, which makes the algorithm much faster
∗Stanford University, Supported in part by NDSEG Fellow-
ship, NSF Grant ITR-0331640, and grants from Media-X and
SNRC.
†Stanford University, Supported in part by NSF Grant ITR-
0331640, and grants from Media-X and SNRC.
than most of its competitors.
Unfortunately, the empirical speed and simplicity
of the k-means algorithm come at the price of accuracy.
There are many natural examples for which the algo-
rithm generates arbitrarily bad clusterings (i.e.,
φ
φOPT is
unbounded even when n and k are ﬁxed). Furthermore,
these examples do not rely on an adversarial placement
of the starting centers, and the ratio can be unbounded
with high probability even with the standard random-
ized seeding technique.
In this paper, we propose a way of initializing
k-means by choosing random starting centers with
very speciﬁc probabilities.
Speciﬁcally, we choose a
point p as a center with probability proportional to p’s
contribution to the overall potential. Letting φ denote
the potential after choosing centers in this way, we show
the following.
Theorem 1.1. For any set of data points, E[φ] ≤
8(ln k + 2)φOP T .
This sampling is both fast and simple, and it already
achieves approximation guarantees that k-means can-
not.
We propose using it to seed the initial centers
for k-means, leading to a combined algorithm we call
k-means++.
This complements a very recent result of Ostrovsky
et al. [24], who independently proposed much the same
algorithm. Whereas they showed this randomized seed-
ing is O(1)-competitive on data sets following a certain
separation condition, we show it is O(log k)-competitive
on all data sets.
We also show that the analysis for Theorem 1.1 is
tight up to a constant factor, and that it can be eas-
ily extended to various potential functions in arbitrary
metric spaces.
In particular, we can also get a sim-
ple O(log k) approximation algorithm for the k-median
objective. Furthermore, we provide preliminary experi-
mental data showing that in practice, k-means++ really
does outperform k-means in terms of both accuracy and
speed, often by a substantial margin.
1.1
Related work As a fundamental problem in
machine learning, k-means has a rich history. Because
of its simplicity and its observed speed, Lloyd’s method
[20] remains the most popular approach in practice,

--- Page 2 ---
despite its limited accuracy. The convergence time of
Lloyd’s method has been the subject of a recent series
of papers [2, 4, 8, 14]; in this work we focus on improving
its accuracy.
In the theory community, Inaba et al. [16] were
the ﬁrst to give an exact algorithm for the k-means
problem, with the running time of O(nkd). Since then, a
number of polynomial time approximation schemes have
been developed (see [9, 13, 19, 21] and the references
therein). While the authors develop interesting insights
into the structure of the clustering problem, their
algorithms are highly exponential (or worse) in k, and
are unfortunately impractical even for relatively small
n, k and d.
Kanungo et al. [17] proposed an O(n3ϵ−d) algorithm
that is (9 + ϵ)-competitive.
However, n3 compares
unfavorably with the almost linear running time of
Lloyd’s method, and the exponential dependence on d
can also be problematic. For these reasons, Kanungo et
al. also suggested a way of combining their techniques
with Lloyd’s algorithm, but in order to avoid the
exponential dependence on d, their approach sacriﬁces
all approximation guarantees.
Mettu and Plaxton [22] also achieved a constant-
probability O(1) approximation using a technique called
successive sampling. They match our running time of
O(nkd), but only if k is suﬃciently large and the spread
is suﬃciently small. In practice, our approach is simpler,
and our experimental results seem to be better in terms
of both speed and accuracy.
Very recently, Ostrovsky et al. [24] independently
proposed an algorithm that is essentially identical to
ours, although their analysis is quite diﬀerent. Letting
φOPT,k denote the optimal potential for a k-clustering
on a given data set, they prove k-means++ is O(1)-
competitive in the case where
φOPT,k
φOPT,k−1
≤ϵ2.
The
intuition here is that if this condition does not hold,
then the data is not well suited for clustering with the
given value for k.
Combining this result with ours gives a strong
characterization of the algorithm’s performance.
In
particular, k-means++ is never worse than O(log k)-
competitive, and on very well formed data sets, it
improves to being O(1)-competitive.
Overall, the seeding technique we propose is similar
in spirit to that used by Meyerson [23] for online facility
location, and Mishra et al. [12] and Charikar et al. [6]
in the context of k-median clustering.
However, our
analysis is quite diﬀerent from those works.
2
Preliminaries
In this section, we formally deﬁne the k-means problem,
as well as the k-means and k-means++ algorithms.
For the k-means problem, we are given an integer k
and a set of n data points X ⊂Rd. We wish to choose
k centers C so as to minimize the potential function,
φ =
X
x∈X
min
c∈C ∥x −c∥2.
Choosing these centers implicitly deﬁnes a clustering
– for each center, we set one cluster to be the set of
data points that are closer to that center than to any
other. As noted above, ﬁnding an exact solution to the
k-means problem is NP-hard.
Throughout the paper, we will let COPT denote the
optimal clustering for a given instance of the k-means
problem, and we will let φOPT denote the corresponding
potential.
Given a clustering C with potential φ, we
also let φ(A) denote the contribution of A ⊂X to the
potential (i.e., φ(A) = P
x∈A minc∈C∥x −c∥2).
2.1
The k-means algorithm The k-means method
is a simple and fast algorithm that attempts to locally
improve an arbitrary k-means clustering. It works as
follows.
1. Arbitrarily choose k initial centers C = {c1, . . . , ck}.
2. For each i ∈{1, . . . , k}, set the cluster Ci to be the
set of points in X that are closer to ci than they
are to cj for all j ̸= i.
3. For each i ∈{1, . . . , k}, set ci to be the center of
mass of all points in Ci: ci =
1
|Ci|
P
x∈Ci x.
4. Repeat Steps 2 and 3 until C no longer changes.
It is standard practice to choose the initial centers
uniformly at random from X. For Step 2, ties may be
broken arbitrarily, as long as the method is consistent.
Steps 2 and 3 are both guaranteed to decrease φ, so
the algorithm makes local improvements to an arbitrary
clustering until it is no longer possible to do so. To see
that Step 3 does in fact decreases φ, it is helpful to recall
a standard result from linear algebra (see [14]).
Lemma 2.1. Let S be a set of points with center of
mass c(S), and let z be an arbitrary point.
Then,
P
x∈S∥x −z∥2 −P
x∈S∥x −c(S)∥2 = |S| · ∥c(S) −z∥2.
Monotonicity for Step 3 follows from taking S to be a
single cluster and z to be its initial center.
As discussed above, the k-means algorithm is at-
tractive in practice because it is simple and it is gener-
ally fast. Unfortunately, it is guaranteed only to ﬁnd a
local optimum, which can often be quite poor.
2.2
The k-means++ algorithm The k-means algo-
rithm begins with an arbitrary set of cluster centers.
We propose a speciﬁc way of choosing these centers. At

--- Page 3 ---
any given time, let D(x) denote the shortest distance
from a data point x to the closest center we have al-
ready chosen. Then, we deﬁne the following algorithm,
which we call k-means++.
1a. Choose an initial center c1 uniformly at random
from X.
1b. Choose the next center ci, selecting ci = x′ ∈X
with probability
D(x′)2
P
x∈X D(x)2 .
1c. Repeat Step 1b until we have chosen a total of k
centers.
2-4. Proceed as with the standard k-means algorithm.
We call the weighting used in Step 1b simply “D2
weighting”.
3
k-means++ is O(log k)-competitive
In this section, we prove our main result.
Theorem 3.1. If C is constructed with k-means++,
then the corresponding potential function φ satisﬁes
E[φ] ≤8(ln k + 2)φOPT.
In fact, we prove this holds after only Step 1 of the
algorithm above.
Steps 2 through 4 can then only
decrease φ. Not surprisingly, our experiments show this
local optimization is important in practice, although it
is diﬃcult to quantify this theoretically.
Our analysis consists of two parts. First, we show
that k-means++ is competitive in those clusters of COPT
from which it chooses a center. This is easiest in the
case of our ﬁrst center, which is chosen uniformly at
random.
Lemma 3.1. Let A be an arbitrary cluster in COPT, and
let C be the clustering with just one center, which is
chosen uniformly at random from A. Then, E[φ(A)] =
2φOPT(A).
Proof. Let c(A) denote the center of mass of the data
points in A.
By Lemma 2.1, we know that since
COPT is optimal, it must be using c(A) as the center
corresponding to the cluster A. Using the same lemma
again, we see E[φ(A)] is given by,
X
a0∈A
1
|A| ·
 X
a∈A
∥a −a0∥2
!
=
1
|A|
X
a0∈A
 X
a∈A
∥a −c(A)∥2 + |A| · ∥a0 −c(A)∥2
!
=
2
X
a∈A
∥a −c(A)∥2,
and the result follows.
Our next step is to prove an analog of Lemma 3.1
for the remaining centers, which are chosen with D2
weighting.
Lemma 3.2. Let A be an arbitrary cluster in COPT, and
let C be an arbitrary clustering.
If we add a random
center to C from A, chosen with D2 weighting, then
E[φ(A)] ≤8φOPT(A).
Proof. The probability that we choose some ﬁxed a0 as
our center, given that we are choosing our center from
A, is precisely
D(a0)2
P
a∈A D(a)2 . Furthermore, after choos-
ing the center a0, a point a will contribute precisely
min(D(a), ∥a −a0∥)2 to the potential. Therefore,
E[φ(A)] =
X
a0∈A
D(a0)2
P
a∈A D(a)2
X
a∈A
min(D(a), ∥a −a0∥)2.
Note by the triangle inequality that D(a0)
≤
D(a) + ∥a −a0∥for all a, a0.
From this, the power-
mean inequality1 implies that D(a0)2
≤2D(a)2 +
2∥a −a0∥2.
Summing over all a, we then have that
D(a0)2 ≤
2
|A|
P
a∈A D(a)2 +
2
|A|
P
a∈A∥a −a0∥2, and
hence, E[φ(A)] is at most,
2
|A| ·
X
a0∈A
P
a∈A D(a)2
P
a∈A D(a)2 ·
X
a∈A
min(D(a), ∥a −a0∥)2
+ 2
|A| ·
X
a0∈A
P
a∈A∥a −a0∥2
P
a∈A D(a)2
·
X
a∈A
min(D(a), ∥a −a0∥)2.
In the ﬁrst expression, we substitute min(D(a), ∥a −
a0∥)2 ≤∥a −a0∥2, and in the second expression, we
substitute min(D(a), ∥a −a0∥)2 ≤D(a)2. Simplifying,
we then have,
E[φ(A)]
≤
4
|A| ·
X
a0∈A
X
a∈A
∥a −a0∥2
=
8φOPT(A).
The last step here follows from Lemma 3.1.
We have now shown that seeding by D2 weighting
is competitive as long as it chooses centers from each
cluster of COPT, which completes the ﬁrst half of our
argument. We now use induction to show the total error
in general is at most O(log k).
1The power-mean inequality states for any real numbers
a1, · · · , am that Σa2
i ≥
1
m(Σai)2.
It follows from the Cauchy-
Schwarz inequality. We are only using the case m = 2 here, but
we will need the general case for Lemma 3.3.

--- Page 4 ---
Lemma 3.3. Let C be an arbitrary clustering. Choose
u > 0 “uncovered” clusters from COPT, and let Xu
denote the set of points in these clusters.
Also let
Xc = X −Xu. Now suppose we add t ≤u random centers
to C, chosen with D2 weighting. Let C′ denote the the
resulting clustering, and let φ′ denote the corresponding
potential. Then, E[φ′] is at most,

φ(Xc) + 8φOPT(Xu)

· (1 + Ht) + u −t
u
· φ(Xu).
Here, Ht denotes the harmonic sum, 1 + 1
2 + · · · + 1
t .
Proof. We prove this by induction, showing that if the
result holds for (t −1, u) and (t −1, u −1), then it
also holds for (t, u).
Therefore, it suﬃces to check
t = 0, u > 0 and t = u = 1 as our base cases.
If t = 0 and u > 0, the result follows from the fact
that 1 + Ht =
u−t
u
= 1.
Next, suppose t = u = 1.
We choose our one new center from the one uncovered
cluster with probability exactly
φ(Xu)
φ
.
In this case,
Lemma 3.2 guarantees that E[φ′] ≤φ(Xc)+8φOPT(Xu).
Since φ′ ≤φ even if we choose a center from a covered
cluster, we have,
E[φ′]
≤
φ(Xu)
φ
·

φ(Xc) + 8φOPT(Xu)

+ φ(Xc)
φ
· φ
≤
2φ(Xc) + 8φOPT(Xu).
Since 1 + Ht = 2 here, we have shown the result holds
for both base cases.
We now proceed to prove the inductive step. It is
convenient here to consider two cases. First suppose we
choose our ﬁrst center from a covered cluster. As above,
this happens with probability exactly φ(Xc)
φ
. Note that
this new center can only decrease φ.
Bearing this in
mind, apply the inductive hypothesis with the same
choice of covered clusters, but with t decreased by one.
It follows that our contribution to E[φ′] in this case is
at most,
φ(Xc)
φ
·

φ(Xc) + 8φOPT(Xu)

· (1 + Ht−1)
+ u −t + 1
u
· φ(Xu)

.
On the other hand, suppose we choose our ﬁrst
center from some uncovered cluster A. This happens
with probability
φ(A)
φ .
Let pa denote the probability
that we choose a ∈A as our center, given the center is
somewhere in A, and let φa denote φ(A) after we choose
a as our center.
Once again, we apply our inductive
hypothesis, this time adding A to the set of covered
clusters, as well as decreasing both t and u by 1.
It
follows that our contribution to E[φOPT] in this case is
at most,
φ(A)
φ
·
X
a∈A
pa

φ(Xc) + φa + 8φOPT(Xu) −8φOPT(A)

· (1 + Ht−1) + u −t
u −1 ·

φ(Xu) −φ(A)

≤φ(A)
φ
·

φ(Xc) + 8φOPT(Xu)

· (1 + Ht−1)
+ u −t
u −1 ·

φ(Xu) −φ(A)

.
The
last
step
here
follows
from
the
fact
that
P
a∈A paφa ≤8φOPT(A), which is implied by Lemma
3.2.
Now,
the
power-mean
inequality
implies
that
P
A⊂Xu φ(A)2 ≥1
u · φ(Xu)2. Therefore, if we sum over
all uncovered clusters A, we obtain a potential contri-
bution of at most,
φ(Xu)
φ
·

φ(Xc) + 8φOPT(Xu)

· (1 + Ht−1)
+ 1
φ · u −t
u −1 ·

φ(Xu)2 −1
u · φ(Xu)2

=
φ(Xu)
φ
·

φ(Xc) + 8φOPT(Xu)

· (1 + Ht−1)
+ u −t
u
· φ(Xu)

.
Combining the potential contribution to E[φ′] from
both cases, we now obtain the desired bound:
E[φ′] ≤

φ(Xc) + 8φOPT(Xu)

· (1 + Ht−1)
+ u −t
u
· φ(Xu) + φ(Xc)
φ
· φ(Xu)
u
≤

φ(Xc) + 8φOPT(Xu)

·

1 + Ht−1 + 1
u

+ u −t
u
· φ(Xu).
The inductive step now follows from the fact that 1
u ≤1
t .
We specialize Lemma 3.3 to obtain our main result.
Theorem 3.1 If C is constructed with k-means++,
then the corresponding potential function φ satisﬁes
E[φ] ≤8(ln k + 2)φOPT.
Proof. Consider the clustering C after we have com-
pleted Step 1. Let A denote the COPT cluster in which
we chose the ﬁrst center.
Applying Lemma 3.3 with

--- Page 5 ---
t = u = k −1 and with A being the only covered clus-
ter, we have,
E[φOPT] ≤

φ(A) + 8φOPT −8φOPT(A)

· (1 + Hk−1).
The result now follows from Lemma 3.1, and from the
fact that Hk−1 ≤1 + ln k.
4
A matching lower bound
In this section, we show that the D2 seeding used
by k-means++ is no better than Ω(log k)-competitive
in expectation, thereby proving Theorem 3.1 is tight
within a constant factor.
Fix k, and then choose n, ∆, δ with n ≫k and ∆≫
δ. We construct X with n points. First choose k centers
c1, c2, . . . , ck such that ∥ci −cj∥2 = ∆2 −
  n−k
n

· δ2
for all i ̸= j.
Now, for each ci, add data points
xi,1, xi,2, · · · , xi, n
k arranged in a regular simplex with
center ci, side length δ, and radius
q
n−k
2n · δ. If we do
this in orthogonal dimensions for each i, we then have,
∥xi,i′ −xj,j′∥=
 δ
if i=j, or
∆
otherwise.
We prove our seeding technique is in expectation
Ω(log k) worse than the optimal clustering in this case.
Clearly, the optimal clustering has centers {ci},
which leads to an optimal potential of φOPT = n−k
2
· δ2.
Conversely, using an induction similar to that of Lemma
3.3, we show D2 seeding cannot match this bound. As
before, we bound the expected potential in terms of
the number of centers left to choose and the number
of uncovered clusters (those clusters of C0 from which
we have not chosen a center).
Lemma 4.1. Let C be an arbitrary clustering on X with
k −t ≥1 centers, but with u clusters from COPT
uncovered.
Now suppose we add t random centers to
C, chosen with D2 weighting.
Let C′ denote the the
resulting clustering, and let φ′ denote the corresponding
potential.
Furthermore, let α =
n−k2
n
, β =
∆2−2kδ2
∆2
and
H′
u = Pu
i=1
k−i
ki . Then, E[φ′] is at least,
αt+1 ·

nδ2 · (1 + H′
u) · β +
n
k ∆2 −2nδ2
· (u −t)

.
Proof. We prove this by induction on t. If t = 0, note
that,
φ′ = φ =

n −u · n
k −k

· δ2 + u · n
k · ∆2.
Since n−u· n
k ≥n
k , we have n−u· n
k −k
n−u· n
k
≥
n
k −k
n
k
= α. Also,
α, β ≤1. Therefore,
φ′ ≥α ·

n −u · n
k

· δ2 · β + u · n
k · ∆2
.
Finally, since nδ2u ≥u · n
k · δ2 · β and nδ2u ≥nδ2H′
uβ,
we have,
φ′ ≥α ·

nδ2 · (1 + H′
u) · β +
n
k ∆2 −2nδ2
· u

.
This completes the base case.
We now proceed to prove the inductive step. As
with Lemma 3.3, we consider two cases. The probability
that our ﬁrst center is chosen from an uncovered cluster
is,
u · n
k · ∆2
u · n
k · ∆2 + (k −u) · n
k · δ2 −(k −t)δ2
≥
u∆2
u∆2 + (k −u)δ2
≥
α ·
u∆2
u∆2 + (k −u)δ2 .
Applying our inductive hypothesis with t and u both
decreased by 1, we obtain a potential contribution from
this case of at least,
u∆2
u∆2 + (k −u)δ2 · αt+1 ·

nδ2 · (1 + H′
u−1) · β
+
n
k ∆2 −2nδ2
· (u −t)

.
The probability that our ﬁrst center is chosen from
a covered cluster is
(k −u) · n
k · δ2 −(k −t)δ2
u · n
k · ∆2 + (k −u) · n
k · δ2 −(k −t)δ2
≥
(k −u) · n
k · δ2 −(k −t)δ2
(k −u) · n
k · δ2
·
(k −u)δ2
u∆2 + (k −u)δ2
≥
α ·
(k −u)δ2
u∆2 + (k −u)δ2 .
Applying our inductive hypothesis with t decreased by 1
but with u constant, we obtain a potential contribution
from this case of at least,
(k −u)δ2
u∆2 + (k −u)δ2 · αt+1 ·

nδ2 · (1 + H′
u) · β
+
n
k ∆2 −2nδ2
· (u −t + 1)

.
Therefore, E[φ′] is at least,
αt+1 ·

nδ2 · (1 + H′
u) · β +
n
k ∆2 −2nδ2
· (u −t)

+
αt+1
u∆2 + (k −u)δ2 ·

(k −u)δ2 ·
n
k ∆2 −2nδ2

−u∆2 ·

H′(u) −H′(u −1)

· nδ2 · β

.

--- Page 6 ---
However, H′
u −H′
u−1 = k−u
ku and β = ∆2−2kδ2
∆2
, so
u∆2 ·

H′(u) −H′(u −1)

· nδ2 · β
= (k −u)δ2 ·
n
k ∆2 −2nδ2
,
and the result follows.
As in the previous section, we obtain the desired
result by specializing the induction.
Theorem 4.1. D2 seeding is no better than 2(ln k)-
competitive.
Proof. Suppose a clustering with potential φ is con-
structed using k-means++ on X described above. Ap-
ply Lemma 4.1 with u = t = k −1 after the ﬁrst
center has been chosen.
Noting that 1 + H′
k−1 =
1 + Pk−1
i=1
  1
i −1
k

= Hk > ln k, we then have,
E[φ] ≥αkβ · nδ2 · ln k.
Now, ﬁx k and δ but let n and ∆approach inﬁnity.
Then α and β both approach 1, and the result follows
from the fact that φOPT = n−k
2
· δ2.
5
Generalizations
Although the k-means algorithm itself applies only
in vector spaces with the potential function φ
=
P
x∈X minc∈C∥x −c∥2, we note that our seeding tech-
nique does not have the same limitations. In this sec-
tion, we discuss extending our results to arbitrary met-
ric spaces with the more general potential function,
φ[ℓ] = P
x∈X minc∈C∥x −c∥ℓfor ℓ≥1. In particular,
note that the case of ℓ= 1 is the k-medians potential
function.
These generalizations require only one change to
the algorithm itself. Instead of using D2 seeding, we
switch to Dℓseeding – i.e., we choose x0 as a center
with probability
D(x0)ℓ
P
x∈X D(x)ℓ.
For the analysis, the most important change ap-
pears in Lemma 3.1. Our original proof uses an inner
product structure that is not available in the general
case. However, a slightly weaker result can be proven
using only the triangle inequality.
Lemma 5.1. Let A be an arbitrary cluster in COPT, and
let C be the clustering with just one center, which is cho-
sen uniformly at random from A. Then, E[φ[ℓ](A)] ≤
2ℓφ[ℓ]
OPT(A).
Proof. Let c denote the center of A in COPT. Then,
E[φ[ℓ](A)]
=
1
|A|
X
a0∈A
X
a∈A
∥a −a0∥ℓ
≤
2ℓ−1
|A|
X
a0∈A
X
a∈A
 ∥a −c∥ℓ+ ∥a0 −c∥ℓ
=
2ℓφ[ℓ]
OPT(A).
The second step here follows from the triangle inequality
and the power-mean inequality.
The rest of our upper bound analysis carries
through without change, except that in the proof of
Lemma 3.2, we lose a factor of 2ℓ−1 from the power-
mean inequality, instead of just 2. Putting everything
together, we obtain the general theorem.
Theorem 5.1. If C is constructed with Dℓseeding,
then the corresponding potential function φ[ℓ] satisﬁes,
E[φ[ℓ]] ≤22ℓ(ln k + 2)φ[ℓ]
OPT.
6
Empirical results
In order to evaluate k-means++ in practice, we have
implemented and tested it in C++ [3]. In this section,
we discuss the results of these preliminary experiments.
We found that D2 seeding substantially improves both
the running time and the accuracy of k-means.
6.1
Datasets We
evaluated
the
performance
of
k-means and k-means++ on four datasets.
The ﬁrst dataset, Norm25, is synthetic. To generate
it, we chose 25 “true” centers uniformly at random
from a 15-dimensional hypercube of side length 500.
We then added points from Gaussian distributions of
variance 1 around each true center. Thus, we obtained
a number of well separated Gaussians with the the true
centers providing a good approximation to the optimal
clustering.
We chose the remaining datasets from real-world
examples oﬀthe UC-Irvine Machine Learning Reposi-
tory. The Cloud dataset [7] consists of 1024 points in 10
dimensions, and it is Philippe Collard’s ﬁrst cloud cover
database. The Intrusion dataset [18] consists of 494019
points in 35 dimensions, and it represents features avail-
able to an intrusion detection system. Finally, the Spam
dataset [25] consists of 4601 points in 58 dimensions,
and it represents features available to an e-mail spam
detection system.
For each dataset, we tested k = 10, 25, and 50.
6.2
Metrics Since we were testing randomized seed-
ing processes, we ran 20 trials for each case. We report

--- Page 7 ---
Average φ
Minimum φ
Average T
k
k-means
k-means++
k-means
k-means++
k-means
k-means++
10
1.365 · 105
8.47%
1.174 · 105
0.93%
0.12
46.72%
25
4.233 · 104
99.96%
1.914 · 104
99.92%
0.90
87.79%
50
7.750 · 103
99.81%
1.474 · 101
0.53%
2.04
−1.62%
Table 1:
Experimental results on the Norm25 dataset (n = 10000, d = 15).
For k-means, we list the
actual potential and time in seconds.
For k-means++, we list the percentage improvement over k-means:
100% ·
 1 −k-means++ value
k-means value

.
Average φ
Minimum φ
Average T
k
k-means
k-means++
k-means
k-means++
k-means
k-means++
10
7.921 · 103
22.33%
6.284 · 103
10.37%
0.08
51.09%
25
3.637 · 103
42.76%
2.550 · 103
22.60%
0.11
43.21%
50
1.867 · 103
39.01%
1.407 · 103
23.07%
0.16
41.99%
Table 2: Experimental results on the Cloud dataset (n = 1024, d = 10). For k-means, we list the actual potential
and time in seconds. For k-means++, we list the percentage improvement over k-means.
Average φ
Minimum φ
Average T
k
k-means
k-means++
k-means
k-means++
k-means
k-means++
10
3.387 · 108
93.37%
3.206 · 108
94.40%
63.94
44.49%
25
3.149 · 108
99.20%
3.100 · 108
99.32%
257.34
49.19%
50
3.079 · 108
99.84%
3.076 · 108
99.87%
917.00
66.70%
Table 3: Experimental results on the Intrusion dataset (n = 494019, d = 35). For k-means, we list the actual
potential and time in seconds. For k-means++, we list the percentage improvement over k-means.
Average φ
Minimum φ
Average T
k
k-means
k-means++
k-means
k-means++
k-means
k-means++
10
3.698 · 104
49.43%
3.684 · 104
54.59%
2.36
69.00%
25
3.288 · 104
88.76%
3.280 · 104
89.58%
7.36
79.84%
50
3.183 · 104
95.35%
2.384 · 104
94.30%
12.20
75.76%
Table 4: Experimental results on the Spam dataset (n = 4601, d = 58). For k-means, we list the actual potential
and time in seconds. For k-means++, we list the percentage improvement over k-means.

--- Page 8 ---
the minimum and the average potential (actually di-
vided by the number of points), as well as the mean
running time. Our implementations are standard with
no special optimizations.
6.3
Results The results for k-means and k-means++
are displayed in Tables 1 through 4. We list the absolute
results for k-means, and the percentage improvement
achieved by k-means++ (e.g., a 90% improvement in
the running time is equivalent to a factor 10 speedup).
We observe that k-means++ consistently outperformed
k-means, both by achieving a lower potential value, in
some cases by several orders of magnitude, and also by
having a faster running time. The D2 seeding is slightly
slower than uniform seeding, but it still leads to a faster
algorithm since it helps the local search converge after
fewer iterations.
The synthetic example is a case where standard
k-means does very badly.
Even though there is an
“obvious” clustering, the uniform seeding will inevitably
merge some of these clusters, and the local search will
never be able to split them apart (see [12] for further
discussion of this phenomenon).
The careful seeding
method of k-means++ avoided this problem altogether,
and it almost always attained the optimal clustering on
the synthetic dataset.
The diﬀerence between k-means and k-means++
on the real-world datasets was also substantial.
In
every case, k-means++ achieved at least a 10% accuracy
improvement over k-means, and it often performed
much better.
Indeed, on the Spam and Intrusion
datasets, k-means++ achieved potentials 20 to 1000
times smaller than those achieved by standard k-means.
Each trial also completed two to three times faster, and
each individual trial was much more likely to achieve a
good clustering.
7
Conclusion and future work
We have presented a new way to seed the k-means
algorithm that is O(log k)-competitive with the optimal
clustering. Furthermore, our seeding technique is as fast
and as simple as the k-means algorithm itself, which
makes it attractive in practice.
Towards that end,
we ran preliminary experiments on several real-world
datasets, and we observed that k-means++ substantially
outperformed standard k-means in terms of both speed
and accuracy.
Although our analysis of the expected potential
E[φ] achieved by k-means++ is tight to within a con-
stant factor, a few open questions still remain. Most
importantly, it is standard practice to run the k-means
algorithm multiple times, and then keep only the best
clustering found. This raises the question of whether
k-means++ achieves asymptotically better results if it is
allowed several trials. For example, if k-means++ is run
2k times, our arguments can be modiﬁed to show it is
likely to achieve a constant approximation at least once.
We ask whether a similar bound can be achieved for a
smaller number of trials.
Also, experiments showed that k-means++ generally
performed better if it selected several new centers during
each iteration, and then greedily chose the one that
decreased φ as much as possible.
Unfortunately, our
proofs do not carry over to this scenario. It would be
interesting to see a comparable (or better) asymptotic
result proven here.
Finally, we are currently working on a more thor-
ough experimental analysis. In particular, we are mea-
suring the performance of not only k-means++ and stan-
dard k-means, but also other variants that have been
suggested in the theory community.
Acknowledgements
We would like to thank Rajeev Motwani for his helpful
comments.
References
[1] Pankaj K. Agarwal and Nabil H. Mustafa.
k-means
projective clustering. In PODS ’04: Proceedings of the
twenty-third ACM SIGMOD-SIGACT-SIGART sym-
posium on Principles of database systems, pages 155–
165, New York, NY, USA, 2004. ACM Press.
[2] D. Arthur and S. Vassilvitskii.
Worst-case and
smoothed analysis of the ICP algorithm, with an ap-
plication to the k-means method.
In Symposium on
Foundations of Computer Science, 2006.
[3] David Arthur and Sergei Vassilvitskii.
k-means++
test
code.
http://www.stanford.edu/∼darthur/
kMeansppTest.zip.
[4] David Arthur and Sergei Vassilvitskii.
How slow is
the k-means method?
In SCG ’06: Proceedings of
the twenty-second annual symposium on computational
geometry. ACM Press, 2006.
[5] Pavel Berkhin.
Survey of clustering data mining
techniques.
Technical report, Accrue Software, San
Jose, CA, 2002.
[6] Moses Charikar, Liadan O’Callaghan, and Rina Pani-
grahy. Better streaming algorithms for clustering prob-
lems. In STOC ’03: Proceedings of the thirty-ﬁfth an-
nual ACM symposium on Theory of computing, pages
30–39, New York, NY, USA, 2003. ACM Press.
[7] Philippe Collard’s cloud cover database. ftp://ftp.
ics.uci.edu/pub/machine-learning-databases/
undocumented/taylor/cloud.data.
[8] Sanjoy Dasgupta. How fast is k-means? In Bernhard
Sch¨olkopf and Manfred K. Warmuth, editors, COLT,
volume 2777 of Lecture Notes in Computer Science,
page 735. Springer, 2003.

--- Page 9 ---
[9] W. Fernandez de la Vega, Marek Karpinski, Claire
Kenyon, and Yuval Rabani. Approximation schemes
for clustering problems. In STOC ’03: Proceedings of
the thirty-ﬁfth annual ACM symposium on Theory of
computing, pages 50–58, New York, NY, USA, 2003.
ACM Press.
[10] P. Drineas, A. Frieze, R. Kannan, S. Vempala, and
V. Vinay. Clustering large graphs via the singular value
decomposition. Mach. Learn., 56(1-3):9–33, 2004.
[11] Fr´ed´eric Gibou and Ronald Fedkiw.
A fast hybrid
k-means level set algorithm for segmentation. In 4th
Annual Hawaii International Conference on Statistics
and Mathematics, pages 281–291, 2005.
[12] Sudipto Guha, Adam Meyerson, Nina Mishra, Rajeev
Motwani, and Liadan O’Callaghan.
Clustering data
streams: Theory and practice. IEEE Transactions on
Knowledge and Data Engineering, 15(3):515–528, 2003.
[13] Sariel Har-Peled and Soham Mazumdar. On coresets
for k-means and k-median clustering.
In STOC ’04:
Proceedings of the thirty-sixth annual ACM symposium
on Theory of computing, pages 291–300, New York,
NY, USA, 2004. ACM Press.
[14] Sariel Har-Peled and Bardia Sadri.
How fast is the
k-means method?
In SODA ’05: Proceedings of the
sixteenth annual ACM-SIAM symposium on Discrete
algorithms, pages 877–885, Philadelphia, PA, USA,
2005. Society for Industrial and Applied Mathematics.
[15] R.
Herwig,
A.J.
Poustka,
C.
Muller,
C.
Bull,
H. Lehrach, and J O’Brien. Large-scale clustering of
cdna-ﬁngerprinting data.
Genome Research, 9:1093–
1105, 1999.
[16] Mary Inaba, Naoki Katoh, and Hiroshi Imai. Applica-
tions of weighted voronoi diagrams and randomization
to variance-based k-clustering: (extended abstract). In
SCG ’94: Proceedings of the tenth annual symposium
on Computational geometry, pages 332–339, New York,
NY, USA, 1994. ACM Press.
[17] Tapas Kanungo, David M. Mount, Nathan S. Ne-
tanyahu, Christine D. Piatko, Ruth Silverman, and An-
gela Y. Wu. A local search approximation algorithm
for k-means clustering.
Comput. Geom., 28(2-3):89–
112, 2004.
[18] KDD Cup 1999 dataset.
http://kdd.ics.uci.edu/
/databases/kddcup99/kddcup99.html.
[19] Amit Kumar, Yogish Sabharwal, and Sandeep Sen. A
simple linear time (1 + ϵ)-approximation algorithm for
k-means clustering in any dimensions. In FOCS ’04:
Proceedings of the 45th Annual IEEE Symposium on
Foundations of Computer Science (FOCS’04), pages
454–462, Washington, DC, USA, 2004. IEEE Com-
puter Society.
[20] Stuart P. Lloyd. Least squares quantization in pcm.
IEEE Transactions on Information Theory, 28(2):129–
136, 1982.
[21] Jir´ı Matousek. On approximate geometric k-clustering.
Discrete & Computational Geometry,
24(1):61–84,
2000.
[22] Ramgopal R. Mettu and C. Greg Plaxton.
Optimal
time bounds for approximate clustering.
In Adnan
Darwiche and Nir Friedman, editors, UAI, pages 344–
351. Morgan Kaufmann, 2002.
[23] A. Meyerson. Online facility location. In FOCS ’01:
Proceedings of the 42nd IEEE symposium on Founda-
tions of Computer Science, page 426, Washington, DC,
USA, 2001. IEEE Computer Society.
[24] R. Ostrovsky, Y. Rabani, L. Schulman, and C. Swamy.
The eﬀectiveness of Lloyd-type methods for the k-
Means problem.
In Symposium on Foundations of
Computer Science, 2006.
[25] Spam e-mail database.
http://www.ics.uci.edu/
∼mlearn/databases/spambase/.


=== 2010.14487v1.pdf ===

--- Page 1 ---
arXiv:2010.14487v1  [cs.LG]  27 Oct 2020
Improved Guarantees for k-means++ and k-means++ Parallel
Konstantin Makarychev, Aravind Reddy, and Liren Shan
Department of Computer Science
Northwestern University
Evanston, IL, USA
Abstract
In this paper, we study k-means++ and k-means∥, the two most popular algorithms for the
classic k-means clustering problem. We provide novel analyses and show improved approxima-
tion and bi-criteria approximation guarantees for k-means++ and k-means∥. Our results give a
better theoretical justiﬁcation for why these algorithms perform extremely well in practice. We
also propose a new variant of k-means∥algorithm (Exponential Race k-means++) that has the
same approximation guarantees as k-means++.
1
Introduction
k-means clustering is one of the most commonly encountered unsupervised learning problems. Given
a set of n data points in Euclidean space, our goal is to partition them into k clusters (each
characterized by a center), such that the sum of squares of distances of data points to their nearest
centers is minimized. The most popular heuristic for solving this problem is Lloyd’s algorithm
(Lloyd, 1982), often referred to simply as “the k-means algorithm".
Lloyd’s algorithm uses iterative improvements to ﬁnd a locally optimal k-means clustering. The
performance of Lloyd’s algorithm crucially depends on the quality of the initial clustering, which
is deﬁned by the initial set of centers, called a seed. Arthur and Vassilvitskii (2007) and Ostrovsky,
Rabani, Schulman, and Swamy (2006) developed an elegant randomized seeding algorithm, known
as the k-means++ algorithm. It works by choosing the ﬁrst center uniformly at random from the
data set and then choosing the subsequent k−1 centers by randomly sampling a single point in each
round with the sampling probability of every point proportional to its current cost. That is, the
probability of choosing any data point x is proportional to the squared distance to its closest already
chosen center. This squared distance is often denoted by D2(x). Arthur and Vassilvitskii (2007)
proved that the expected cost of the initial clustering obtained by k-means++ is at most 8 (ln k + 2)
times the cost of the optimal clustering i.e., k-means++ gives an 8 (ln k + 2)-approximation for the
k-means problem. They also provided a family of k-means instances for which the approximation
factor of k-means++ is 2 ln k and thus showed that their analysis of k-means++ is almost tight.
Due to its speed, simplicity, and good empirical performance, k-means++ is the most widely
used algorithm for k-means clustering. It is employed by such machine learning libraries as Apache
The conference version of this paper will appear in the proceedings of the 34th Conference on Neural Information
Processing Systems (NeurIPS 2020). Author order is alphabetical.
1

--- Page 2 ---
Spark MLlib, Google BigQuery, IBM SPSS, Intel DAAL, and Microsoft ML.NET. In addition
to k-means++, these libraries implement a scalable variant of k-means++ called k-means∥(read
“k-means parallel”) designed by Bahmani, Moseley, Vattani, Kumar, and Vassilvitskii (2012). Some-
what surprisingly, k-means∥not only works better in parallel than k-means++ but also slightly
outperforms k-means++ in practice in the single machine setting (see Bahmani et al. (2012) and
Figure 1 below). However, theoretical guarantees for k-means∥are substantially weaker than for
k-means++.
The k-means∥algorithm makes T passes over the data set (usually T = 5). In every round,
it independently draws approximately ℓ= Θ(k) random centers according to the D2 distribution.
After each round it recomputes the distances to the closest chosen centers and updates D2(x) for
all x in the data set. Thus, after T rounds, k-means∥chooses approximately Tℓcenters. It then
selects k centers among Tℓcenters using k-means++.
Our contributions.
In this paper, we improve the theoretical guarantees for k-means++,
k-means∥, and Bi-Criteria k-means++ (which we deﬁne below).
First, we show that the expected cost of the solution output by k-means++ is at most 5(ln k+2)
times the optimal solution’s cost. This improves upon the bound of 8(ln k+2) shown by Arthur and
Vassilvitskii (2007) and directly improves the approximation factors for several algorithms which
use k-means++ as a subroutine like Local Search k-means++ (Lattanzi and Sohler, 2019). To
obtain this result, we give a reﬁned analysis of the expected cost of covered clusters (see Lemma
3.2 in Arthur and Vassilvitskii (2007) and Lemma 4.1 in this paper). We also show that our new
bound on the expected cost of covered clusters is tight (see Lemma C.1).
Then, we address the question of why the observed performance of k-means∥is better than the
performance of k-means++. There are two possible explanations for this fact. (1) This may be the
case because k-means∥picks k centers in two stages. At the ﬁrst stage, it samples ℓT ≥k centers.
At the second stage, it prunes centers and chooses k centers among ℓT centers using k-means++.
(2) This may also be the case because k-means∥updates the distribution function D2(x) once
in every round. That is, it recomputes D2(x) once for every ℓchosen centers, while k-means++
recomputes D2(x) every time it chooses a center. In this paper, we empirically demonstrate that
the ﬁrst explanation is correct. First, we noticed that k-means∥for ℓ· T = k is almost identical
with k-means++ (see Appendix A). Second, we compare k-means∥with another algorithm which
we call Bi-Criteria k-means++ with Pruning. This algorithm also works in two stages: At the
Bi-Criteria k-means++ stage, it chooses k + ∆centers in the data set using k-means++. Then, at
the Pruning stage, it picks k centers among the k +∆centers selected at the ﬁrst stage again using
k-means++. Our experiments on the standard data sets BioTest from KDD-Cup 2004 (Elber, 2004)
and COVTYPE from the UCI ML repository (Dua and Graﬀ, 2017) show that the performance
of k-means∥and Bi-Criteria k-means++ with Pruning are essentially identical (see Figures 1 and
Appendix A).
These results lead to another interesting question: How good are k-means++ and k-means∥
algorithms that sample k+∆instead of k centers? The idea of oversampling using k-means++ was
studied earlier in the literature under the name of bi-criteria approximation. Aggarwal, Deshpande,
and Kannan (2009) showed that with constant probability, sampling k + ∆centers by k-means++
provides a constant-factor approximation if ∆≥βk for some constant β > 0. Wei (2016) improved
on this result by showing an expected approximation ratio of 8(1 + 1.618k/∆). Note that for bi-
criteria algorithms we compare the expected cost of the clustering with k +∆centers they produce
and the cost of the optimal clustering with exactly k centers.
2

--- Page 3 ---
10
20
30
40
50
3
4
5
6
#centers
cost
(a) BioTest
k-means++
BiCriteria k-means++ w/Pruning
k-means∥
10
20
30
40
50
10
15
20
25
30
#centers
cost
(b) COVTYPE
k-means++
BiCriteria k-means++ w/Pruning
k-means∥
Figure 1: Performance of k-means++, k-means∥, and Bi-Criteria k-means++ with pruning on the
BioTest and COVTYPE datasets. For k = 10, 15, · · · , 50, we ran these algorithms for 50 iterations
and took their average. We normalized the clustering costs by dividing them by cost1000(X).
In this paper, we show that the expected bi-criteria approximation ratio for k-means++ with
∆additional centers is at most the minimum of two bounds:
(A) 5

2 + 1
2e + ln 2k
∆

for 1 ≤∆≤2k; and (B) 5

1 +
k
e (∆−1)

for ∆≥1
Both bounds are better than the bound by Wei (2016). The improvement is especially noticeable
for small values of ∆. More speciﬁcally, when the number of additional centers is ∆= k/ log k, our
approximation guarantee is O(log log k) while Wei (2016) gives an O(log k) approximation.
We believe that our results for small values of ∆provide an additional explanation for why
k-means++ works so well in practice. Consider a data scientist who wants to cluster a data set
X with k∗true clusters (i.e. k∗latent groups). Since she does not know the actual value of k∗,
she uses the elbow method (Boehmke and Greenwell, 2019) or some other heuristic to ﬁnd k. Our
results indicate that if she chooses slightly more number of clusters (for instance, 1.05k∗), then she
will get a constant bi-criteria approximation to the optimal clustering.
We also note that our bounds on the approximation factor smoothly transition from the regular
(∆= 0) to bi-criteria (∆> 0) regime. We complement our analysis with an almost matching lower
bound of Θ(log(k/∆)) on the approximation factor of k-means for ∆≤k (see Appendix C).
We then analyze Bi-Criteria k-means∥algorithm, the variant of k-means∥that does not prune
centers at the second stage.
In their original paper, Bahmani, Moseley, Vattani, Kumar, and
Vassilvitskii (2012) showed that the expected cost of the solution for k-means∥with T rounds and
oversampling parameter ℓis at most:
16
1 −αOPTk(X) +
1 + α
2
T
OPT1(X),
where α = exp

−

1 −e−ℓ/(2k)
; OPTk(X) is the cost of the optimal k-means clustering of X;
OPT1(X) is the cost of the optimal clustering of X with 1 center (see Section 2 for details). We
note that OPT1(X) ≫OPTk(X).
For ℓ= k, this result gives a bound of ≈49 OPTk(X) +
0.83T OPT1(X).
Bachem, Lucic, and Krause (2017) improved the approximation guarantee for
3

--- Page 4 ---
ℓ≥k to
26OPTk(X) + 2
 k
eℓ
T
OPT1(X).
In this work, we improve this bound for ℓ≥k and also obtain a better bound for ℓ< k. For ℓ≥k,
we show that the cost of k-means∥without pruning is at most
8OPTk(X) + 2
 k
eℓ
T
OPT1(X).
For ℓ< k, we give a bound of
5
1 −e−ℓ
k
OPTk(X) + 2

e−ℓ
k
T
OPT1(X)
Finally, we give a new parallel variant of the k-means++ algorithm, which we call Exponential
Race k-means++ (k-means++ER). This algorithm is similar to k-means∥. In each round, it also
selects ℓcandidate centers in parallel (some of which may be dropped later) making one pass over
the data set. However, after T rounds, it returns exactly k centers. The probability distribution
of these centers is identical to the distribution of centers output by k-means++. The expected
number of rounds is bounded as follows:
O
k
ℓ+ log OPT1(X)
OPTk(X)

.
This algorithm oﬀers a unifying view on k-means++ and k-means∥. We describe it in Section 7.
Other related work. Dasgupta (2008) and Aloise, Deshpande, Hansen, and Popat (2009) showed
that k-means problem is NP-hard. Awasthi, Charikar, Krishnaswamy, and Sinop (2015) proved that
it is also NP-hard to approximate k-means objective within a factor of (1 + ε) for some constant
ε > 0 (see also Lee, Schmidt, and Wright (2017)). We also mention that k-means was studied not
only for Euclidean spaces but also for arbitrary metric spaces.
There are several known constant factor approximation algorithms for the k-means problem.
Kanungo, Mount, Netanyahu, Piatko, Silverman, and Wu (2004) gave a 9 + ε approximation local
search algorithm. Ahmadian, Norouzi-Fard, Svensson, and Ward (2019) proposed a primal-dual
algorithm with an approximation factor of 6.357.
This is the best known approximation for k-
means. Makarychev, Makarychev, Sviridenko, and Ward (2016) gave constant-factor bi-criteria
approximation algorithms based on linear programming and local search. Note that although these
algorithms run in polynomial time, they do not scale well to massive data sets.
Lattanzi and
Sohler (2019) provided a constant factor approximation by combining the local search idea with
the k-means++ algorithm. Choo, Grunau, Portmann, and Rozhoň (2020) further improved upon
this result by reducing the number of local search steps needed from O(k log log k) to O(k).
Independently and concurrently to our work, Rozhoň (2020) gave an interesting analysis for
k-means∥by viewing it as a balls into bins problem and showed that O(log n/ log log n) rounds
suﬃce to give a constant approximation with high probability.
Acknowledgments. We would like to thank all the reviewers for their helpful comments. Kon-
stantin Makarychev, Aravind Reddy, and Liren Shan were supported in part by NSF grants CCF-
1955351 and HDR TRIPODS CCF-1934931. Aravind Reddy was also supported in part by NSF
CCF-1637585.
4

--- Page 5 ---
2
Preliminaries
Given a set of points X = {x1, x2, · · · , xn} ⊆Rd and an integer k ≥1, the k-means clustering
problem is to ﬁnd a set C of k centers in Rd to minimize
cost(X, C) :=
X
x∈X
min
c∈C ∥x −c∥2.
For any integer i ≥1, let us deﬁne OPTi(X) := min|C|=i cost (X, C) . Thus, OPTk(X) refers to the
cost of the optimal solution for the k-means problem. Let C∗denote a set of optimal centers. We
use {Pi}k
i=1 to denote the clusters induced by the center set C∗.
For any Y ⊆X, the cost of Y with center set C, denoted by cost (Y, C) = P
x∈Y minc∈C∥x−c∥2.
The optimal cost for subset Y with i centers is OPTi(Y). Let µ = P
x∈Y x/ |Y| be the centroid of
the cluster Y. Then, we have the following closed form expression for the optimal cost of Y with
one center (see Appendix B for proof),
OPT1(Y) =
X
x∈Y
∥x −µ∥2 =
P
(x,y)∈Y×Y∥x −y∥2
2 |Y|
.
(1)
k-means++ seeding: The k-means++ algorithm samples the ﬁrst center uniformly at random
from the given points and then samples k −1 centers sequentially from the given points with
probability of each point being sampled proportional to its cost i.e. cost(x, C)/cost(X, C).
Algorithm 1 k-means++ seeding
1: Sample a point c uniformly at random from X and set C1 = {c}.
2: for t = 2 to k do
3:
Sample x ∈X w.p. cost(x, Ct)/cost(X, Ct).
4:
Ct = Ct−1 ∪{x}.
5: end for
6: Return Ck
k-means∥and k-means∥Pois seeding: In the k-means∥algorithm, the ﬁrst center is chosen
uniformly at random from X. But after that, at each round, the algorithm samples each point
independently with probability min {ℓ· cost(x, C)/cost(X, C), 1} where ℓis the oversampling pa-
rameter chosen by the user and it usually lies between 0.1k and 10k. The algorithm runs for T
rounds (where T is also a parameter chosen by the user) and samples around ℓT points, which is
usually strictly larger than k. This oversampled set is then weighted using the original data set X
and a weighted version of k-means++ is run on this set to get the ﬁnal k-centers. We only focus
on the stage in which we get the oversampled set because the guarantees for the second stage come
directly from k-means++.
For the sake of analysis, we also consider a diﬀerent implementation of k-means∥, which we
call k-means∥Pois
(Algorithm 3).
This algorithm diﬀers from k-means∥in that each point is
sampled independently with probability 1 −exp(−ℓ· cost(x, C)/cost(X, C)) rather than min{ℓ·
cost(x, C)/cost(X, C), 1}. In practice, there is essentially no diﬀerence between k-means∥and k-
means∥Pois, since ℓ· cost(x, C)/cost(X, C) is a very small number for all x and thus the sampling
probabilities for k-means∥and k-means∥Pois are almost equal.
5

--- Page 6 ---
Algorithm 2 k-means∥seeding
1: Sample a point c uniformly from X and
set C1 = {c}
2: for t = 1 to T do
3:
Sample each point x into C′ indepen-
dently w.p. min{1, λt(x)} where
λt(x) = ℓ· cost(x, Ct)/cost(X, Ct)
4:
Let Ct+1 = Ct ∪C′.
5: end for
Algorithm 3 k-means∥Pois seeding
1: Sample a point c uniformly from X and
set C1 = {c}
2: for t = 1 to T do
3:
Sample each point x into C′ indepen-
dently w.p. 1 −e−λt(x) where
λt(x) = ℓ· cost(x, Ct)/cost(X, Ct)
4:
Let Ct+1 = Ct ∪C′.
5: end for
In the rest of the paper, we focus only on the seeding step of k-means++, k-means∥, and k-
means∥Pois and ignore Lloyd’s iterations as the approximation guarantees for these algorithms come
entirely from the seeding step.
3
General framework
In this section, we describe a general framework we use to analyze k-means++ and k-means∥Pois.
Consider k-means++ or k-means∥Pois algorithm.
Let Ct be the set of centers chosen by this
algorithm after step t. For the sake of analysis, we assume that Ct is an ordered set or list of
centers, and the order of centers in Ct is the same as the order in which our algorithm chooses these
centers. We explain how to order centers in k-means∥Pois algorithm in Section 6. We denote by T
the stopping time of the algorithm. Observe that after step t of the algorithm, the probabilities of
choosing a new center in k-means++ or a batch of new centers in k-means∥Pois are deﬁned by the
current costs of points in X which, in turn, are completely determined by the current set of centers
Ct. Thus, the states of the algorithm form a Markov chain.
In our analysis, we ﬁx the optimal clustering P = {P1, . . . , Pk} (if this clustering is not unique,
we pick an arbitrary optimal clustering). The optimal cost of each cluster Pi is OPT1(Pi) and the
optimal cost of the entire clustering is OPTk(X) = Pk
i=1 OPT1(Pi).
Following the notation in Arthur and Vassilvitskii (2007), we say that a cluster Pi is hit or
covered by a set of centers C if C ∩Pi ̸= ∅; otherwise, we say that Pi is not hit or uncovered. We
split the cost of each cluster Pi into two components which we call the covered and uncovered costs
of Pi. For a given set of centers C,
The covered or hit cost of Pi,
H(Pi, C) :=
(
cost(Pi, C),
if Pi is covered by C
0,
otherwise.
The uncovered cost of Pi,
U(Pi, C) :=
(
0,
if Pi is covered by C
cost(Pi, C),
otherwise.
Let H(X, C) = Pk
i=1 H(Pi, C) and U(X, C) = Pk
i=1 U(Pi, C). Then,
cost(X, C) = H(X, C) + U(X, C).
For the sake of brevity, we deﬁne costt(Y) := cost(Y, Ct) for any Y ⊆X, Ht(Pi) := H(Pi, Ct), and
Ut(Pi) := U(Pi, Ct). In Section 4, we show that for any t, we have E[Ht(X)] ≤5OPTk(X), which is
an improvement over the bound of 8OPTk(X) given by Arthur and Vassilvitskii (2007). Then, in
6

--- Page 7 ---
Sections 5 and 6, we analyze the expected uncovered cost U(X, CT ) for k-means++ and k-means∥
algorithms.
Consider a center c in C. We say that c is a miss if another center c′ covers the same cluster
in P as c, and c′ appears before c in the ordered set C. We denote the number of misses in C by
M(C) and the the number of clusters in P not covered by centers in C by K(C).
Observe that the stochastic processes Ut(Pi) with discrete time t are non-increasing since the
algorithm never removes centers from the set Ct and therefore the distance from any point x to
Ct never increases. Similarly, the processes Ht(Pi) are non-increasing after the step ti when Pi is
covered ﬁrst time. In this paper, we sometimes use a proxy eHt(Pi) for Ht(Pi), which we deﬁne
as follows. If Pi is covered by Ct, then eHt(Pi) = Hti(Pi), where ti ≤t is the ﬁrst time when Pi
is covered by Ct. If Pi is not covered by Ct, then eHt(Pi) = 5OPT1(Pi). It is easy to see that
Ht(Pi) ≤eHt′(Pi) for all t ≤t′. In Section 4, we also show that eHt(Pi) is a supermartingale i.e.,
E[ eHt′(Pi) | Ct] ≤eHt(Pi) for all t ≤t′.
4
Bound on the cost of covered clusters
In this section, we improve the bound by Arthur and Vassilvitskii (2007) on the expected cost
of a covered cluster in k-means++. Our bound also works for k-means∥Pois algorithm. Pick an
arbitrary cluster Pi in the optimal solution P = {P1, . . . , Pk} and consider an arbitrary state
Ct = {c1, . . . , cN} of the k-means++ or k-means∥Pois algorithm. Let Dt+1 be the set of new centers
the algorithm adds to Ct at step t (for k-means++, Dt+1 contains only one center). Suppose now
that centers in Dt+1 cover Pi i.e. Dt+1 ∩Pi ̸= ∅. We show that the expected cost of cluster Pi
after step (t + 1) conditioned on the event {Dt+1 ∩Pi ̸= ∅} and the current state of the algorithm
Ct is upper bounded by 5OPT1(Pi) i.e.
E [cost(Pi, Ct+1) | Ct, {Dt+1 ∩Pi ̸= ∅}] ≤5OPT1(Pi).
(2)
We now prove the main lemma.
Lemma 4.1. Consider an arbitrary set of centers C = {c1, . . . , cN} ⊆Rd and an arbitrary set
P ⊆X. Pick a random point c in P with probability Pr(c = x) = cost(x, C)/cost(P, C). Let
C′ = C ∪{c}. Then, Ec [cost(P, C′)] ≤5OPT1(P).
Remarks: Lemma 3.2 in the paper by Arthur and Vassilvitskii (2007) gives a bound of 8OPT1(P).
We also show in Appendix C that our bound is tight (see Lemma C.1).
Proof. The cost of any point y after picking center c equals the squared distance from y to the
set of centers C′ = C ∪{c}, which in turn equals min{cost(y, C), ∥y −c∥2}.
Thus, if a point
x ∈P is chosen as a center, then the cost of point y equals min{cost(y, C), ∥x −y∥2}.
Since
Pr(c = x) = cost(x, C)/cost(P, C), we have
Ec
cost(P, C′)
 =
X
x∈P
y∈P
cost(x, C)
cost(P, C) · min{cost(y, C), ∥x −y∥2}.
We write the right hand side in a symmetric form with respect to x and y. To this end, we deﬁne
function f as follows:
f(x, y) = cost(x, C) · min
n
∥x −y∥2, cost(y, C)
o
+ cost(y, C) · min
n
∥x −y∥2, cost(x, C)
o
.
7

--- Page 8 ---
Note that f(x, y) = f(y, x). Then,
Ec
cost(P, C′)
 =
1
2cost(P, C)
X
(x,y)∈P ×P
f(x, y).
We now give an upper bound on f(x, y) and then use this bound to ﬁnish the proof of Lemma 4.1.
Lemma 4.2. For any x, y ∈P, we have f(x, y) ≤5 min {cost(x, C), cost(y, C)} ∥x −y∥2.
Proof. Since f(x, y) is a symmetric function with respect to x and y, we may assume without loss of
generality that cost(x, C) ≤cost(y, C). Then, we need to show that f(x, y) ≤5cost(x, C)∥x −y∥2.
Consider three cases.
Case 1: If cost(x, C) ≤cost(y, C) ≤∥x −y∥2, then
f(x, y) = 2cost(x, C)cost(y, C) ≤2cost(x, C)∥x −y∥2.
Case 2: If cost(x, C) ≤∥x −y∥2 ≤cost(y, C), then
f(x, y) = cost(x, C)∥x −y∥2 + cost(y, C)cost(x, C).
By the triangle inequality, we have
cost(y, C) ≤
q
cost(x, C) + ∥x −y∥
2
≤4∥x −y∥2.
Thus, f(x, y) ≤5cost(x, C)∥x −y∥2.
Case 3: If ∥x −y∥2 ≤cost(x, C) ≤cost(y, C), then
f(x, y) = (cost(x, C) + cost(y, C)) ∥x −y∥2.
By the triangle inequality,
cost(y, C) ≤
q
cost(x, C) + ∥x −y∥
2
≤4cost(x, C).
Thus, we have f(x, y) ≤5cost(x, C)∥x −y∥2.
In all cases, the desired inequality holds. This concludes the proof of Lemma 4.2.
We use Lemma 4.2 to bound the expected cost of P. Let φ∗be a vector in RP with φ∗
x =
cost(x, C) for any x ∈P. Then, f(x, y) ≤5 min
n
φ∗
x, φ∗
y
o
∥x −y∥2. Since cost(P, C) = P
z∈P φ∗
z, we
have
Ec
cost(P, C′)
 ≤
5 P
(x,y)∈P ×P min
n
φ∗
x, φ∗
y
o
∥x −y∥2
2 P
z∈P φ∗z
|
{z
}
5F (φ∗)
.
For arbitrary vector φ ∈RP
≥0, deﬁne the following function:
F(φ) =
P
(x,y)∈P ×P min {φx, φy} ∥x −y∥2
2 P
z∈P φz
.
(3)
8

--- Page 9 ---
We have Ec [cost(P, C′)] ≤5F(φ∗). Thus, to ﬁnish the proof of Lemma 4.1, it suﬃces to show that
F(φ) ≤OPT1(P) for every φ ≥0 and particularly for φ = φ∗. By Lemma 4.3 (which we state and
prove below), function F(φ) is maximized when φ ∈{0, 1}P . Let φ∗∗be a maximizer of F(φ) in
{0, 1}P and P ′ = {x ∈P : φ∗∗
x = 1}. Observe that
F(φ∗∗) =
P
(x,y)∈P ′×P ′∥x −y∥2
2|P ′|
= OPT1(P ′).
Here we used the closed form expression (1) for the optimal cost of cluster P ′. Since P ′ ⊂P, we
have OPT1(P ′) ≤OPT1(P). Thus, F(φ∗) ≤F(φ∗∗) ≤OPT1(P).
Lemma 4.3. There exists a maximizer φ∗∗of F(φ) in the region {φ ≥0} such that φ ∈{0, 1}P .
Proof. Let m = |P| be the size of the cluster P and Π be the set of all bisections or permutations
π : {1, . . . , m} →P. Partition the set {φ ≥0} into m! regions (“cones over order polytopes”):
{φ : φ ≥0} = ∪π∈ΠOπ,
where Oπ = {φ : 0 ≤φπ(1) ≤φπ(2) ≤· · · ≤φπ(m)}. We show that for every π ∈Π, there exists a
maximizer φ∗∗of F(φ) in the region Oπ, such that φ∗∗∈{0, 1}P . Therefore, there exists a global
maximizer φ∗∗that belongs {0, 1}P
Fix a π ∈Π. Denote by V the hyperplane {φ : P
x∈P φx = 1}. Observe that F is a scale
invariant function i.e., F(φ) = F(λφ) for every λ > 0. Thus, for every φ ∈Oπ, there exists a
φ′ ∈Oπ ∩V (namely, φ′ = φ/(P
x∈P φx)) such that F(φ′) = F(φ). Hence, max{F(φ) : φ ∈Oπ} =
max{F(φ) : φ ∈Oπ ∩V }. Note that for φ ∈V , the denominator of (3) equals 2, and for φ ∈Oπ,
the numerator of (3) is a linear function of φ. Therefore, F(φ) is a linear function in the convex
set Oπ ∩V . Consequently, one of the maximizers of F must be an extreme point of Oπ ∩V .
The polytope Oπ ∩V is deﬁned by m inequalities and one equality. Thus, for every extreme
point φ of this polytope, all inequalities φπ(i) ≤φπ(i+1) but one must be tight. In other words, for
some j < m, we have
0 = φπ(1) = · · · = φπ(j) < φπ(j+1) = · · · = φπ(m).
(4)
Therefore, there exists a maximizer φ of F(φ) in Oπ ∩V satisfying (4) for some j. After rescaling
φ – multiplying all coordinates of φ by (m −j) – we obtain a vector φ∗∗whose ﬁrst j coordinates
φ∗∗
π(1), . . . , φ∗∗
π(j) are zeroes and the last m −j coordinates φ∗∗
π(j+1), . . . , φ∗∗
π(m) are ones. Thus, φ∗∗∈
{0, 1}P . Since F is rescaling invariant, F(φ∗∗) = F(φ). This concludes the proof.
Replacing the bound in Lemma 3.2 from the analysis of Arthur and Vassilvitskii (2007) by our
bound from Lemma 4.1 gives the following result (see also Lemma 5.6).
Theorem 4.4. The approximation factor of k-means++ is at most 5(ln k + 2).
We now state an important corollary of Lemma 4.1.
Corollary 4.5.
For every P ∈P, the process eHt(P) for k-means++ is a supermartingale i.e.,
E
h
eHt+1(X) | Ct
i
≤eHt(X).
9

--- Page 10 ---
Proof. The value of eHt(X) changes only if at step t, we cover a yet uncovered cluster P. In this
case, the value of eHt+1(P) changes by the new cost of P minus 5OPT(P). By Lemma 4.1 this
quantity is non-positive in expectation.
Since the process eHt(P) is a supermartingale, we have E[ eHt(P)] ≤eH0(P) = 5OPT1(P). Hence,
E[Ht(P)] ≤E[ eHt(P)] = 5OPT1(P). Thus, E[Ht(X)] ≤5OPTk(X). Since costt(X) = Ht(X) +
Ut(X) and we have a bound on the expectation of the covered cost, Ht(X), in the remaining
sections, we shall only analyze the uncovered cost Ut(X).
5
Bi-criteria approximation of k-means++
In this section, we give a bi-criteria approximation guarantee for k-means++.
Theorem 5.1. Let costk+∆(X) be the cost of the clustering with k + ∆centers sampled by the
k-means++ algorithm. Then, for ∆≥1, the expected cost E [costk+∆(X)] is upper bounded by
(below (a)+ denotes max(a, 0)).
min
n
2 + 1
2e +

ln 2k
∆
+
, 1 +
k
e (∆−1)
o
5OPTk(X).
Note that the above approximation guarantee is the minimum of two bounds: (1) 2+ 1
2e + ln 2k
∆
for 1 ≤∆≤2k; and (2) 1 +
k
e(∆−1) for ∆≥1. The second bound is stronger than the ﬁrst bound
when ∆/k ⪆0.085.
5.1
Proof overview of Theorem 5.1
We now present a high level overview of the proof and then give a formal proof. Our proof consists
of three steps.
First, we prove bound (2) on the expected cost of the clustering returned by k-means++ after
k + ∆rounds. We argue that the expected cost of the covered clusters is bounded by 5OPTk(X)
(see Section 3) and thus it is suﬃcient to bound the expected cost of uncovered clusters. Consider
an optimal cluster P ∈P. We need to estimate the probability that it is not covered after k + ∆
rounds. We upper bound this probability by the probability that the algorithm does not cover P
before it makes ∆misses (note: after k + ∆rounds k-means++ must make at least ∆misses).
In this overview, we make the following simplifying assumptions (which turn out to be satisﬁed
in the worst case for bi-criteria k-means++): Suppose that the uncovered cost of cluster P does not
decrease before it is covered and equals U(P) and, moreover, the total cost of all covered clusters
almost does not change and equals H(X) (this may be the case if one large cluster contributes
most of the covered cost, and that cluster is covered at the ﬁrst step of k-means++). Under these
assumptions, the probability that k-means++ chooses ∆centers in the already covered clusters
and does not choose a single center in P equals (H(X)/(U(P) + H(X)))∆. If k-means++ does
not choose a center in P, the uncovered cost of cluster P is U(P); otherwise, the uncovered cost of
cluster P is 0. Thus, the expected uncovered cost of P is (H(X)/(U(P)+H(X)))∆U(P). It is easy
to show that (H(X)/(U(P) + H(X)))∆U(P) ≤H(X)/(e(∆−1)). Thus, the expected uncovered
cost of all clusters is at most
k
(e(∆−1))E[H(X)] ≤
k
(e(∆−1))5OPTk(X).
10

--- Page 11 ---
Then, we use ideas from Arthur and Vassilvitskii (2007), Dasgupta (2013) to prove the following
statement: Let us count the cost of uncovered clusters only when the number of misses after k
rounds of k-means++ is greater than ∆/2. Then the expected cost of uncovered clusters is at most
O(log(k/∆)) · OPTk(X). That is, E[H(Uk(X) · 1{M(Ck) ≥∆/2}] ≤O(log(k/∆)) · OPTk(X).
Finally, we combine the previous two steps to get bound (1). We argue that if the number of
misses after k rounds of k-means++ is less than ∆/2, then almost all clusters are covered. Hence,
we can apply bound (2) to k′ ≤∆/2 uncovered clusters and ∆remaining rounds of k-means++
and get a 5(1+1/(2e)) approximation. If the number of misses is greater than ∆/2, then the result
from the previous step yields an O(log(k/∆)) approximation.
5.2
Analysis of k-means++
In this section, we analyze the bi-criteria k-means++ algorithm and prove Theorem 5.1. To this end,
we establish the ﬁrst and second bounds from Theorem 5.1 on the expected cost of the clustering
after k + ∆rounds of k-means. We will start with the second bound.
5.2.1
Bi-criteria bound for large ∆
Lemma 5.2. The following bi-criteria bound holds
E [costk+∆(X)] ≤5

1 +
k
e (∆−1)

OPTk(X).
Consider the discrete time Markov chain Ct associated with k-means++ algorithm (see Sec-
tion 3). Let P ∈P be an arbitrary cluster in the optimal solution. Partition all states of the
Markov chain into k + ∆disjoint groups M0, M1, · · · , Mk+∆−1 and H. Each set Mi contains all
states C with i misses that do not cover P: Mi = {C : M(C) = i, P ∩C = ∅} . The set H contains
all states C that cover P: H = {C : P ∩C ̸= ∅}.
We now deﬁne a new Markov chain Xt. To this end, we ﬁrst expand the set of states {C}. For
every state C of the process Ct, we create two additional “virtual” states Ca and Cb. Then, we let
X2t = Ct for every even step 2t, and
X2t+1 =
(
Ca
t ,
if Ct+1 ∈Mi
Cb
t ,
if Ct+1 ∈Mi+1 ∪H.
for every odd step 2t + 1. We stop Xt when Ct stops or when Ct hits the set H (i.e., Ct ∈H).
Loosely speaking, Xt follows Markov chain Ct but makes additional intermediate stops. When Ct
moves from one state in Mi to another state in Mi, X2t+1 stops in Ca
t ; and when Ct moves from
a state in Mi to a state in Mi+1 or H, X2t+1 stops in Cb
t .
Write transition probabilities for Xt:
P [X2t+1 = Ca | X2t = C] = U(X, C) −U(P, C)
cost(X, C)
,
P
h
X2t+1 = Cb | X2t = C
i
= U(P, C) + H(X, C)
cost(X, C)
,
and for all C ∈Mi and C′ = C ∪{x} ∈Mi,
P
X2t+2 = C′ | X2t+1 = Ca =
cost(x, C)
U(X, C) −U(P, C),
11

--- Page 12 ---
for all C ∈Mi and C′ = C ∪{x} ∈Mi+1 ∪H,
P
h
X2t+2 = C′ | X2t+1 = Cbi
=
cost(x, C)
U(P, C) + H(X, C).
Above, U(X, C) −U(P, C) is the cost of points in all uncovered clusters except for P. If we pick a
center from these clusters, we will necessarily cover a new cluster, and therefore X2t+2 will stay in
Mi. Similarly, U(P, C) + H(X, C) is the cost of all covered clusters plus the cost of P. If we pick
a center from these clusters, then X2t+2 will move to Mi+1 or H.
Deﬁne another Markov chain {Yt}. The transition probabilities of {Yt} are the same as the
transition probabilities of Xt except Y never visits states in H and therefore for C ∈Mi and
C′ = C ∪{x} ∈Mi+1, we have
P
h
Y2t+2 = C′ | Y2t+1 = Cbi
= cost(x, C)
H(X, C) .
We now prove a lemma that relates probabilities of visiting states by Xt and Yt.
Lemma 5.3.
For every t ≤k + ∆and states C′ ∈Mi, C′′ ∈M∆, we have
P [C′′ ∈{Xj} | X2t = C′]
P [C′′ ∈{Yj} | Y2t = C′] ≤
 
eH(X, C′′)
eH(X, C′′) + U(P, C′′)
!∆−i
where {C′′ ∈{Xj}} and {C′′ ∈{Yj}} denote the events X visits C′′ and Y visits C′′, respectively.
Proof. Consider the unique path p from C′ to C′′ in the state space of X (note that the transition
graphs for X and Y are directed trees). The probability of transitioning from C′ to C′′ for X and
Y equals the product of respective transition probabilities for every edge on the path. Recall that
transitions probabilities for X and Y are the same for all states but Cb, where C ∈∪jMj. The
number of such states on the path p is equal to the number transitions from Mj to Mj+1, since X
and Y can get from Mj to Mj+1 only through a state Cb on the boundary of Mj and Mj+1. The
number of transitions from Mj to Mj+1 equals ∆−i. For each state Cb on the path, the ratio of
transition probabilities from Cb to the next state C ∪{x} for Markov chains X and Y equals
H(X, C)
U(P, C) + H(X, C) ≤
eH(X, C′′)
U(P, C′′) + eH(X, C′′)
,
here we used that (a) U(P, C) ≥U(P, C′′) since Ut(P) is a non-increasing process; and (b)
H(P, C) ≤eH(P, C′′) since Ht(P) ≤eHt′(P) if t ≤t′ (see Section 3).
We now prove an analog of Corollary 4.5 for eH(X, Yj).
Lemma 5.4.
eH(X, Yt) is a supermartingale.
Proof. If Yj = C, then Yj+1 can only be in
n
Ca, Cbo
. Since eH(X, Ca) = eH(X, Cb) = eH(X, C), we
have E
h
eH(X, Yj+1) | Yj = C
i
= eH(X, Yj).
If Yj = Ca, then Yj+1 = C′ where the new center c should be in uncovered clusters with respect
to Ct.
E
H(P ′, Yj+1) | Yj = Ca, c ∈P ′ ≤5OPT1(P ′),
12

--- Page 13 ---
which implies
E
h
eH(P ′, Yj+1) | Yj = Ca, c ∈P ′i
≤eH(P ′, Yj).
Therefore, we have
E
h
eH(X, Yj+1) | Yj = Cai
≤eH(X, Yj).
If Yj = Cb, then for any possible state C′ of Yj+1, the new center should be in covered clusters with
respect to C. By deﬁnition, we must have eH(X, C′) = eH(X, C) = eH(X, Cb). Thus, it holds that
E
h
eH(X, Yj+1) | Yj = Cbi
= eH(X, Yj).
Combining all these cases, we get
n
eH(X, Yj)
o
is a supermartingale.
We now use Lemma 5.3 and Lemma 5.4 to bound the expected uncovered cost of P after k + ∆
rounds of k-means++.
Lemma 5.5.
For any cluster P ∈P and t ≤k + ∆, we have
E [Uk+∆(P) | Ct] ≤
eHt(X)
e(∆−M(Ct) −1).
Proof. Since k-means++ samples k + ∆centers and the total number of clusters in the optimal
solution P is k, k-means++ must make ∆misses. Hence, the process {Xt} which follows k-means++
must either visit a state in M≥∆or stop in H (recall that we stop process Xt if it reaches H).
If {Xt} stops in group H, then the cluster P is covered which means that Uk+∆(P) = 0. Let
∂M∆be the frontier of M∆i.e., the states that Xt visits ﬁrst when it reaches M∆(recall that
the transition graph of Xt is a tree). The expected cost E [Uk+∆(P) | Ct] is upper bounded by the
expected uncovered cost of P at time when Ct reaches ∂M∆. Thus,
E [Uk+∆(P) | Ct] ≤
X
C∈∂M∆
P [C ∈{Xj} | Ct] U(P, C).
Observe that by Lemma 5.3, for any C ∈∂M∆, we have
P [C ∈{Xj} | Ct] U(P, C) ≤P [C ∈{Yj} | Ct]
 
eH(X, C)
eH(X, C) + U(P, C)
!∆′
U(P, C).
Let f(x) = x(1/(1+ x))∆′. Then, f(x) is maximized at x = 1/(∆′ −1) and the maximum value
f(1/(∆′ −1)) = 1/(e(∆′ −1)). Therefore, for every C ∈∂M∆, we have
P[C ∈{Xj} | Ct]U(P, C) ≤P [C ∈{Yj} | Ct] f
 
U(P, C)
eH(X, C)
!
eH(X, C)
≤P [C ∈{Yj} | Ct]
eH(X, C)
e(∆′ −1).
Let τ = min {j : Yj ∈∂M∆} be the stopping time when Yj ﬁrst visits ∂M∆. We get
X
C∈∂M∆
P [C ∈{Yj} | Ct] eH(X, C) = E
h
eH(X, Yτ) | Ct
i
.
13

--- Page 14 ---
By Lemma 5.4, eH(X, Yj) is a supermartingale. Thus, by the optional stopping theorem,
E
h
eH(X, Yτ) | Ct
i
≤eH(X, Ct).
Therefore, we have
E [Uk+∆(P) | Ct] ≤
eHt(X)
e(∆−M(Ct) −1),
This concludes the proof.
We now add up bounds from Lemma 5.5 with t = 0 for all clusters P ∈P and obtain Lemma 5.2.
5.3
Bi-criteria bound for small ∆
In this section, we give another bi-criteria approximation guarantee for k-means++.
Lemma 5.6. Let costk+∆(X) be the cost of the the clustering resulting from sampling k+∆centers
according to the k-means++ algorithm (for ∆∈{1, . . . , 2k}). Then,
E [costk+∆(X)] ≤5

2 + 1
2e + ln 2k
∆

OPTk(X).
Proof. Consider k-means++ clustering algorithm and the corresponding random process Ct. Fix
a κ ∈{1, . . . , k}. Let τ be the ﬁrst iteration1 (stopping time) when K(Cτ) ≤κ if K(Ck) ≤κ; and
τ = k, otherwise. We refer the reader to Section 3 for deﬁnitions of M(Ct), Ut(X) = U(X, Ct), and
K(Ct).
We separately analyze the cost of uncovered clusters after the ﬁrst τ steps and the last k′ −τ
steps, where k′ = k + ∆is the total number of centers chosen by k-means++.
The ﬁrst step of our proof follows the analysis of k-means++ by Dasgupta (2013), and by
Arthur and Vassilvitskii (2007). Deﬁne a potential function Ψ (see Dasgupta 2013):
Ψt := M(Ct)U(X, Ct)
K(Ct)
.
If K(Ct) = 0, then M(Ct) and U(X, Ct) must be 0 and we let Ψt = 0
We use the following result by Dasgupta (2013) to estimate E[Ψτ(X)] in Lemma 5.8.
Lemma 5.7 (Dasgupta (2013)). For any 0 ≤t ≤k, we have
E [Ψt+1 −Ψt | Ct] ≤H(X, Ct)
K(Ct) .
Lemma 5.8. Then, the following bound holds:
E[Ψτ(X)] ≤5

1 + ln

k
κ + 1

OPTk(X).
1Recall, that K(Ct) is a non-increasing stochastic process with K(C0) = k.
14

--- Page 15 ---
Proof. Note that Ψ1 = 0 as M(C1) = 0. Thus,
E[Ψτ] ≤
τ−1
X
t=1
E
Ψt+1 −Ψt
 ≤E
h τ−1
X
t=1
H(X, Ct)
K(Ct)
i
.
Using the inequality H(X, Ct) ≤eHk(X) (see Section 3), we get:
E[Ψτ] ≤E
h τ−1
X
t=1
eHk(X)
K(Ct)
i
≤E
h
eHk(X) ·
τ−1
X
t=1
1
K(Ct)
i
.
Observe that K(C1), . . . , K(Cτ−1) is a non-increasing sequence in which two consecutive terms are
either equal or K(Ci+1) = K(Ci) −1. Moreover, K(C1) = k and K(Cτ−1) > κ. Therefore, by
Lemma 5.9 (see below), for every realization C0, C1, . . . , Cτ, we have:
τ−1
X
t=1
1
K(Ct) ≤1 + log k/(κ+1).
Thus,
E[Ψτ] ≤(1 + log k/(κ+1))E[ eHk(X)] ≤5(1 + log k/(κ+1)) OPTk(X).
This concludes the proof.
Let κ = ⌊(∆−1)/2⌋. By Lemma 5.8, we have
E
hM(Cτ)Uτ(X)
K(Cτ)
i
≤5

1 + ln 2k
∆

OPTk(X).
Since Ut(X) is a non-increasing stochastic process, we have E[Uk+∆(X)] ≤E[Uτ(X)]. Thus,
E
hM(Cτ)
K(Cτ) · Uk+∆(X)
i
≤5

1 + ln 2k
∆

OPTk(X).
Our goal is to bound E[Uk′(X)]. Write,
E[Uk′(X)] = E
hM(Cτ)
K(Cτ) · Uk′(X)
i
+ E
hK(Cτ) −M(Cτ)
K(Cτ)
· Uk′(X)
i
.
The ﬁrst term on the right hand side is upper bounded by 5
 1+ ln 2k
∆
OPTk(X). We now estimate
the second term, which we denote by (∗).
Note that K(Ct) −M(Ct) = k −t, since the number of uncovered clusters after t steps of
k-means++ equals the number of misses plus the number of steps remaining. Particularly, if τ = k,
we have K(Cτ) −M(Cτ) = K(Ck) −M(Ck) = 0. Consequently, if τ = k, then the second term (∗)
equals 0. Thus, we only need to consider the case, when τ < k. Note that in this case K(Cτ) = κ.
By Lemma 5.2 (applied to all uncovered clusters), we have
E[Uk′(X) | Cτ, τ] ≤
K(Cτ)
e(∆′ −1)
eHτ(X),
where ∆′ = ∆−M(Cτ).
15

--- Page 16 ---
Thus,
E
hK(Cτ) −M(Cτ)
K(Cτ)
· Uk′(X) | Cτ, τ
i
≤K(Cτ) −M(Cτ)
K(Cτ)
·
K(Cτ)
e(∆′ −1) · eHτ(X) = (∗∗).
Plugging in K(Cτ) = κ and the expression for ∆′ (see above), and using that κ ≤(∆−1)/2, we
get
(∗∗) =
κ −M(Cτ)
e(∆−M(Cτ) −1). · eHτ(X) ≤1
2e
eHτ(X).
Finally, taking the expectation over all Cτ, we obtain the bound
E
hK(Cτ) −M(Cτ)
K(Cτ)
· Uk′(X)
i
≤5OPT1(X)
2e
.
Thus, E[Uk′(X)] ≤5(1 + 1/2e + ln 2k/∆)OPTk(X). Therefore,
E[costk′(X)] = E[Hk′(X)] + Uk′(X) ≤5
 2 + 1
2e + ln 2k
∆
 OPTk(X).
We now prove Lemma 5.9.
Lemma 5.9.
For any t ≤k integers a1 ≥a2 ≥· · · ≥at such that a1 = k, at > κ and ai −ai+1 ∈
{0, 1} for all 1 ≤i < t, the following inequality holds
t
X
i=1
1
ai
≤1 + log

k
κ + 1

.
Proof. It is easy to see that the sum is maximized when t = k, and the sequence a1, . . . , ak is as
follows:
1
k,
1
k −1, . . . ,
1
κ + 2
|
{z
}
(k−(κ+1)) terms
,
1
κ + 1, . . . ,
1
κ + 1
|
{z
}
(κ+1) terms
.
The sum of the ﬁrst (k −(κ + 1)) terms is upper bounded by
Z 1/k
1/(κ+1)
1
x dx = ln
k
κ + 1.
The sum of the last (κ + 1) terms is 1.
6
Analysis of k-means∥
In this section, we give a sketch of analysis for the k-means∥algorithm. Speciﬁcally, we show upper
bounds on the expected cost of the solution after T rounds.
16

--- Page 17 ---
Theorem 6.1. The expected cost of the clustering returned by k-means∥algorithm after T rounds
are upper bounded as follows:
for ℓ< k,
E [costT+1(X)] ≤

e−ℓ
k
T
E [cost1(X)] + 5OPTk(X)
1 −e−ℓ
k
;
for ℓ≥k,
E [costT+1(X)] ≤
 k
eℓ
T
E [cost1(X)] + 5OPTk(X)
1 −k/eℓ
.
Remark: For the second bound (ℓ≥k), the additive term 5OPTk(X)/(1 −k/(eℓ)) ≤8OPTk(X).
The probability that a point is sampled by k-means∥is strictly greater than the probability
that it is sampled by k-means∥Pois since 1 −e−λ < λ for all λ > 0. Thus, for every round, we can
couple k-means∥Pois and k-means∥so that each point sampled by k-means∥Pois is also sampled by
k-means∥. Thus, the expected cost returned by k-means∥is at most the expected cost returned
by k-means∥Pois. In the following analysis, we show an upper bound for the expected cost of the
solution returned by k-means∥Pois.
As a thought experiment, consider a modiﬁed k-means∥Pois algorithm. This algorithm is given
the set X, parameter k, and additionally the optimal solution P = {P1, . . . , Pk}. Although this
modiﬁed algorithm is useless in practice as we do not know the optimal solution in advance, it will
be helpful for our analysis.
In every round t, the modiﬁed algorithm ﬁrst draws independent Poisson random variables
Zt(Pi) ∼Pois(λt(Pi)) for every cluster i ∈{1, . . . , k} with rate λt(Pi) = P
x∈Pi λt(x). Then, for
each i ∈{1, . . . , k}, it samples Zt(Pi) points x ∈Pi with repetitions from Pi, picking every point
x with probability λt(x)/λt(Pi) and adds them to the set of centers Ct. We assume that points in
every set Ct are ordered in the same way as they were chosen by this algorithm.
We claim that the distribution of the output sets CT of this algorithm is exactly the same as
in the original k-means∥Pois algorithm. Therefore, we can analyze the modiﬁed algorithm instead
of k-means∥Pois, using the framework described in Sections 3.
Lemma 6.2.
The sets Ct in the original and modiﬁed k-means∥Pois algorithms are identically
distributed.
Proof. Consider |Pi| independent Poisson point processes Nx(a) with rates λt(x), where x ∈Pi
(here, we use variable a for time). Suppose we add a center x at step t of the algorithm if Nx(t) ≥1.
On the one hand, the probability that we choose x is equal to 1 −e−λt(x) which is exactly the
probability that k-means∥Pois picks x as a center at step t. On the other hand, the sum NPi =
P
x∈Pi Nx is a Poisson point process with rate λt(Pi). Thus, the total number of jumps in the
interval [0, 1] of processes Nx with x ∈Pi is distributed as Zt(Pi). Moreover, the probability that
Nx jumps at time a conditioned on the event that NPi jumps at time a is λt(x)/λt(Pi). Thus, for
every jump of NPi, we choose one random center x with probability λt(x)/λt(Pi).
Lemma 6.3.
For k-means∥algorithm with parameter ℓ, the following bounds hold:
for ℓ< k,
E [costt+1(X)] ≤e−ℓ
k · E [costt(X)] + 5OPTk(X);
for ℓ≥k,
E [costt+1(X)] ≤
 k
eℓ

· E [costt(X)] + 5OPTk(X).
Proof. Since the expected cost returned by k-means∥is at most the expected cost returned by
k-means∥Pois, we analyze the expected cost of the clustering after one step of k-means∥Pois.
17

--- Page 18 ---
If the algorithm covers cluster Pi at round t, then at the next round, its uncovered cost equals
0. The number of centers chosen in Pi is determined by the Poisson random variable Zt+1(Pi).
Hence, Pi is uncovered at round t + 1 only if Zt+1(Pi) = 0. Since Ut(Pi) is non-increasing in t and
Ut(Pi) ≤costt(Pi), we have
E [Ut+1(Pi) | Ct] ≤P [Zt+1(Pi) = 0] Ut(Pi) ≤exp

−ℓcostt(Pi)
costt(X)

costt(Pi).
Deﬁne two function: f(x) = e−x · x; and g(x) = f(x) for x ∈[0, 1] and g(x) = e−1 for x ∈[1, ∞).
Then,
E [Ut+1(X) | Ct] ≤
 
1
k
k
X
i=1
f
ℓcostt(Pi)
costt(X)
!
kcostt(X)
ℓ
.
Since g(x) ≤f(x), and g(x) is concave for x ≥0, we have
E [Ut+1(X) | Ct] ≤
 
1
k
k
X
i=1
g
ℓcostt(Pi)
costt(X)
!
kcostt(X)
ℓ
≤g
 ℓ
k
 kcostt(X)
ℓ
.
Here, we use that P
i costt(Pi) = costt(X).
Therefore, for ℓ≤k, we have
E [Ut+1(X) | Ct] ≤

e−ℓ
k

costt(X);
and for ℓ≥k, we have
E [Ut+1(X) | Ct] ≤
 k
eℓ

costt(X).
Similar to Corollary 4.5, the process eHt(P) for k-means∥Pois is also a supermartingale, which
implies E [Ht+1(X)] ≤5OPTk(X). This concludes the proof.
Proof of Theorem 6.1. Applying the bound from Lemma 6.3 for t times, we get the following results.
For ℓ≤k,
E [costt+1(X)] ≤

e−ℓ
k
t
E [cost1(X)] + 5OPTk(X)ηt,
where ηt = Pt
j=1

e−ℓ
k
j−1 <
1
1−e−ℓ
k . For ℓ≥k,
E [costt+1(X)] ≤
 k
eℓ
t
E [cost1(X)] + 5OPTk(X)ηt,
where ηt = Pt
j=1

k
eℓ
j−1 ≤
1
1−k
eℓ.
Corollary 6.4.
Consider a data set X with more than k distinct points. Let
T = ln E
 cost1(X)
OPTk(X))

and ℓ> k. Then, after T rounds of k-means∥, the expected cost of clustering E [costT (X)] is at
most 9OPTk(X).
18

--- Page 19 ---
7
Exponential Race k-means++ and Reservoir Sampling
In this section, we show how to implement k-means++ algorithm in parallel using R passes over the
data set. This implementation, which we refer to as k-means++ER (exponential race k-means++),
is very similar to k-means∥, but has stronger theoretical guarantees. Like k-means∥, in every round,
k-means++ER tentatively selects ℓcenters, in expectation. However, in the same round, it removes
some of the just selected centers (without making another pass over the data set). Consequently,
by the end of each iteration, the algorithm keeps at most k centers.
We can run k-means++ER till it samples exactly k centers; in which case, the distribution of k
sampled centers is identical to the distribution of the regular k-means++, and the expected number
of rounds or passes over the data set R is upper bounded by
O
k
ℓ+ log OPT1(X)
OPTk(X)

.
We note that R is never greater than k. We can also run this algorithm for at most R∗rounds.
Then, the expected cost of the clustering is at most
5(ln k + 2) OPTk(X) + 5R∗
 4k
eℓR∗
R∗
· OPT1(X).
7.1
Algorithm
In this section, we give a high level description of our k-means++ER algorithm. In Section 7.2, we
show how to eﬃciently implement k-means++ER using lazy updates and explain why our algorithm
makes R passes over the data set.
The algorithm simulates n continuous-time stochastic processes.
Each stochastic process is
associated with one of the points in the data set. We denote the process corresponding to x ∈X
by Pt(x). Stochastic process Pt(x) is a Poisson process with variable arrival rate λt(x).
The algorithm chooses the ﬁrst center c1 uniformly at random in X and sets the arrival rate
of each process Pt(x) to be λt(x) = cost(x, {c1}). Then, it waits till one of the Poisson processes
Pt(x) jumps. When process Pt(x) jumps, the algorithm adds the point x ∈X (corresponding to
that process) to the set of centers Ct and updates the arrival rates of all processes to be
λt(y) = cost(y, Ct)
for all y ∈X. Note that if y is a center, then the arrival rate λt(y) is 0.
The algorithm also maintains a round counter R. In the lazy version of this algorithm (which
we describe in the next section), the algorithm makes a pass over the data set and samples a new
batch of centers every time this counter is incremented. Additionally, at the end of each round, the
algorithm checks if it chose at least one center in that round, and in the unlikely event that it did
not, it selects one center with probability proportional to the costs of the points.
Initially, the algorithm sets R = 0, t0 = 0, and t1 = ℓ/cost(X, {c1}). Then, at each time point
ti (i ≥1), we increment R and compute
ti+1 = ti + ℓ/cost(X, Cti),
where Cti is the set of all centers selected before time ti. We refer to the time frame [ti−1, ti] for
i ≥1 as the i-th round. The algorithm stops when one of the following conditions holds true (1) the
19

--- Page 20 ---
number of sampled centers is k; or (2) the round counter R equals the prespeciﬁed threshold R∗,
which may be ﬁnite or inﬁnite.
Before analyzing this algorithm, we mention that every Poisson process Pt with a variable
arrival rate λt can be coupled with a Poisson process Qs with rate 1. To this end, we substitute
the variable
s(t) =
Z t
0
λτdτ,
and let
Pt ≡Qs(t).
Observe that the expected number of arrivals for process Qs in the inﬁnitesimal interval [s, s + ds]
is ds = λtdt which is exactly the same as for process Pt.
It is convenient to think about the variables s as “current position”, t as “current time”, and λt
as “current speed” of s. To generate process Pt(x), we can ﬁrst generate Poisson process Qs(x) with
arrival rate 1 and then move the position st(x) with speed λt(x). The process Pt(x) = Qst(x)(x) is
a Poisson process with variable arrival rate λt(x).
Theorem 7.1. I. If the number of rounds is not bounded (i.e., R∗= ∞), then the distribution of
centers returned by k-means++ER is identical to the distribution of centers returned by k-means++.
II. Moreover, the expected number of rounds R is upper bounded by
(1 + ok(1)) ·

⌈k
ℓ⌉+ log 2 OPT1(X)
OPTk(X)

,
and never exceeds k.
III. If the threshold R∗is given (R∗< ∞), then the cost of the solution after R∗rounds is upper
bounded by
5(ln k + 2) OPTk(X) + 2R∗
 4k
eℓR∗
R∗
· OPT1(X).
Proof of Part I. For the sake of analysis, we assume that after the algorithm outputs solution C, it
does not terminate, but instead continues to simulate Poisson processes Pt(x). It also continues to
update the set Ct (but, of course, not the solution) and the arrival rates λt(x) till the set Ct contains
k centers. Once |Ct| = k, the algorithm stops updating the set of centers Ct and arrival rates but
still simulates continuous-time processes Pt(x). Clearly, this additional phase of the algorithm does
not aﬀect the solution since it starts after the solution is already returned to the user.
We prove by induction on i that the ﬁrst i centers c1, . . . , ci have exactly the same joint distri-
bution as in k-means++. Indeed, the ﬁrst center c1 is drawn uniformly at random from the data
set X as in k-means++. Suppose centers c1, . . . , ci are already selected. Then, we choose the next
center ci+1 at the time of the next jump of one of the Poisson processes Pt(x). Observe that the
conditional probability that a particular process Pt(x) jumps given that one of the processes Pt(y)
(y ∈X) jumps is proportional to λt(x), which in turn equals the current cost(x, Ct) of point x.
Hence, the distribution of center ci+1 is the same as in k-means++. This completes the proof of
item I.
Proof of Part II. We now show items II and III. Deﬁne process
Pt(X) =
X
x∈X
Pt(x).
20

--- Page 21 ---
Its rate λt(X) equals P
x∈X λt(x). We couple this process with a Poisson Qs(X) with arrival rate
1 as discussed above. We want to estimate the number of centers chosen by the algorithm in the
ﬁrst R′ rounds. To this end, we count the number of jumps of the Poisson process Pt(X) (recall
that we add a new center to Ct whenever Pt(X) jumps unless |Ct| already contains k centers). The
number of jumps equals PtR′ which, in turn, equals QsR′ where sR′(X) is the position of s(X) at
time tR′:
sR′(X) =
Z tR′
0
λτ(X) dτ =
R′−1
X
i=0
Z ti+1
ti
λτ(X) dτ ≥
R′−1
X
i=0
(ti+1 −ti) · λti+1(X).
Here, we used that λt(X) is non-increasing, and thus, λti+1(X) ≤λτ(X) for all τ ∈[ti, ti+1]. We
now recall that (ti+1 −ti) = ℓ/cost(X, Cti) and λti+1(X) = cost(X, Cti+1). Hence,
sR′(X) ≥ℓ
R′−1
X
i=0
cost(X, Cti+1)
cost(X, Cti) .
By the inequality of arithmetic and geometric means, we have
sR′(X) ≥ℓ· R′
 R′−1
Y
i=0
cost(X, Cti+1)
cost(X, Cti)
!1/R′
= ℓ· R′
 
cost(X, CtR′ )
cost(X, Ct0)
!1/R′
(5)
= ℓ· R′
 
cost(X, CtR′ )
cost(X, {c1})
!1/R′
.
We now use this equation to prove items II and III. For item II, we let random variable R′ to
be
R′ = 2e⌈k/ℓ⌉+ log cost(X, {c1})
OPTk(X)
.
Note that R′ depends on the ﬁrst center c1 (which is chosen in the very beginning of the algorithm)
but not on the Poisson processes Pt(x).
Since, Ct always contains at most k centers, we have
cost(x, CtR′ ) ≥OPTk(X), and consequently
sR′(X) ≥ℓ· R′
 
OPTk(X)
cost(X, {c1})
!1/R′
> ℓ· 2e⌈k/ℓ⌉· 1/e ≥2k.
The expected number of jumps of the Poisson process Qs(X) in the interval [0, sR′(X)] equals
QsR(X)(X). Observe that
QsR(X)(X) ≥Q2k(X)
and Q2k(X) is a Poisson random variable with parameter 2k. By the Chernoﬀbound2, it makes
fewer than k jumps with exponentially small probability in k; namely, with probability at most
(e/2)−k. Thus, with probability at least 1 −(e/2)−k, the algorithm selects k centers in the ﬁrst R′
rounds. Moreover, if it does not happen in the ﬁrst R∗rounds, then it selects k centers by the end
of the second R′ rounds again with probability at least 1 −(e/2)−k and so on. Hence, the expected
2We use the bound Pr{P ≤k} ≤e−λ eλ/kk, where P is a Poisson random variable with parameter λ and k < λ.
See e.g., Theorem 5.4.2 in Mitzenmacher and Upfal (2017).
21

--- Page 22 ---
number of rounds till it selects k centers is (1 + ok(1))R′. Finally, observe that the expectation of
cost(X, {c1}) over the choice of the ﬁrst center equals 2 OPTk(X). Since log(·) is a convex function,
we have
E[R′] ≤2e⌈k/ℓ⌉+ log 2 OPT1(X)
OPTk(X) .
Therefore, we showed that the expected number of rounds is upper bounded by the right hand side
of the expression above times a multiplicative factor of (1+ok(1)). A slightly more careful analysis
gives a bound of
(1 + ok(1))
 
e⌈k/ℓ⌉+ log 2 OPT1(X)
OPTk(X)
!
.
This concludes the proof of item II.
Proof of Part III. We now prove item III. Denote T = tR∗. Consider the event
E =
algorithm samples k centers in the ﬁrst R∗rounds
	.
Let ¯E be the complimentary events to E. Then,
E
cost(X, CT )
 = E
cost(X, CT ) · 1(E)
 + E
cost(X, CT ) · 1( ¯E)
.
We now separately upper bound each of the terms on the right hand side. It is easy to upper bound
the ﬁrst term:
E[cost(X, CT ) · 1(E)] ≤5(ln k + 2) · OPTk(X),
because the distribution of centers returned by k-means++ER is identical to the distribution of
centers returned by k-means++. We now bound the second term. Denote by Dρ the event
Dρ =

cost(X, CT ) ≥
 ρk
ℓR∗
R∗
cost(X, {c1})

.
We prove the following claim.
Claim 7.2. The following inequality holds for every real number ρ ∈[1, ℓR∗/k] and any choice of
the ﬁrst center c1:
Pr
  ¯E and Dρ | c1
 ≤e−(ρ−1)kρk−1.
Proof. We use inequality (5) with R′ = R∗:
sR∗(X) ≥ℓ· R∗
 
cost(X, CT )
cost(X, {c1})
!1/R∗
.
It implies that sR∗(X) ≥ρk if event Dρ occurs. On the other hand if ¯E occurs, then the number
of centers chosen by the end of round R∗is less than k and, consequently, the number of jumps of
Pt(X) in the interval [0, T] is less than k:
PT (X) ≡QsR∗(X)(X) < k.
22

--- Page 23 ---
Hence, we can bound Pr( ¯E and Dρ | c1) as follows:
Pr( ¯E and Dρ) ≤Pr
 Dρ and QsR∗(X) < k | c1
 ≤
≤Pr
 Dρ and Qρk(X) < k | c1
 ≤Pr
 Qρk(X) < k | c1
.
Random variable Qρk(X) has the Poisson distribution with parameter ρk and is independent of c1.
By the Chernoﬀbound, the probability that Qρk(X) ≤k −1 is at most (as in Part II of the proof):
Pr
Qρk(X) ≤k −1
	 ≤e−ρk eρk
k −1
k−1
= e−(ρ−1)k−1ρk−1 ·

k
k −1
k−1
|
{z
}
≤e
≤e−(ρ−1)kρk−1.
This completes the proof of Claim 7.2.
Let
Z =
ℓR∗
k
R∗
· cost(X, CT )
cost(X, {c1}).
Then, by Claim 7.2,
Pr
  ¯E and Z ≥ρR∗| c1
 ≤e−(ρ−1)kρk−1.
(6)
Write,
E
1( ¯E) · Z | c1
 =
Z ∞
0
Pr
 1( ¯E) and Z ≥r | c1
dr ≤1 +
Z ∞
1
Pr
 1( ¯E) and Z ≥r | c1
 dr.
We now substitute r = ρR∗and then use (6):
E
Z · 1( ¯E) | c1
 ≤1 + R∗
Z ∞
1
Pr
  ¯E and Z ≥ρR∗| c1
 · ρR∗−1dρ
≤1 + R∗
Z ∞
1
e−(ρ−1)kρk+R∗−2dρ.
We note that R∗< k, since our algorithm chooses at least one center in each round. Thus,
by Lemma 7.3 (which we prove below), the integral on the right hand side is upper bounded by
eR∗/2 · (4/e)R∗. Hence,
E
Z · 1( ¯E) | c1
 ≤1 + R∗·
4
e
R∗−2
.
Multiplying both sides of the inequality by (k/ℓR∗)R∗·cost(X, {c1}) and taking the expectation over
c1, we get the desired inequality:
E
cost(X, CT ) · 1( ¯E)
 ≤

1 + R∗
4
e
R∗ k
ℓR∗
R∗
Ec1
cost(X, {c1}

=

1 + R∗4
e
R∗−2 k
ℓR∗
R∗
· 2 OPT1(X)
< 2R∗
 4k
eℓR∗
R∗
OPT1(X).
This ﬁnishes the proof of Theorem 7.1.
23

--- Page 24 ---
Lemma 7.3. For R∗< k, we have
Z ∞
1
e−(ρ−1)kρk+R∗−2dρ ≤e
2
4
e
R∗
.
Proof. Since e−(ρ−1)ρ ≤1 for all ρ ≥1, we have e−(ρ−1)kρk ≤e−(ρ−1)R∗ρR∗for any R∗< k. Thus,
we have
Z ∞
1
e−(ρ−1)kρk+R∗−2dρ ≤
Z ∞
1
e−(ρ−1)R∗ρ2R∗−3dρ = eR∗Z ∞
1
e−ρR∗ρ2R∗−3dρ
= eR∗Z ∞
1
(e−ρρ2)R∗ρ−3dρ.
Observe that e−ρρ2 ≤4/e2 for any ρ ≥1. Hence,
(e−ρρ2)R∗= (e−ρρ2)R∗−1 · e−ρρ2 ≤(4/e2)R∗−1e−ρρ2.
Thus,
Z ∞
1
e−(ρ−1)kρk+R∗−2dρ ≤4R∗−1 · eR∗
e2(R∗−1)
·
Z ∞
1
e−ρ
ρ
dρ = 4R∗−1
eR∗−2 · 1
4 =
4
e
R∗−2
.
7.2
Lazy implementation of k-means++ER
We now describe how we can eﬃciently implement the k-means++ER algorithm using a lazy reser-
voir sampling. We remind the reader that the time of the ﬁrst jump of a Poisson process with
parameter λ is distributed as the exponential distribution with parameter λ. Imagine for a mo-
ment, that the arrival rates of our Poisson processes were constant. Then, in order to select the
ﬁrst k jumps, we would generate independent exponential random variables with parameters λ(x)
for all x and choose k smallest values among them. This algorithm is known as the reservoir sam-
pling(see Efraimidis and Spirakis (2006)). To adapt this algorithm to our needs, we need to update
the arrival rates of the exponential random variables. Loosely speaking, we do so by generating
exponential random variables with rate 1 for Poisson processes Qs(x) which are described above
and then updating the speeds λt(x) of variables st(x). We now formally describe the algorithm.
In the beginning of every round i, we recompute costs of all points in the data set.
Then,
we draw an independent exponential random variable Sx with rate 1 for every point x, and let
St(x) = Sx . We set
τt(x) = St(x)
λt(x).
Think of St(x) as the distance st(x) needs to travel till process Qs(x) jumps; λt(x) is the speed
of point st(x); and τt(x) is the time left till Qs(x) = Pt(x) jumps if the speed λt does not change.
Among all points x ∈X, we select a tentative set of centers Z for this round. The set Z contains all
points x with ti−1 + τt(x) ≤ti. This is the set of all points for which their Poisson processes would
jump in the current round if their arrival rates remained the same till the end of the round. Since
the arrival rates can only decrease in our algorithm, we know for sure that for points x outside of
Z, the corresponding processes Pt(x) will not jump in this round. Thus, we can safely ignore those
points during the current round.
24

--- Page 25 ---
We also note that in the unlikely event that the initial set Z is empty, we choose x with the
smallest time τt(x) and add it to the set of centers Ct. (This is equivalent to choosing a point
with probability proportional to cost(x, Ct) by the memorylessness property of the exponential
distribution).
The steps we described above – updating costs cost(x, Ct), drawing exponential random vari-
ables Sx, and selecting points in the set Z – can be performed in parallel using one pass over the
data set. In the rest of the current round, our algorithm deals only with the set Z whose size in
expectation is at most ℓ(see below).
While the set Z is not empty we do the following. We choose x ∈Z with the smallest value of
τt(x). This x corresponds to the process that jumps ﬁrst. Then, we perform the following updates:
We add x to the set of centers Ct. We set the “current time” t to t = t′ + τt′(x), where t′ is the
time of the previous update. If x is the ﬁrst center selected in the current round, then we let t′ to
be the time when the round started (i.e., ti−1). We recompute the arrival rates (speeds) λt(x) for
each x in Z. Finally, we update the values of all τt(x) for x ∈Z using the formula
τt(x) = St(x) −λt′(x) · (t −t′)
λt(x)
,
here λt′(x) · (t −t′) is the distance variable st(x) moved from the position where it was at time t′;
St(x) −λt′(x) · (t −t′) is the remaining distance st(x) needs to travel till the process Qt(x) jumps;
and τt(x) is the remaining time till Pt(x) jumps if we do not update its arrival rate. After we
update τt(x), we prune the set Z. Speciﬁcally, we remove from set Z all points x with t+τt(x) > ti.
As before, we know for sure that if x is removed from Z, then the corresponding processes Pt(x)
will not jump in the current round.
This algorithm simulates the process we described in the previous section. The key observation
is that Poisson processes Pt(x) we associate with points x removed from Z cannot jump in this
round and thus can be safely removed from our consideration. We now show that the expected size
of the set Z is at most ℓ. In the next section, we analyze the running time of this algorithm.
Then we show that the expected size of the set Z in the beginning of each round i + 1 is at
most ℓ. Since every point x belongs to Z with probability
Pr{x ∈Z} = Pr

Sx
cost(x, Cti) ≤
ℓ
cost(X, Cti)

= Pr

Sx ≤ℓ· cost(x, Cti)
cost(X, Cti)

.
The right hand side is the probability that the Poisson process Qs(x) with rate 1 jumps in the inter-
val of length ℓ· cost(x, Cti)/cost(X, Cti) which is upper bounded by the expected number of jumps
of Qs(x) in this interval. The expected number of jumps exactly equals ℓ· cost(x, Cti)/cost(X, Cti).
Thus, the expected size of Z is upper bounded as
E|Z| =
X
z∈X
Pr{z ∈Z} ≤
X
z∈X
ℓ· cost(z, Cti)
cost(X, Cti) = ℓ.
7.3
Run time analysis
According to our analysis above, the number of new centers chosen at each round of k-means++ER
is at most the size of set Z, which is O(ℓ) with high probability. In the beginning of every round,
we need to update costs of all data points, which requires O(nℓd) time. In each round, we also
25

--- Page 26 ---
need to maintain the rates of all points in set Z, which needs O(ℓ2d) time. Thus, the total running
time for k-means++ER with R rounds is O(Rnℓd). We note that before running our algorithm, we
can reduce the dimension d of the space to O(log k) using the Johnson–Lindenstrauss transform
(see Johnson and Lindenstrauss (1984)). This will increase the approximation factor by a factor of
(1 + ε) but make the algorithm considerably faster (see Makarychev et al. (2019), Becchetti et al.
(2019), and Boutsidis et al. (2010)).
References
A. Aggarwal, A. Deshpande, and R. Kannan. Adaptive sampling for k-means clustering. In Approx-
imation, Randomization, and Combinatorial Optimization. Algorithms and Techniques, pages
15–28. Springer, 2009.
S. Ahmadian, A. Norouzi-Fard, O. Svensson, and J. Ward. Better guarantees for k-means and
euclidean k-median by primal-dual algorithms. SIAM Journal on Computing, pages FOCS17–97,
2019.
D. Aloise, A. Deshpande, P. Hansen, and P. Popat.
Np-hardness of euclidean sum-of-squares
clustering. Machine learning, 75(2):245–248, 2009.
D. Arthur and S. Vassilvitskii. k-means++: The advantages of careful seeding. In Proceedings of
the eighteenth annual ACM-SIAM symposium on Discrete algorithms, pages 1027–1035. Society
for Industrial and Applied Mathematics, 2007.
P. Awasthi, M. Charikar, R. Krishnaswamy, and A. K. Sinop. The hardness of approximation of
euclidean k-means. In 31st International Symposium on Computational Geometry (SoCG 2015).
Schloss Dagstuhl-Leibniz-Zentrum fuer Informatik, 2015.
O. Bachem, M. Lucic, and A. Krause. Distributed and provably good seedings for k-means in
constant rounds.
In Proceedings of the 34th International Conference on Machine Learning-
Volume 70, pages 292–300. JMLR. org, 2017.
B. Bahmani, B. Moseley, A. Vattani, R. Kumar, and S. Vassilvitskii. Scalable k-means++. Pro-
ceedings of the VLDB Endowment, 5(7):622–633, 2012.
L. Becchetti, M. Bury, V. Cohen-Addad, F. Grandoni, and C. Schwiegelshohn. Oblivious dimension
reduction for k-means: beyond subspaces and the johnson–lindenstrauss lemma. In Proceedings
of the 51st Annual ACM SIGACT Symposium on Theory of Computing, pages 1039–1050, 2019.
B. Boehmke and B. M. Greenwell. Hands-on machine learning with R. CRC Press, 2019.
C. Boutsidis, A. Zouzias, and P. Drineas. Random projections for k-means clustering. In Advances
in Neural Information Processing Systems, pages 298–306, 2010.
T. Brunsch and H. Röglin. A bad instance for k-means++. Theoretical Computer Science, 505:
19–26, 2013.
D. Choo, C. Grunau, J. Portmann, and V. Rozhoň. k-means++: few more steps yield constant
approximation. In Proceedings of the 37th International Conference on Machine Learning, pages
7849–7057. JMLR. org, 2020.
26

--- Page 27 ---
S. Dasgupta. The hardness of k-means clustering. Department of Computer Science and Engineer-
ing, University of California, San Diego, 2008.
S. Dasgupta.
UCSD CSE 291,
Lecture Notes:
Geometric
Algorithms,
2013.
URL:
https://cseweb.ucsd.edu/~dasgupta/291-geom/kmeans.pdf. Last visited on 2020/06/01.
D. Dua and C. Graﬀ. UCI ML repository, 2017. URL http://archive.ics.uci.edu/ml.
P. S. Efraimidis and P. G. Spirakis. Weighted random sampling with a reservoir. Information
Processing Letters, 97(5):181–185, 2006.
R. Elber. Kdd-Cup, 2004. URL http://osmot.cs.cornell.edu/kddcup/.
W. B. Johnson and J. Lindenstrauss. Extensions of lipschitz mappings into a hilbert space. Con-
temporary mathematics, 26(189-206):1, 1984.
T. Kanungo, D. M. Mount, N. S. Netanyahu, C. D. Piatko, R. Silverman, and A. Y. Wu.
A
local search approximation algorithm for k-means clustering. Computational Geometry, 28(2):
89 – 112, 2004.
ISSN 0925-7721.
doi: https://doi.org/10.1016/j.comgeo.2004.03.003.
URL
http://www.sciencedirect.com/science/article/pii/S0925772104000215.
S. Lattanzi and C. Sohler.
A better k-means++ algorithm via local search.
In International
Conference on Machine Learning, pages 3662–3671, 2019.
E. Lee, M. Schmidt, and J. Wright. Improved and simpliﬁed inapproximability for k-means. Infor-
mation Processing Letters, 120:40–43, 2017.
S. Lloyd. Least squares quantization in pcm. IEEE transactions on information theory, 28(2):
129–137, 1982.
K. Makarychev, Y. Makarychev, M. Sviridenko, and J. Ward. A bi-criteria approximation algorithm
for k-means. Approximation, Randomization, and Combinatorial Optimization. Algorithms and
Techniques, 2016.
K. Makarychev, Y. Makarychev, and I. Razenshteyn. Performance of johnson–lindenstrauss trans-
form for k-means and k-medians clustering. In Proceedings of the 51st Annual ACM SIGACT
Symposium on Theory of Computing, pages 1027–1038, 2019.
M. Mitzenmacher and E. Upfal. Probability and computing: Randomization and probabilistic tech-
niques in algorithms and data analysis. Cambridge university press, 2017.
R. Ostrovsky, Y. Rabani, L. Schulman, and C. Swamy. The eﬀectiveness of lloyd-type methods
for the k-means problem. In 2006 47th Annual IEEE Symposium on Foundations of Computer
Science (FOCS’06), 2006.
V. Rozhoň.
Simple and sharp analysis of k-means||.
In Proceedings of the 37th International
Conference on Machine Learning, pages 7828–7837. JMLR. org, 2020.
D. Wei. A constant-factor bi-criteria approximation guarantee for k-means++. In Advances in
Neural Information Processing Systems, pages 604–612, 2016.
27

--- Page 28 ---
Appendix
In this appendix, we present our experiments, give proofs omitted in the main part of the paper,
and provide complimentary lower bounds.
A
Experiments
In this section, we present plots that show that the performance of k-means∥and “k-means++
with oversampling and pruning” algorithms are very similar in practice. Below, we compare the
following algorithms on the datasets BioTest from KDD Cup 2004 (Elber, 2004) and COVTYPE
from the UCI ML repository (Dua and Graﬀ, 2017):
• Regular k-means++. The performance of this algorithm is shown with a solid black line on
the plots below.
• k-means∥without pruning. This algorithm samples k centers using k-means∥with T = 5
rounds and ℓ= k/T.
• k-means∥. This algorithm ﬁrst samples 5k centers using k-means∥and then subsamples k
centers using k-means++. The performance of this algorithm is shown with a dashed blue
line on the plots below.
• k-means++ with oversampling and pruning. This algorithm ﬁrst samples 5k centers using
k-means++ and then subsamples k centers using k-means++.
The performance of this
algorithm is shown with a thin red line on the plots below.
For each k = 5, 10, · · · , 200, we ran these algorithms for 50 iterations and took their average.
We normalized all costs by dividing them by the cost of k-means++ with k = 1000 centers.
0
50
100
150
200
2
4
6
8
10
#centers
cost
BioTest
k-means++
BiCriteria k-means++ w/Pruning
k-means∥
10
20
30
40
50
3
4
5
6
#centers
cost
BioTest
k-means++
BiCriteria k-means++ w/Pruning
k-means∥
28

--- Page 29 ---
0
50
100
150
200
0
20
40
#centers
cost
COVTYPE
k-means++
BiCriteria k-means++ w/Pruning
k-means∥
10
20
30
40
50
10
15
20
25
30
#centers
cost
COVTYPE
k-means++
BiCriteria k-means++ w/Pruning
k-means∥
0
50
100
150
200
2
4
6
8
10
#centers
cost
BioTest
k-means++
k-means∥without Pruning
10
20
30
40
50
4
6
8
10
#centers
cost
BioTest
k-means++
k-means∥without Prunning
0
50
100
150
200
0
20
40
60
#centers
cost
COVTYPE
k-means++
k-means∥without Prunning
10
20
30
40
50
10
20
30
40
50
60
#centers
cost
COVTYPE
k-means++
k-means∥without Pruning
29

--- Page 30 ---
B
Details for Preliminaries
For any set of points Y ⊂Rd, let µ = P
x∈Y x/ |Y| be the centroid of the cluster Y. Then, the
optimal cost of Y with one center,
OPT1(Y) =
X
x∈Y
∥x −µ∥2 =
P
(x,y)∈Y×Y∥x −y∥2
2 |Y|
.
This is a well known formula which is often used for analyzing of k-means algorithms. For com-
pleteness, we give a proof below.
Proof. Consider any point z ∈Rd, then we have:
cost(Y, {z}) =
X
x∈Y
∥x −z∥2 =
X
x∈Y
∥(x −µ) + (µ −z)∥2
=
X
x∈Y

∥x −µ∥2 + ∥µ −z∥2 + 2 ⟨x −µ, µ −z⟩

=
X
x∈Y
∥x −µ∥2 + |Y| · ∥µ −z∥2 + 2
* X
x∈Y
(x −µ), µ −z
+
=
X
x∈Y
∥x −µ∥2 + |Y| · ∥µ −z∥2.
Thus, the optimal choice of z to minimize cost(Y, {z}) is µ and OPT1(Y) = P
x∈Y∥x −µ∥2.
X
x∈Y
∥x −µ∥2 =
X
x∈Y
⟨x −µ, x −µ⟩=
X
x∈Y
⟨x, x −µ⟩
=
X
x∈Y
*
x, x −
X
y∈Y
y
|Y|
+
=
1
|Y|
X
(x,y)∈Y×Y
⟨x, x −y⟩
=
1
2 |Y|


X
(x,y)∈Y×Y
⟨x, x −y⟩+
X
(x,y)∈Y×Y
⟨y, y −x⟩


=
P
(x,y)∈Y×Y∥x −y∥2
2 |Y|
.
C
Lower bounds
C.1
Lower bound on the cost of covered clusters
We show the following lower bound on the expected cost of a covered cluster in k-means++. There-
fore, the 5-approximation in Lemma 4.1 is tight.
Theorem C.1. For any ε > 0, there exists an instance of k-means such that for a set P ∈X
and a set of centers C ∈Rd, if a new center c is sampled from P with probability Pr(c = x) =
cost(x, C)/cost(P, C), then
Ec [cost(P, C ∪{c})] ≥(5 −ε)OPT1(P).
30

--- Page 31 ---
Proof. Consider the following one dimensional example, where P contains t points at 0 and one
point at 1, and the closest center already chosen in C to P is at −1.
−1
0
t
1
1
The new center c will be chosen at 0 with probability
t
t+4, and at 1 with probability
4
t+4. Then,
the expected cost of P is
Ec [cost(P, C ∪{c})] = 1 ·
t
t + 4 + t ·
4
t + 4 =
5t
t + 4;
and the optimal cost of P is OPT1(P) ≤1. Thus, by choosing t ≥4(5 −ε)/ε, we have
Ec [cost(P, C ∪{c})] ≥(5 −ε)OPT1(P).
C.2
Lower bound on the bi-criteria approximation
In this section, we show that the bi-criteria approximation bound of O(ln k
∆) is tight up to constant
factor. Our proof follows the approach by Brunsch and Röglin (2013). We show the following
theorem.
Theorem C.2.
For every k > 1 and ∆≤k, there exists an instance X of k-means such that the
bi-criteria k-means++ algorithm with k + ∆centers returns a solution of cost greater than
1
8 log k
∆· OPTk(X)
with probability at least 1 −e−
√
k/2.
Remark: This implies that the expected cost of bi-criteria k-means with k + ∆centers is at
least
1 −e−
√
k/2
8
· log k
∆· OPTk(X).
Proof. For every k and ∆≥
√
k, we consider the following instance. The ﬁrst cluster is a scaled
version of the standard simplex with N ≫k vertices centered at the origin, which is called the heavy
cluster. The length of the edges in this simplex is 1/
√
N −1. Each of the remaining k −1 clusters
contains a single point on k −1 axes, which are called light clusters. These clusters are located at
distance √α from the center of the heavy cluster and
√
2α from each other, where α = ln(k/∆)
4∆
.
For the sake of analysis, let us run k-means++ till we cover all clusters. At the ﬁrst step, the
k-means++ algorithm almost certainly selects a center from the heavy cluster since N ≫k. Then,
at each step, the algorithm can select a center either from one of uncovered light clusters or from
the heavy cluster. In the former case, we say that the algorithm hits a light cluster, and in the latter
case we say that the algorithm misses a light cluster. Below, we show that with high probability
the algorithm makes at least 2∆misses before it covers all but ∆light clusters.
31

--- Page 32 ---
Lemma C.3. Let ∆≥
√
k. By the time the k-means++ algorithm covers all but ∆light clusters,
it makes greater than 2∆misses with probability at least 1 −e−
√
k/2.
Proof sketch. Let ε = 1/
√
N. Observe that k-means++ almost certainly covers all clusters in εN
steps (since N ≫k). So in the rest of this proof sketch, we assume that the number chosen centers
is at most εN and, consequently, at least (1 −ε)N points in the heavy cluster are not selected as
centers. Hence, the cost of the heavy cluster is at least 1 −ε.
Consider a step of the algorithm when exactly u light clusters remain uncovered. At this step,
the total cost of all light clusters is αu (we assume for simplicity that distance between the light
clusters and the closest chosen center in the heavy cluster is the same as the distance to the origin).
The cost of the heavy cluster is at least 1 −ε. The probability that the algorithm chooses a center
from the heavy cluster and thus misses a light cluster is at least (1 −ε)/(1 + αu).
Deﬁne random variables {Xu} as follows. Let Xu = 1 if the algorithm misses a cluster at least
once when the number of uncovered light clusters is u; and let Xu = 0, otherwise. Then, {Xu} are
independent Bernoulli random variables. For each u, we have P [Xu = 1] ≥(1 −ε)/(1 + αu).
Observe that the total number of misses is lower bounded by Pk−1
u=∆Xu. Then, we have
E
" k−1
X
u=∆
Xu
#
≥(1 −ε)
k−1
X
u=∆
1
1 + αu ≥(1 −ε)
Z k
∆
du
1 + αu
= (1 −ε)α−1 ln 1 + αk
1 + α∆
≥(1 −ε)α−1 ln k
∆= 4(1 −ε)∆.
Let µ = E
hPk−1
u=∆Xu
i
≥4(1 −ε)∆. By the Chernoﬀbound for Bernoulli random variables, we
have
P
"
k
X
u=∆
Xu ≤2∆
#
≤e−µ
 eµ
2∆
2∆
.
Since f(x) = e−x( ex
2∆)2∆is a monotone decreasing function for x ≥2∆, we have
P
"
k
X
u=∆
Xu ≤2∆
#
≤e−(2−4ε)∆· 22∆≤e−∆/2.
Hence, with probability as least 1 −e−
√
k/2, the number of misses is greater than 2∆.
For every k and ∆≥
√
k, consider the instance we constructed. By Lemma C.3, the algorithm
chooses more than k+∆centers to cover all but ∆light clusters with probability at least 1−e−
√
k/2.
Thus, at the time when the algorithm chose k + ∆centers, the number of uncovered light clusters
was greater than ∆. Hence, in the clustering with k + ∆centers sampled by k-means++, the total
cost is at least 1
4 ln (k/∆), while the cost of the optimal solution with k clusters is 1. For every k
and ∆<
√
k, the total cost is at least 1
4 ln(k/∆′) with ∆′ =
√
k extra centers, which concludes the
proof.
32


=== round-0001/verifier.feedback.md ===
High-level assessment and triage
Both reports pursue the same core idea: exploit the bi-criteria (oversampling) guarantee of Makarychev–Reddy–Shan (MRS, 2020) to bound the expected k-means++ ratio when k is drawn uniformly from [k0,2k0). The per-k reduction (choose a baseline s≤k with OPT(s) close to OPT(k) and treat the remaining k−s steps as oversampling) is sound and is the right lever. The dyadic plateau decomposition (intervals on which OPT(·) stays within a factor 2) is a natural way to control OPT(k) vs OPT(s). The arithmetic averaging of ln(2m/Δ) across Δ on a plateau, via Stirling bounds, is also correct in spirit and gives the characteristic ln(2m/L) term.

Corrections and rigor gaps
- Equality s(k)=a on a plateau: This is not generally true. What is true is s(k)≤a (since OPT(a)≤2·OPT(k) on a factor-2 plateau), hence Δ(k)=k−s(k)≥k−a. Because both MRS branches are monotone (nondecreasing in s and nonincreasing in Δ), replacing s(k) by a and Δ(k) by k−a only weakens the bound; thus the plateau analysis goes through after this fix.
- Δ=0 handling: On a plateau of length L≥2, the Δ=0 event has probability 1/L and contributes O((ln m)/L) to the average; this is negligible on long plateaus. However, in the global average across [k0,2k0), one must still sum these endpoint contributions over all plateaus. If there are many short plateaus (especially L=1), the Δ=0 contributions alone can sum to Θ(log k0). Any global bound must therefore explicitly account for plateau endpoints.
- Stirling step: Use an inequality form (e.g., ln(n!)≥n ln n−n+1 for n≥1) to turn “≈” into a clean “≤,” yielding a rigorous bound (1/(L−1))∑_{Δ=1}^{L−1} ln(2m/Δ) ≤ 1 + ln(2m/(L−1)). This avoids asymptotic o(1) placeholders.
- Constants from MRS: Both reports quote the 5 constant. To be completely safe, phrase the results with absolute constants C_bi (for Theorem 5.1) and C_fix (for the fixed-k bound), and only later specialize to C_bi=5, C_fix≈5 once the citation is double-checked.

What is solid enough to curate now
- A per-k oversampling-based inequality: for each k, choose s(k) minimal with OPT(s(k))≤2·OPT(k); then if Δ(k)≥1, E[ALG(k)]≤2·C_bi·min{2+1/(2e)+ln(2 s(k)/Δ(k)), 1+s(k)/(e(Δ(k)−1))}·OPT(k); for Δ(k)=0, use the standard O(ln k) bound. This is a direct corollary of MRS and monotonicity of OPT(·).
- A per-plateau averaged bound (L≥2): For a plateau I=[m,m+L), average over k∈I yields E[ALG/OPT]≤const + const·ln^+(2m/(L−1)) + O((ln m)/L), where ln^+(x)=max{ln x,0}. This captures the oversampling benefit cleanly.
- A decomposition across [k0,2k0): averaging the per-plateau bounds with weights L_j/k0 gives an explicit functional in terms of starts m_j and lengths L_j, plus an additive term that accounts for Δ=0 endpoints and L_j=1 plateaus. This demonstrates why long plateaus yield O(log log k0), and also why an adversary can force Θ(log k0) via many short plateaus.

Promising directions and concrete next steps
1) Constants/clean statements: Write the per-k and per-plateau statements with explicit constants C_bi (from MRS Thm 5.1) and C_fix (fixed-k). Verify the exact MRS formulas (both branches) and the constant 5.
2) Plateau averaging details: Record a fully rigorous inequality for the Stirling step and for the Δ=0 endpoint, keeping an explicit bound valid for all L≥2 and a separate branch for L=1.
3) Structural conditions ⇒ O(log log k0): Identify mild regularity that ensures a constant fraction of [k0,2k0) lies in long plateaus (e.g., L_j≥m_j/polylog(k0)). Candidates: constraints on discrete derivatives ΔOPT(k):=OPT(k)−OPT(k+1), or quasiconvexity of k·OPT(k). Turn these into a lemma implying Σ_j L_j ln(2m_j/L_j) is O(k0 log log k0).
4) Heavy-coverage program: Formalize the “scale separation” condition and prove that, with high probability, k-means++ covers all k1 heavy clusters before any collision. Couple the MRS potential/supermartingale framework to a coupon-collector over heavy clusters. Quantify the w.h.p. guarantee and glue it to a bi-criteria tail bound with (1+ε)·k2 oversampling.
5) Lower bound attempt: Construct instances with Δ(k)=1 for a 1−o(1) fraction of k∈[k0,2k0), forcing the random-k average to be Ω(log k0). A robust adaptation of Arthur–Vassilvitskii’s instance with steadily shrinking OPT(k) would settle optimality of the smoothing.

I have curated rigorous versions of the per-k bound and the per-plateau average (with explicit, safe constants and precise handling of endpoints) into output.md, together with the global decomposition corollary. These are nontrivial, checkable steps toward the final goal.

=== notes.md ===
Problem and notation
- X ⊂ R^d finite dataset; cost(C) = ∑_{x∈X} min_{c∈C} ||x−c||^2.
- OPT_k := min_{|C|=k} cost(C), nonincreasing in k.
- ALG_k := cost of k-means++ after k centers; E_++[·] denotes expectation over its randomness.
- We study E_{k∼U({k0,…,2k0−1})} E_++[ ALG_k / OPT_k ].

Key external input (Makarychev–Reddy–Shan 2020)
- There is an absolute constant C_bi (quoted as 5 in MRS) such that for any r≥1, Δ≥1,
  E_++[cost after r+Δ centers] ≤ C_bi · min{ 2 + 1/(2e) + ln(2r/Δ), 1 + r/(e(Δ−1)) } · OPT_r.
- For Δ=0 (fixed-k), there is an absolute constant C_fix (also ≲5 in MRS) with E_++[ALG_k] ≤ C_fix · (ln k + O(1)) · OPT_k.

Per-k oversampling bound (robust to s-misalignment)
- For any k, define s(k) := min{ s∈{1,…,k} : OPT_s ≤ 2·OPT_k }. Let Δ(k)=k−s(k).
- If Δ(k)≥1, then
  E_++[ALG_k] ≤ 2·C_bi · min{ 2 + 1/(2e) + ln(2 s(k)/Δ(k)), 1 + s(k)/(e(Δ(k)−1)) } · OPT_k.
- If Δ(k)=0, fall back to E_++[ALG_k] ≤ C_fix · (ln k + O(1)) · OPT_k.
Note: On any factor-2 plateau I=[m,m+L) (i.e., OPT_k∈(OPT_m/2,OPT_m] for k∈I), we have s(k)≤m and Δ(k)≥k−m; since the MRS bound is monotone in (s,Δ), replacing (s(k),Δ(k)) by (m,k−m) yields a valid upper bound.

Per-plateau averaging and Stirling control
- For a factor-2 plateau I=[m,m+L), L≥2, uniform k over I gives
  E_{k,++}[ALG_k/OPT_k] ≤ 2·C_bi · (2 + 1/(2e) + (1/(L−1))∑_{Δ=1}^{L−1} ln(2m/Δ)) + (C_fix(ln m + O(1)))/L.
- Using ln(n!) ≥ n ln n − n + 1 (n≥1),
  (1/(L−1))∑_{Δ=1}^{L−1} ln(2m/Δ) = ln(2m) − ln((L−1)!)/(L−1) ≤ 1 + ln(2m/(L−1)).
- Hence E_{k,++}[ALG_k/OPT_k] ≤ const + const·ln^+(2m/(L−1)) + O((ln m)/L), where ln^+(x)=max{ln x,0} and all const’s are absolute.

Decomposition over [k0,2k0)
- Partition [k0,2k0) into maximal factor-2 plateaus I_j=[m_j, m_j+L_j). For L_j≥2, the per-plateau average is bounded as above. For L_j=1, the single k in I_j contributes ≤ C_fix(ln m_j + O(1)). Averaging with weights L_j/k0 yields a global bound explicitly in terms of {(m_j,L_j)}; in the worst case many L_j=1 plateaus can force Θ(log k0).

Consequences and regimes
- Long plateaus: If some I=[m,m+L) satisfies L ≥ m/polylog(k0), then E_{k∈I,++}[ALG/OPT] = O(log log k0). If a constant fraction of [k0,2k0) is covered by such plateaus, the global average is O(log log k0).
- Worst case without structure: If many plateaus are short (e.g., L_j=1), the global average can be Θ(log k0), aligning with classic worst-case fixed-k behavior.

Open directions
- Structural conditions on OPT(·) implying many long plateaus (e.g., bounded discrete derivative decay, quasiconvexity/regularity of k·OPT_k) to get unconditional O(log log k0).
- Heavy-coverage lemma (conjectural): under strong scale separation OPT_{k1} ≫ OPT_k, show k-means++ hits all k1 heavy clusters w.h.p. before collisions, then apply bi-criteria on the remaining part.
- Lower bounds: Construct instances with Δ(k)=1 for a 1−o(1) fraction of k in [k0,2k0), giving an Ω(log k0) average and clarifying optimality of random-k smoothing.


=== output.md ===
Theorems curated in this file depend on the following standard notation and external input.
- For a finite dataset X⊂R^d, write OPT_k for the optimal k-means cost with k centers, and ALG_k for the cost produced by k-means++ after k centers; E_++[·] is expectation over the algorithm’s randomness.
- External input (Makarychev–Reddy–Shan, 2020): There exists an absolute constant C_bi such that for all integers r≥1 and Δ≥1,
  E_++[cost after r+Δ centers] ≤ C_bi · min{ 2 + 1/(2e) + ln(2r/Δ), 1 + r/(e(Δ−1)) } · OPT_r.
  There is also an absolute constant C_fix with E_++[ALG_k] ≤ C_fix·(ln k + O(1))·OPT_k for all k (the fixed-k bound). We treat C_bi and C_fix as absolute numerical constants.

Theorem 1 (Per-k oversampling bound via a 2-approximate anchor).
Fix k≥1 and define
  s(k) := min{ s∈{1,…,k} : OPT_s ≤ 2·OPT_k },    Δ(k) := k − s(k).
Then:
- If Δ(k) ≥ 1, we have
  E_++[ALG_k] ≤ 2·C_bi · min{ 2 + 1/(2e) + ln(2 s(k)/Δ(k)), 1 + s(k)/(e(Δ(k)−1)) } · OPT_k.
- If Δ(k) = 0, we have E_++[ALG_k] ≤ C_fix·(ln k + O(1))·OPT_k.
Proof.
For Δ(k)≥1, apply the MRS bi-criteria inequality with r = s(k) and Δ = k−s(k):
  E_++[ALG_k] = E_++[cost after s(k)+Δ(k) centers]
  ≤ C_bi · min{ 2 + 1/(2e) + ln(2 s(k)/Δ(k)), 1 + s(k)/(e(Δ(k)−1)) } · OPT_{s(k)}.
By definition of s(k), OPT_{s(k)} ≤ 2·OPT_k, giving the stated bound. For Δ(k)=0 the fixed-k bound yields the second claim. ∎

Theorem 2 (Averaging over a factor-2 plateau).
Let I = {m, m+1, …, m+L−1} with L≥2 and assume OPT_k ∈ (OPT_m/2, OPT_m] for all k∈I. Then for k uniform in I and over the k-means++ randomness,
  E_{k∈I} E_++[ ALG_k / OPT_k ] ≤ A0 + A1 · ln^+( 2m/(L−1) ) + (A2 · ln m)/L,
where ln^+(x) := max{ln x,0} and A0,A1,A2 are absolute constants depending only on C_bi,C_fix.
Proof.
Average the bound from Theorem 1 over k∈I. For Δ:=k−m≥1, we may upper bound s(k) by m and lower bound Δ(k) by Δ (since s(k)≤m and Δ(k)≥Δ on a factor-2 plateau), and use only the logarithmic branch of MRS to get
  E_++[ALG_{m+Δ}/OPT_{m+Δ}] ≤ 2·C_bi · ( 2 + 1/(2e) + ln(2m/Δ) ).
Averaging Δ from 1 to L−1 gives
  (1/(L−1))∑_{Δ=1}^{L−1} ln(2m/Δ) = ln(2m) − ln((L−1)!)/(L−1) ≤ 1 + ln(2m/(L−1)),
where we used ln(n!) ≥ n ln n − n + 1 for n=L−1≥1. Multiplying by (L−1)/L and adding the Δ=0 endpoint contribution C_fix(ln m + O(1))/L yields the claim with suitable absolute constants A0,A1,A2. Replacing ln by ln^+ only weakens the bound. ∎

Corollary 3 (Decomposition across [k0,2k0)).
Partition the integer interval [k0,2k0) into maximal factor-2 plateaus I_j = [m_j, m_j+L_j), j=1,…,J, so OPT_k ∈ (OPT_{m_j}/2, OPT_{m_j}] for k∈I_j, and ∑_j L_j = k0. Then, for k uniform on [k0,2k0),
  E_k E_++[ ALG_k / OPT_k ]
  ≤ (1/k0) ∑_{j : L_j≥2} L_j [ A0 + A1 · ln^+( 2 m_j/(L_j−1) ) ]
    + (1/k0) ∑_{j : L_j≥2} A2 · ln m_j
    + (1/k0) ∑_{j : L_j=1} C_fix · (ln m_j + O(1)).
In particular, if a constant fraction of [k0,2k0) is covered by plateaus satisfying L_j ≥ m_j / polylog(k0), then the contribution of those plateaus is O(log log k0), and the remaining fraction must be controlled separately; in the worst case with many L_j=1 plateaus the global average can be Θ(log k0).
Proof.
Average the bound of Theorem 2 over plateaus with weights L_j/k0, and handle L_j=1 plateaus by the fixed-k bound. ∎



Current round tag: Round 0002 — 2025-08-27T15:48:20.447570Z
Return ONLY valid JSON with a single field:
{ "progress_md": "<your progress notes for this round>" }

Read output.md. If you spot gaps, errors, or missing justifications in output.md, point them out clearly inside progress_md.
