--- SYSTEM ---
You are a research mathematician. 

You are being run in a loop with a verifier that checks rigor. Your goal is to make **incremental, auditable progress**. You are not expected to solve the problem at once, but to generate new, testable ideas.

Inputs: 
- task statement 
- notes.md (summary of past progress; optional) 
- output.md (rigorously proven results curated so far; optional) 
- summary of past rounds (optional) 
- possibly some reference papers.

What counts as progress:
- Extract small lemmas/heuristics from literature and state them cleanly with one-line "why useful here".
- Explore small examples & try to break your own claims with toy counterexamples.
- Prove special cases or nontrivial bounds.
- If an approach fails, explain crisply why.
- Point out flaws in notes.md or output.md (but do not rewrite output.md yourself).

**Discipline.** 
- Read notes, outputs, summaries carefully before proposing new work. 
- Reference papers if relevant, but focus on *incremental, checkable steps*. 
- Do not output Markdown code fences, only raw JSON. 
- Length: at least ~200 words. 
- Organize your reasoning with short headings (Ideas, Examples, Obstacles, Next steps), make clear what your claims are and how they are supported. 
- Remember: the verifier curates notes and outputs, you only suggest.

**Return strictly JSON**:
{
  "progress_md": "Your progress notes for this round in Markdown (KaTeX allowed). Point out any gaps in output.md clearly. Do not modify output.md directly."
}


--- USER ---
Work on this problem context:

=== task.tex ===
\newcommand{\X}{X}
\newcommand{\R}{\mathbb{R}}
\newcommand{\cost}{\phi}
\newcommand{\OPT}{\mathrm{OPT}}
\newcommand{\ALG}{\mathrm{ALG}}
%\newcommand{\E}{\mathbb{E}}

\section*{Algorithm, model, and task}

\paragraph{Objective.}
Fix a dataset $\X=\{x_1,\dots,x_n\}\subseteq\R^d$ and the standard $k$-means cost
\begin{equation*}
\cost(C) \;=\; \sum_{x\in \X} \min_{c\in C} \|x-c\|_2^2
\qquad\text{for a center set } C\subseteq\R^d,\ |C|=k.
\end{equation*}
Let $\OPT(k)=\min_{|C|=k}\cost(C)$ and let $\ALG(k)$ denote the cost of the clustering returned by the $k$-means++ seeding.

\paragraph{$k$-means++ seeding (Arthur--Vassilvitskii).}
Pick the first center uniformly from $\X$. Then, iteratively, sample the next center $c$ from $\X$ with probability proportional to $D(x)^2$, where $D(x)$ is the distance from $x$ to its closest previously chosen center. Stop after $k$ centers are chosen; assign each $x$ to its nearest chosen center.

\paragraph{Smoothed random-$k$ experiment.}
Given an integer $k_0\ge 1$, draw $k$ uniformly at random from $\{k_0,k_0{+}1,\dots,2k_0{-}1\}$, run $k$-means++ with this $k$ on $\X$, and set the random variable
\begin{equation*}
\alpha \;=\; \frac{\ALG(k)}{\OPT(k)} \;\ge\; 1.
\end{equation*}

\section*{The task}
Prove the strongest possible asymptotic upper bound on
\begin{equation*}
\E_{k\sim U([k_0,2k_0)),\,\text{$k$-means++}}\!\!\Big[\; \tfrac{\ALG(k)}{\OPT(k)} \;\Big],
\end{equation*}
as a function of $k_0$. Alternatively, prove the strongest possible upper bound on the value that $\tfrac{\ALG(k)}{\OPT(k)}$ achieves with probability at least $0.01$. 

\section*{Context and prior results}

\paragraph{Baseline (fixed $k$).}
For standard $k$-means++ with $k$ centers, $\E[\ALG(k)]\le O(\log k)\cdot \OPT(k)$ and there are instances with $\E[\ALG(k)]\ge \Omega(\log k)\cdot \OPT(k)$.

\paragraph{Oversampling phenomenon.}
Let $k'=(1{+}\varepsilon)k$. Results in the spirit of Theorem~5.1 of an attached paper show that sampling $k'$ centers by $D^2$-sampling (and evaluating against $\OPT(k)$) yields an expected $O(\log (1/\varepsilon))$-approximation. Intuitively, a mild amount of \emph{oversampling} dramatically sharpens the guarantee.

\paragraph{Why randomizing $k$ might help.}
As $k$ increases, $\OPT(k)$ decreases. Group the values of $OPT(k)$ by $\lfloor \log_2 (OPT(k)) \rfloor$. If there is a long "plateau" where $OPT$ stays roughly the same, up to $2$-factor, $k$-means++ effectively oversamples relative to the number of ``active'' clusters present on that scale. This could lead to a better guarantee. 


\section*{Possible approaches and technical ideas}

\paragraph{Important note}
Take the following section only as an inspiration of what might work. Take it with a huge grain of salt. 

\paragraph{1.\ Plateau decomposition of $\OPT(k)$}
Let $k_0=k^{(0)}<k^{(1)}<\cdots<k^{(m)}\le 2k_0$ be the indices at which $\OPT(\cdot)$ drops by a constant factor; define plateaus $I_j=[k^{(j)},k^{(j+1)}\!)$ on which $\OPT(k)$ is constant.
\begin{itemize}
  \item \textbf{Long vs.\ short plateaus.} Call $I_j$ \emph{long} if $|I_j|\ge k_0/\mathrm{poly}\log k_0$, else \emph{short}.
  \item \textbf{Averaging benefit on long plateaus.} If $k$ is uniform over a long plateau and $\OPT$ does not change, then running $k$-means++ with parameter $k$ is akin to oversampling relative to the \emph{effective} cluster count on that scale. Using Theorem~5.1 from the reference with $\varepsilon \approx |I_j|/k$ suggests an $O(\log\log k_0)$ \emph{conditional} bound on that plateau.
  \item \textbf{Bounding the mass of short plateaus.} Either there are few short plateaus at each scale (so their contribution in the uniform average is small), or there are many in a row; in the latter case, we can try to invoke a strengthened oversampling statement (see next) to regain a $\log\log$ bound.
\end{itemize}

\paragraph{2.\ A coverage lemma for the ``heavy'' part}
\label{subsec:coverage}
Aim to prove a statement of the form:

\medskip
\noindent\emph{Hypothesis (strengthening of Thm.~5.1).}
Let $k=k_1{+}k_2$. Suppose $\OPT(k_1) \ge k^{10}\,\OPT(k)$ (a strong scale separation).
Run $k$-means++ for $k'=k_1 + (1+\epsilon)k_2 $ steps. Then, with probability $1 - 1/k^{O(1)}$, the first $k_1$ steps place one center in each of the $k_1$ ``heavy'' clusters before any collision, and overall the returned cost is $O(\log(1/\varepsilon))\cdot \OPT(k)$.

\medskip
\noindent\textbf{Why plausible.}
Under strong separation, the contribution to $D^2$ from uncovered heavy clusters dominates. By the standard $k$-means++ potential argument, the chance to hit an already covered heavy cluster in the early steps is small. 

=== progress.md ===
...
≥0.01-probability” guarantee whenever that plateau occupies at least ≈ 2% of [k0,2k0).

## Global decomposition with hybrid averaging
Let I_j=[m_j,m_j+L_j) be the factor-2 plateaus covering [k0,2k0) with lengths L_j. The hybrid bound yields
E_{k∼U([k0,2k0))} E_++[ALG_k/OPT_k]
≤ (1/k0) Σ_{j: L_j≥3} L_j [ K0 + K1·min{ ln^+(2m_j/(L_j−1)), (m_j ln L_j)/L_j } ]
   + (1/k0) Σ_{j: L_j≥2} K2·ln m_j + (1/k0) Σ_{j: L_j∈{1,2}} C_fix·(ln m_j + O(1)).
This strictly strengthens Corollary 3 by allowing the (m_j ln L_j)/L_j regime to kick in when L_j is large relative to m_j.

## Toward a high-probability “heavy coverage” step: a micro-lemma
Let P_H be k1 optimal clusters (“heavy”), P_L the rest. Suppose Σ_{P∈P_H} OPT1(P) ≥ R·Σ_{Q∈P_L} OPT1(Q) with R≥k^C for a large constant C. Let U_t(H) be uncovered heavy cost at step t, and H_t(*) the covered cost proxy from MRS (Cor. 4.5): E[eH_{t+1}(*) | C_t] ≤ eH_t(*).
Claim (expected collision bound, sketch). In the first k1 steps,
E[# times we sample a point from an already-covered heavy cluster] ≤ O(1/R) · k1.
Idea: At any t before all heavy clusters are covered, the probability to hit a covered heavy cluster is ≤ (covered-heavy-cost)/(U_t(H) + rest). By Lemma 4.1 and Cor. 4.5, covered-heavy-cost is at most a 5× sum of OPT1 over covered heavy clusters in expectation, while U_t(H) remains a constant fraction of Σ_{P∈P_H} OPT1(P) until the last few heavy clusters are hit. Summing these probabilities over t yields the O(k1/R) bound. A rigorous proof needs a stopping-time argument that tracks U_t(H) via the supermartingale eH_t and a conditioning on the first time the number of uncovered heavy clusters drops to a given value.
Why useful here: This is a concrete, checkable step toward showing that under strong scale separation, k-means++ covers each heavy cluster before collision with high probability, enabling an O(log(1/ε)) tail via bi-criteria on the light remainder.

## Lower-bound direction (status)
A robust Ω(log k0) instance under random-k smoothing likely requires Δ(k)=1 for a 1−o(1) fraction of k∈[k0,2k0). A candidate is a nested family of nearly orthogonal tiny-variance simplices placed at carefully chosen radii so that OPT(k−1) ≈ (1+Θ(1/ln k0))·OPT(k) for most k in the range. I propose to formalize a discrete schedule of radii ensuring each additional center yields only a multiplicative ≈(1−1/(c ln k0)) drop in OPT, yielding only microscopic plateaus; verifying this rigorously is the next step for a lower bound.

## Next steps (concrete and verifiable)
1) Incorporate the Hybrid plateau average lemma into output.md as an explicit theorem, with all constants tracked and a short proof (as above), and add the monotonicity one-liner.
2) Add a short corollary: On a plateau with L ≥ c·m, at least a 1−1/c fraction of k achieves E_++[ALG/OPT] ≤ 2·C_bi·(1+1/(e c)). Then translate by Markov into a 0.01-probability global statement under a simple mass condition on such plateaus.
3) Begin formalizing the expected-collision micro-lemma under the scale separation ratio R, with a precise supermartingale/optional stopping argument (using eH_t from MRS). The goal is an explicit bound of the form P[≥ one heavy collision] ≤ R^{−Ω(1)}.
4) Explore the refined global bound numerically on synthetic OPT(·) profiles to validate the improvement from the (m ln L)/L term and identify worst-case patterns.

## Why this is incremental progress
- Provides a strictly stronger plateau-average bound using both MRS branches, with explicit constants and steps that are easy to verify.
- Extracts a practical quantile guarantee (for the “≥0.01 probability” variant) under a simple plateau-length condition.
- Identifies and justifies an implicit monotonicity used in output.md.
- Advances the heavy-coverage program by isolating a concrete expected-collision estimate to be proved next.

## Recap of curated results and scope
- We have a clean per-k bound (Theorem 1 in output.md) via the anchor s(k):=min{s≤k: OPT_s ≤ 2·OPT_k}, translating “effective oversampling” into the MRS bi-criteria guarantee with a factor-2 slack.
- We have a per-plateau average bound (Theorem 2) using only the logarithmic branch of MRS and a Stirling inequality, plus a global decomposition across plateaus (Corollary 3).

## Small fixes/clarifications for output.md
- Monotonicity justification: In Theorem 2’s proof, replacing (s(k),Δ(k)) by (m,k−m) is valid because the MRS two-branch bound f(s,Δ):=min{2+1/(2e)+ln(2s/Δ), 1+s/(e(Δ−1))} is nondecreasing in s and nonincreasing in Δ for Δ≥2; the log-branch is also nonincreasing in Δ for Δ≥1. Thus f(s(k),Δ(k)) ≤ f(m,k−m). This monotonicity could be stated explicitly for completeness.
- Endpoint constant: The Δ=0 contribution can be bounded by C_fix(ln m + 2)/L, so A2 can be set to C_fix and the implicit O(1) made explicit as +2. This may help downstream constant accounting.
- Range note: On [k0,2k0), any plateau start m≥k0 and length L≤2k0−m≤m. This observation is useful when invoking the “large-Δ branch” below: since Δ≤L−1≤m−1, the second branch never yields values below 1+1/e.

## Lemma A (Monotonicity of the MRS bi-criteria bound)
Statement. Let f(s,Δ):=min{ 2+1/(2e)+ln(2s/Δ), 1+s/(e(Δ−1)) } defined for integers s≥1, Δ≥1 (interpret the second term only for Δ≥2). Then for fixed s, f is nonincreasing in Δ; for fixed Δ≥2, f is nondecreasing in s.
Why useful here. This formally justifies the substitution s(k)≤m and Δ(k)≥k−m in Theorem 2.
Sketch. Each branch is coordinate-wise monotone: ln(2s/Δ) increases in s and decreases in Δ; s/(e(Δ−1)) increases in s and decreases in Δ for Δ≥2. The pointwise minimum of functions with the same monotonicity preserves these directions.

## Proposition B (Two-branch per-plateau averaging with a tunable split)
Setup. On a 2-plateau I=[m,m+L) (L≥2), for Δ:=k−m∈{1,…,L−1}, we have the per-k bound (Theorem 1, with s(k)≤m, Δ(k)≥Δ):
E_++[ALG_{m+Δ}/OPT_{m+Δ}] ≤ 2·C_bi·min{ a + ln(2m/Δ), 1 + m/(e(Δ−1)) }, where a:=2 + 1/(2e).
For any integer τ with 1≤τ≤L−1,
(1/(L−1)) Σ_{Δ=1}^{L−1} min{ a + ln(2m/Δ), 1 + m/(e(Δ−1)) }
≤ a + (τ/(L−1))·ln(2m/τ) + ((L−1−τ)/(L−1)) + (m/(e(L−1)))·(H_{L−2} − H_{τ−1}),
where H_r is the r-th harmonic number, H_0:=0.
Proof idea. Split the sum at τ: for Δ≤τ use the logarithmic branch and upper bound ln(2m/Δ) by ln(2m/τ); for Δ≥τ+1 use the 1+m/(e(Δ−1)) branch and sum the 1/(Δ−1)-tail by H_{L−2}−H_{τ−1}.

- Consequence 1 (recovering Theorem 2): Choosing τ=L−1 (where the “large-Δ” branch is unused) yields the Stirling-based bound used in Theorem 2: average ≤ a + 1 + ln(2m/(L−1)).
- Consequence 2 (benefit when L is a constant fraction of m): Since L≤m on [k0,2k0), set τ=⌊c·L⌋ with c∈(0,1) fixed. Then H_{L−2}−H_{τ−1} ≤ ln((L−1)/(cL−1)) + O(1/L) = ln(1/c) + O(1/L). The right-hand side becomes
  a + c·ln(2m/(cL)) + (1−c) + (m/(e(L−1)))·(ln(1/c)+o(1)).
  If L≥α m (α∈(0,1]), then m/(L−1)≤1/α and ln(2m/(cL)) ≤ ln(2/(cα)), giving an explicit O(1) bound in terms of α (see Corollary C below).

## Corollary C (Explicit constants on long plateaus)
If L≥α m for some α∈(0,1], then for k uniform over I and algorithmic randomness,
E[ALG_k/OPT_k] ≤ 2·C_bi·( a + 1 + ln(2/α) ) + (C_fix(ln m + 2))/L.
Taking C_bi=C_fix=5, this simplifies to
E[ALG_k/OPT_k] ≤ 10·(3 + 1/(2e) + ln(2/α)) + 5(ln m + 2)/L.
In particular, if α is an absolute constant, the plateau-average is an absolute constant (e.g., for α=1, the leading term is ≤ 10·(3 + 1/(2e) + ln 2) ≈ 38.8).
Proof. Apply Proposition B with τ=L−1 (or any fixed fraction of L) and the observation m/(L−1)≤1/α, ln(2m/(L−1)) ≤ ln(2/α) + ln(L/(L−1)) ≤ ln(2/α) + O(1/L). Add the Δ=0 endpoint term.

## Corollary D (Quantile-in-Δ guarantee on a plateau)
Fix any η∈(0,1). On I=[m,m+L) with L≥2, for all Δ≥η(L−1), the large-Δ branch yields
E_++[ALG_{m+Δ}/OPT_{m+Δ}] ≤ 2·C_bi·(1 + m/(e·η(L−1))).
Therefore, for at least a (1−η) fraction of k∈I, we have the bound above. If moreover L≥α m, this becomes ≤ 2·C_bi·(1 + 1/(eαη)). For C_bi=5 and, say, η=0.01, α=1, this gives an explicit 1%‑quantile bound ≤ 10·(1 + 100/e) ≈ 10·(1 + 36.8) ≈ 378.
Why useful. This provides a rigorous “with probability ≥ 1−η over k uniform on the plateau” guarantee using the second branch. It is numerically loose but constant when L is a constant fraction of m.

## Suggested curation items
- Add Lemma A explicitly to output.md to justify the monotonic substitutions.
- Add Proposition B (two-branch split) and Corollary C (explicit constants for L≥α m). These strengthen Theorem 2 in regimes with long plateaus by giving explicit constants that do not grow with k0 (beyond a negligible L^{-1} endpoint term).
- Optionally add Corollary D (quantile-in-Δ within a plateau) as a step toward the “probability ≥ 0.01” formulation in the task.

## Examples and sanity checks
- Long dyadic plateau: If OPT halves only once across [k0,2k0), then L≈m (α≈1). Corollary C gives an O(1) plateau-average; Theorem 2 also gives O(1); Corollary D yields an explicit 1% bound ≤ ~378 (with C_bi=5), improvable by tightening constants.
- Short-plateau regime: If all plateaus have L_j=1, Corollary 3 indicates the global average can be Θ(log k0). Proposition B reverts to the Theorem 2 bound with τ=L−1.

## Toward a high-probability “heavy-coverage” lemma (progress)
Goal. Under strong scale separation (e.g., OPT_{k1} ≥ k^C·OPT_k for large C), show that the first k1 steps select one center in each of the k1 heavy optimal clusters without collision with probability ≥ 1−k^{-Ω(1)}.
Incremental step (testable sub-claim). Let H_t be the set of heavy clusters still uncovered at step t≤k1, and U_t(heavy) their uncovered cost. Then using Corollary 4.5 of MRS (supermartingale eH_t(P) per cluster) and linearity, we can define eH_t(heavy):=Σ_{P∈H_t} eH_t(P), which is a supermartingale dominating H_t(heavy). If the total “light” mass satisfies H_t(light)+U_t(light) ≤ OPT_k · polylog k and OPT_{k1} ≥ k^C·OPT_k, then for all t≤k1,
  E[ 1 − U_{t}(heavy)/(U_t(heavy)+H_t(all)) ] ≤ k^{-Ω(1)}.
Interpretation. The term in brackets is the one-step miss probability within heavy clusters. Summing over t or using a multiplicative Chernoff-type bound for time-inhomogeneous trials suggests a k^{-Ω(1)} bound on any miss among heavy clusters. Formalizing this requires quantifying the drift of eH_t(heavy) and the coupling between heavy and light processes; this is the next technical hurdle.
Next steps: (i) pin down a concrete scale-separation threshold (C) and a light-mass bound that make the miss probability summably small; (ii) formalize a stopping-time argument over the first k1 steps, applying optional stopping to eH_t(heavy) and bounding the cumulative collision probability.

## Next concrete actions
1) Curate Lemma A (monotonicity) and Proposition B (two-branch plateau averaging) into output.md, with explicit constants (e.g., C_bi=C_fix=5) to yield numerically interpretable bounds (Corollary C and D).
2) For the “0.01 probability” goal, record Corollary D as a per-plateau quantile bound. Then state a global condition: if at least 1% of k lie in plateaus with L≥α m, we get an explicit constant bound for those k.
3) Tighten constants in Corollary C by optimizing the split τ in Proposition B (solve for τ minimizing the RHS or pick τ≈(m/e)∧(L−1) to balance branches). Provide a closed-form upper bound using ln and 1/τ estimates.
4) Continue the heavy-coverage program: attempt to bound the expected number of heavy-collisions via a martingale or potential argument and convert to a high-probability bound via Freedman/Azuma inequalities adapted to bounded differences in eH_t(heavy).
5) Optional: instantiate Theorem 2’s constants numerically: A0=10·(2+1/(2e))≈21.84, A1=10, A2=C_fix≈5, replacing O(1) by +2 in the endpoint term, to facilitate downstream aggregation and “back-of-envelope” guarantees.

## Round 0003 — 2025-08-27T19:55:00Z

Overview
- I read the curated output.md. The core per-k anchor bound (Theorem 1), the pure-log plateau average (Theorem 2), the MRS-monotonicity lemma, and the tunable split (Theorem 3) with its corollaries are now in place. This gives a flexible toolkit to bound the smoothed-in-k objective via plateau decomposition.
- In this round, I (i) isolate a couple of small clarifications and an elementary structural lemma useful for downstream applications; (ii) formalize a “min-of-extremes” corollary that we can invoke plateauby-plateau without choosing τ; (iii) derive explicit, checkable 1%–probability guarantees under a simple plateau-length condition with constants; and (iv) propose a targeted micro-lemma (with assumptions) toward the heavy-coverage program with a clean expected-collisions bound, ready to be lifted to high probability in future rounds.

Gaps and clarifications for output.md
- Edge-length remark on [k0,2k0). It will be repeatedly convenient to note that any factor-2 plateau I=[m,m+L) contained in [k0,2k0) necessarily satisfies L ≤ 2k0 − m ≤ m (since m ≥ k0). This was briefly mentioned in notes but not stated in output.md. Suggest adding this as a Remark after Theorem 2/Theorem 3 to justify simplifications like m/(L−1) ≤ 1.
- Explicit “min-of-extremes” corollary. Corollary 4 tabulates the two extreme choices τ= L−1 and τ=1. It is useful to add an explicit corollary stating that the plateau-average is bounded by the minimum of these two expressions. I state this precisely below (Corollary A) because many downstream inequalities can be derived by directly taking this min and then simplifying regime-by-regime.

New small lemmas and corollaries (clean, checkable)

Lemma A (Geometric constraint on plateau length).
Let I=[m,m+L) be a factor-2 plateau contained in [k0,2k0). Then L ≤ 2k0 − m ≤ m. In particular, m/(L−1) ≥ 1 for L ≥ 2.
Proof. Since the largest element in I is m+L−1 ≤ 2k0−1, we have L ≤ 2k0−m. As m ≥ k0, we get 2k0−m ≤ m. The “in particular” follows because L−1 ≤ m−1 for L≥2.

Corollary A (Min-of-extremes plateau bound).
Under the hypotheses of Theorem 3 with L≥3, for k uniform on I and algorithmic randomness,
E[ ALG_k/OPT_k ] ≤ min{ B_log, B_hyb } + (C_fix · (ln m + O(1)))/L,
where
- B_log := 2·C_bi · ( a + 1 + ln^+( 2m/(L−1) ) ),
- B_hyb := 2·C_bi · ( 1 + (a+ln(2m))/(L−1) + (m/(e(L−1))) · H_{L−2} ),
with a := 2 + 1/(2e) and H_q ≤ ln q + 1.
Proof. From Theorem 3, the two extreme choices τ=L−1 and τ=1 give the two RHS bounds (Corollary 4). Taking the minimum only tightens the inequality. Replace ln by ln^+ in B_log to weaken slightly and avoid negative logs; this is already consistent with Theorem 2.
Why useful. This freestates the “hybrid” advantage without committing to τ. It also enables simple regime reductions:
- If L ≥ α m with α∈(0,1], then B_log ≤ 2·C_bi·(a+1+ln(2/α)).
- If L is large and m/(L−1) ≪ 1/ln L, then B_hyb ≤ 2·C_bi·(1 + o(1) + (m/(e(L−1))) ln L) = O(1).

Corollary B (Explicit constants for probability ≥ 1% on a long plateau).
Fix η∈(0,1). On a factor-2 plateau I=[m,m+L) with L≥2 and for T := max{2, ⌈η(L−1)⌉+1}, Corollary 6 implies that for at least a (1−η−O(1/L)) fraction of k∈I,
E_++[ ALG_k/OPT_k ] ≤ 2·C_bi · ( 1 + m/( e (T−1) ) ).
In particular, if L ≥ α m for some α∈(0,1], then for at least a (1−η−O(1/L)) fraction of k,
E_++[ ALG_k/OPT_k ] ≤ 2·C_bi · ( 1 + 1/( e α η ) ).
Therefore, using Markov,
P_{k∈I,++}[ ALG_k/OPT_k ≤ 100 · 2·C_bi · ( 1 + 1/( e α η ) ) ] ≥ (1−η−O(1/L)) · 0.99.
Consequently, if I covers a γ-fraction of [k0,2k0) (i.e., L/k0 ≥ γ), then
P_{k∼U([k0,2k0)),++}[ ALG_k/OPT_k ≤ 200·C_bi · ( 1 + 1/( e α η ) ) ] ≥ γ(1−η−O(1/L)) · 0.99.
Concrete instantiation. Take C_bi=5 and α=1 (i.e., L ≥ m) and η=0.99. Then with probability ≥ ≈ 0.99·(1−0.99) ≈ 0.01 on k within that plateau we get E_++[ALG/OPT] ≤ 10·(1 + 1/(0.99 e)) ≈ 13.7. Markov converts this into
P_{k,++}[ ALG/OPT ≤ 100 · 13.7 ] ≥ 0.99 on those k. Thus, if such a plateau occupies ≥ 1% of [k0,2k0), we obtain a 1% overall probability bound with an explicit constant ≤ 1370.
Why useful. This gives a rigorous 1%–probability guarantee under a single checkable condition L ≥ α m on a plateau that covers a constant fraction of the window. Constants can be improved by tuning η or by using the min-of-extremes bound when α is moderately large.

Examples and sanity checks
- α = 1 (maximal plateau within [k0,2k0)). Corollary A with B_log: E ≤ 2·C_bi·(a+1+ln 2) + (C_fix·(ln m)/L). With C_bi=C_fix=5, a≈2.183, this is ≤ 10·(3.183 + 0.693) + o(1) ≈ 38.8.
- Intermediate α. For α=1/10, B_log gives ≤ 10·(3.183 + ln 20) ≈ 10·(3.183 + 2.996) ≈ 61.8; B_hyb may improve this further when L is sufficiently large relative to m ln L.

Toward optimizing the τ-split (next technical step)
Goal. Select τ = τ(m,L) minimizing the RHS in Theorem 3. Writing the bound (omitting absolute constants and the negligible endpoint term) as
G(τ) := (τ/(L−1)) [a + 1 + ln(2m) − ln τ] + ((L−1−τ)/(L−1)) + (m/(e(L−1))) [H_{L−2} − H_{τ−1}],
we can approximate H_{q} − H_{p} ≈ ln(q/p) to get
G(τ) ≈ 1 + (τ/(L−1)) [a′ + ln(2m/τ)] + (m/(e(L−1))) ln((L−2)/(τ−1)).
- Balancing heuristic. Differentiating the approximation suggests choosing τ so that
ln(2m/τ) ≈ 1 + (m/e) · 1/(τ−1),
which informally places τ near the smaller of L−1 and a scale ≈ m/ln L if L is very large. This predicts the boundary-optimality of τ=L−1 in the regime m/L ≫ ln L and of a small constant τ in the regime L ≫ m ln L, corroborating Corollary A’s min-of-extremes behavior.
Actionable objective. Provide a rigorous upper envelope of the form
E_{k∈I} E_++[ALG/OPT] ≤ K0 + K1 · min{ ln^+(2m/(L−1)), (m ln L)/L } + K2 · (ln m)/L,
by selecting τ = 1 when (m ln L)/L ≤ c, else τ = L−1. Since Corollary A already yields these two regimes, formalizing the min and tracking constants is straightforward.

Heavy-coverage program: an expected-collisions micro-lemma under a dominance hypothesis
We propose an intermediate, verifiable statement that isolates the heart of the “coverage before collisions” phenomenon without yet deriving it purely from OPT-scale separation.

Hypothesis H(β,k1):
Let the optimal k-partition be P = P_H ⊎ P_L with |P_H| = k1. Consider k-means++ run. Let U_t(H) be the uncovered cost of heavy clusters at time t; H_t(H) the covered heavy cost proxy (use eH_t as in output.md). Assume that for all t prior to covering all heavy clusters,
U_t(H) ≥ β · ( H_t(all) + U_t(L) ).
Interpretation. The uncovered heavy mass dominates the rest by factor β at all intermediate times until heavy coverage is complete. This captures a strong-but-natural “heavy dominance” condition.

Lemma C (expected heavy-collision bound under H(β,k1)).
Assume H(β,k1). Then the expected number of times k-means++ samples in a previously covered heavy cluster (a “heavy collision”) before all heavy clusters are covered is at most k1/β. In particular, if β ≥ k1·κ, the expected number is ≤ 1/κ.
Sketch proof.
- At any step t before heavy coverage completes, conditional on the filtration, the probability that we sample from a covered heavy cluster is at most H_t(H) / ( U_t(H) + H_t(all) + U_t(L) ). By H(β,k1), this is ≤ H_t(H) / ((1+1/β)U_t(H)) ≤ 1/β, since H_t(H) ≤ U_t(H) just before the last uncovered heavy is hit (or use the supermartingale eH_t to bound H_t(H) in terms of U_t(H) with a constant factor). A more conservative inequality — replacing H_t(H) by eH_t(H) and noting eH_t(H) ≤ 5·OPT_{k1}(X) while U_t(H) remains ≥ (β/(1+β))·cost_t(X) — yields the same 1/β order.
- Summing over the at most k1 draws that land in heavy clusters (each step either hits a new heavy cluster or collides within heavy), linearity of expectation gives the stated bound.
Usefulness. Together with the MRS bi-criteria step applied after heavy coverage (oversampling the light part), this would yield an O(log(1/ε)) guarantee provided β is polynomially large in k (or even β ≳ k1), precisely the intuition behind Subsection 2 of task.tex. The next step is to derive H(β,k1) from a scale-separation condition (e.g., OPT_{k1} ≥ k^C · OPT_k) using the eH_t supermartingale and the uncovered-cost monotonicity; I will attempt this derivation next.

Next steps (concrete and verifiable)
1) Add Lemma A (L ≤ m) and Corollary A (min-of-extremes) to output.md, with short proofs (as above).
2) State a global 1%–probability corollary (Corollary B) in output.md: if a plateau I covers a γ-fraction of [k0,2k0) and satisfies L ≥ α m, then with probability at least 0.99·(γ(1−η−O(1/L))) ≥ 0.01 (for suitable η,γ) we get a constant-factor bound; make constants explicit with C_bi=C_fix=5.
3) Formalize the “upper envelope” min{ ln^+(2m/(L−1)), (m ln L)/L } bound from Corollary A, tracking constants K0,K1,K2, by simply taking the min of the two extremes and simplifying H_{L−2} ≤ ln(L−2)+1.
4) Heavy-coverage: Under a dominance hypothesis H(β,k1), finish a rigorous proof of Lemma C by replacing the heuristic H_t(H) ≤ U_t(H) with a clean inequality (e.g., via eH_t and stopping-time arguments) and quantify the number of heavy steps ≤ k1. Then study sufficient conditions (in terms of OPT-scale separation) that imply H(β,k1) with β polynomial in k.
5) Lower bounds: Evaluate the plateau calculus on synthetic OPT profiles (e.g., OPT_k ∝ k^{-p} with p>0) to validate constant/ log log behavior; in parallel, probe whether one can enforce Δ(k) small on most k by constructing instances with frequent large one-step drops, and quantify the resulting average using Theorem 1 (per-k anchor) directly.

Why this advances the program
- Corollary A provides a ready-to-use bound that often gives real improvements without τ-optimization.
- Corollary B supplies rigorous “≥ 1% probability” guarantees with explicit constants, addressing the alternative objective in the task statement.
- Lemma A is a simple geometric fact used implicitly; recording it helps avoid slips when simplifying constants.
- Lemma C frames the heavy-coverage argument into a concrete, checkable expectation bound under a clear dominance assumption, paving the way for deriving this assumption from OPT-scale separation using the MRS supermartingale machinery.

## Round 0003 — Optimization of the plateau-averaging bound, probability guarantees, and a micro-step toward heavy-coverage

### Quick audit of output.md
- The monotonicity lemma is now present and correct, which closes the key gap needed in Theorem 2’s proof.
- The tunable split (Theorem 3) fixes the earlier averaging inaccuracies; its proof correctly handles the Δ=1 term and the harmonic tail.
- Edge cases: Theorem 3 allows L≥2; the use of H_{L−2} is consistent (H_0=0 for L=2). The large-Δ branch is applied only for Δ≥2 as required.
- Minor note: In Theorem 2’s proof, the factor (L−1)/L arises because averaging over the plateau includes the Δ=0 endpoint with probability 1/L; this is implicitly used and consistent.

I see no correctness issues that require changes to output.md. Below are incremental, testable improvements and additions built on the curated results.

### Idea 1: A clean “optimized-τ” corollary of Theorem 3
Goal: Provide a simple, closed-form plateau bound by instantiating τ in Theorem 3 to either “near-full-log” or “mid-split,” yielding a min-of-two functional that is easy to aggregate across plateaus.

Proposed Corollary (to add after Corollary 4). Let I=[m,m+L) with L≥3. Then for k uniform in I and the algorithmic randomness,
- Choose τ1 := L−1 (pure log branch). Then
  E[ALG/OPT] ≤ 2 C_bi (a + 1 + ln(2m/(L−1))) + C_fix (ln m + O(1))/L.
- Choose τ2 := ⌊L/2⌋ (balanced split). Using H_{L−2} − H_{τ2−1} ≤ ln((L−1)/(τ2−1)) + 1 ≤ ln 2 + 1, we get
  E[ALG/OPT] ≤ 2 C_bi ( (1/2)(a + 1 + ln(4m/L)) + 1/2 + (m/(e(L−1)))(ln 2 + 1) ) + C_fix (ln m + O(1))/L.
Taking the minimum of these two explicit bounds yields
  E[ALG/OPT] ≤ K0 + K1 · min{ ln^+(2m/(L−1)), ln^+(4m/L) + (m/L) } + K2 · (ln m)/L,
for absolute constants K0,K1,K2 depending only on C_bi,C_fix (e.g., with C_bi=C_fix=5, K0≈35, K1≈10, K2≈5).

Why useful and how to verify:
- This gives a simple min-of-two regime bound with an explicit (m/L) term rather than (m ln L)/L. It isolates the transition: if L is a constant fraction of m, both branches are O(1); if L≪m, the logarithmic branch dominates, as expected.
- Proof is a direct instantiation of Theorem 3 with τ=L−1 and τ=⌊L/2⌋, plus H_q ≤ ln q + 1 and ln( (L−1)/(τ2−1) ) ≤ ln 2 + o(1).

Optional refinement (third choice τ3 := min{L−1, ⌊m/e⌋}). For this selection the harmonic tail is zero when L−1 ≤ ⌊m/e⌋, giving a bound of order (m/L) with a constant inside the logarithm term ln(2e); this can be added as a third candidate branch to tighten constants further in some short-plateau regimes. I leave this out of the main corollary for simplicity but can provide it on request.

### Idea 2: Global min-of-two mixture across plateaus
Using the corollary above inside the global decomposition (Corollary 3), we obtain
E_k,++[ALG/OPT] ≤ (1/k0) Σ_j L_j [ K0 + K1 · min{ ln^+(2 m_j/(L_j−1)), ln^+(4 m_j/L_j) + (m_j/L_j) } ] + (K2/k0) Σ_j ln m_j.

Why useful: This sharpened aggregation makes explicit that extremely long plateaus (L_j≈m_j) contribute a constant (from ln terms ≈ O(1) and m_j/L_j≈O(1)), while short plateaus revert to the log term ln(2 m_j/L_j). It recovers O(log log k0) under the same “long plateau” structural condition and clarifies that no unconditional improvement over O(log k0) is possible from plateau calculus alone.

Verification plan: Numerically instantiate with synthetic OPT profiles to compare against Theorem 2; but analytically the bound follows by substituting the corollary on each I_j and summing with weights L_j/k0.

### Idea 3: Explicit probability-≥0.01 guarantees under a plateau mass condition
We can strengthen Corollary 6 to a concrete 1% probability guarantee over the joint (k,++) randomness under a simple condition.
- Fix a plateau I=[m,m+L) with L≥α m for some absolute α∈(0,1]. Pick η=0.01 in Corollary 6, so T=max{2, ⌈0.01(L−1)⌉+1} and for at least a (1−η−O(1/L)) fraction of k in I we have
  E_++[ALG/OPT] ≤ 2 C_bi (1 + m/(e(T−1))) ≤ 2 C_bi (1 + 1/(e α η)) ≤ 2 C_bi (1 + 100/(e α)).
- With C_bi=5 and α=1, this gives E_++[ALG/OPT] ≤ 10(1+100/e) ≈ 10·(1+36.79) ≈ 378 on at least ~99% of k in I.
- By Markov’s inequality, for any c≥1, P_{++}[ALG/OPT ≤ c·378] ≥ 1−1/c on those k.
- If such a plateau occupies a fraction p of [k0,2k0), the global joint (k,++) probability is ≥ p·(0.99)(1−1/c). For example, p=0.02 and c=2 already give probability ≥ 0.0198 for a factor ≤ 756.

This addresses the task’s “probability ≥ 0.01” variant under a verifiable condition (a single long plateau). Constants can be tuned by selecting η and c.

### Idea 4: Micro-lemma toward heavy-coverage under scale separation
We propose a precise sub-claim to advance the conjectural “heavy coverage” lemma. Let the optimal k-partition be P=H∪L with |H|=k1 “heavy” clusters and |L|=k2. Suppose Σ_{Q∈H} OPT1(Q) ≥ R · Σ_{Q∈L} OPT1(Q) for R ≥ k^C (C large). Consider the k-means++ run, and let U_t(H) be the uncovered heavy cost at step t, eH_t(H)=Σ_{Q∈H} eH_t(Q) (supermartingale from MRS Cor. 4.5).

Claim (expected heavy-collision bound under a persistence condition). If there exists a constant β∈(0,1) such that for all t≤k1 (until the last few heavy clusters), E[U_t(H)] ≥ β Σ_{Q∈H} OPT1(Q), then the expected number of times k-means++ selects a point from a covered heavy cluster before all heavy clusters are hit is at most O(k1/(β R)).

Sketch of argument (checkable steps):
- At step t, the probability to sample from a covered heavy cluster is at most E[H_t(covered H)] / E[U_t(H) + H_t(covered H) + cost_t(L)]. By MRS Lemma 4.1 and the supermartingale property, E[H_t(covered H)] ≤ 5 Σ_{Q∈H} OPT1(Q) uniformly in t.
- The denominator is ≥ E[U_t(H)] + Σ_{Q∈L} OPT1(Q) (since covered costs on L are nonnegative). Under the β-persistence and the scale separation Σ_H ≥ R Σ_L, this is ≥ β Σ_H + Σ_L ≥ (β + 1/R) Σ_H.
- Thus the per-step collision probability is ≤ 5 Σ_H / ((β + 1/R) Σ_H) ≤ 5/β for small R, and ≤ 5 R / (β R + 1) ≤ 5/(β) · (1/R) when R is large. Being precise: for R≥1/β we have ≤ 10/(β R). Summing over t up to k1 steps gives E[collisions among heavy] ≤ O(k1/(β R)).

Usefulness and what remains: This turns scale separation into a quantitative bound on expected heavy-collisions, contingent on the (β-persistence) condition that uncovered heavy cost stays a constant fraction of Σ_H until near the end. The next step is to justify (β-persistence) under scale separation—intuitively, centers selected outside H should not significantly reduce U_t(H) before we start hitting H, but this needs a geometric or potential-based argument. A path is to show that the expected reduction in U_t(H) from a center outside H is o(Σ_H) when R is large, using the same pairwise-bound technique as in MRS Lemma 4.1 aggregated over H.

This micro-lemma is an incremental step: it isolates the coupon-collector-like collision control given any β-persistence bound, paving the way for a high-probability heavy-coverage statement via Freedman/Azuma once (β-persistence) is established and bounded-difference increments of eH_t(H) are quantified.

### Idea 5: Lower-bound direction tailored to random-k smoothing (plan refinement)
Objective: Build an instance where Δ(k)=1 on a 1−o(1) fraction of k∈[k0,2k0), forcing an Ω(log k0) average even under random-k smoothing.
- Construction sketch: Start from a Brunsch–Röglin-type instance with one large “heavy” simplex cluster and many singleton “light” clusters positioned at radii calibrated so that each additional center mostly “unlocks” one light cluster at a time with only a tiny multiplicative drop in OPT (e.g., OPT_{k−1}/OPT_k ≈ 1 + Θ(1/ln k0)).
- Goal property: For most k in the window, s(k)=k−1, hence Δ(k)=1 and Theorem 1 collapses to the fixed-k O(ln k) bound.
- Next steps: Specify the radii schedule r_i to ensure OPT’s discrete derivative remains within a (1+O(1/ln k0)) multiplicative gap for ≳0.9 k0 steps; then compute E_k E_++[ALG/OPT] ≥ c ln k0 via Arthur–Vassilvitskii’s lower-bound methodology, adapted to the window.

### Concrete next steps (verifiable)
1) Add the “optimized-τ corollary” to output.md with explicit constants and a short proof (as above). This gives an easy-to-use min-of-two bound: logarithmic vs. an (m/L)-driven term.
2) Incorporate the global min-of-two mixture bound and document the regimes it improves upon Theorem 2 (particularly when L is a moderate constant fraction of m).
3) Formalize the 0.01-probability guarantee in the style of Idea 3, with explicit constants for C_bi=5; specify the dependence on the plateau mass p to obtain a global guarantee.
4) Prove the expected heavy-collision bound under the β-persistence assumption cleanly (full write-up). Then investigate proving β-persistence from scale separation R≥k^C using the same pairwise-cost and supermartingale techniques as in MRS’s covered-cost analysis.
5) Begin crystallizing the random-k lower bound: settle a concrete radii schedule ensuring microscopic OPT drops across most of the window and work through the Δ(k)=1 frequency.

### Why these are incremental and auditable
- All new claims are direct instantiations of Theorem 3 or straightforward corollaries (τ=L−1, τ=⌊L/2⌋), with explicit constants; they can be checked algebraically.
- The probability guarantee is a direct, quantifiable application of Corollary 6 and Markov’s inequality.
- The micro-lemma isolates one probabilistic piece (expected heavy-collisions), making the remaining gap (β-persistence) explicit and testable in subsequent rounds.

### Quick audit of output.md (this round)
- The min-of-extremes bound (Corollary 7) and the derived upper envelope (Corollary 8) look correct and rigorously supported by Theorem 3 and H_q ≤ ln q + 1. The “plateau length within [k0,2k0)” Remark is now correctly stated with m/(L−1) ≥ 1.
- The probability corollary (Corollary 9) is also correct in its methodology (quantile-in-Δ + Markov). It cleanly separates the plateau-level and global mixing statements.
- Missing but useful: a rigorously stated “balanced-split” instantiation of Theorem 3 with τ=⌊L/2⌋ (with safe constants) that can improve constants in some intermediate regimes, and an explicit instantiation of constants in Corollary 8 when C_bi=C_fix=5 to facilitate downstream aggregation.

### New, checkable refinements (to be curated)

1) Balanced-split corollary with safe constants (intermediate regime)
- Statement (new). In the setting of Theorem 3 with L≥3 and τ:=⌊L/2⌋, for k uniform on I and ++ randomness,
  E[ALG/OPT] ≤ 2 C_bi [ (2/3)(a + 1 + ln(4m/(L−1))) + 1/2 + (m/(e(L−1)))(ln 3 + 1) ] + (C_fix(ln m + O(1)))/L,
  where a=2+1/(2e).
- Proof sketch (fully checkable): Start from Theorem 3, choose τ=⌊L/2⌋. Use the uniform bounds valid for all L≥3: τ/(L−1) ≤ 2/3; (L−1−τ)/(L−1) ≤ 1/2; H_{L−2}−H_{τ−1} ≤ ln 3 + 1; and ln(2m/τ) ≤ ln(4m/(L−1)). Substitute these into Theorem 3 and simplify. This yields a rigorous bound that can outperform both extremes in some midrange regimes (large L but not a fixed fraction of m), at the expense of slightly bigger constants on the logarithmic term.
- Why useful: Strengthens Corollary 7 by adding a third, explicitly computable candidate bound in the min, sometimes improving over the pure-log extreme when L is moderately large, and over the τ=1 extreme when m/(L−1) is not too big.

2) Explicit constants for the upper-envelope (Corollary 8) when C_bi=C_fix=5
- Claim (explicit instantiation). With C_bi=C_fix=5 and L≥2,
  E_{k∈I} E_++[ALG/OPT] ≤ 35 + 10·min{ ln^+(2m/(L−1)), (m ln L)/L } + 5·(ln m)/L.
- Justification: From Corollary 7, the logarithmic extreme gives 2·C_bi·(a+1) ≤ 2·5·(2+1/(2e)+1) ≤ 32 (more carefully: 10·(3+1/(2e)) ≈ 31.83), we round up to 35 to absorb O(1/L) terms and endpoint constants. The coefficient in front of ln^+(2m/(L−1)) is 2·C_bi=10. The hybrid extreme contributes a (m ln L)/(e(L−1)) term with factor 2·C_bi/e ≤ 10/e ≈ 3.68; we upper-envelope it by 10 for simplicity, safe since L−1≈L for L≥2. The endpoint Δ=0 term adds ≤ C_fix(ln m + 2)/L = 5(ln m + 2)/L, which we cover by 5·(ln m)/L after absorbing constants into the 35. This produces a fully explicit, conservative version of Corollary 8 ready for aggregation.
- Use: Facilitates clean global inequalities without carrying symbolic constants; helpful when deriving numeric probability thresholds from Corollary 9.

3) Plateau-level 1% guarantee: explicit instantiation
- From Corollary 9, choosing η=0.99 and c=100, on a single plateau I=[m,m+L) we get
  P_{k∼U(I),++}[ ALG/OPT ≤ 100·B_I ] ≥ (1−0.99−O(1/L))·0.99 ≈ 0.01 − O(1/L),
  with B_I = 2·C_bi·(1 + m/(e(T−1))) and T=⌈0.99(L−1)⌉+1. If L ≥ α m, then B_I ≤ 2·C_bi·(1 + 1/(α e · 0.99)) ≤ 10·(1 + 1.02/αe). For α=1 and C_bi=5, this gives B_I ≲ 10·(1+0.376) ≈ 13.76, hence ALG/OPT ≤ 1376 with probability ≥ 1% for k uniform on that plateau. If the plateau occupies a γ-fraction of [k0,2k0), the global (k,++) probability is ≥ γ times this quantity.
- Why useful: A ready-to-use “0.01-probability” bound with explicit constants under the simple condition L ≥ α m on one plateau covering a nonnegligible fraction γ of the window.

### Special-case heavy-coverage under metric separation (progress toward Subsec. 2)
- Motivation. The conjectural “heavy coverage” lemma under scale separation is difficult to prove in full generality due to possible reductions of U_t(H) by centers sampled outside heavy clusters. As an incremental step, we formalize and prove a special-case lemma under explicit metric separation, making the dominance/persistence condition verifiable.
- Setup. Let H be the set of k1 optimal clusters (heavy), and L the rest. For each heavy cluster P∈H, let diam2(P):=max_{x∈P}||x−µ_P||^2, and impose the separation condition: for some λ≥10,
  dist(µ_P, y) ≥ λ · sqrt(diam2(P)) for all P∈H and all y∉P.
  This enforces that points outside P are at least λ radii away from its centroid. Let k=k1+k2 and run k-means++ for k′=k1+(1+ε)k2 steps.
- Lemma (special-case heavy coverage). Under the above separation with λ≥10,
  (i) For any step t before covering all k1 heavy clusters, conditioned on the filtration, the probability that the next sample falls in an uncovered heavy cluster is at least 1 − O(1/λ^2).
  (ii) Consequently, with probability ≥ 1 − k1·O(1/λ^2), k-means++ selects one center in each heavy cluster within the first k1 selections that hit heavy clusters (i.e., before any heavy collision).
  (iii) Conditioned on (ii), the remaining k′−k1 steps oversample the light part by a factor (1+ε), hence by MRS Theorem 5.1, the final cost is ≤ 5·O(ln(1/ε))·OPT_k.
- Proof sketch (checkable constants): For (i), before covering P∈H, every x∈P has D(x)^2 ≥ ||x−µ_P||^2 (no heavy center chosen), while for any y∉P, D(y) ≥ dist(y,µ_P)−||x−µ_P|| ≥ (λ−1)·sqrt(diam2(P)) by triangle inequality. Summing over all uncovered heavy clusters and using that OPT1(P)=∑_{x∈P}||x−µ_P||^2, we get that the D^2-mass of uncovered heavy points is at least ∑_{P∈H_uncovered} OPT1(P), while the mass outside these heavy clusters is at most a 1/(λ−1)^2 fraction per point relative to diameters; aggregating, the uncovered-heavy mass dominates by a factor 1−O(1/λ^2). Thus the next D^2-sample lands in uncovered heavy with probability ≥ 1 − O(1/λ^2). Summing the collision probabilities over the k1 heavy hits yields (ii). Part (iii) follows from the bi-criteria bound (MRS Theorem 5.1). A short writeup can fix constants (e.g., replacing O(1/λ^2) by ≤ 5/λ^2) by carefully bounding the total outside mass in terms of ∑_{P∈H} OPT1(P).
- Why useful: Proves the desired phenomenon (coverage before collisions and O(ln(1/ε)) tail) in a clean geometric regime; provides a stepping stone toward deriving persistence from purely scale-separation conditions.

### Lower-bound direction: toward robustness under random-k smoothing
- Goal. Construct a dataset X for which Δ(k)=k−s(k)=1 for a 1−o(1) fraction of k∈[k0,2k0), implying that the plateau calculus cannot improve beyond Θ(log k0) unconditionally.
- Candidate blueprint (adapting MRS App. C.2):
  - Use one “heavy” simplex (many points) centered at the origin with extremely small edge length (so OPT1(heavy) ≈ 1 after scaling) and many “light” singleton clusters placed on coordinate axes at radii r_j chosen so that adding a center primarily “unlocks” one light cluster at a time with a small multiplicative drop in OPT.
  - Calibrate radii r_j so that OPT(k−1)/OPT(k)=1+Θ(1/ln k0) for most k in [k0,2k0), making the OPT sequence nearly flat and ensuring s(k)=k−1 typically.
- Next, compute E_{k,++}[ALG/OPT] over this window using Arthur–Vassilvitskii’s lower-bound technique (the “misses” process), to target a matching Ω(log k0) expectation.
- Concrete next steps: (i) write the exact OPT_k for this family; (ii) verify the discrete derivative control across the window; (iii) adapt the MRS miss-counting lower bound to average over k.

### Examples and sanity checks
- Long dyadic plateau: With OPT halving only once on [k0,2k0), L≈m. Using the explicit constants above, E_{k∈I}E_++[ALG/OPT] ≤ 35 + 10 ln 2 + o(1) ≈ 41.9.
- Intermediate L (balanced split advantage): Take L=m/√ln m. The balanced-split bound gives a term (m/(e(L−1)))(ln 3+1) ≈ O(√ln m), whereas the pure-log extreme gives 10 ln(2√ln m) = O(ln ln m). In this regime, the logarithmic extreme remains asymptotically better; the τ=⌊L/2⌋ bound, however, can numerically improve constants when m/(L−1) is small but L not a constant fraction of m.

### Obstacles and gaps
- Unconditional improvement: Without structural constraints on OPT(·), the plateau calculus still allows Θ(log k0) worst-case averages (e.g., when all L_j=1). No universal O(log log k0) upper bound is currently proven.
- Heavy-coverage beyond metric separation: Deriving a persistence bound U_t(H) ≳ Σ_H purely from OPT-scale separation remains open. The special-case lemma addresses geometric separation but not the general case.

### Next steps (actionable)
1) Add the balanced-split corollary (with τ=⌊L/2⌋) and its proof (with safe constants) to output.md; note it as an optional third candidate in the min.
2) Record the explicit-constants version of Corollary 8 for C_bi=C_fix=5 to support numerical corollaries and global aggregation.
3) Write a short, fully rigorous proof of the special-case heavy-coverage lemma under metric separation (choose concrete constants, e.g., λ≥20 to make probabilities crisp), and add it as a separate lemma with an application corollary (O(ln(1/ε)) under oversampling of light part).
4) Begin formalizing the random-k lower bound: fix a radii schedule {r_j}, prove Δ(k)=1 for ≥ 0.9k0 values in [k0,2k0), and compute the averaged Ω(log k0) lower bound via the miss-counting framework.
5) Explore whether mild regularity assumptions on OPT(·) (e.g., lower bounds on plateau lengths at dyadic scales) can be derived from general geometric principles to yield unconditional O(log log k0) bounds via Corollary 8.

## Round 0004 — 2025-08-27T19:48:23.817717Z

Overview and audit of output.md
- The curated file now contains: (i) the per-k anchor bound (Theorem 1), (ii) per-plateau averaging (Theorem 2), (iii) the monotonicity lemma, (iv) a tunable τ-split (Theorem 3), (v) the hybrid extremes (Corollary 4), (vi) long-plateau constants (Corollary 5), (vii) a quantile-in-Δ lemma (Corollary 6), (viii) the min-of-extremes bound (Corollary 7), (ix) an upper envelope (Corollary 8), and (x) a plateau-level and globally mixed probability statement (Corollary 9). The remark “L ≤ m on [k0,2k0)” is present with the correct implication m/(L−1) ≥ 1.
- I see no correctness issues that require retraction. Constants are not yet fully optimized but are kept symbolic, which is fine for now.

New refinement: an improved “third branch” via choosing τ ≈ m/ln L
Why useful here. Corollary 8 gives the envelope K0 + K1·min{ ln^+(2m/(L−1)), (m ln L)/L } + K2·(ln m)/L. The (m ln L)/L term is sometimes loose. By an explicit choice τ := ⌊m/ln L⌋ in Theorem 3 (when admissible), we can replace (m ln L)/L by (m (1 + ln ln L))/L, which is strictly smaller for L≥e^e, tightening intermediate regimes.

Statement (proposed Corollary: τ = ⌊m/ln L⌋ branch).
- Setting τ* := min{ L−1, ⌊m/ln L⌋ } in Theorem 3, for L≥3 and m≥L, there exist absolute constants K0,K1,K2 (depending only on C_bi,C_fix) such that
  E_{k∈I} E_++[ALG_k/OPT_k]
  ≤ K0 + K1·(m/L)·(1 + ln ln L) + K2·(ln m)/L.
- In particular, when τ* = ⌊m/ln L⌋ ≤ L−1 (i.e., m ≤ (L−1) ln L), one can take
  E_{k∈I} E_++[ALG_k/OPT_k]
  ≤ 2 C_bi·[ 1 + (m/((L−1) ln L))·(a+1+ln(4 ln L)) + (m/(e(L−1)))·(1 + ln ln L) ] + (C_fix·(ln m + 2))/L,
  with a := 2 + 1/(2e).

Proof sketch (fully checkable from Theorem 3).
- Start from Theorem 3 with general τ. Put τ := ⌊m/ln L⌋. We use the following bounds for L≥3 and m≥L:
  1) τ ≥ m/ln L − 1 ⇒ 2m/τ ≤ 4 ln L ⇒ ln(2m/τ) ≤ ln(4 ln L).
  2) τ/(L−1) ≤ (m/ln L)/(L−1) ≤ m/(L ln L).
  3) H_{L−2} − H_{τ−1} ≤ ln((L−2)/(τ−1)) + 1 ≤ ln(L/τ) + 1 ≤ ln( (L ln L)/m ) + 1 ≤ 1 + ln ln L, since L ≤ m.
- Plugging into Theorem 3’s RHS:
  (τ/(L−1))(a + 1 + ln(2m/τ)) ≤ (m/(L ln L))(a+1+ln(4 ln L));
  ((L−1−τ)/(L−1)) ≤ 1;
  (m/(e(L−1)))(H_{L−2} − H_{τ−1}) ≤ (m/(e(L−1)))(1 + ln ln L).
- Add the Δ=0 endpoint term C_fix(ln m + O(1))/L. Absorb additive constants into K0 and account for L−1 ≈ L (since L≥3) to get the displayed form.
- If τ* = L−1 (i.e., m > (L−1) ln L), we revert to the pure-log branch in Corollary 4.

Consequences and regime comparison
- Min-of-three envelope. Taken together with Corollary 7, we can use a min of three branches: (i) pure-log: ln^+(2m/(L−1)), (ii) hybrid (τ=1): (m ln L)/L, (iii) the new τ=⌊m/ln L⌋: (m (1 + ln ln L))/L. This uniformly strengthens Corollary 8 by allowing the improved middle regime.
- Where it helps: When L is moderately large relative to m (e.g., L ≥ m^γ with γ∈(0,1)), both hybrid branches yield O(1). The improvement is most visible for intermediate L where the previous envelope gave O((m ln L)/L) but ln ln L ≪ ln L, so the new bound is smaller by a factor ≈ ln L / ln ln L.
- Checkable example. Let L = m/√ln m (so L ≤ m and L≥3 for m large). Then
  • Pure-log: ln(2m/(L−1)) ≈ ln(2√ln m) = O(ln ln m).
  • Old hybrid: (m ln L)/L ≈ (m ln(m/√ln m))/(m/√ln m) ≈ √ln m · (ln m − (1/2) ln ln m) = Θ(√ln m · ln m) [very loose].
  • New branch: (m (1 + ln ln L))/L ≈ √ln m · (1 + ln(ln m − (1/2) ln ln m)) = Θ(√ln m · ln ln m).
  Hence min{·} now picks the O(ln ln m) pure-log branch anyway, but the new branch dramatically tightens the hybrid alternative and will be advantageous in other intermediate regimes (e.g., L = m / polylog m).

Global aggregation (plateau-wise min-of-three)
- Using the decomposition of Corollary 3, we can strengthen Corollary 8 as follows. There exist absolute constants K0,K1,K2 such that for the covering plateaus I_j=[m_j,m_j+L_j),
  E_{k∼U([k0,2k0))} E_++[ALG_k/OPT_k]
  ≤ (1/k0) ∑_j L_j [ K0 + K1·min{ ln^+(2m_j/(L_j−1)), (m_j ln L_j)/L_j, (m_j (1 + ln ln L_j))/L_j } ]
    + (K2/k0) ∑_j ln m_j.
- This follows by taking, for each plateau, the minimum over the three explicit instantiations τ=L−1, τ=1, τ=⌊m_j/ln L_j⌋, and summing with weights L_j/k0.

Probability-≥0.01 guarantees (clarification)
- Corollary 9 provides the plateau-level and globally mixed guarantees via (η,c). A simple instantiation for “≥0.01 probability”: If a single plateau I occupies a γ≥0.02 fraction of [k0,2k0) and satisfies L ≥ α m, then choosing η=0.5 and c=2 gives
  P[ ALG_k/OPT_k ≤ 2·B_I ] ≥ γ·(0.5−O(1/L))·0.5 ≥ 0.01 for all sufficiently large L,
  with B_I ≤ 2·C_bi (1 + 1/(α e η)) + o(1). This is numerically loose but completely explicit. Tuning (η,c) can improve constants as desired.

Small technical lemmas and clarifications (ready-to-curate)
- Stirling floor correction in the τ=⌊m/ln L⌋ branch: Using τ ≥ m/ln L − 1 and m ≥ L ≥ 3, the inequality 2m/τ ≤ 4 ln L holds (since m/ln L ≥ L/ln L ≥ 3/ln 3 > 2, hence m/ln L − 1 ≥ (1/2)·m/ln L), justifying ln(2m/τ) ≤ ln(4 ln L) cleanly.
- Harmonic tail control: H_{L−2} − H_{τ−1} ≤ ln(L/τ) + 1 works uniformly for L≥3, τ≥1; with τ=⌊m/ln L⌋ and L ≤ m, this gives ≤ 1 + ln ln L.

Heavy-coverage program: a precise, checkable sub-lemma (pathwise bound)
- Pathwise covered-heavy control. For any set H of heavy optimal clusters, define eH_t(P) as in MRS. Then pathwise we have H_t(P) ≤ eH_t(P) ≤ 5·OPT1(P) for every heavy P and every t (the first inequality is by definition of eH, the second is Lemma 4.1 applied at first hit). Hence
  H_t(H_cov) ≤ 5·∑_{P∈H} OPT1(P) deterministically for all t.
- Conditional per-step collision probability bound. At step t conditioned on filtration F_t,
  P[“sample from a covered heavy cluster” | F_t] = H_t(H_cov) / cost_t(X)
  ≤ 5·(∑_{P∈H} OPT1(P)) / cost_t(X).
- This converts the heavy-collision question into lower bounding cost_t(X) (denominator) by a function of the current uncovered-heavy mass U_t(H) (plus possibly light mass). Under a quantitative “dominance/persistence” hypothesis of the form cost_t(X) ≥ β·U_t(H) for all t before heavy coverage completes (β≥1), we immediately get
  P[collision in heavy at step t | F_t] ≤ 5·(∑_H OPT1(P)) / (β·U_t(H)).
  Summing only over steps in which a heavy pick occurs and using U_t(H) ≥ ∑_{P∈H_uncovered} OPT1(P), we find that the expected number of heavy collisions before covering all heavy clusters is bounded by a telescoping potential argument akin to a variable-probability coupon collector. This is a concrete, checkable path: it remains to prove the β-persistence from a scale-separation condition (e.g., OPT_{k1} ≥ k^C·OPT_k) via the eH_t supermartingale and to carry out the summation rigorously (I will do this next).

Examples and sanity checks
- Dyadic plateau (one long plateau): L≈m. All three branches give O(1) (pure-log dominates), consistent with Corollary 5.
- Geometric intermediate L: If L = m / polylog m, then the pure-log branch gives O(log log m); the new τ=⌊m/ln L⌋ branch gives O( (m/L)·ln ln L ) = O( polylog(m)·ln ln m ), which is larger; hence the min will pick the pure-log branch, as desired.
- Short plateaus L=O(1): All hybrid branches devolve to O(m/L), while pure-log gives O(ln m). Min therefore selects O(ln m), matching known worst cases.

Gaps and open tasks (clearly identified)
- output.md currently presents the min-of-two envelope (Corollary 8). The improved τ=⌊m/ln L⌋ branch is not yet recorded; adding it would strengthen the per-plateau bound in some regimes.
- The heavy-coverage argument remains conditional. The pathwise bound H_t(H_cov) ≤ 5∑_H OPT1(P) is now explicit and correct. The key missing piece is to prove a denominator persistence bound cost_t(X) ≥ β·U_t(H) (or a comparable lower bound) from a scale-separation assumption; I will attempt this via the eH_t supermartingale and a careful analysis of how sampling outside H affects U_t(H).
- Lower bound under random-k smoothing: still open; designing an instance with many short plateaus across the entire window requires careful OPT(·) control. This remains an item for a separate development.

Concrete next steps (verifiable)
1) Curate the new τ=⌊m/ln L⌋ corollary into output.md (as a third explicit instantiation of Theorem 3) and add the “min-of-three” per-plateau envelope and its global aggregation counterpart. All inequalities are elementary and already itemized above.
2) Provide explicit numerical constants K0,K1,K2 when C_bi=C_fix=5 for the new branch, e.g., K0≈40, K1≈10, K2≈5 (to be checked and tightened in write-up).
3) For the probability-≥0.01 variant, add a short remark instantiating Corollary 9 with (η,c) tuned to hit 1% under a simple plateau mass condition (as outlined above), giving a concrete constant.
4) Heavy-coverage: formalize “β-persistence from scale separation” and sum conditional collision probabilities over heavy draws until coverage; aim for an explicit bound on expected heavy collisions (e.g., polynomially small in k under scale separation), then lift to high probability via Freedman–Azuma.
5) Explore, even empirically, OPT profiles where the new τ=⌊m/ln L⌋ branch strictly improves the previous envelope, to guide any further analytical tightening (e.g., replacing ln(4 ln L) by ln ln(2m) in some ranges).



=== summary.md ===

## round-0001 — model summary — 2025-08-27T15:48:20.397295Z
Both provers developed a dyadic “plateau” approach to bound E_{k∼U([k0,2k0))}[ALG(k)/OPT(k)] using the Makarychev–Reddy–Shan (2020) bi-criteria oversampling guarantee. They define 2-plateaus (OPT(k) within a factor 2) and show a per-plateau averaged bound: for a plateau starting at m of length L, the k-means++ average ratio is O(1 + ln(2m/L)) (constants from MRS). A per-k formulation via s(k):=min{s:OPT(s)≤2OPT(k)} and Δ(k)=k−s(k) gives E[ALG(k)] ≤ 10·min{2+1/(2e)+ln(2s/Δ), 1 + s/(e(Δ−1))}·OPT(k) for Δ≥1 (Δ=0 falls back to O(ln k)). Consequences: long plateaus (L ≥ m / polylog m) yield O(log log k0) averages; but many short plateaus can force only the trivial O(log k0) bound, so extra structure (or a “heavy-coverage” high‑probability lemma) is needed for an unconditional improvement. Files relied on: prover-01.out.json, prover-02.out.json. Refs: Makarychev–Reddy–Shan (2020); Arthur–Vassilvitskii (2007).

## round-0002 — model summary — 2025-08-27T18:54:16.798946Z
Round recap: Developed a “hybrid” plateau-averaging lemma that combines both MRS branches to give explicit, checkable plateau averages. For a factor-2 plateau I=[m,m+L) the authors show
E_{k∈I} E_++[ALG_k/OPT_k] ≤ 2·C_bi·min{Ā,B̄} + C_fix(ln m + O(1))/L,
with Ā = 2 + 1/(2e) + ln(2m) − ln((L−1)!)/(L−1) (≤ 3 + 1/(2e) + ln(2m/(L−1))) and
B̄ = 1 + (2 + 1/(2e) + ln(2m))/(L−1) + (m/(e(L−1)))·H_{L−2}.
This yields a two‑regime consolidation (logarithmic branch vs. large‑Δ branch) giving O(1) in both natural regimes, an explicit plateau quantile/probability bound (convertible via Markov; e.g. L ≥ 101·m gives ≈98% on that plateau), and a strengthened global decomposition summing per‑plateau min terms. Also advanced a micro‑lemma toward heavy‑coverage: expected heavy collisions ≤ O(k1/R) under scale separation R ≥ k^C. Verifier: partial success. Cited: prover-02.out.json.

## round-0003 — model summary — 2025-08-27T19:44:49.694910Z
Round 0003 consolidated the plateau-averaging machinery and produced several immediately usable corollaries and a clear plan for advancing heavy-coverage and lower bounds. Key technical fixes: the monotonicity lemma is now present (closing a gap in Theorem 2) and Theorem 3's tunable τ-split correctly handles the Δ=1 endpoint and the harmonic tail. Two practical corollaries were proposed: an optimized-τ (min-of-two) plateau bound with explicit constants (log vs hybrid/m/L regimes) and a global min-of-two aggregation across plateaus. A concrete 1%–probability guarantee was derived under a single long-plateau condition with numeric instantiations (e.g., C_bi=5). Finally, a micro-lemma bounds expected heavy-cluster collisions under a β-persistence hypothesis; the next technical task is to derive β-persistence from OPT-scale separation. Verifier: partial success. Cited files: prover-01.out.json, prover-02.out.json.

## round-0004 — model summary — 2025-08-27T19:53:06.658788Z
This round produced several checkable refinements and concrete instantiations. Key technical additions: a new τ := ⌊m/ln L⌋ branch that replaces the (m ln L)/L hybrid term by (m(1+ln ln L))/L (tightening intermediate regimes); a global per-plateau min-of-three envelope (pure-log, τ=1 hybrid, τ=⌊m/ln L⌋); an optional balanced-split corollary (τ=⌊L/2⌋) for midrange L; explicit numeric constants when C_bi=C_fix=5 (e.g., E[...] ≤ 35 + 10·min{ln^+(2m/(L−1)), (m ln L)/L} + 5·(ln m)/L); a concrete ≥1% probability instantiation; and progress on heavy-coverage: a pathwise bound H_t(H_cov) ≤ 5·∑OPT1 and conversion to conditional collision probabilities, plus a special-case metric-separation lemma (λ≥10) that gives high-probability coverage-before-collisions and O(ln(1/ε)) final cost under oversampling. A lower-bound blueprint (heavy simplex + light singletons) aims to produce Ω(log k0) averaged lower bounds under random-k smoothing. Files used: prover-01.out.json, prover-02.out.json.



=== notes.md ===
Problem and notation
- X ⊂ R^d finite dataset; cost(C) = ∑_{x∈X} min_{c∈C} ||x−c||^2.
- OPT_k := min_{|C|=k} cost(C), nonincreasing in k.
- ALG_k := cost of k-means++ after k centers; E_++[·] denotes expectation over its randomness.
- We study E_{k∼U({k0,…,2k0−1})} E_++[ ALG_k / OPT_k ].

Key external input (Makarychev–Reddy–Shan 2020)
- There is an absolute constant C_bi (quoted as 5 in MRS) such that for any r≥1, Δ≥1,
  E_++[cost after r+Δ centers] ≤ C_bi · min{ 2 + 1/(2e) + ln(2r/Δ), 1 + r/(e(Δ−1)) } · OPT_r.
- For Δ=0 (fixed-k), there is an absolute constant C_fix (also ≲5 in MRS) with E_++[ALG_k] ≤ C_fix · (ln k + O(1)) · OPT_k.

Per-k oversampling bound (robust to s-misalignment)
- For any k, define s(k) := min{ s∈{1,…,k} : OPT_s ≤ 2·OPT_k }. Let Δ(k)=k−s(k).
- If Δ(k)≥1, then
  E_++[ALG_k] ≤ 2·C_bi · min{ 2 + 1/(2e) + ln(2 s(k)/Δ(k)), 1 + s(k)/(e(Δ(k)−1)) } · OPT_k.
- If Δ(k)=0, fall back to E_++[ALG_k] ≤ C_fix · (ln k + O(1)) · OPT_k.
Note: On any factor-2 plateau I=[m,m+L) (i.e., OPT_k∈(OPT_m/2,OPT_m] for k∈I), we have s(k)≤m and Δ(k)≥k−m; since the MRS bound is monotone in (s,Δ), replacing (s(k),Δ(k)) by (m,k−m) yields a valid upper bound.

Per-plateau averaging and Stirling control
- For a factor-2 plateau I=[m,m+L), L≥2, uniform k over I gives
  E_{k,++}[ALG_k/OPT_k] ≤ 2·C_bi · (2 + 1/(2e) + (1/(L−1))∑_{Δ=1}^{L−1} ln(2m/Δ)) + (C_fix(ln m + O(1)))/L.
- Using ln(n!) ≥ n ln n − n + 1 (n≥1),
  (1/(L−1))∑_{Δ=1}^{L−1} ln(2m/Δ) = ln(2m) − ln((L−1)!)/(L−1) ≤ 1 + ln(2m/(L−1)).
- Hence E_{k,++}[ALG_k/OPT_k] ≤ const + const·ln^+(2m/(L−1)) + O((ln m)/L), where ln^+(x)=max{ln x,0} and all const’s are absolute.

Decomposition over [k0,2k0)
- Partition [k0,2k0) into maximal factor-2 plateaus I_j=[m_j, m_j+L_j). For L_j≥2, the per-plateau average is bounded as above. For L_j=1, the single k in I_j contributes ≤ C_fix(ln m_j + O(1)). Averaging with weights L_j/k0 yields a global bound explicitly in terms of {(m_j,L_j)}; in the worst case many L_j=1 plateaus can force Θ(log k0).

Consequences and regimes
- Long plateaus: If some I=[m,m+L) satisfies L ≥ m/polylog(k0), then E_{k∈I,++}[ALG/OPT] = O(log log k0). If a constant fraction of [k0,2k0) is covered by such plateaus, the global average is O(log log k0).
- Worst case without structure: If many plateaus are short (e.g., L_j=1), the global average can be Θ(log k0), aligning with classic worst-case fixed-k behavior.

Open directions
- Structural conditions on OPT(·) implying many long plateaus (e.g., bounded discrete derivative decay, quasiconvexity/regularity of k·OPT_k) to get unconditional O(log log k0).
- Heavy-coverage lemma (conjectural): under strong scale separation OPT_{k1} ≫ OPT_k, show k-means++ hits all k1 heavy clusters w.h.p. before collisions, then apply bi-criteria on the remaining part.
- Lower bounds: Construct instances with Δ(k)=1 for a 1−o(1) fraction of k in [k0,2k0), giving an Ω(log k0) average and clarifying optimality of random-k smoothing.
Refinements and corrections for plateau averaging and monotonicity

- Monotonicity of the MRS bi-criteria bound. Define for integers s≥1 and Δ≥1 (interpreting the second branch only for Δ≥2):
  f(s,Δ) := min{ a + ln(2s/Δ), 1 + s/(e(Δ−1)) }, where a := 2 + 1/(2e).
  Then f is nondecreasing in s and nonincreasing in Δ. Consequently, on any factor-2 plateau I=[m,m+L) and for k=m+Δ with Δ≥1, the per-k bound from Theorem 1 satisfies
  f(s(k),Δ(k)) ≤ f(m,Δ), so we may safely replace (s(k),Δ(k)) by (m,Δ) in upper bounds.

- Correct τ-split averaging (fixing Proposition B). On a factor-2 plateau I=[m,m+L) with L≥2, for any integer τ∈{1,…,L−1}:
  (1/(L−1)) ∑_{Δ=1}^{L−1} min{ a+ln(2m/Δ), 1 + m/(e(Δ−1)) } (with the second term read only for Δ≥2)
  ≤ (τ/(L−1)) [ a + 1 + ln(2m/τ) ]
    + ((L−1−τ)/(L−1))
    + (m/(e(L−1))) · (H_{L−2} − H_{τ−1}).
  Proof sketch: Split the sum at τ and use Stirling ln(τ!) ≥ τ ln τ − τ + 1 to bound ∑_{Δ≤τ} ln(2m/Δ) ≤ τ(1+ln(2m/τ)). For the tail, sum 1/(Δ−1) as H_{L−2}−H_{τ−1}. This corrects two issues in earlier drafts: (i) the constant a must be multiplied by τ/(L−1), and (ii) ln(2m/Δ) cannot be upper bounded by ln(2m/τ) when Δ≤τ.

- Hybrid extremes recovered from the τ-split:
  • Choosing τ=L−1 gives the pure-logarithmic average a+1+ln(2m/(L−1)).
  • Choosing τ=1 gives the “hybrid extreme” 1 + (a+ln(2m))/(L−1) + (m/(e(L−1))) H_{L−2}.
  Hence we may take the minimum of these two regimes for a tighter bound, plateau by plateau.

- Long-plateau corollary. If L≥α m with α∈(0,1], then a+1+ln(2m/(L−1)) ≤ a+1+ln(2/α)+O(1/L), so the plateau-average is O(1) up to the endpoint term of order (ln m)/L.

- Quantile-in-Δ bound (careful threshold). For any η∈(0,1), let T := max{2, ⌈η(L−1)⌉+1}. Then, for all Δ≥T, by the large-Δ branch
  E_++[ALG_{m+Δ}/OPT_{m+Δ}] ≤ 2·C_bi·(1 + m/(e(T−1))).
  The fraction of k∈I with Δ≥T is (L−T+1)/L = 1 − η − O(1/L). This yields a rigorous “with probability ≥ 1−η−O(1/L) over k uniform on the plateau” expected-ratio bound; Markov then converts it into a constant-factor joint (k,++) probability if desired.

- Global decomposition improvement. Using the τ-split per plateau and optimizing τ as a function of (m,L) leads to a refined global bound that, informally, scales like
  K0 + K1 · min{ ln^+(2m/(L−1)), (m ln L)/L } + K2 · (ln m)/L,
  up to absolute constants K0,K1,K2. Formalizing this from Theorem 3 is a next step.

Open directions and next steps
- Optimize τ in the τ-split bound and write a clean, closed-form per-plateau bound. Then aggregate over plateaus to strengthen Corollary 3.
- Probability guarantees: phrase clean conditions under which at least a 1% measure of k (in [k0,2k0) or on a specific plateau) yields a constant expected ratio, and apply Markov to obtain joint (k,++) probability.
- Heavy-coverage: formalize the supermartingale for heavy uncovered mass (precise citation from MRS), bound expected heavy-collisions by O(k1/R), and lift to high probability via Freedman/Azuma with a stopping time.
- Lower bounds: construct OPT(·) with OPT_{k−1}/OPT_k ≈ 1 − Θ(1/ln k0) for most k to force Δ(k)=1 on a 1−o(1) fraction of the window, yielding an Ω(log k0) average.
Additions and clarifications

- Geometric constraint on plateau length within the window. If a factor-2 plateau I=[m,m+L) is contained in [k0,2k0), then L ≤ 2k0−m ≤ m. In particular, for L≥2 we have m/(L−1) ≥ 1. This prevents sign errors when simplifying expressions involving m/(L−1).

- Min-of-extremes and an upper envelope. From Theorem 3 and Corollary 4 we can (and will) use the minimum of the two extreme instantiations τ=L−1 (pure log) and τ=1 (hybrid extreme). Using H_q ≤ ln q + 1, this yields a clean per-plateau upper envelope of the form
  E_{k∈I} E_++[ALG_k/OPT_k] ≤ K0 + K1 · min{ ln^+(2m/(L−1)), (m ln L)/L } + K2 · (ln m)/L,
for absolute constants K0,K1,K2 depending only on C_bi,C_fix. This is the regime split we will use in global aggregation: the log term dominates when L≪m, while the (m ln L)/L term is O(1) when L is a constant fraction of m.

- Safe balanced-split (τ=⌊L/2⌋) template (optional). The following uniform bounds hold for L≥3 and τ=⌊L/2⌋:
  • τ/(L−1) ≤ 2/3 and (L−1−τ)/(L−1) ≤ 1/2.
  • ln(2m/τ) ≤ ln(4m/(L−1)).
  • H_{L−2} − H_{τ−1} ≤ ln 3 + 1 (with small-L cases checked directly).
Plugging these into Theorem 3 yields the rigorous estimate
  E_{k∈I} E_++[ALG_k/OPT_k]
  ≤ 2C_bi [ (2/3)(a + 1 + ln(4m/(L−1))) + 1/2 + (m/(e(L−1)))(ln 3 + 1) ]
    + (C_fix(\ln m + O(1)))/L.
This improves the (m ln L)/L term to a pure (m/L) term at the expense of a larger coefficient on the logarithmic piece. It may be tighter than the min-of-extremes envelope in intermediate regimes. If used, constants must be kept as above (1/2 is not a safe coefficient for τ/(L−1) when L is even).

- Probability ≥ 1% guarantees under a long-plateau condition. From Corollary 6: fix η∈(0,1) and define T := max{2, ⌈η(L−1)⌉+1}. For at least a (1−η−O(1/L)) fraction of k∈I we have
  E_++[ALG_k/OPT_k] ≤ 2C_bi (1 + m/(e(T−1))).
By Markov, for any c≥1,
  P_{++}[ ALG_k/OPT_k ≤ c · 2C_bi (1 + m/(e(T−1))) ] ≥ 1 − 1/c
for each such k. Therefore, jointly over k uniform on I and ++ randomness,
  P[ ALG_k/OPT_k ≤ c · 2C_bi (1 + m/(e(T−1))) ] ≥ (1−η−O(1/L))(1−1/c).
If L ≥ α m, then m/(T−1) ≤ 1/(α η) up to 1+O(1/L) factors, giving an explicit constant bound. If the plateau covers a γ-fraction of [k0,2k0), the global probability is ≥ γ times the RHS. Example: with C_bi=5, α=1, η=0.99, c=100, we get a ≥1% guarantee with a factor ≤ 100·2·5·(1 + 1/(0.99 e)).

- Heavy-coverage program (caveats and plan). Any per-step “collision within heavy” probability bound must be stated conditionally on the current filtration F_t: under D^2 sampling,
  P[collision in H at step t | F_t] = cost_t(covered H) / cost_t(all).
Avoid ratio-of-expectations. A plausible route is to bound cost_t(covered H) in terms of the uncovered heavy mass U_t(H) via the MRS eH_t supermartingale (in expectation and eventually with concentration), while lower bounding cost_t(all) by U_t(H) + U_t(L). To obtain a β-persistence-type condition from scale separation, quantify the expected reduction of U_t(H) when sampling outside H and show it is o(U_t(H)) under strong separation (e.g., OPT_{k1} ≥ k^C · OPT_k). With these in place, the expected number of heavy collisions can be bounded by summing the conditional probabilities until all heavy clusters are hit, and then lifted to high probability by Freedman/Azuma.

- Lower bounds. To show random-k smoothing cannot beat Θ(log k0) in general, aim for an instance where Δ(k)=1 for a 1−o(1) fraction of k in [k0,2k0). Concretely, calibrate radii so OPT_{k−1}/OPT_k ≈ 1 + Θ(1/ln k0) across most of the window, ensuring s(k)=k−1 on most k. Then adapt Arthur–Vassilvitskii’s lower-bound calculation to the averaged setting.
New per-plateau refinement: τ≈m/ln L branch and an updated envelope

- Additional τ-instantiation. On a factor-2 plateau I=[m,m+L) with L≥3, choose τ*:=min{L−1, ⌊m/ln L⌋}. Then Theorem 3 yields
  E_{k∈I} E_++[ALG/OPT]
  ≤ 2C_bi [ (τ*/(L−1))(a+1+ln(2m/τ*)) + ((L−1−τ*)/(L−1)) + (m/(e(L−1)))(H_{L−2} − H_{τ*−1}) ]
    + (C_fix(\ln m + O(1)))/L,
  where a:=2+1/(2e).
  In the subcase m ≤ (L−1) ln L (hence τ*=⌊m/ln L⌋) and assuming m≥L (which holds for plateaus contained in [k0,2k0)), we can bound
  • ln(2m/τ*) ≤ ln(4 ln L)
  • τ*/(L−1) ≤ m/((L−1) ln L)
  • H_{L−2} − H_{τ*−1} ≤ 1 + ln ln L
  to conclude
  E_{k∈I} E_++[ALG/OPT]
  ≤ 2C_bi [ 1 + (m/((L−1) ln L))(a+1+\ln(4\ln L)) + (m/(e(L−1)))(1 + \ln\ln L) ]
    + (C_fix(\ln m + O(1)))/L.
  This improves the (m\ln L)/L tail from the τ=1 branch to ≈ (m/L)(1+\ln\ln L).

- Updated min-of-three envelope (plateau-wise). Together with the τ=L−1 (pure-log) and τ=1 (hybrid) instantiations, we may use
  E_{k∈I} E_++[ALG/OPT] ≤ K0 + K1·min{ ln^+(2m/(L−1)), (m\ln L)/L, (m(1+\ln\ln L))/L } + K2·(\ln m)/L,
  for absolute constants K0,K1,K2 depending only on C_bi,C_fix. This strictly strengthens the previous min-of-two envelope in regimes where ln ln L ≪ ln L.

- Global aggregation (window-wise). For the maximal factor-2 plateau partition [k0,2k0)=\bigsqcup_j I_j with I_j=[m_j,m_j+L_j), one may take, plateau by plateau, the minimum of the three branches above (with the third used for L_j≥3), and sum with weights L_j/k0:
  E_{k\sim U([k0,2k0))} E_++[ALG/OPT]
  ≤ (1/k0) \sum_j L_j [ K0 + K1·min{ ln^+(2m_j/(L_j−1)), (m_j\ln L_j)/L_j, (m_j(1+\ln\ln L_j))/L_j } ]
    + (K2/k0) \sum_j \ln m_j.

- Balanced split reminder. The τ=⌊L/2⌋ instantiation yields
  E_{k∈I} E_++[ALG/OPT]
  ≤ 2C_bi [ (2/3)(a+1+\ln(4m/(L−1))) + 1/2 + (m/(e(L−1)))(\ln 3 + 1) ] + (C_fix(\ln m + O(1)))/L,
  valid for all L≥3. This removes the ln L factor in the tail at the expense of a larger constant on the log piece.

- Probabilistic 1% instantiation (correction). From Corollary 9, for any η∈(0,1) and c≥1,
  P_{k∈I,++}[ ALG/OPT ≤ c·B_I ] ≥ (1−η−O(1/L))(1−1/c), with B_I=2C_bi(1 + m/(e(T−1))), T=\max\{2,\lceil η(L−1)\rceil+1\}.
  To guarantee at least 1% mass uniformly (up to O(1/L)), take η=0.98 and c=2, giving (1−η)(1−1/c)=0.02·0.5=0.01. If, in addition, L≥α m, then B_I ≤ 2C_bi(1 + 1/(α e η)) up to O(1/L).

Caveat on heavy-coverage sketches. The pathwise control and metric-separation heuristics are promising but currently lack fully rigorous denominators: one must explicitly quantify cost_t(X) in terms of U_t(H) under precise conditions, and avoid uncontrolled uses of triangle inequality against existing centers outside H. These ideas remain in-development items in notes, not yet suitable for output.


=== output.md ===
Theorems curated in this file depend on the following standard notation and external input.
- For a finite dataset X⊂R^d, write OPT_k for the optimal k-means cost with k centers, and ALG_k for the cost produced by k-means++ after k centers; E_++[·] is expectation over the algorithm’s randomness.
- External input (Makarychev–Reddy–Shan, 2020): There exists an absolute constant C_bi such that for all integers r≥1 and Δ≥1,
  E_++[cost after r+Δ centers] ≤ C_bi · min{ 2 + 1/(2e) + ln(2r/Δ), 1 + r/(e(Δ−1)) } · OPT_r.
  There is also an absolute constant C_fix with E_++[ALG_k] ≤ C_fix·(ln k + O(1))·OPT_k for all k (the fixed-k bound). We treat C_bi and C_fix as absolute numerical constants.

Theorem 1 (Per-k oversampling bound via a 2-approximate anchor).
Fix k≥1 and define
  s(k) := min{ s∈{1,…,k} : OPT_s ≤ 2·OPT_k },    Δ(k) := k − s(k).
Then:
- If Δ(k) ≥ 1, we have
  E_++[ALG_k] ≤ 2·C_bi · min{ 2 + 1/(2e) + ln(2 s(k)/Δ(k)), 1 + s(k)/(e(Δ(k)−1)) } · OPT_k.
- If Δ(k) = 0, we have E_++[ALG_k] ≤ C_fix·(ln k + O(1))·OPT_k.
Proof.
For Δ(k)≥1, apply the MRS bi-criteria inequality with r = s(k) and Δ = k−s(k):
  E_++[ALG_k] = E_++[cost after s(k)+Δ(k) centers]
  ≤ C_bi · min{ 2 + 1/(2e) + ln(2 s(k)/Δ(k)), 1 + s(k)/(e(Δ(k)−1)) } · OPT_{s(k)}.
By definition of s(k), OPT_{s(k)} ≤ 2·OPT_k, giving the stated bound. For Δ(k)=0 the fixed-k bound yields the second claim. ∎

Theorem 2 (Averaging over a factor-2 plateau).
Let I = {m, m+1, …, m+L−1} with L≥2 and assume OPT_k ∈ (OPT_m/2, OPT_m] for all k∈I. Then for k uniform in I and over the k-means++ randomness,
  E_{k∈I} E_++[ ALG_k / OPT_k ] ≤ A0 + A1 · ln^+( 2m/(L−1) ) + (A2 · ln m)/L,
where ln^+(x) := max{ln x,0} and A0,A1,A2 are absolute constants depending only on C_bi,C_fix.
Proof.
Average the bound from Theorem 1 over k∈I. For Δ:=k−m≥1, we may upper bound s(k) by m and lower bound Δ(k) by Δ (since s(k)≤m and Δ(k)≥Δ on a factor-2 plateau), and use only the logarithmic branch of MRS to get
  E_++[ALG_{m+Δ}/OPT_{m+Δ}] ≤ 2·C_bi · ( 2 + 1/(2e) + ln(2m/Δ) ).
Averaging Δ from 1 to L−1 gives
  (1/(L−1))∑_{Δ=1}^{L−1} ln(2m/Δ) = ln(2m) − ln((L−1)!)/(L−1) ≤ 1 + ln(2m/(L−1)),
where we used ln(n!) ≥ n ln n − n + 1 for n=L−1≥1. Multiplying by (L−1)/L and adding the Δ=0 endpoint contribution C_fix(ln m + O(1))/L yields the claim with suitable absolute constants A0,A1,A2. Replacing ln by ln^+ only weakens the bound. ∎

Corollary 3 (Decomposition across [k0,2k0)).
Partition the integer interval [k0,2k0) into maximal factor-2 plateaus I_j = [m_j, m_j+L_j), j=1,…,J, so OPT_k ∈ (OPT_{m_j}/2, OPT_{m_j}] for k∈I_j, and ∑_j L_j = k0. Then, for k uniform on [k0,2k0),
  E_k E_++[ ALG_k / OPT_k ]
  ≤ (1/k0) ∑_{j : L_j≥2} L_j [ A0 + A1 · ln^+( 2 m_j/(L_j−1) ) ]
    + (1/k0) ∑_{j : L_j≥2} A2 · ln m_j
    + (1/k0) ∑_{j : L_j=1} C_fix · (ln m_j + O(1)).
In particular, if a constant fraction of [k0,2k0) is covered by plateaus satisfying L_j ≥ m_j / polylog(k0), then the contribution of those plateaus is O(log log k0), and the remaining fraction must be controlled separately; in the worst case with many L_j=1 plateaus the global average can be Θ(log k0).
Proof.
Average the bound of Theorem 2 over plateaus with weights L_j/k0, and handle L_j=1 plateaus by the fixed-k bound. ∎
Lemma (Monotonicity of the MRS bi-criteria bound).
Let a := 2 + 1/(2e), and for integers s≥1, Δ≥1 define
  F(s,Δ) := min{ a + ln(2s/Δ), 1 + s/(e(Δ−1)) },
where the second term is interpreted only for Δ≥2 (or taken as +∞ for Δ=1). Then:
- For fixed Δ, F is nondecreasing in s.
- For fixed s, F is nonincreasing in Δ (for all Δ≥1 in the above interpretation).
Proof.
For Δ≥2, both functions s ↦ a + ln(2s/Δ) and s ↦ 1 + s/(e(Δ−1)) are nondecreasing in s, hence their pointwise minimum is nondecreasing. For Δ≥2, both functions Δ ↦ a + ln(2s/Δ) and Δ ↦ 1 + s/(e(Δ−1)) are nonincreasing in Δ, hence the minimum is nonincreasing. For the transition from Δ=1 to Δ=2, note that F(s,1) = a + ln(2s) ≥ a + ln(s) ≥ a + ln(2s/2) ≥ F(s,2), since the logarithmic branch decreases with Δ and adding the second branch at Δ=2 can only decrease the minimum. Thus F is nonincreasing in Δ for all Δ≥1. ∎

Theorem 3 (Plateau averaging with a tunable split of the two MRS branches).
Let I = {m, m+1, …, m+L−1} be a factor-2 plateau (i.e., OPT_k ∈ (OPT_m/2, OPT_m] for k∈I) with L≥2. For any integer τ with 1≤τ≤L−1, for k uniform on I and over k-means++ randomness we have
  E_{k∈I} E_++[ ALG_k / OPT_k ]
  ≤ 2·C_bi · [ (τ/(L−1)) · ( a + 1 + ln(2m/τ) )
                + ((L−1−τ)/(L−1))
                + (m/(e(L−1))) · ( H_{L−2} − H_{τ−1} ) ]
    + (C_fix · (ln m + O(1)))/L,
where H_r denotes the r-th harmonic number (H_0:=0).
Proof.
Fix k=m+Δ with Δ∈{1,…,L−1}. By Theorem 1, and Lemma (monotonicity),
  E_++[ALG_{m+Δ}/OPT_{m+Δ}] ≤ 2·C_bi · min{ a + ln(2m/Δ), 1 + m/(e(Δ−1)) },
with the second branch applicable for Δ≥2. Averaging Δ uniformly over {1,…,L−1}, split the sum at τ:
  (1/(L−1)) ∑_{Δ=1}^{L−1} min{…}
  ≤ (1/(L−1)) ∑_{Δ=1}^{τ} [ a + ln(2m/Δ) ]
    + (1/(L−1)) ∑_{Δ=τ+1}^{L−1} [ 1 + m/(e(Δ−1)) ].
For the first sum, use ∑_{Δ=1}^{τ} ln(2m/Δ) = τ ln(2m) − ln(τ!) ≤ τ[1 + ln(2m/τ)] by ln(τ!) ≥ τ ln τ − τ. For the second, ∑_{Δ=τ+1}^{L−1} 1 = L−1−τ and ∑_{Δ=τ+1}^{L−1} 1/(Δ−1) = H_{L−2} − H_{τ−1}. This yields the displayed bound. Finally, incorporate the Δ=0 endpoint k=m which has probability 1/L and contributes at most C_fix·(ln m + O(1))/L by the fixed-k bound. ∎

Corollary 4 (Hybrid extremes recovered; strengthening Theorem 2).
In the setting of Theorem 3:
- Choosing τ = L−1 gives
  E_{k∈I} E_++[ ALG_k / OPT_k ] ≤ 2·C_bi · ( a + 1 + ln(2m/(L−1)) ) + (C_fix · (ln m + O(1)))/L.
  This recovers Theorem 2 (up to constant reparameterization by a and the explicit +1 from Stirling).
- Choosing τ = 1 gives
  E_{k∈I} E_++[ ALG_k / OPT_k ] ≤ 2·C_bi · ( 1 + (a+ln(2m))/(L−1) + (m/(e(L−1))) · H_{L−2} )
    + (C_fix · (ln m + O(1)))/L.
Using H_q ≤ ln q + 1, this yields a complementary regime where the bound scales like 1 + O((ln m)/L) + O((m ln L)/L).
Proof.
Both bounds are immediate from Theorem 3 with the specified τ and the inequality H_q ≤ ln q + 1 for q≥1. ∎

Corollary 5 (Explicit constants on long plateaus).
If L ≥ α m for some fixed α∈(0,1], then for k uniform on I,
  E_{k∈I} E_++[ ALG_k / OPT_k ]
  ≤ 2·C_bi · ( a + 1 + ln(2/α) ) + (C_fix · (ln m + O(1)))/L.
Proof.
Apply Corollary 4 with τ=L−1 and use ln(2m/(L−1)) ≤ ln(2/α) + O(1/L). The endpoint term is unchanged. ∎

Corollary 6 (Quantile-in-Δ guarantee on a plateau).
Fix η∈(0,1) and let T := max{2, ⌈η(L−1)⌉+1}. Then for all Δ∈{T,…,L−1},
  E_++[ ALG_{m+Δ} / OPT_{m+Δ} ] ≤ 2·C_bi · ( 1 + m/( e (T−1) ) ).
Consequently, for k uniform on I, the fraction of k for which the above inequality holds is at least (L−T+1)/L = 1 − η − O(1/L).
Proof.
For Δ≥T, the large-Δ branch gives 1 + m/(e(Δ−1)) ≤ 1 + m/(e(T−1)). Combine with Theorem 1 and monotonicity, and note that the set {k: Δ(k)≥T} has size L−T+1 among the L indices in I. ∎
Remark (Plateau length within [k0,2k0)).
Let I = [m,m+L) be a factor-2 plateau contained in [k0,2k0). Then L ≤ 2k0 − m ≤ m. In particular, for L≥2 we have m/(L−1) ≥ 1.
Proof.
Since m+L−1 ≤ 2k0−1, we have L ≤ 2k0−m. As m ≥ k0, it follows that 2k0−m ≤ m. The final claim is immediate. ∎

Corollary 7 (Min-of-extremes plateau bound).
In the setting of Theorem 3 with L≥2, for k uniform on I and the k-means++ randomness,
  E_{k∈I} E_++[ ALG_k / OPT_k ]
  ≤ min{ B_log, B_hyb } + (C_fix · (ln m + O(1)))/L,
where
  B_log := 2·C_bi · ( a + 1 + ln^+( 2m/(L−1) ) ),
  B_hyb := 2·C_bi · ( 1 + (a+\ln(2m))/(L−1) + (m/(e(L−1))) · H_{L−2} ),
and a := 2 + 1/(2e), H_q denotes the q-th harmonic number with H_0:=0, and ln^+(x):=max{ln x,0}.
Proof.
Take τ=L−1 and τ=1 in Theorem 3 to obtain the two bounds (cf. Corollary 4). Replacing ln by ln^+ only increases the RHS. Taking the minimum preserves validity. ∎

Corollary 8 (Upper envelope from the min-of-extremes).
In the setting of Corollary 7, there exist absolute constants K0,K1,K2 (depending only on C_bi and C_fix) such that
  E_{k∈I} E_++[ ALG_k / OPT_k ]
  ≤ K0 + K1 · min{ ln^+( 2m/(L−1) ), (m ln L)/L } + K2 · (\ln m)/L.
Proof.
From Corollary 7 with the logarithmic extreme,
  E ≤ 2·C_bi·(a+1) + 2·C_bi·ln^+(2m/(L−1)) + (C_fix·(\ln m + O(1)))/L.
From the hybrid extreme and H_{L−2} ≤ ln(L−2)+1 ≤ ln L + 1,
  E ≤ 2·C_bi·\Big( 1 + (a+\ln(2m))/(L−1) + (m/(e(L−1))) (\ln L + 1) \Big)
    + (C_fix·(\ln m + O(1)))/L.
Absorb constants into K0 and the terms (ln m)/(L−1), (m/(L−1)) into K2·(\ln m)/L and K1·(m ln L)/L, respectively, using L−1≈L up to absolute factors for L≥2. Taking the minimum of the two displays yields the claim. ∎

Corollary 9 (Plateau-level probability guarantee via quantiles and Markov; global mixing).
Let I=[m,m+L) be a factor-2 plateau with L≥2 and fix η∈(0,1). Define T := max{2, ⌈η(L−1)⌉+1}. Then for at least a (1−η−O(1/L)) fraction of k∈I we have
  E_++[ ALG_k / OPT_k ] ≤ B_I := 2·C_bi · ( 1 + m/( e (T−1) ) ).
Consequently, for any c≥1,
  P_{k\sim U(I),\,++}\big[ ALG_k/OPT_k ≤ c · B_I \big] ≥ (1−η−O(1/L)) · (1 − 1/c).
If, moreover, L ≥ α m for some α∈(0,1], then B_I ≤ 2·C_bi · (1 + 1/(α e η)) + O(1/L). If a plateau I covers a γ-fraction of [k0,2k0) (i.e., L/k0 ≥ γ), then for k uniform on [k0,2k0) and the ++ randomness,
  P\big[ ALG_k/OPT_k ≤ c · B_I \big] ≥ γ · (1−η−O(1/L)) · (1 − 1/c).
Proof.
The first assertion is Corollary 6: a (1−η−O(1/L)) fraction of k satisfy the displayed per-k bound with Δ≥T, hence with E_++ bounded by B_I. For any such k, Markov’s inequality gives P_{++}[ALG_k/OPT_k ≤ c·B_I] ≥ 1−1/c. Averaging uniformly over k on I yields the claim. If L ≥ α m, then T−1 ≥ η(L−1) ≥ η(α m−1), which implies m/(T−1) ≤ 1/(α η) + O(1/L). The global bound follows by restricting to k in I, which occurs with probability ≥ γ. ∎
Corollary 10 (A τ≈m/ln L branch on a plateau).
Let I=[m,m+L) be a factor-2 plateau with L≥3. Define τ*:=min{L−1, ⌊m/ln L⌋}. Then for k uniform on I and over the k-means++ randomness,
  E_{k∈I} E_++[ ALG_k / OPT_k ]
  ≤ 2·C_bi · [ (τ*/(L−1)) · ( a + 1 + ln(2m/τ*) )
                + ((L−1−τ*)/(L−1))
                + (m/(e(L−1))) · ( H_{L−2} − H_{τ*−1} ) ]
    + (C_fix · (ln m + O(1)))/L,
where a:=2 + 1/(2e), and H_r is the r-th harmonic number (H_0:=0). In particular, if m ≤ (L−1) ln L and (as is the case for plateaus inside [k0,2k0)) m≥L, then
  E_{k∈I} E_++[ ALG_k / OPT_k ]
  ≤ 2·C_bi · [ 1 + (m/((L−1) ln L)) · ( a + 1 + ln(4 ln L) )
                + (m/(e(L−1))) · ( 1 + ln ln L ) ]
    + (C_fix · (ln m + O(1)))/L.
Proof.
Apply Theorem 3 with τ=τ*. The displayed inequality is just Theorem 3. In the subcase m ≤ (L−1) ln L, τ*=⌊m/ln L⌋ satisfies τ* ≥ m/ln L − 1, hence 2m/τ* ≤ 4 ln L and ln(2m/τ*) ≤ ln(4 ln L). Also τ*/(L−1) ≤ m/((L−1) ln L). Finally, H_{L−2} − H_{τ*−1} ≤ ln(L/τ*) + 1 ≤ ln((L ln L)/m) + 1 ≤ 1 + ln ln L, using m≥L. Substituting these bounds into Theorem 3 yields the “in particular” display. ∎

Corollary 11 (Updated per-plateau upper envelope: min-of-three).
In the setting of Corollary 7, there exist absolute constants K0,K1,K2 (depending only on C_bi and C_fix) such that for k uniform on I=[m,m+L) with L≥2,
  E_{k∈I} E_++[ ALG_k / OPT_k ]
  ≤ K0 + K1 · min{ ln^+( 2m/(L−1) ), (m ln L)/L, (m(1 + ln ln L))/L } + K2 · (\ln m)/L.
Here ln^+(x):=max{ln x,0}. The third term in the minimum is intended for L≥3; when L=2 the minimum of the first two terms suffices. 
Proof.
Combine: (i) the logarithmic extreme τ=L−1 (Corollary 4), (ii) the τ=1 hybrid extreme (Corollary 4 with H_{L−2}≤ln L+1), and (iii) Corollary 10 in the subcase τ*=⌊m/ln L⌋ (which holds exactly when m ≤ (L−1) ln L). In case (iii), the two m-dependent contributions are bounded by a constant multiple of (m/L)(1+ln ln L), since (m/((L−1) ln L))(a+1+ln(4 ln L)) ≤ (const)·(m/L) and (m/(e(L−1)))(1+ln ln L) ≤ (const)·(m/L)(1+ln ln L), with all constants depending only on C_bi. Absorb all additive O(1) and O((\ln m)/L) terms into K0 and K2. Taking the minimum of the three displays yields the claim. ∎

Corollary 12 (Explicit constants for the envelope when C_bi=C_fix=5).
Assume C_bi=C_fix=5 and L≥2. Then for k uniform on I=[m,m+L),
  E_{k∈I} E_++[ ALG_k / OPT_k ]
  ≤ 35 + 10 · min{ ln^+(2m/(L−1)), (m ln L)/L, (m(1+\ln\ln L))/L } + 5 · (\ln m)/L.
Proof.
From Corollary 7 (logarithmic extreme), E ≤ 2·C_bi·(a+1) + 2·C_bi·ln^+(2m/(L−1)) + (C_fix·(\ln m + O(1)))/L. With a=2+1/(2e) and C_bi=C_fix=5, 2·C_bi·(a+1) ≈ 31.84; round up to 35 to absorb the endpoint O(1/L) terms. The coefficient in front of ln^+(·) is 2·C_bi=10. For the τ=1 branch, the (m ln L)/L contribution carries a factor 2·C_bi/e ≤ 10/e < 10; rounding up to 10 is safe after replacing L−1 by L. For the τ≈m/ln L branch (Corollary 10), the m-dependent part is ≤ const·(m/L)(1+\ln\ln L) with const depending only on C_bi; choosing 10 dominates those constants as well. The endpoint term is ≤ 5·(\ln m + 2)/L and we keep 5·(\ln m)/L after absorbing the additive 10/L into 35. Taking the minimum of the three candidate bounds proves the display. ∎

Remark (Numerical 1% probability instantiation on a single plateau).
In Corollary 9, choose η=0.98 and c=2. Then for k uniform on I and ++ randomness,
  P[ ALG_k/OPT_k ≤ 2 · B_I ] ≥ (1−η−O(1/L)) · (1 − 1/2) = (0.02−O(1/L)) · 0.5 ≥ 0.01 − O(1/L),
with B_I = 2·C_bi·(1 + m/(e(T−1))) and T = max{2, ⌈0.98(L−1)⌉+1}. If additionally L ≥ α m, then B_I ≤ 2·C_bi·(1 + 1/(α e · 0.98)) + O(1/L), giving a fully explicit constant-factor guarantee at ≥1% mass (up to O(1/L)).


=== 1.pdf ===

--- Page 1 ---
k-means++: The Advantages of Careful Seeding
David Arthur ∗
Sergei Vassilvitskii†
Abstract
The k-means method is a widely used clustering technique
that seeks to minimize the average squared distance between
points in the same cluster. Although it oﬀers no accuracy
guarantees, its simplicity and speed are very appealing in
practice. By augmenting k-means with a very simple, ran-
domized seeding technique, we obtain an algorithm that is
Θ(log k)-competitive with the optimal clustering.
Prelim-
inary experiments show that our augmentation improves
both the speed and the accuracy of k-means, often quite
dramatically.
1
Introduction
Clustering is one of the classic problems in machine
learning and computational geometry. In the popular
k-means formulation, one is given an integer k and a set
of n data points in Rd. The goal is to choose k centers
so as to minimize φ, the sum of the squared distances
between each point and its closest center.
Solving this problem exactly is NP-hard, even with
just two clusters [10], but twenty-ﬁve years ago, Lloyd
[20] proposed a local search solution that is still very
widely used today (see for example [1, 11, 15]). Indeed,
a recent survey of data mining techniques states that it
“is by far the most popular clustering algorithm used in
scientiﬁc and industrial applications” [5].
Usually referred to simply as k-means, Lloyd’s
algorithm begins with k arbitrary centers, typically
chosen uniformly at random from the data points. Each
point is then assigned to the nearest center, and each
center is recomputed as the center of mass of all points
assigned to it. These two steps (assignment and center
calculation) are repeated until the process stabilizes.
One can check that the total error φ is monotoni-
cally decreasing, which ensures that no clustering is re-
peated during the course of the algorithm. Since there
are at most kn possible clusterings, the process will al-
ways terminate. In practice, very few iterations are usu-
ally required, which makes the algorithm much faster
∗Stanford University, Supported in part by NDSEG Fellow-
ship, NSF Grant ITR-0331640, and grants from Media-X and
SNRC.
†Stanford University, Supported in part by NSF Grant ITR-
0331640, and grants from Media-X and SNRC.
than most of its competitors.
Unfortunately, the empirical speed and simplicity
of the k-means algorithm come at the price of accuracy.
There are many natural examples for which the algo-
rithm generates arbitrarily bad clusterings (i.e.,
φ
φOPT is
unbounded even when n and k are ﬁxed). Furthermore,
these examples do not rely on an adversarial placement
of the starting centers, and the ratio can be unbounded
with high probability even with the standard random-
ized seeding technique.
In this paper, we propose a way of initializing
k-means by choosing random starting centers with
very speciﬁc probabilities.
Speciﬁcally, we choose a
point p as a center with probability proportional to p’s
contribution to the overall potential. Letting φ denote
the potential after choosing centers in this way, we show
the following.
Theorem 1.1. For any set of data points, E[φ] ≤
8(ln k + 2)φOP T .
This sampling is both fast and simple, and it already
achieves approximation guarantees that k-means can-
not.
We propose using it to seed the initial centers
for k-means, leading to a combined algorithm we call
k-means++.
This complements a very recent result of Ostrovsky
et al. [24], who independently proposed much the same
algorithm. Whereas they showed this randomized seed-
ing is O(1)-competitive on data sets following a certain
separation condition, we show it is O(log k)-competitive
on all data sets.
We also show that the analysis for Theorem 1.1 is
tight up to a constant factor, and that it can be eas-
ily extended to various potential functions in arbitrary
metric spaces.
In particular, we can also get a sim-
ple O(log k) approximation algorithm for the k-median
objective. Furthermore, we provide preliminary experi-
mental data showing that in practice, k-means++ really
does outperform k-means in terms of both accuracy and
speed, often by a substantial margin.
1.1
Related work As a fundamental problem in
machine learning, k-means has a rich history. Because
of its simplicity and its observed speed, Lloyd’s method
[20] remains the most popular approach in practice,

--- Page 2 ---
despite its limited accuracy. The convergence time of
Lloyd’s method has been the subject of a recent series
of papers [2, 4, 8, 14]; in this work we focus on improving
its accuracy.
In the theory community, Inaba et al. [16] were
the ﬁrst to give an exact algorithm for the k-means
problem, with the running time of O(nkd). Since then, a
number of polynomial time approximation schemes have
been developed (see [9, 13, 19, 21] and the references
therein). While the authors develop interesting insights
into the structure of the clustering problem, their
algorithms are highly exponential (or worse) in k, and
are unfortunately impractical even for relatively small
n, k and d.
Kanungo et al. [17] proposed an O(n3ϵ−d) algorithm
that is (9 + ϵ)-competitive.
However, n3 compares
unfavorably with the almost linear running time of
Lloyd’s method, and the exponential dependence on d
can also be problematic. For these reasons, Kanungo et
al. also suggested a way of combining their techniques
with Lloyd’s algorithm, but in order to avoid the
exponential dependence on d, their approach sacriﬁces
all approximation guarantees.
Mettu and Plaxton [22] also achieved a constant-
probability O(1) approximation using a technique called
successive sampling. They match our running time of
O(nkd), but only if k is suﬃciently large and the spread
is suﬃciently small. In practice, our approach is simpler,
and our experimental results seem to be better in terms
of both speed and accuracy.
Very recently, Ostrovsky et al. [24] independently
proposed an algorithm that is essentially identical to
ours, although their analysis is quite diﬀerent. Letting
φOPT,k denote the optimal potential for a k-clustering
on a given data set, they prove k-means++ is O(1)-
competitive in the case where
φOPT,k
φOPT,k−1
≤ϵ2.
The
intuition here is that if this condition does not hold,
then the data is not well suited for clustering with the
given value for k.
Combining this result with ours gives a strong
characterization of the algorithm’s performance.
In
particular, k-means++ is never worse than O(log k)-
competitive, and on very well formed data sets, it
improves to being O(1)-competitive.
Overall, the seeding technique we propose is similar
in spirit to that used by Meyerson [23] for online facility
location, and Mishra et al. [12] and Charikar et al. [6]
in the context of k-median clustering.
However, our
analysis is quite diﬀerent from those works.
2
Preliminaries
In this section, we formally deﬁne the k-means problem,
as well as the k-means and k-means++ algorithms.
For the k-means problem, we are given an integer k
and a set of n data points X ⊂Rd. We wish to choose
k centers C so as to minimize the potential function,
φ =
X
x∈X
min
c∈C ∥x −c∥2.
Choosing these centers implicitly deﬁnes a clustering
– for each center, we set one cluster to be the set of
data points that are closer to that center than to any
other. As noted above, ﬁnding an exact solution to the
k-means problem is NP-hard.
Throughout the paper, we will let COPT denote the
optimal clustering for a given instance of the k-means
problem, and we will let φOPT denote the corresponding
potential.
Given a clustering C with potential φ, we
also let φ(A) denote the contribution of A ⊂X to the
potential (i.e., φ(A) = P
x∈A minc∈C∥x −c∥2).
2.1
The k-means algorithm The k-means method
is a simple and fast algorithm that attempts to locally
improve an arbitrary k-means clustering. It works as
follows.
1. Arbitrarily choose k initial centers C = {c1, . . . , ck}.
2. For each i ∈{1, . . . , k}, set the cluster Ci to be the
set of points in X that are closer to ci than they
are to cj for all j ̸= i.
3. For each i ∈{1, . . . , k}, set ci to be the center of
mass of all points in Ci: ci =
1
|Ci|
P
x∈Ci x.
4. Repeat Steps 2 and 3 until C no longer changes.
It is standard practice to choose the initial centers
uniformly at random from X. For Step 2, ties may be
broken arbitrarily, as long as the method is consistent.
Steps 2 and 3 are both guaranteed to decrease φ, so
the algorithm makes local improvements to an arbitrary
clustering until it is no longer possible to do so. To see
that Step 3 does in fact decreases φ, it is helpful to recall
a standard result from linear algebra (see [14]).
Lemma 2.1. Let S be a set of points with center of
mass c(S), and let z be an arbitrary point.
Then,
P
x∈S∥x −z∥2 −P
x∈S∥x −c(S)∥2 = |S| · ∥c(S) −z∥2.
Monotonicity for Step 3 follows from taking S to be a
single cluster and z to be its initial center.
As discussed above, the k-means algorithm is at-
tractive in practice because it is simple and it is gener-
ally fast. Unfortunately, it is guaranteed only to ﬁnd a
local optimum, which can often be quite poor.
2.2
The k-means++ algorithm The k-means algo-
rithm begins with an arbitrary set of cluster centers.
We propose a speciﬁc way of choosing these centers. At

--- Page 3 ---
any given time, let D(x) denote the shortest distance
from a data point x to the closest center we have al-
ready chosen. Then, we deﬁne the following algorithm,
which we call k-means++.
1a. Choose an initial center c1 uniformly at random
from X.
1b. Choose the next center ci, selecting ci = x′ ∈X
with probability
D(x′)2
P
x∈X D(x)2 .
1c. Repeat Step 1b until we have chosen a total of k
centers.
2-4. Proceed as with the standard k-means algorithm.
We call the weighting used in Step 1b simply “D2
weighting”.
3
k-means++ is O(log k)-competitive
In this section, we prove our main result.
Theorem 3.1. If C is constructed with k-means++,
then the corresponding potential function φ satisﬁes
E[φ] ≤8(ln k + 2)φOPT.
In fact, we prove this holds after only Step 1 of the
algorithm above.
Steps 2 through 4 can then only
decrease φ. Not surprisingly, our experiments show this
local optimization is important in practice, although it
is diﬃcult to quantify this theoretically.
Our analysis consists of two parts. First, we show
that k-means++ is competitive in those clusters of COPT
from which it chooses a center. This is easiest in the
case of our ﬁrst center, which is chosen uniformly at
random.
Lemma 3.1. Let A be an arbitrary cluster in COPT, and
let C be the clustering with just one center, which is
chosen uniformly at random from A. Then, E[φ(A)] =
2φOPT(A).
Proof. Let c(A) denote the center of mass of the data
points in A.
By Lemma 2.1, we know that since
COPT is optimal, it must be using c(A) as the center
corresponding to the cluster A. Using the same lemma
again, we see E[φ(A)] is given by,
X
a0∈A
1
|A| ·
 X
a∈A
∥a −a0∥2
!
=
1
|A|
X
a0∈A
 X
a∈A
∥a −c(A)∥2 + |A| · ∥a0 −c(A)∥2
!
=
2
X
a∈A
∥a −c(A)∥2,
and the result follows.
Our next step is to prove an analog of Lemma 3.1
for the remaining centers, which are chosen with D2
weighting.
Lemma 3.2. Let A be an arbitrary cluster in COPT, and
let C be an arbitrary clustering.
If we add a random
center to C from A, chosen with D2 weighting, then
E[φ(A)] ≤8φOPT(A).
Proof. The probability that we choose some ﬁxed a0 as
our center, given that we are choosing our center from
A, is precisely
D(a0)2
P
a∈A D(a)2 . Furthermore, after choos-
ing the center a0, a point a will contribute precisely
min(D(a), ∥a −a0∥)2 to the potential. Therefore,
E[φ(A)] =
X
a0∈A
D(a0)2
P
a∈A D(a)2
X
a∈A
min(D(a), ∥a −a0∥)2.
Note by the triangle inequality that D(a0)
≤
D(a) + ∥a −a0∥for all a, a0.
From this, the power-
mean inequality1 implies that D(a0)2
≤2D(a)2 +
2∥a −a0∥2.
Summing over all a, we then have that
D(a0)2 ≤
2
|A|
P
a∈A D(a)2 +
2
|A|
P
a∈A∥a −a0∥2, and
hence, E[φ(A)] is at most,
2
|A| ·
X
a0∈A
P
a∈A D(a)2
P
a∈A D(a)2 ·
X
a∈A
min(D(a), ∥a −a0∥)2
+ 2
|A| ·
X
a0∈A
P
a∈A∥a −a0∥2
P
a∈A D(a)2
·
X
a∈A
min(D(a), ∥a −a0∥)2.
In the ﬁrst expression, we substitute min(D(a), ∥a −
a0∥)2 ≤∥a −a0∥2, and in the second expression, we
substitute min(D(a), ∥a −a0∥)2 ≤D(a)2. Simplifying,
we then have,
E[φ(A)]
≤
4
|A| ·
X
a0∈A
X
a∈A
∥a −a0∥2
=
8φOPT(A).
The last step here follows from Lemma 3.1.
We have now shown that seeding by D2 weighting
is competitive as long as it chooses centers from each
cluster of COPT, which completes the ﬁrst half of our
argument. We now use induction to show the total error
in general is at most O(log k).
1The power-mean inequality states for any real numbers
a1, · · · , am that Σa2
i ≥
1
m(Σai)2.
It follows from the Cauchy-
Schwarz inequality. We are only using the case m = 2 here, but
we will need the general case for Lemma 3.3.

--- Page 4 ---
Lemma 3.3. Let C be an arbitrary clustering. Choose
u > 0 “uncovered” clusters from COPT, and let Xu
denote the set of points in these clusters.
Also let
Xc = X −Xu. Now suppose we add t ≤u random centers
to C, chosen with D2 weighting. Let C′ denote the the
resulting clustering, and let φ′ denote the corresponding
potential. Then, E[φ′] is at most,

φ(Xc) + 8φOPT(Xu)

· (1 + Ht) + u −t
u
· φ(Xu).
Here, Ht denotes the harmonic sum, 1 + 1
2 + · · · + 1
t .
Proof. We prove this by induction, showing that if the
result holds for (t −1, u) and (t −1, u −1), then it
also holds for (t, u).
Therefore, it suﬃces to check
t = 0, u > 0 and t = u = 1 as our base cases.
If t = 0 and u > 0, the result follows from the fact
that 1 + Ht =
u−t
u
= 1.
Next, suppose t = u = 1.
We choose our one new center from the one uncovered
cluster with probability exactly
φ(Xu)
φ
.
In this case,
Lemma 3.2 guarantees that E[φ′] ≤φ(Xc)+8φOPT(Xu).
Since φ′ ≤φ even if we choose a center from a covered
cluster, we have,
E[φ′]
≤
φ(Xu)
φ
·

φ(Xc) + 8φOPT(Xu)

+ φ(Xc)
φ
· φ
≤
2φ(Xc) + 8φOPT(Xu).
Since 1 + Ht = 2 here, we have shown the result holds
for both base cases.
We now proceed to prove the inductive step. It is
convenient here to consider two cases. First suppose we
choose our ﬁrst center from a covered cluster. As above,
this happens with probability exactly φ(Xc)
φ
. Note that
this new center can only decrease φ.
Bearing this in
mind, apply the inductive hypothesis with the same
choice of covered clusters, but with t decreased by one.
It follows that our contribution to E[φ′] in this case is
at most,
φ(Xc)
φ
·

φ(Xc) + 8φOPT(Xu)

· (1 + Ht−1)
+ u −t + 1
u
· φ(Xu)

.
On the other hand, suppose we choose our ﬁrst
center from some uncovered cluster A. This happens
with probability
φ(A)
φ .
Let pa denote the probability
that we choose a ∈A as our center, given the center is
somewhere in A, and let φa denote φ(A) after we choose
a as our center.
Once again, we apply our inductive
hypothesis, this time adding A to the set of covered
clusters, as well as decreasing both t and u by 1.
It
follows that our contribution to E[φOPT] in this case is
at most,
φ(A)
φ
·
X
a∈A
pa

φ(Xc) + φa + 8φOPT(Xu) −8φOPT(A)

· (1 + Ht−1) + u −t
u −1 ·

φ(Xu) −φ(A)

≤φ(A)
φ
·

φ(Xc) + 8φOPT(Xu)

· (1 + Ht−1)
+ u −t
u −1 ·

φ(Xu) −φ(A)

.
The
last
step
here
follows
from
the
fact
that
P
a∈A paφa ≤8φOPT(A), which is implied by Lemma
3.2.
Now,
the
power-mean
inequality
implies
that
P
A⊂Xu φ(A)2 ≥1
u · φ(Xu)2. Therefore, if we sum over
all uncovered clusters A, we obtain a potential contri-
bution of at most,
φ(Xu)
φ
·

φ(Xc) + 8φOPT(Xu)

· (1 + Ht−1)
+ 1
φ · u −t
u −1 ·

φ(Xu)2 −1
u · φ(Xu)2

=
φ(Xu)
φ
·

φ(Xc) + 8φOPT(Xu)

· (1 + Ht−1)
+ u −t
u
· φ(Xu)

.
Combining the potential contribution to E[φ′] from
both cases, we now obtain the desired bound:
E[φ′] ≤

φ(Xc) + 8φOPT(Xu)

· (1 + Ht−1)
+ u −t
u
· φ(Xu) + φ(Xc)
φ
· φ(Xu)
u
≤

φ(Xc) + 8φOPT(Xu)

·

1 + Ht−1 + 1
u

+ u −t
u
· φ(Xu).
The inductive step now follows from the fact that 1
u ≤1
t .
We specialize Lemma 3.3 to obtain our main result.
Theorem 3.1 If C is constructed with k-means++,
then the corresponding potential function φ satisﬁes
E[φ] ≤8(ln k + 2)φOPT.
Proof. Consider the clustering C after we have com-
pleted Step 1. Let A denote the COPT cluster in which
we chose the ﬁrst center.
Applying Lemma 3.3 with

--- Page 5 ---
t = u = k −1 and with A being the only covered clus-
ter, we have,
E[φOPT] ≤

φ(A) + 8φOPT −8φOPT(A)

· (1 + Hk−1).
The result now follows from Lemma 3.1, and from the
fact that Hk−1 ≤1 + ln k.
4
A matching lower bound
In this section, we show that the D2 seeding used
by k-means++ is no better than Ω(log k)-competitive
in expectation, thereby proving Theorem 3.1 is tight
within a constant factor.
Fix k, and then choose n, ∆, δ with n ≫k and ∆≫
δ. We construct X with n points. First choose k centers
c1, c2, . . . , ck such that ∥ci −cj∥2 = ∆2 −
  n−k
n

· δ2
for all i ̸= j.
Now, for each ci, add data points
xi,1, xi,2, · · · , xi, n
k arranged in a regular simplex with
center ci, side length δ, and radius
q
n−k
2n · δ. If we do
this in orthogonal dimensions for each i, we then have,
∥xi,i′ −xj,j′∥=
 δ
if i=j, or
∆
otherwise.
We prove our seeding technique is in expectation
Ω(log k) worse than the optimal clustering in this case.
Clearly, the optimal clustering has centers {ci},
which leads to an optimal potential of φOPT = n−k
2
· δ2.
Conversely, using an induction similar to that of Lemma
3.3, we show D2 seeding cannot match this bound. As
before, we bound the expected potential in terms of
the number of centers left to choose and the number
of uncovered clusters (those clusters of C0 from which
we have not chosen a center).
Lemma 4.1. Let C be an arbitrary clustering on X with
k −t ≥1 centers, but with u clusters from COPT
uncovered.
Now suppose we add t random centers to
C, chosen with D2 weighting.
Let C′ denote the the
resulting clustering, and let φ′ denote the corresponding
potential.
Furthermore, let α =
n−k2
n
, β =
∆2−2kδ2
∆2
and
H′
u = Pu
i=1
k−i
ki . Then, E[φ′] is at least,
αt+1 ·

nδ2 · (1 + H′
u) · β +
n
k ∆2 −2nδ2
· (u −t)

.
Proof. We prove this by induction on t. If t = 0, note
that,
φ′ = φ =

n −u · n
k −k

· δ2 + u · n
k · ∆2.
Since n−u· n
k ≥n
k , we have n−u· n
k −k
n−u· n
k
≥
n
k −k
n
k
= α. Also,
α, β ≤1. Therefore,
φ′ ≥α ·

n −u · n
k

· δ2 · β + u · n
k · ∆2
.
Finally, since nδ2u ≥u · n
k · δ2 · β and nδ2u ≥nδ2H′
uβ,
we have,
φ′ ≥α ·

nδ2 · (1 + H′
u) · β +
n
k ∆2 −2nδ2
· u

.
This completes the base case.
We now proceed to prove the inductive step. As
with Lemma 3.3, we consider two cases. The probability
that our ﬁrst center is chosen from an uncovered cluster
is,
u · n
k · ∆2
u · n
k · ∆2 + (k −u) · n
k · δ2 −(k −t)δ2
≥
u∆2
u∆2 + (k −u)δ2
≥
α ·
u∆2
u∆2 + (k −u)δ2 .
Applying our inductive hypothesis with t and u both
decreased by 1, we obtain a potential contribution from
this case of at least,
u∆2
u∆2 + (k −u)δ2 · αt+1 ·

nδ2 · (1 + H′
u−1) · β
+
n
k ∆2 −2nδ2
· (u −t)

.
The probability that our ﬁrst center is chosen from
a covered cluster is
(k −u) · n
k · δ2 −(k −t)δ2
u · n
k · ∆2 + (k −u) · n
k · δ2 −(k −t)δ2
≥
(k −u) · n
k · δ2 −(k −t)δ2
(k −u) · n
k · δ2
·
(k −u)δ2
u∆2 + (k −u)δ2
≥
α ·
(k −u)δ2
u∆2 + (k −u)δ2 .
Applying our inductive hypothesis with t decreased by 1
but with u constant, we obtain a potential contribution
from this case of at least,
(k −u)δ2
u∆2 + (k −u)δ2 · αt+1 ·

nδ2 · (1 + H′
u) · β
+
n
k ∆2 −2nδ2
· (u −t + 1)

.
Therefore, E[φ′] is at least,
αt+1 ·

nδ2 · (1 + H′
u) · β +
n
k ∆2 −2nδ2
· (u −t)

+
αt+1
u∆2 + (k −u)δ2 ·

(k −u)δ2 ·
n
k ∆2 −2nδ2

−u∆2 ·

H′(u) −H′(u −1)

· nδ2 · β

.

--- Page 6 ---
However, H′
u −H′
u−1 = k−u
ku and β = ∆2−2kδ2
∆2
, so
u∆2 ·

H′(u) −H′(u −1)

· nδ2 · β
= (k −u)δ2 ·
n
k ∆2 −2nδ2
,
and the result follows.
As in the previous section, we obtain the desired
result by specializing the induction.
Theorem 4.1. D2 seeding is no better than 2(ln k)-
competitive.
Proof. Suppose a clustering with potential φ is con-
structed using k-means++ on X described above. Ap-
ply Lemma 4.1 with u = t = k −1 after the ﬁrst
center has been chosen.
Noting that 1 + H′
k−1 =
1 + Pk−1
i=1
  1
i −1
k

= Hk > ln k, we then have,
E[φ] ≥αkβ · nδ2 · ln k.
Now, ﬁx k and δ but let n and ∆approach inﬁnity.
Then α and β both approach 1, and the result follows
from the fact that φOPT = n−k
2
· δ2.
5
Generalizations
Although the k-means algorithm itself applies only
in vector spaces with the potential function φ
=
P
x∈X minc∈C∥x −c∥2, we note that our seeding tech-
nique does not have the same limitations. In this sec-
tion, we discuss extending our results to arbitrary met-
ric spaces with the more general potential function,
φ[ℓ] = P
x∈X minc∈C∥x −c∥ℓfor ℓ≥1. In particular,
note that the case of ℓ= 1 is the k-medians potential
function.
These generalizations require only one change to
the algorithm itself. Instead of using D2 seeding, we
switch to Dℓseeding – i.e., we choose x0 as a center
with probability
D(x0)ℓ
P
x∈X D(x)ℓ.
For the analysis, the most important change ap-
pears in Lemma 3.1. Our original proof uses an inner
product structure that is not available in the general
case. However, a slightly weaker result can be proven
using only the triangle inequality.
Lemma 5.1. Let A be an arbitrary cluster in COPT, and
let C be the clustering with just one center, which is cho-
sen uniformly at random from A. Then, E[φ[ℓ](A)] ≤
2ℓφ[ℓ]
OPT(A).
Proof. Let c denote the center of A in COPT. Then,
E[φ[ℓ](A)]
=
1
|A|
X
a0∈A
X
a∈A
∥a −a0∥ℓ
≤
2ℓ−1
|A|
X
a0∈A
X
a∈A
 ∥a −c∥ℓ+ ∥a0 −c∥ℓ
=
2ℓφ[ℓ]
OPT(A).
The second step here follows from the triangle inequality
and the power-mean inequality.
The rest of our upper bound analysis carries
through without change, except that in the proof of
Lemma 3.2, we lose a factor of 2ℓ−1 from the power-
mean inequality, instead of just 2. Putting everything
together, we obtain the general theorem.
Theorem 5.1. If C is constructed with Dℓseeding,
then the corresponding potential function φ[ℓ] satisﬁes,
E[φ[ℓ]] ≤22ℓ(ln k + 2)φ[ℓ]
OPT.
6
Empirical results
In order to evaluate k-means++ in practice, we have
implemented and tested it in C++ [3]. In this section,
we discuss the results of these preliminary experiments.
We found that D2 seeding substantially improves both
the running time and the accuracy of k-means.
6.1
Datasets We
evaluated
the
performance
of
k-means and k-means++ on four datasets.
The ﬁrst dataset, Norm25, is synthetic. To generate
it, we chose 25 “true” centers uniformly at random
from a 15-dimensional hypercube of side length 500.
We then added points from Gaussian distributions of
variance 1 around each true center. Thus, we obtained
a number of well separated Gaussians with the the true
centers providing a good approximation to the optimal
clustering.
We chose the remaining datasets from real-world
examples oﬀthe UC-Irvine Machine Learning Reposi-
tory. The Cloud dataset [7] consists of 1024 points in 10
dimensions, and it is Philippe Collard’s ﬁrst cloud cover
database. The Intrusion dataset [18] consists of 494019
points in 35 dimensions, and it represents features avail-
able to an intrusion detection system. Finally, the Spam
dataset [25] consists of 4601 points in 58 dimensions,
and it represents features available to an e-mail spam
detection system.
For each dataset, we tested k = 10, 25, and 50.
6.2
Metrics Since we were testing randomized seed-
ing processes, we ran 20 trials for each case. We report

--- Page 7 ---
Average φ
Minimum φ
Average T
k
k-means
k-means++
k-means
k-means++
k-means
k-means++
10
1.365 · 105
8.47%
1.174 · 105
0.93%
0.12
46.72%
25
4.233 · 104
99.96%
1.914 · 104
99.92%
0.90
87.79%
50
7.750 · 103
99.81%
1.474 · 101
0.53%
2.04
−1.62%
Table 1:
Experimental results on the Norm25 dataset (n = 10000, d = 15).
For k-means, we list the
actual potential and time in seconds.
For k-means++, we list the percentage improvement over k-means:
100% ·
 1 −k-means++ value
k-means value

.
Average φ
Minimum φ
Average T
k
k-means
k-means++
k-means
k-means++
k-means
k-means++
10
7.921 · 103
22.33%
6.284 · 103
10.37%
0.08
51.09%
25
3.637 · 103
42.76%
2.550 · 103
22.60%
0.11
43.21%
50
1.867 · 103
39.01%
1.407 · 103
23.07%
0.16
41.99%
Table 2: Experimental results on the Cloud dataset (n = 1024, d = 10). For k-means, we list the actual potential
and time in seconds. For k-means++, we list the percentage improvement over k-means.
Average φ
Minimum φ
Average T
k
k-means
k-means++
k-means
k-means++
k-means
k-means++
10
3.387 · 108
93.37%
3.206 · 108
94.40%
63.94
44.49%
25
3.149 · 108
99.20%
3.100 · 108
99.32%
257.34
49.19%
50
3.079 · 108
99.84%
3.076 · 108
99.87%
917.00
66.70%
Table 3: Experimental results on the Intrusion dataset (n = 494019, d = 35). For k-means, we list the actual
potential and time in seconds. For k-means++, we list the percentage improvement over k-means.
Average φ
Minimum φ
Average T
k
k-means
k-means++
k-means
k-means++
k-means
k-means++
10
3.698 · 104
49.43%
3.684 · 104
54.59%
2.36
69.00%
25
3.288 · 104
88.76%
3.280 · 104
89.58%
7.36
79.84%
50
3.183 · 104
95.35%
2.384 · 104
94.30%
12.20
75.76%
Table 4: Experimental results on the Spam dataset (n = 4601, d = 58). For k-means, we list the actual potential
and time in seconds. For k-means++, we list the percentage improvement over k-means.

--- Page 8 ---
the minimum and the average potential (actually di-
vided by the number of points), as well as the mean
running time. Our implementations are standard with
no special optimizations.
6.3
Results The results for k-means and k-means++
are displayed in Tables 1 through 4. We list the absolute
results for k-means, and the percentage improvement
achieved by k-means++ (e.g., a 90% improvement in
the running time is equivalent to a factor 10 speedup).
We observe that k-means++ consistently outperformed
k-means, both by achieving a lower potential value, in
some cases by several orders of magnitude, and also by
having a faster running time. The D2 seeding is slightly
slower than uniform seeding, but it still leads to a faster
algorithm since it helps the local search converge after
fewer iterations.
The synthetic example is a case where standard
k-means does very badly.
Even though there is an
“obvious” clustering, the uniform seeding will inevitably
merge some of these clusters, and the local search will
never be able to split them apart (see [12] for further
discussion of this phenomenon).
The careful seeding
method of k-means++ avoided this problem altogether,
and it almost always attained the optimal clustering on
the synthetic dataset.
The diﬀerence between k-means and k-means++
on the real-world datasets was also substantial.
In
every case, k-means++ achieved at least a 10% accuracy
improvement over k-means, and it often performed
much better.
Indeed, on the Spam and Intrusion
datasets, k-means++ achieved potentials 20 to 1000
times smaller than those achieved by standard k-means.
Each trial also completed two to three times faster, and
each individual trial was much more likely to achieve a
good clustering.
7
Conclusion and future work
We have presented a new way to seed the k-means
algorithm that is O(log k)-competitive with the optimal
clustering. Furthermore, our seeding technique is as fast
and as simple as the k-means algorithm itself, which
makes it attractive in practice.
Towards that end,
we ran preliminary experiments on several real-world
datasets, and we observed that k-means++ substantially
outperformed standard k-means in terms of both speed
and accuracy.
Although our analysis of the expected potential
E[φ] achieved by k-means++ is tight to within a con-
stant factor, a few open questions still remain. Most
importantly, it is standard practice to run the k-means
algorithm multiple times, and then keep only the best
clustering found. This raises the question of whether
k-means++ achieves asymptotically better results if it is
allowed several trials. For example, if k-means++ is run
2k times, our arguments can be modiﬁed to show it is
likely to achieve a constant approximation at least once.
We ask whether a similar bound can be achieved for a
smaller number of trials.
Also, experiments showed that k-means++ generally
performed better if it selected several new centers during
each iteration, and then greedily chose the one that
decreased φ as much as possible.
Unfortunately, our
proofs do not carry over to this scenario. It would be
interesting to see a comparable (or better) asymptotic
result proven here.
Finally, we are currently working on a more thor-
ough experimental analysis. In particular, we are mea-
suring the performance of not only k-means++ and stan-
dard k-means, but also other variants that have been
suggested in the theory community.
Acknowledgements
We would like to thank Rajeev Motwani for his helpful
comments.
References
[1] Pankaj K. Agarwal and Nabil H. Mustafa.
k-means
projective clustering. In PODS ’04: Proceedings of the
twenty-third ACM SIGMOD-SIGACT-SIGART sym-
posium on Principles of database systems, pages 155–
165, New York, NY, USA, 2004. ACM Press.
[2] D. Arthur and S. Vassilvitskii.
Worst-case and
smoothed analysis of the ICP algorithm, with an ap-
plication to the k-means method.
In Symposium on
Foundations of Computer Science, 2006.
[3] David Arthur and Sergei Vassilvitskii.
k-means++
test
code.
http://www.stanford.edu/∼darthur/
kMeansppTest.zip.
[4] David Arthur and Sergei Vassilvitskii.
How slow is
the k-means method?
In SCG ’06: Proceedings of
the twenty-second annual symposium on computational
geometry. ACM Press, 2006.
[5] Pavel Berkhin.
Survey of clustering data mining
techniques.
Technical report, Accrue Software, San
Jose, CA, 2002.
[6] Moses Charikar, Liadan O’Callaghan, and Rina Pani-
grahy. Better streaming algorithms for clustering prob-
lems. In STOC ’03: Proceedings of the thirty-ﬁfth an-
nual ACM symposium on Theory of computing, pages
30–39, New York, NY, USA, 2003. ACM Press.
[7] Philippe Collard’s cloud cover database. ftp://ftp.
ics.uci.edu/pub/machine-learning-databases/
undocumented/taylor/cloud.data.
[8] Sanjoy Dasgupta. How fast is k-means? In Bernhard
Sch¨olkopf and Manfred K. Warmuth, editors, COLT,
volume 2777 of Lecture Notes in Computer Science,
page 735. Springer, 2003.

--- Page 9 ---
[9] W. Fernandez de la Vega, Marek Karpinski, Claire
Kenyon, and Yuval Rabani. Approximation schemes
for clustering problems. In STOC ’03: Proceedings of
the thirty-ﬁfth annual ACM symposium on Theory of
computing, pages 50–58, New York, NY, USA, 2003.
ACM Press.
[10] P. Drineas, A. Frieze, R. Kannan, S. Vempala, and
V. Vinay. Clustering large graphs via the singular value
decomposition. Mach. Learn., 56(1-3):9–33, 2004.
[11] Fr´ed´eric Gibou and Ronald Fedkiw.
A fast hybrid
k-means level set algorithm for segmentation. In 4th
Annual Hawaii International Conference on Statistics
and Mathematics, pages 281–291, 2005.
[12] Sudipto Guha, Adam Meyerson, Nina Mishra, Rajeev
Motwani, and Liadan O’Callaghan.
Clustering data
streams: Theory and practice. IEEE Transactions on
Knowledge and Data Engineering, 15(3):515–528, 2003.
[13] Sariel Har-Peled and Soham Mazumdar. On coresets
for k-means and k-median clustering.
In STOC ’04:
Proceedings of the thirty-sixth annual ACM symposium
on Theory of computing, pages 291–300, New York,
NY, USA, 2004. ACM Press.
[14] Sariel Har-Peled and Bardia Sadri.
How fast is the
k-means method?
In SODA ’05: Proceedings of the
sixteenth annual ACM-SIAM symposium on Discrete
algorithms, pages 877–885, Philadelphia, PA, USA,
2005. Society for Industrial and Applied Mathematics.
[15] R.
Herwig,
A.J.
Poustka,
C.
Muller,
C.
Bull,
H. Lehrach, and J O’Brien. Large-scale clustering of
cdna-ﬁngerprinting data.
Genome Research, 9:1093–
1105, 1999.
[16] Mary Inaba, Naoki Katoh, and Hiroshi Imai. Applica-
tions of weighted voronoi diagrams and randomization
to variance-based k-clustering: (extended abstract). In
SCG ’94: Proceedings of the tenth annual symposium
on Computational geometry, pages 332–339, New York,
NY, USA, 1994. ACM Press.
[17] Tapas Kanungo, David M. Mount, Nathan S. Ne-
tanyahu, Christine D. Piatko, Ruth Silverman, and An-
gela Y. Wu. A local search approximation algorithm
for k-means clustering.
Comput. Geom., 28(2-3):89–
112, 2004.
[18] KDD Cup 1999 dataset.
http://kdd.ics.uci.edu/
/databases/kddcup99/kddcup99.html.
[19] Amit Kumar, Yogish Sabharwal, and Sandeep Sen. A
simple linear time (1 + ϵ)-approximation algorithm for
k-means clustering in any dimensions. In FOCS ’04:
Proceedings of the 45th Annual IEEE Symposium on
Foundations of Computer Science (FOCS’04), pages
454–462, Washington, DC, USA, 2004. IEEE Com-
puter Society.
[20] Stuart P. Lloyd. Least squares quantization in pcm.
IEEE Transactions on Information Theory, 28(2):129–
136, 1982.
[21] Jir´ı Matousek. On approximate geometric k-clustering.
Discrete & Computational Geometry,
24(1):61–84,
2000.
[22] Ramgopal R. Mettu and C. Greg Plaxton.
Optimal
time bounds for approximate clustering.
In Adnan
Darwiche and Nir Friedman, editors, UAI, pages 344–
351. Morgan Kaufmann, 2002.
[23] A. Meyerson. Online facility location. In FOCS ’01:
Proceedings of the 42nd IEEE symposium on Founda-
tions of Computer Science, page 426, Washington, DC,
USA, 2001. IEEE Computer Society.
[24] R. Ostrovsky, Y. Rabani, L. Schulman, and C. Swamy.
The eﬀectiveness of Lloyd-type methods for the k-
Means problem.
In Symposium on Foundations of
Computer Science, 2006.
[25] Spam e-mail database.
http://www.ics.uci.edu/
∼mlearn/databases/spambase/.


=== 2010.14487v1.pdf ===

--- Page 1 ---
arXiv:2010.14487v1  [cs.LG]  27 Oct 2020
Improved Guarantees for k-means++ and k-means++ Parallel
Konstantin Makarychev, Aravind Reddy, and Liren Shan
Department of Computer Science
Northwestern University
Evanston, IL, USA
Abstract
In this paper, we study k-means++ and k-means∥, the two most popular algorithms for the
classic k-means clustering problem. We provide novel analyses and show improved approxima-
tion and bi-criteria approximation guarantees for k-means++ and k-means∥. Our results give a
better theoretical justiﬁcation for why these algorithms perform extremely well in practice. We
also propose a new variant of k-means∥algorithm (Exponential Race k-means++) that has the
same approximation guarantees as k-means++.
1
Introduction
k-means clustering is one of the most commonly encountered unsupervised learning problems. Given
a set of n data points in Euclidean space, our goal is to partition them into k clusters (each
characterized by a center), such that the sum of squares of distances of data points to their nearest
centers is minimized. The most popular heuristic for solving this problem is Lloyd’s algorithm
(Lloyd, 1982), often referred to simply as “the k-means algorithm".
Lloyd’s algorithm uses iterative improvements to ﬁnd a locally optimal k-means clustering. The
performance of Lloyd’s algorithm crucially depends on the quality of the initial clustering, which
is deﬁned by the initial set of centers, called a seed. Arthur and Vassilvitskii (2007) and Ostrovsky,
Rabani, Schulman, and Swamy (2006) developed an elegant randomized seeding algorithm, known
as the k-means++ algorithm. It works by choosing the ﬁrst center uniformly at random from the
data set and then choosing the subsequent k−1 centers by randomly sampling a single point in each
round with the sampling probability of every point proportional to its current cost. That is, the
probability of choosing any data point x is proportional to the squared distance to its closest already
chosen center. This squared distance is often denoted by D2(x). Arthur and Vassilvitskii (2007)
proved that the expected cost of the initial clustering obtained by k-means++ is at most 8 (ln k + 2)
times the cost of the optimal clustering i.e., k-means++ gives an 8 (ln k + 2)-approximation for the
k-means problem. They also provided a family of k-means instances for which the approximation
factor of k-means++ is 2 ln k and thus showed that their analysis of k-means++ is almost tight.
Due to its speed, simplicity, and good empirical performance, k-means++ is the most widely
used algorithm for k-means clustering. It is employed by such machine learning libraries as Apache
The conference version of this paper will appear in the proceedings of the 34th Conference on Neural Information
Processing Systems (NeurIPS 2020). Author order is alphabetical.
1

--- Page 2 ---
Spark MLlib, Google BigQuery, IBM SPSS, Intel DAAL, and Microsoft ML.NET. In addition
to k-means++, these libraries implement a scalable variant of k-means++ called k-means∥(read
“k-means parallel”) designed by Bahmani, Moseley, Vattani, Kumar, and Vassilvitskii (2012). Some-
what surprisingly, k-means∥not only works better in parallel than k-means++ but also slightly
outperforms k-means++ in practice in the single machine setting (see Bahmani et al. (2012) and
Figure 1 below). However, theoretical guarantees for k-means∥are substantially weaker than for
k-means++.
The k-means∥algorithm makes T passes over the data set (usually T = 5). In every round,
it independently draws approximately ℓ= Θ(k) random centers according to the D2 distribution.
After each round it recomputes the distances to the closest chosen centers and updates D2(x) for
all x in the data set. Thus, after T rounds, k-means∥chooses approximately Tℓcenters. It then
selects k centers among Tℓcenters using k-means++.
Our contributions.
In this paper, we improve the theoretical guarantees for k-means++,
k-means∥, and Bi-Criteria k-means++ (which we deﬁne below).
First, we show that the expected cost of the solution output by k-means++ is at most 5(ln k+2)
times the optimal solution’s cost. This improves upon the bound of 8(ln k+2) shown by Arthur and
Vassilvitskii (2007) and directly improves the approximation factors for several algorithms which
use k-means++ as a subroutine like Local Search k-means++ (Lattanzi and Sohler, 2019). To
obtain this result, we give a reﬁned analysis of the expected cost of covered clusters (see Lemma
3.2 in Arthur and Vassilvitskii (2007) and Lemma 4.1 in this paper). We also show that our new
bound on the expected cost of covered clusters is tight (see Lemma C.1).
Then, we address the question of why the observed performance of k-means∥is better than the
performance of k-means++. There are two possible explanations for this fact. (1) This may be the
case because k-means∥picks k centers in two stages. At the ﬁrst stage, it samples ℓT ≥k centers.
At the second stage, it prunes centers and chooses k centers among ℓT centers using k-means++.
(2) This may also be the case because k-means∥updates the distribution function D2(x) once
in every round. That is, it recomputes D2(x) once for every ℓchosen centers, while k-means++
recomputes D2(x) every time it chooses a center. In this paper, we empirically demonstrate that
the ﬁrst explanation is correct. First, we noticed that k-means∥for ℓ· T = k is almost identical
with k-means++ (see Appendix A). Second, we compare k-means∥with another algorithm which
we call Bi-Criteria k-means++ with Pruning. This algorithm also works in two stages: At the
Bi-Criteria k-means++ stage, it chooses k + ∆centers in the data set using k-means++. Then, at
the Pruning stage, it picks k centers among the k +∆centers selected at the ﬁrst stage again using
k-means++. Our experiments on the standard data sets BioTest from KDD-Cup 2004 (Elber, 2004)
and COVTYPE from the UCI ML repository (Dua and Graﬀ, 2017) show that the performance
of k-means∥and Bi-Criteria k-means++ with Pruning are essentially identical (see Figures 1 and
Appendix A).
These results lead to another interesting question: How good are k-means++ and k-means∥
algorithms that sample k+∆instead of k centers? The idea of oversampling using k-means++ was
studied earlier in the literature under the name of bi-criteria approximation. Aggarwal, Deshpande,
and Kannan (2009) showed that with constant probability, sampling k + ∆centers by k-means++
provides a constant-factor approximation if ∆≥βk for some constant β > 0. Wei (2016) improved
on this result by showing an expected approximation ratio of 8(1 + 1.618k/∆). Note that for bi-
criteria algorithms we compare the expected cost of the clustering with k +∆centers they produce
and the cost of the optimal clustering with exactly k centers.
2

--- Page 3 ---
10
20
30
40
50
3
4
5
6
#centers
cost
(a) BioTest
k-means++
BiCriteria k-means++ w/Pruning
k-means∥
10
20
30
40
50
10
15
20
25
30
#centers
cost
(b) COVTYPE
k-means++
BiCriteria k-means++ w/Pruning
k-means∥
Figure 1: Performance of k-means++, k-means∥, and Bi-Criteria k-means++ with pruning on the
BioTest and COVTYPE datasets. For k = 10, 15, · · · , 50, we ran these algorithms for 50 iterations
and took their average. We normalized the clustering costs by dividing them by cost1000(X).
In this paper, we show that the expected bi-criteria approximation ratio for k-means++ with
∆additional centers is at most the minimum of two bounds:
(A) 5

2 + 1
2e + ln 2k
∆

for 1 ≤∆≤2k; and (B) 5

1 +
k
e (∆−1)

for ∆≥1
Both bounds are better than the bound by Wei (2016). The improvement is especially noticeable
for small values of ∆. More speciﬁcally, when the number of additional centers is ∆= k/ log k, our
approximation guarantee is O(log log k) while Wei (2016) gives an O(log k) approximation.
We believe that our results for small values of ∆provide an additional explanation for why
k-means++ works so well in practice. Consider a data scientist who wants to cluster a data set
X with k∗true clusters (i.e. k∗latent groups). Since she does not know the actual value of k∗,
she uses the elbow method (Boehmke and Greenwell, 2019) or some other heuristic to ﬁnd k. Our
results indicate that if she chooses slightly more number of clusters (for instance, 1.05k∗), then she
will get a constant bi-criteria approximation to the optimal clustering.
We also note that our bounds on the approximation factor smoothly transition from the regular
(∆= 0) to bi-criteria (∆> 0) regime. We complement our analysis with an almost matching lower
bound of Θ(log(k/∆)) on the approximation factor of k-means for ∆≤k (see Appendix C).
We then analyze Bi-Criteria k-means∥algorithm, the variant of k-means∥that does not prune
centers at the second stage.
In their original paper, Bahmani, Moseley, Vattani, Kumar, and
Vassilvitskii (2012) showed that the expected cost of the solution for k-means∥with T rounds and
oversampling parameter ℓis at most:
16
1 −αOPTk(X) +
1 + α
2
T
OPT1(X),
where α = exp

−

1 −e−ℓ/(2k)
; OPTk(X) is the cost of the optimal k-means clustering of X;
OPT1(X) is the cost of the optimal clustering of X with 1 center (see Section 2 for details). We
note that OPT1(X) ≫OPTk(X).
For ℓ= k, this result gives a bound of ≈49 OPTk(X) +
0.83T OPT1(X).
Bachem, Lucic, and Krause (2017) improved the approximation guarantee for
3

--- Page 4 ---
ℓ≥k to
26OPTk(X) + 2
 k
eℓ
T
OPT1(X).
In this work, we improve this bound for ℓ≥k and also obtain a better bound for ℓ< k. For ℓ≥k,
we show that the cost of k-means∥without pruning is at most
8OPTk(X) + 2
 k
eℓ
T
OPT1(X).
For ℓ< k, we give a bound of
5
1 −e−ℓ
k
OPTk(X) + 2

e−ℓ
k
T
OPT1(X)
Finally, we give a new parallel variant of the k-means++ algorithm, which we call Exponential
Race k-means++ (k-means++ER). This algorithm is similar to k-means∥. In each round, it also
selects ℓcandidate centers in parallel (some of which may be dropped later) making one pass over
the data set. However, after T rounds, it returns exactly k centers. The probability distribution
of these centers is identical to the distribution of centers output by k-means++. The expected
number of rounds is bounded as follows:
O
k
ℓ+ log OPT1(X)
OPTk(X)

.
This algorithm oﬀers a unifying view on k-means++ and k-means∥. We describe it in Section 7.
Other related work. Dasgupta (2008) and Aloise, Deshpande, Hansen, and Popat (2009) showed
that k-means problem is NP-hard. Awasthi, Charikar, Krishnaswamy, and Sinop (2015) proved that
it is also NP-hard to approximate k-means objective within a factor of (1 + ε) for some constant
ε > 0 (see also Lee, Schmidt, and Wright (2017)). We also mention that k-means was studied not
only for Euclidean spaces but also for arbitrary metric spaces.
There are several known constant factor approximation algorithms for the k-means problem.
Kanungo, Mount, Netanyahu, Piatko, Silverman, and Wu (2004) gave a 9 + ε approximation local
search algorithm. Ahmadian, Norouzi-Fard, Svensson, and Ward (2019) proposed a primal-dual
algorithm with an approximation factor of 6.357.
This is the best known approximation for k-
means. Makarychev, Makarychev, Sviridenko, and Ward (2016) gave constant-factor bi-criteria
approximation algorithms based on linear programming and local search. Note that although these
algorithms run in polynomial time, they do not scale well to massive data sets.
Lattanzi and
Sohler (2019) provided a constant factor approximation by combining the local search idea with
the k-means++ algorithm. Choo, Grunau, Portmann, and Rozhoň (2020) further improved upon
this result by reducing the number of local search steps needed from O(k log log k) to O(k).
Independently and concurrently to our work, Rozhoň (2020) gave an interesting analysis for
k-means∥by viewing it as a balls into bins problem and showed that O(log n/ log log n) rounds
suﬃce to give a constant approximation with high probability.
Acknowledgments. We would like to thank all the reviewers for their helpful comments. Kon-
stantin Makarychev, Aravind Reddy, and Liren Shan were supported in part by NSF grants CCF-
1955351 and HDR TRIPODS CCF-1934931. Aravind Reddy was also supported in part by NSF
CCF-1637585.
4

--- Page 5 ---
2
Preliminaries
Given a set of points X = {x1, x2, · · · , xn} ⊆Rd and an integer k ≥1, the k-means clustering
problem is to ﬁnd a set C of k centers in Rd to minimize
cost(X, C) :=
X
x∈X
min
c∈C ∥x −c∥2.
For any integer i ≥1, let us deﬁne OPTi(X) := min|C|=i cost (X, C) . Thus, OPTk(X) refers to the
cost of the optimal solution for the k-means problem. Let C∗denote a set of optimal centers. We
use {Pi}k
i=1 to denote the clusters induced by the center set C∗.
For any Y ⊆X, the cost of Y with center set C, denoted by cost (Y, C) = P
x∈Y minc∈C∥x−c∥2.
The optimal cost for subset Y with i centers is OPTi(Y). Let µ = P
x∈Y x/ |Y| be the centroid of
the cluster Y. Then, we have the following closed form expression for the optimal cost of Y with
one center (see Appendix B for proof),
OPT1(Y) =
X
x∈Y
∥x −µ∥2 =
P
(x,y)∈Y×Y∥x −y∥2
2 |Y|
.
(1)
k-means++ seeding: The k-means++ algorithm samples the ﬁrst center uniformly at random
from the given points and then samples k −1 centers sequentially from the given points with
probability of each point being sampled proportional to its cost i.e. cost(x, C)/cost(X, C).
Algorithm 1 k-means++ seeding
1: Sample a point c uniformly at random from X and set C1 = {c}.
2: for t = 2 to k do
3:
Sample x ∈X w.p. cost(x, Ct)/cost(X, Ct).
4:
Ct = Ct−1 ∪{x}.
5: end for
6: Return Ck
k-means∥and k-means∥Pois seeding: In the k-means∥algorithm, the ﬁrst center is chosen
uniformly at random from X. But after that, at each round, the algorithm samples each point
independently with probability min {ℓ· cost(x, C)/cost(X, C), 1} where ℓis the oversampling pa-
rameter chosen by the user and it usually lies between 0.1k and 10k. The algorithm runs for T
rounds (where T is also a parameter chosen by the user) and samples around ℓT points, which is
usually strictly larger than k. This oversampled set is then weighted using the original data set X
and a weighted version of k-means++ is run on this set to get the ﬁnal k-centers. We only focus
on the stage in which we get the oversampled set because the guarantees for the second stage come
directly from k-means++.
For the sake of analysis, we also consider a diﬀerent implementation of k-means∥, which we
call k-means∥Pois
(Algorithm 3).
This algorithm diﬀers from k-means∥in that each point is
sampled independently with probability 1 −exp(−ℓ· cost(x, C)/cost(X, C)) rather than min{ℓ·
cost(x, C)/cost(X, C), 1}. In practice, there is essentially no diﬀerence between k-means∥and k-
means∥Pois, since ℓ· cost(x, C)/cost(X, C) is a very small number for all x and thus the sampling
probabilities for k-means∥and k-means∥Pois are almost equal.
5

--- Page 6 ---
Algorithm 2 k-means∥seeding
1: Sample a point c uniformly from X and
set C1 = {c}
2: for t = 1 to T do
3:
Sample each point x into C′ indepen-
dently w.p. min{1, λt(x)} where
λt(x) = ℓ· cost(x, Ct)/cost(X, Ct)
4:
Let Ct+1 = Ct ∪C′.
5: end for
Algorithm 3 k-means∥Pois seeding
1: Sample a point c uniformly from X and
set C1 = {c}
2: for t = 1 to T do
3:
Sample each point x into C′ indepen-
dently w.p. 1 −e−λt(x) where
λt(x) = ℓ· cost(x, Ct)/cost(X, Ct)
4:
Let Ct+1 = Ct ∪C′.
5: end for
In the rest of the paper, we focus only on the seeding step of k-means++, k-means∥, and k-
means∥Pois and ignore Lloyd’s iterations as the approximation guarantees for these algorithms come
entirely from the seeding step.
3
General framework
In this section, we describe a general framework we use to analyze k-means++ and k-means∥Pois.
Consider k-means++ or k-means∥Pois algorithm.
Let Ct be the set of centers chosen by this
algorithm after step t. For the sake of analysis, we assume that Ct is an ordered set or list of
centers, and the order of centers in Ct is the same as the order in which our algorithm chooses these
centers. We explain how to order centers in k-means∥Pois algorithm in Section 6. We denote by T
the stopping time of the algorithm. Observe that after step t of the algorithm, the probabilities of
choosing a new center in k-means++ or a batch of new centers in k-means∥Pois are deﬁned by the
current costs of points in X which, in turn, are completely determined by the current set of centers
Ct. Thus, the states of the algorithm form a Markov chain.
In our analysis, we ﬁx the optimal clustering P = {P1, . . . , Pk} (if this clustering is not unique,
we pick an arbitrary optimal clustering). The optimal cost of each cluster Pi is OPT1(Pi) and the
optimal cost of the entire clustering is OPTk(X) = Pk
i=1 OPT1(Pi).
Following the notation in Arthur and Vassilvitskii (2007), we say that a cluster Pi is hit or
covered by a set of centers C if C ∩Pi ̸= ∅; otherwise, we say that Pi is not hit or uncovered. We
split the cost of each cluster Pi into two components which we call the covered and uncovered costs
of Pi. For a given set of centers C,
The covered or hit cost of Pi,
H(Pi, C) :=
(
cost(Pi, C),
if Pi is covered by C
0,
otherwise.
The uncovered cost of Pi,
U(Pi, C) :=
(
0,
if Pi is covered by C
cost(Pi, C),
otherwise.
Let H(X, C) = Pk
i=1 H(Pi, C) and U(X, C) = Pk
i=1 U(Pi, C). Then,
cost(X, C) = H(X, C) + U(X, C).
For the sake of brevity, we deﬁne costt(Y) := cost(Y, Ct) for any Y ⊆X, Ht(Pi) := H(Pi, Ct), and
Ut(Pi) := U(Pi, Ct). In Section 4, we show that for any t, we have E[Ht(X)] ≤5OPTk(X), which is
an improvement over the bound of 8OPTk(X) given by Arthur and Vassilvitskii (2007). Then, in
6

--- Page 7 ---
Sections 5 and 6, we analyze the expected uncovered cost U(X, CT ) for k-means++ and k-means∥
algorithms.
Consider a center c in C. We say that c is a miss if another center c′ covers the same cluster
in P as c, and c′ appears before c in the ordered set C. We denote the number of misses in C by
M(C) and the the number of clusters in P not covered by centers in C by K(C).
Observe that the stochastic processes Ut(Pi) with discrete time t are non-increasing since the
algorithm never removes centers from the set Ct and therefore the distance from any point x to
Ct never increases. Similarly, the processes Ht(Pi) are non-increasing after the step ti when Pi is
covered ﬁrst time. In this paper, we sometimes use a proxy eHt(Pi) for Ht(Pi), which we deﬁne
as follows. If Pi is covered by Ct, then eHt(Pi) = Hti(Pi), where ti ≤t is the ﬁrst time when Pi
is covered by Ct. If Pi is not covered by Ct, then eHt(Pi) = 5OPT1(Pi). It is easy to see that
Ht(Pi) ≤eHt′(Pi) for all t ≤t′. In Section 4, we also show that eHt(Pi) is a supermartingale i.e.,
E[ eHt′(Pi) | Ct] ≤eHt(Pi) for all t ≤t′.
4
Bound on the cost of covered clusters
In this section, we improve the bound by Arthur and Vassilvitskii (2007) on the expected cost
of a covered cluster in k-means++. Our bound also works for k-means∥Pois algorithm. Pick an
arbitrary cluster Pi in the optimal solution P = {P1, . . . , Pk} and consider an arbitrary state
Ct = {c1, . . . , cN} of the k-means++ or k-means∥Pois algorithm. Let Dt+1 be the set of new centers
the algorithm adds to Ct at step t (for k-means++, Dt+1 contains only one center). Suppose now
that centers in Dt+1 cover Pi i.e. Dt+1 ∩Pi ̸= ∅. We show that the expected cost of cluster Pi
after step (t + 1) conditioned on the event {Dt+1 ∩Pi ̸= ∅} and the current state of the algorithm
Ct is upper bounded by 5OPT1(Pi) i.e.
E [cost(Pi, Ct+1) | Ct, {Dt+1 ∩Pi ̸= ∅}] ≤5OPT1(Pi).
(2)
We now prove the main lemma.
Lemma 4.1. Consider an arbitrary set of centers C = {c1, . . . , cN} ⊆Rd and an arbitrary set
P ⊆X. Pick a random point c in P with probability Pr(c = x) = cost(x, C)/cost(P, C). Let
C′ = C ∪{c}. Then, Ec [cost(P, C′)] ≤5OPT1(P).
Remarks: Lemma 3.2 in the paper by Arthur and Vassilvitskii (2007) gives a bound of 8OPT1(P).
We also show in Appendix C that our bound is tight (see Lemma C.1).
Proof. The cost of any point y after picking center c equals the squared distance from y to the
set of centers C′ = C ∪{c}, which in turn equals min{cost(y, C), ∥y −c∥2}.
Thus, if a point
x ∈P is chosen as a center, then the cost of point y equals min{cost(y, C), ∥x −y∥2}.
Since
Pr(c = x) = cost(x, C)/cost(P, C), we have
Ec
cost(P, C′)
 =
X
x∈P
y∈P
cost(x, C)
cost(P, C) · min{cost(y, C), ∥x −y∥2}.
We write the right hand side in a symmetric form with respect to x and y. To this end, we deﬁne
function f as follows:
f(x, y) = cost(x, C) · min
n
∥x −y∥2, cost(y, C)
o
+ cost(y, C) · min
n
∥x −y∥2, cost(x, C)
o
.
7

--- Page 8 ---
Note that f(x, y) = f(y, x). Then,
Ec
cost(P, C′)
 =
1
2cost(P, C)
X
(x,y)∈P ×P
f(x, y).
We now give an upper bound on f(x, y) and then use this bound to ﬁnish the proof of Lemma 4.1.
Lemma 4.2. For any x, y ∈P, we have f(x, y) ≤5 min {cost(x, C), cost(y, C)} ∥x −y∥2.
Proof. Since f(x, y) is a symmetric function with respect to x and y, we may assume without loss of
generality that cost(x, C) ≤cost(y, C). Then, we need to show that f(x, y) ≤5cost(x, C)∥x −y∥2.
Consider three cases.
Case 1: If cost(x, C) ≤cost(y, C) ≤∥x −y∥2, then
f(x, y) = 2cost(x, C)cost(y, C) ≤2cost(x, C)∥x −y∥2.
Case 2: If cost(x, C) ≤∥x −y∥2 ≤cost(y, C), then
f(x, y) = cost(x, C)∥x −y∥2 + cost(y, C)cost(x, C).
By the triangle inequality, we have
cost(y, C) ≤
q
cost(x, C) + ∥x −y∥
2
≤4∥x −y∥2.
Thus, f(x, y) ≤5cost(x, C)∥x −y∥2.
Case 3: If ∥x −y∥2 ≤cost(x, C) ≤cost(y, C), then
f(x, y) = (cost(x, C) + cost(y, C)) ∥x −y∥2.
By the triangle inequality,
cost(y, C) ≤
q
cost(x, C) + ∥x −y∥
2
≤4cost(x, C).
Thus, we have f(x, y) ≤5cost(x, C)∥x −y∥2.
In all cases, the desired inequality holds. This concludes the proof of Lemma 4.2.
We use Lemma 4.2 to bound the expected cost of P. Let φ∗be a vector in RP with φ∗
x =
cost(x, C) for any x ∈P. Then, f(x, y) ≤5 min
n
φ∗
x, φ∗
y
o
∥x −y∥2. Since cost(P, C) = P
z∈P φ∗
z, we
have
Ec
cost(P, C′)
 ≤
5 P
(x,y)∈P ×P min
n
φ∗
x, φ∗
y
o
∥x −y∥2
2 P
z∈P φ∗z
|
{z
}
5F (φ∗)
.
For arbitrary vector φ ∈RP
≥0, deﬁne the following function:
F(φ) =
P
(x,y)∈P ×P min {φx, φy} ∥x −y∥2
2 P
z∈P φz
.
(3)
8

--- Page 9 ---
We have Ec [cost(P, C′)] ≤5F(φ∗). Thus, to ﬁnish the proof of Lemma 4.1, it suﬃces to show that
F(φ) ≤OPT1(P) for every φ ≥0 and particularly for φ = φ∗. By Lemma 4.3 (which we state and
prove below), function F(φ) is maximized when φ ∈{0, 1}P . Let φ∗∗be a maximizer of F(φ) in
{0, 1}P and P ′ = {x ∈P : φ∗∗
x = 1}. Observe that
F(φ∗∗) =
P
(x,y)∈P ′×P ′∥x −y∥2
2|P ′|
= OPT1(P ′).
Here we used the closed form expression (1) for the optimal cost of cluster P ′. Since P ′ ⊂P, we
have OPT1(P ′) ≤OPT1(P). Thus, F(φ∗) ≤F(φ∗∗) ≤OPT1(P).
Lemma 4.3. There exists a maximizer φ∗∗of F(φ) in the region {φ ≥0} such that φ ∈{0, 1}P .
Proof. Let m = |P| be the size of the cluster P and Π be the set of all bisections or permutations
π : {1, . . . , m} →P. Partition the set {φ ≥0} into m! regions (“cones over order polytopes”):
{φ : φ ≥0} = ∪π∈ΠOπ,
where Oπ = {φ : 0 ≤φπ(1) ≤φπ(2) ≤· · · ≤φπ(m)}. We show that for every π ∈Π, there exists a
maximizer φ∗∗of F(φ) in the region Oπ, such that φ∗∗∈{0, 1}P . Therefore, there exists a global
maximizer φ∗∗that belongs {0, 1}P
Fix a π ∈Π. Denote by V the hyperplane {φ : P
x∈P φx = 1}. Observe that F is a scale
invariant function i.e., F(φ) = F(λφ) for every λ > 0. Thus, for every φ ∈Oπ, there exists a
φ′ ∈Oπ ∩V (namely, φ′ = φ/(P
x∈P φx)) such that F(φ′) = F(φ). Hence, max{F(φ) : φ ∈Oπ} =
max{F(φ) : φ ∈Oπ ∩V }. Note that for φ ∈V , the denominator of (3) equals 2, and for φ ∈Oπ,
the numerator of (3) is a linear function of φ. Therefore, F(φ) is a linear function in the convex
set Oπ ∩V . Consequently, one of the maximizers of F must be an extreme point of Oπ ∩V .
The polytope Oπ ∩V is deﬁned by m inequalities and one equality. Thus, for every extreme
point φ of this polytope, all inequalities φπ(i) ≤φπ(i+1) but one must be tight. In other words, for
some j < m, we have
0 = φπ(1) = · · · = φπ(j) < φπ(j+1) = · · · = φπ(m).
(4)
Therefore, there exists a maximizer φ of F(φ) in Oπ ∩V satisfying (4) for some j. After rescaling
φ – multiplying all coordinates of φ by (m −j) – we obtain a vector φ∗∗whose ﬁrst j coordinates
φ∗∗
π(1), . . . , φ∗∗
π(j) are zeroes and the last m −j coordinates φ∗∗
π(j+1), . . . , φ∗∗
π(m) are ones. Thus, φ∗∗∈
{0, 1}P . Since F is rescaling invariant, F(φ∗∗) = F(φ). This concludes the proof.
Replacing the bound in Lemma 3.2 from the analysis of Arthur and Vassilvitskii (2007) by our
bound from Lemma 4.1 gives the following result (see also Lemma 5.6).
Theorem 4.4. The approximation factor of k-means++ is at most 5(ln k + 2).
We now state an important corollary of Lemma 4.1.
Corollary 4.5.
For every P ∈P, the process eHt(P) for k-means++ is a supermartingale i.e.,
E
h
eHt+1(X) | Ct
i
≤eHt(X).
9

--- Page 10 ---
Proof. The value of eHt(X) changes only if at step t, we cover a yet uncovered cluster P. In this
case, the value of eHt+1(P) changes by the new cost of P minus 5OPT(P). By Lemma 4.1 this
quantity is non-positive in expectation.
Since the process eHt(P) is a supermartingale, we have E[ eHt(P)] ≤eH0(P) = 5OPT1(P). Hence,
E[Ht(P)] ≤E[ eHt(P)] = 5OPT1(P). Thus, E[Ht(X)] ≤5OPTk(X). Since costt(X) = Ht(X) +
Ut(X) and we have a bound on the expectation of the covered cost, Ht(X), in the remaining
sections, we shall only analyze the uncovered cost Ut(X).
5
Bi-criteria approximation of k-means++
In this section, we give a bi-criteria approximation guarantee for k-means++.
Theorem 5.1. Let costk+∆(X) be the cost of the clustering with k + ∆centers sampled by the
k-means++ algorithm. Then, for ∆≥1, the expected cost E [costk+∆(X)] is upper bounded by
(below (a)+ denotes max(a, 0)).
min
n
2 + 1
2e +

ln 2k
∆
+
, 1 +
k
e (∆−1)
o
5OPTk(X).
Note that the above approximation guarantee is the minimum of two bounds: (1) 2+ 1
2e + ln 2k
∆
for 1 ≤∆≤2k; and (2) 1 +
k
e(∆−1) for ∆≥1. The second bound is stronger than the ﬁrst bound
when ∆/k ⪆0.085.
5.1
Proof overview of Theorem 5.1
We now present a high level overview of the proof and then give a formal proof. Our proof consists
of three steps.
First, we prove bound (2) on the expected cost of the clustering returned by k-means++ after
k + ∆rounds. We argue that the expected cost of the covered clusters is bounded by 5OPTk(X)
(see Section 3) and thus it is suﬃcient to bound the expected cost of uncovered clusters. Consider
an optimal cluster P ∈P. We need to estimate the probability that it is not covered after k + ∆
rounds. We upper bound this probability by the probability that the algorithm does not cover P
before it makes ∆misses (note: after k + ∆rounds k-means++ must make at least ∆misses).
In this overview, we make the following simplifying assumptions (which turn out to be satisﬁed
in the worst case for bi-criteria k-means++): Suppose that the uncovered cost of cluster P does not
decrease before it is covered and equals U(P) and, moreover, the total cost of all covered clusters
almost does not change and equals H(X) (this may be the case if one large cluster contributes
most of the covered cost, and that cluster is covered at the ﬁrst step of k-means++). Under these
assumptions, the probability that k-means++ chooses ∆centers in the already covered clusters
and does not choose a single center in P equals (H(X)/(U(P) + H(X)))∆. If k-means++ does
not choose a center in P, the uncovered cost of cluster P is U(P); otherwise, the uncovered cost of
cluster P is 0. Thus, the expected uncovered cost of P is (H(X)/(U(P)+H(X)))∆U(P). It is easy
to show that (H(X)/(U(P) + H(X)))∆U(P) ≤H(X)/(e(∆−1)). Thus, the expected uncovered
cost of all clusters is at most
k
(e(∆−1))E[H(X)] ≤
k
(e(∆−1))5OPTk(X).
10

--- Page 11 ---
Then, we use ideas from Arthur and Vassilvitskii (2007), Dasgupta (2013) to prove the following
statement: Let us count the cost of uncovered clusters only when the number of misses after k
rounds of k-means++ is greater than ∆/2. Then the expected cost of uncovered clusters is at most
O(log(k/∆)) · OPTk(X). That is, E[H(Uk(X) · 1{M(Ck) ≥∆/2}] ≤O(log(k/∆)) · OPTk(X).
Finally, we combine the previous two steps to get bound (1). We argue that if the number of
misses after k rounds of k-means++ is less than ∆/2, then almost all clusters are covered. Hence,
we can apply bound (2) to k′ ≤∆/2 uncovered clusters and ∆remaining rounds of k-means++
and get a 5(1+1/(2e)) approximation. If the number of misses is greater than ∆/2, then the result
from the previous step yields an O(log(k/∆)) approximation.
5.2
Analysis of k-means++
In this section, we analyze the bi-criteria k-means++ algorithm and prove Theorem 5.1. To this end,
we establish the ﬁrst and second bounds from Theorem 5.1 on the expected cost of the clustering
after k + ∆rounds of k-means. We will start with the second bound.
5.2.1
Bi-criteria bound for large ∆
Lemma 5.2. The following bi-criteria bound holds
E [costk+∆(X)] ≤5

1 +
k
e (∆−1)

OPTk(X).
Consider the discrete time Markov chain Ct associated with k-means++ algorithm (see Sec-
tion 3). Let P ∈P be an arbitrary cluster in the optimal solution. Partition all states of the
Markov chain into k + ∆disjoint groups M0, M1, · · · , Mk+∆−1 and H. Each set Mi contains all
states C with i misses that do not cover P: Mi = {C : M(C) = i, P ∩C = ∅} . The set H contains
all states C that cover P: H = {C : P ∩C ̸= ∅}.
We now deﬁne a new Markov chain Xt. To this end, we ﬁrst expand the set of states {C}. For
every state C of the process Ct, we create two additional “virtual” states Ca and Cb. Then, we let
X2t = Ct for every even step 2t, and
X2t+1 =
(
Ca
t ,
if Ct+1 ∈Mi
Cb
t ,
if Ct+1 ∈Mi+1 ∪H.
for every odd step 2t + 1. We stop Xt when Ct stops or when Ct hits the set H (i.e., Ct ∈H).
Loosely speaking, Xt follows Markov chain Ct but makes additional intermediate stops. When Ct
moves from one state in Mi to another state in Mi, X2t+1 stops in Ca
t ; and when Ct moves from
a state in Mi to a state in Mi+1 or H, X2t+1 stops in Cb
t .
Write transition probabilities for Xt:
P [X2t+1 = Ca | X2t = C] = U(X, C) −U(P, C)
cost(X, C)
,
P
h
X2t+1 = Cb | X2t = C
i
= U(P, C) + H(X, C)
cost(X, C)
,
and for all C ∈Mi and C′ = C ∪{x} ∈Mi,
P
X2t+2 = C′ | X2t+1 = Ca =
cost(x, C)
U(X, C) −U(P, C),
11

--- Page 12 ---
for all C ∈Mi and C′ = C ∪{x} ∈Mi+1 ∪H,
P
h
X2t+2 = C′ | X2t+1 = Cbi
=
cost(x, C)
U(P, C) + H(X, C).
Above, U(X, C) −U(P, C) is the cost of points in all uncovered clusters except for P. If we pick a
center from these clusters, we will necessarily cover a new cluster, and therefore X2t+2 will stay in
Mi. Similarly, U(P, C) + H(X, C) is the cost of all covered clusters plus the cost of P. If we pick
a center from these clusters, then X2t+2 will move to Mi+1 or H.
Deﬁne another Markov chain {Yt}. The transition probabilities of {Yt} are the same as the
transition probabilities of Xt except Y never visits states in H and therefore for C ∈Mi and
C′ = C ∪{x} ∈Mi+1, we have
P
h
Y2t+2 = C′ | Y2t+1 = Cbi
= cost(x, C)
H(X, C) .
We now prove a lemma that relates probabilities of visiting states by Xt and Yt.
Lemma 5.3.
For every t ≤k + ∆and states C′ ∈Mi, C′′ ∈M∆, we have
P [C′′ ∈{Xj} | X2t = C′]
P [C′′ ∈{Yj} | Y2t = C′] ≤
 
eH(X, C′′)
eH(X, C′′) + U(P, C′′)
!∆−i
where {C′′ ∈{Xj}} and {C′′ ∈{Yj}} denote the events X visits C′′ and Y visits C′′, respectively.
Proof. Consider the unique path p from C′ to C′′ in the state space of X (note that the transition
graphs for X and Y are directed trees). The probability of transitioning from C′ to C′′ for X and
Y equals the product of respective transition probabilities for every edge on the path. Recall that
transitions probabilities for X and Y are the same for all states but Cb, where C ∈∪jMj. The
number of such states on the path p is equal to the number transitions from Mj to Mj+1, since X
and Y can get from Mj to Mj+1 only through a state Cb on the boundary of Mj and Mj+1. The
number of transitions from Mj to Mj+1 equals ∆−i. For each state Cb on the path, the ratio of
transition probabilities from Cb to the next state C ∪{x} for Markov chains X and Y equals
H(X, C)
U(P, C) + H(X, C) ≤
eH(X, C′′)
U(P, C′′) + eH(X, C′′)
,
here we used that (a) U(P, C) ≥U(P, C′′) since Ut(P) is a non-increasing process; and (b)
H(P, C) ≤eH(P, C′′) since Ht(P) ≤eHt′(P) if t ≤t′ (see Section 3).
We now prove an analog of Corollary 4.5 for eH(X, Yj).
Lemma 5.4.
eH(X, Yt) is a supermartingale.
Proof. If Yj = C, then Yj+1 can only be in
n
Ca, Cbo
. Since eH(X, Ca) = eH(X, Cb) = eH(X, C), we
have E
h
eH(X, Yj+1) | Yj = C
i
= eH(X, Yj).
If Yj = Ca, then Yj+1 = C′ where the new center c should be in uncovered clusters with respect
to Ct.
E
H(P ′, Yj+1) | Yj = Ca, c ∈P ′ ≤5OPT1(P ′),
12

--- Page 13 ---
which implies
E
h
eH(P ′, Yj+1) | Yj = Ca, c ∈P ′i
≤eH(P ′, Yj).
Therefore, we have
E
h
eH(X, Yj+1) | Yj = Cai
≤eH(X, Yj).
If Yj = Cb, then for any possible state C′ of Yj+1, the new center should be in covered clusters with
respect to C. By deﬁnition, we must have eH(X, C′) = eH(X, C) = eH(X, Cb). Thus, it holds that
E
h
eH(X, Yj+1) | Yj = Cbi
= eH(X, Yj).
Combining all these cases, we get
n
eH(X, Yj)
o
is a supermartingale.
We now use Lemma 5.3 and Lemma 5.4 to bound the expected uncovered cost of P after k + ∆
rounds of k-means++.
Lemma 5.5.
For any cluster P ∈P and t ≤k + ∆, we have
E [Uk+∆(P) | Ct] ≤
eHt(X)
e(∆−M(Ct) −1).
Proof. Since k-means++ samples k + ∆centers and the total number of clusters in the optimal
solution P is k, k-means++ must make ∆misses. Hence, the process {Xt} which follows k-means++
must either visit a state in M≥∆or stop in H (recall that we stop process Xt if it reaches H).
If {Xt} stops in group H, then the cluster P is covered which means that Uk+∆(P) = 0. Let
∂M∆be the frontier of M∆i.e., the states that Xt visits ﬁrst when it reaches M∆(recall that
the transition graph of Xt is a tree). The expected cost E [Uk+∆(P) | Ct] is upper bounded by the
expected uncovered cost of P at time when Ct reaches ∂M∆. Thus,
E [Uk+∆(P) | Ct] ≤
X
C∈∂M∆
P [C ∈{Xj} | Ct] U(P, C).
Observe that by Lemma 5.3, for any C ∈∂M∆, we have
P [C ∈{Xj} | Ct] U(P, C) ≤P [C ∈{Yj} | Ct]
 
eH(X, C)
eH(X, C) + U(P, C)
!∆′
U(P, C).
Let f(x) = x(1/(1+ x))∆′. Then, f(x) is maximized at x = 1/(∆′ −1) and the maximum value
f(1/(∆′ −1)) = 1/(e(∆′ −1)). Therefore, for every C ∈∂M∆, we have
P[C ∈{Xj} | Ct]U(P, C) ≤P [C ∈{Yj} | Ct] f
 
U(P, C)
eH(X, C)
!
eH(X, C)
≤P [C ∈{Yj} | Ct]
eH(X, C)
e(∆′ −1).
Let τ = min {j : Yj ∈∂M∆} be the stopping time when Yj ﬁrst visits ∂M∆. We get
X
C∈∂M∆
P [C ∈{Yj} | Ct] eH(X, C) = E
h
eH(X, Yτ) | Ct
i
.
13

--- Page 14 ---
By Lemma 5.4, eH(X, Yj) is a supermartingale. Thus, by the optional stopping theorem,
E
h
eH(X, Yτ) | Ct
i
≤eH(X, Ct).
Therefore, we have
E [Uk+∆(P) | Ct] ≤
eHt(X)
e(∆−M(Ct) −1),
This concludes the proof.
We now add up bounds from Lemma 5.5 with t = 0 for all clusters P ∈P and obtain Lemma 5.2.
5.3
Bi-criteria bound for small ∆
In this section, we give another bi-criteria approximation guarantee for k-means++.
Lemma 5.6. Let costk+∆(X) be the cost of the the clustering resulting from sampling k+∆centers
according to the k-means++ algorithm (for ∆∈{1, . . . , 2k}). Then,
E [costk+∆(X)] ≤5

2 + 1
2e + ln 2k
∆

OPTk(X).
Proof. Consider k-means++ clustering algorithm and the corresponding random process Ct. Fix
a κ ∈{1, . . . , k}. Let τ be the ﬁrst iteration1 (stopping time) when K(Cτ) ≤κ if K(Ck) ≤κ; and
τ = k, otherwise. We refer the reader to Section 3 for deﬁnitions of M(Ct), Ut(X) = U(X, Ct), and
K(Ct).
We separately analyze the cost of uncovered clusters after the ﬁrst τ steps and the last k′ −τ
steps, where k′ = k + ∆is the total number of centers chosen by k-means++.
The ﬁrst step of our proof follows the analysis of k-means++ by Dasgupta (2013), and by
Arthur and Vassilvitskii (2007). Deﬁne a potential function Ψ (see Dasgupta 2013):
Ψt := M(Ct)U(X, Ct)
K(Ct)
.
If K(Ct) = 0, then M(Ct) and U(X, Ct) must be 0 and we let Ψt = 0
We use the following result by Dasgupta (2013) to estimate E[Ψτ(X)] in Lemma 5.8.
Lemma 5.7 (Dasgupta (2013)). For any 0 ≤t ≤k, we have
E [Ψt+1 −Ψt | Ct] ≤H(X, Ct)
K(Ct) .
Lemma 5.8. Then, the following bound holds:
E[Ψτ(X)] ≤5

1 + ln

k
κ + 1

OPTk(X).
1Recall, that K(Ct) is a non-increasing stochastic process with K(C0) = k.
14

--- Page 15 ---
Proof. Note that Ψ1 = 0 as M(C1) = 0. Thus,
E[Ψτ] ≤
τ−1
X
t=1
E
Ψt+1 −Ψt
 ≤E
h τ−1
X
t=1
H(X, Ct)
K(Ct)
i
.
Using the inequality H(X, Ct) ≤eHk(X) (see Section 3), we get:
E[Ψτ] ≤E
h τ−1
X
t=1
eHk(X)
K(Ct)
i
≤E
h
eHk(X) ·
τ−1
X
t=1
1
K(Ct)
i
.
Observe that K(C1), . . . , K(Cτ−1) is a non-increasing sequence in which two consecutive terms are
either equal or K(Ci+1) = K(Ci) −1. Moreover, K(C1) = k and K(Cτ−1) > κ. Therefore, by
Lemma 5.9 (see below), for every realization C0, C1, . . . , Cτ, we have:
τ−1
X
t=1
1
K(Ct) ≤1 + log k/(κ+1).
Thus,
E[Ψτ] ≤(1 + log k/(κ+1))E[ eHk(X)] ≤5(1 + log k/(κ+1)) OPTk(X).
This concludes the proof.
Let κ = ⌊(∆−1)/2⌋. By Lemma 5.8, we have
E
hM(Cτ)Uτ(X)
K(Cτ)
i
≤5

1 + ln 2k
∆

OPTk(X).
Since Ut(X) is a non-increasing stochastic process, we have E[Uk+∆(X)] ≤E[Uτ(X)]. Thus,
E
hM(Cτ)
K(Cτ) · Uk+∆(X)
i
≤5

1 + ln 2k
∆

OPTk(X).
Our goal is to bound E[Uk′(X)]. Write,
E[Uk′(X)] = E
hM(Cτ)
K(Cτ) · Uk′(X)
i
+ E
hK(Cτ) −M(Cτ)
K(Cτ)
· Uk′(X)
i
.
The ﬁrst term on the right hand side is upper bounded by 5
 1+ ln 2k
∆
OPTk(X). We now estimate
the second term, which we denote by (∗).
Note that K(Ct) −M(Ct) = k −t, since the number of uncovered clusters after t steps of
k-means++ equals the number of misses plus the number of steps remaining. Particularly, if τ = k,
we have K(Cτ) −M(Cτ) = K(Ck) −M(Ck) = 0. Consequently, if τ = k, then the second term (∗)
equals 0. Thus, we only need to consider the case, when τ < k. Note that in this case K(Cτ) = κ.
By Lemma 5.2 (applied to all uncovered clusters), we have
E[Uk′(X) | Cτ, τ] ≤
K(Cτ)
e(∆′ −1)
eHτ(X),
where ∆′ = ∆−M(Cτ).
15

--- Page 16 ---
Thus,
E
hK(Cτ) −M(Cτ)
K(Cτ)
· Uk′(X) | Cτ, τ
i
≤K(Cτ) −M(Cτ)
K(Cτ)
·
K(Cτ)
e(∆′ −1) · eHτ(X) = (∗∗).
Plugging in K(Cτ) = κ and the expression for ∆′ (see above), and using that κ ≤(∆−1)/2, we
get
(∗∗) =
κ −M(Cτ)
e(∆−M(Cτ) −1). · eHτ(X) ≤1
2e
eHτ(X).
Finally, taking the expectation over all Cτ, we obtain the bound
E
hK(Cτ) −M(Cτ)
K(Cτ)
· Uk′(X)
i
≤5OPT1(X)
2e
.
Thus, E[Uk′(X)] ≤5(1 + 1/2e + ln 2k/∆)OPTk(X). Therefore,
E[costk′(X)] = E[Hk′(X)] + Uk′(X) ≤5
 2 + 1
2e + ln 2k
∆
 OPTk(X).
We now prove Lemma 5.9.
Lemma 5.9.
For any t ≤k integers a1 ≥a2 ≥· · · ≥at such that a1 = k, at > κ and ai −ai+1 ∈
{0, 1} for all 1 ≤i < t, the following inequality holds
t
X
i=1
1
ai
≤1 + log

k
κ + 1

.
Proof. It is easy to see that the sum is maximized when t = k, and the sequence a1, . . . , ak is as
follows:
1
k,
1
k −1, . . . ,
1
κ + 2
|
{z
}
(k−(κ+1)) terms
,
1
κ + 1, . . . ,
1
κ + 1
|
{z
}
(κ+1) terms
.
The sum of the ﬁrst (k −(κ + 1)) terms is upper bounded by
Z 1/k
1/(κ+1)
1
x dx = ln
k
κ + 1.
The sum of the last (κ + 1) terms is 1.
6
Analysis of k-means∥
In this section, we give a sketch of analysis for the k-means∥algorithm. Speciﬁcally, we show upper
bounds on the expected cost of the solution after T rounds.
16

--- Page 17 ---
Theorem 6.1. The expected cost of the clustering returned by k-means∥algorithm after T rounds
are upper bounded as follows:
for ℓ< k,
E [costT+1(X)] ≤

e−ℓ
k
T
E [cost1(X)] + 5OPTk(X)
1 −e−ℓ
k
;
for ℓ≥k,
E [costT+1(X)] ≤
 k
eℓ
T
E [cost1(X)] + 5OPTk(X)
1 −k/eℓ
.
Remark: For the second bound (ℓ≥k), the additive term 5OPTk(X)/(1 −k/(eℓ)) ≤8OPTk(X).
The probability that a point is sampled by k-means∥is strictly greater than the probability
that it is sampled by k-means∥Pois since 1 −e−λ < λ for all λ > 0. Thus, for every round, we can
couple k-means∥Pois and k-means∥so that each point sampled by k-means∥Pois is also sampled by
k-means∥. Thus, the expected cost returned by k-means∥is at most the expected cost returned
by k-means∥Pois. In the following analysis, we show an upper bound for the expected cost of the
solution returned by k-means∥Pois.
As a thought experiment, consider a modiﬁed k-means∥Pois algorithm. This algorithm is given
the set X, parameter k, and additionally the optimal solution P = {P1, . . . , Pk}. Although this
modiﬁed algorithm is useless in practice as we do not know the optimal solution in advance, it will
be helpful for our analysis.
In every round t, the modiﬁed algorithm ﬁrst draws independent Poisson random variables
Zt(Pi) ∼Pois(λt(Pi)) for every cluster i ∈{1, . . . , k} with rate λt(Pi) = P
x∈Pi λt(x). Then, for
each i ∈{1, . . . , k}, it samples Zt(Pi) points x ∈Pi with repetitions from Pi, picking every point
x with probability λt(x)/λt(Pi) and adds them to the set of centers Ct. We assume that points in
every set Ct are ordered in the same way as they were chosen by this algorithm.
We claim that the distribution of the output sets CT of this algorithm is exactly the same as
in the original k-means∥Pois algorithm. Therefore, we can analyze the modiﬁed algorithm instead
of k-means∥Pois, using the framework described in Sections 3.
Lemma 6.2.
The sets Ct in the original and modiﬁed k-means∥Pois algorithms are identically
distributed.
Proof. Consider |Pi| independent Poisson point processes Nx(a) with rates λt(x), where x ∈Pi
(here, we use variable a for time). Suppose we add a center x at step t of the algorithm if Nx(t) ≥1.
On the one hand, the probability that we choose x is equal to 1 −e−λt(x) which is exactly the
probability that k-means∥Pois picks x as a center at step t. On the other hand, the sum NPi =
P
x∈Pi Nx is a Poisson point process with rate λt(Pi). Thus, the total number of jumps in the
interval [0, 1] of processes Nx with x ∈Pi is distributed as Zt(Pi). Moreover, the probability that
Nx jumps at time a conditioned on the event that NPi jumps at time a is λt(x)/λt(Pi). Thus, for
every jump of NPi, we choose one random center x with probability λt(x)/λt(Pi).
Lemma 6.3.
For k-means∥algorithm with parameter ℓ, the following bounds hold:
for ℓ< k,
E [costt+1(X)] ≤e−ℓ
k · E [costt(X)] + 5OPTk(X);
for ℓ≥k,
E [costt+1(X)] ≤
 k
eℓ

· E [costt(X)] + 5OPTk(X).
Proof. Since the expected cost returned by k-means∥is at most the expected cost returned by
k-means∥Pois, we analyze the expected cost of the clustering after one step of k-means∥Pois.
17

--- Page 18 ---
If the algorithm covers cluster Pi at round t, then at the next round, its uncovered cost equals
0. The number of centers chosen in Pi is determined by the Poisson random variable Zt+1(Pi).
Hence, Pi is uncovered at round t + 1 only if Zt+1(Pi) = 0. Since Ut(Pi) is non-increasing in t and
Ut(Pi) ≤costt(Pi), we have
E [Ut+1(Pi) | Ct] ≤P [Zt+1(Pi) = 0] Ut(Pi) ≤exp

−ℓcostt(Pi)
costt(X)

costt(Pi).
Deﬁne two function: f(x) = e−x · x; and g(x) = f(x) for x ∈[0, 1] and g(x) = e−1 for x ∈[1, ∞).
Then,
E [Ut+1(X) | Ct] ≤
 
1
k
k
X
i=1
f
ℓcostt(Pi)
costt(X)
!
kcostt(X)
ℓ
.
Since g(x) ≤f(x), and g(x) is concave for x ≥0, we have
E [Ut+1(X) | Ct] ≤
 
1
k
k
X
i=1
g
ℓcostt(Pi)
costt(X)
!
kcostt(X)
ℓ
≤g
 ℓ
k
 kcostt(X)
ℓ
.
Here, we use that P
i costt(Pi) = costt(X).
Therefore, for ℓ≤k, we have
E [Ut+1(X) | Ct] ≤

e−ℓ
k

costt(X);
and for ℓ≥k, we have
E [Ut+1(X) | Ct] ≤
 k
eℓ

costt(X).
Similar to Corollary 4.5, the process eHt(P) for k-means∥Pois is also a supermartingale, which
implies E [Ht+1(X)] ≤5OPTk(X). This concludes the proof.
Proof of Theorem 6.1. Applying the bound from Lemma 6.3 for t times, we get the following results.
For ℓ≤k,
E [costt+1(X)] ≤

e−ℓ
k
t
E [cost1(X)] + 5OPTk(X)ηt,
where ηt = Pt
j=1

e−ℓ
k
j−1 <
1
1−e−ℓ
k . For ℓ≥k,
E [costt+1(X)] ≤
 k
eℓ
t
E [cost1(X)] + 5OPTk(X)ηt,
where ηt = Pt
j=1

k
eℓ
j−1 ≤
1
1−k
eℓ.
Corollary 6.4.
Consider a data set X with more than k distinct points. Let
T = ln E
 cost1(X)
OPTk(X))

and ℓ> k. Then, after T rounds of k-means∥, the expected cost of clustering E [costT (X)] is at
most 9OPTk(X).
18

--- Page 19 ---
7
Exponential Race k-means++ and Reservoir Sampling
In this section, we show how to implement k-means++ algorithm in parallel using R passes over the
data set. This implementation, which we refer to as k-means++ER (exponential race k-means++),
is very similar to k-means∥, but has stronger theoretical guarantees. Like k-means∥, in every round,
k-means++ER tentatively selects ℓcenters, in expectation. However, in the same round, it removes
some of the just selected centers (without making another pass over the data set). Consequently,
by the end of each iteration, the algorithm keeps at most k centers.
We can run k-means++ER till it samples exactly k centers; in which case, the distribution of k
sampled centers is identical to the distribution of the regular k-means++, and the expected number
of rounds or passes over the data set R is upper bounded by
O
k
ℓ+ log OPT1(X)
OPTk(X)

.
We note that R is never greater than k. We can also run this algorithm for at most R∗rounds.
Then, the expected cost of the clustering is at most
5(ln k + 2) OPTk(X) + 5R∗
 4k
eℓR∗
R∗
· OPT1(X).
7.1
Algorithm
In this section, we give a high level description of our k-means++ER algorithm. In Section 7.2, we
show how to eﬃciently implement k-means++ER using lazy updates and explain why our algorithm
makes R passes over the data set.
The algorithm simulates n continuous-time stochastic processes.
Each stochastic process is
associated with one of the points in the data set. We denote the process corresponding to x ∈X
by Pt(x). Stochastic process Pt(x) is a Poisson process with variable arrival rate λt(x).
The algorithm chooses the ﬁrst center c1 uniformly at random in X and sets the arrival rate
of each process Pt(x) to be λt(x) = cost(x, {c1}). Then, it waits till one of the Poisson processes
Pt(x) jumps. When process Pt(x) jumps, the algorithm adds the point x ∈X (corresponding to
that process) to the set of centers Ct and updates the arrival rates of all processes to be
λt(y) = cost(y, Ct)
for all y ∈X. Note that if y is a center, then the arrival rate λt(y) is 0.
The algorithm also maintains a round counter R. In the lazy version of this algorithm (which
we describe in the next section), the algorithm makes a pass over the data set and samples a new
batch of centers every time this counter is incremented. Additionally, at the end of each round, the
algorithm checks if it chose at least one center in that round, and in the unlikely event that it did
not, it selects one center with probability proportional to the costs of the points.
Initially, the algorithm sets R = 0, t0 = 0, and t1 = ℓ/cost(X, {c1}). Then, at each time point
ti (i ≥1), we increment R and compute
ti+1 = ti + ℓ/cost(X, Cti),
where Cti is the set of all centers selected before time ti. We refer to the time frame [ti−1, ti] for
i ≥1 as the i-th round. The algorithm stops when one of the following conditions holds true (1) the
19

--- Page 20 ---
number of sampled centers is k; or (2) the round counter R equals the prespeciﬁed threshold R∗,
which may be ﬁnite or inﬁnite.
Before analyzing this algorithm, we mention that every Poisson process Pt with a variable
arrival rate λt can be coupled with a Poisson process Qs with rate 1. To this end, we substitute
the variable
s(t) =
Z t
0
λτdτ,
and let
Pt ≡Qs(t).
Observe that the expected number of arrivals for process Qs in the inﬁnitesimal interval [s, s + ds]
is ds = λtdt which is exactly the same as for process Pt.
It is convenient to think about the variables s as “current position”, t as “current time”, and λt
as “current speed” of s. To generate process Pt(x), we can ﬁrst generate Poisson process Qs(x) with
arrival rate 1 and then move the position st(x) with speed λt(x). The process Pt(x) = Qst(x)(x) is
a Poisson process with variable arrival rate λt(x).
Theorem 7.1. I. If the number of rounds is not bounded (i.e., R∗= ∞), then the distribution of
centers returned by k-means++ER is identical to the distribution of centers returned by k-means++.
II. Moreover, the expected number of rounds R is upper bounded by
(1 + ok(1)) ·

⌈k
ℓ⌉+ log 2 OPT1(X)
OPTk(X)

,
and never exceeds k.
III. If the threshold R∗is given (R∗< ∞), then the cost of the solution after R∗rounds is upper
bounded by
5(ln k + 2) OPTk(X) + 2R∗
 4k
eℓR∗
R∗
· OPT1(X).
Proof of Part I. For the sake of analysis, we assume that after the algorithm outputs solution C, it
does not terminate, but instead continues to simulate Poisson processes Pt(x). It also continues to
update the set Ct (but, of course, not the solution) and the arrival rates λt(x) till the set Ct contains
k centers. Once |Ct| = k, the algorithm stops updating the set of centers Ct and arrival rates but
still simulates continuous-time processes Pt(x). Clearly, this additional phase of the algorithm does
not aﬀect the solution since it starts after the solution is already returned to the user.
We prove by induction on i that the ﬁrst i centers c1, . . . , ci have exactly the same joint distri-
bution as in k-means++. Indeed, the ﬁrst center c1 is drawn uniformly at random from the data
set X as in k-means++. Suppose centers c1, . . . , ci are already selected. Then, we choose the next
center ci+1 at the time of the next jump of one of the Poisson processes Pt(x). Observe that the
conditional probability that a particular process Pt(x) jumps given that one of the processes Pt(y)
(y ∈X) jumps is proportional to λt(x), which in turn equals the current cost(x, Ct) of point x.
Hence, the distribution of center ci+1 is the same as in k-means++. This completes the proof of
item I.
Proof of Part II. We now show items II and III. Deﬁne process
Pt(X) =
X
x∈X
Pt(x).
20

--- Page 21 ---
Its rate λt(X) equals P
x∈X λt(x). We couple this process with a Poisson Qs(X) with arrival rate
1 as discussed above. We want to estimate the number of centers chosen by the algorithm in the
ﬁrst R′ rounds. To this end, we count the number of jumps of the Poisson process Pt(X) (recall
that we add a new center to Ct whenever Pt(X) jumps unless |Ct| already contains k centers). The
number of jumps equals PtR′ which, in turn, equals QsR′ where sR′(X) is the position of s(X) at
time tR′:
sR′(X) =
Z tR′
0
λτ(X) dτ =
R′−1
X
i=0
Z ti+1
ti
λτ(X) dτ ≥
R′−1
X
i=0
(ti+1 −ti) · λti+1(X).
Here, we used that λt(X) is non-increasing, and thus, λti+1(X) ≤λτ(X) for all τ ∈[ti, ti+1]. We
now recall that (ti+1 −ti) = ℓ/cost(X, Cti) and λti+1(X) = cost(X, Cti+1). Hence,
sR′(X) ≥ℓ
R′−1
X
i=0
cost(X, Cti+1)
cost(X, Cti) .
By the inequality of arithmetic and geometric means, we have
sR′(X) ≥ℓ· R′
 R′−1
Y
i=0
cost(X, Cti+1)
cost(X, Cti)
!1/R′
= ℓ· R′
 
cost(X, CtR′ )
cost(X, Ct0)
!1/R′
(5)
= ℓ· R′
 
cost(X, CtR′ )
cost(X, {c1})
!1/R′
.
We now use this equation to prove items II and III. For item II, we let random variable R′ to
be
R′ = 2e⌈k/ℓ⌉+ log cost(X, {c1})
OPTk(X)
.
Note that R′ depends on the ﬁrst center c1 (which is chosen in the very beginning of the algorithm)
but not on the Poisson processes Pt(x).
Since, Ct always contains at most k centers, we have
cost(x, CtR′ ) ≥OPTk(X), and consequently
sR′(X) ≥ℓ· R′
 
OPTk(X)
cost(X, {c1})
!1/R′
> ℓ· 2e⌈k/ℓ⌉· 1/e ≥2k.
The expected number of jumps of the Poisson process Qs(X) in the interval [0, sR′(X)] equals
QsR(X)(X). Observe that
QsR(X)(X) ≥Q2k(X)
and Q2k(X) is a Poisson random variable with parameter 2k. By the Chernoﬀbound2, it makes
fewer than k jumps with exponentially small probability in k; namely, with probability at most
(e/2)−k. Thus, with probability at least 1 −(e/2)−k, the algorithm selects k centers in the ﬁrst R′
rounds. Moreover, if it does not happen in the ﬁrst R∗rounds, then it selects k centers by the end
of the second R′ rounds again with probability at least 1 −(e/2)−k and so on. Hence, the expected
2We use the bound Pr{P ≤k} ≤e−λ eλ/kk, where P is a Poisson random variable with parameter λ and k < λ.
See e.g., Theorem 5.4.2 in Mitzenmacher and Upfal (2017).
21

--- Page 22 ---
number of rounds till it selects k centers is (1 + ok(1))R′. Finally, observe that the expectation of
cost(X, {c1}) over the choice of the ﬁrst center equals 2 OPTk(X). Since log(·) is a convex function,
we have
E[R′] ≤2e⌈k/ℓ⌉+ log 2 OPT1(X)
OPTk(X) .
Therefore, we showed that the expected number of rounds is upper bounded by the right hand side
of the expression above times a multiplicative factor of (1+ok(1)). A slightly more careful analysis
gives a bound of
(1 + ok(1))
 
e⌈k/ℓ⌉+ log 2 OPT1(X)
OPTk(X)
!
.
This concludes the proof of item II.
Proof of Part III. We now prove item III. Denote T = tR∗. Consider the event
E =
algorithm samples k centers in the ﬁrst R∗rounds
	.
Let ¯E be the complimentary events to E. Then,
E
cost(X, CT )
 = E
cost(X, CT ) · 1(E)
 + E
cost(X, CT ) · 1( ¯E)
.
We now separately upper bound each of the terms on the right hand side. It is easy to upper bound
the ﬁrst term:
E[cost(X, CT ) · 1(E)] ≤5(ln k + 2) · OPTk(X),
because the distribution of centers returned by k-means++ER is identical to the distribution of
centers returned by k-means++. We now bound the second term. Denote by Dρ the event
Dρ =

cost(X, CT ) ≥
 ρk
ℓR∗
R∗
cost(X, {c1})

.
We prove the following claim.
Claim 7.2. The following inequality holds for every real number ρ ∈[1, ℓR∗/k] and any choice of
the ﬁrst center c1:
Pr
  ¯E and Dρ | c1
 ≤e−(ρ−1)kρk−1.
Proof. We use inequality (5) with R′ = R∗:
sR∗(X) ≥ℓ· R∗
 
cost(X, CT )
cost(X, {c1})
!1/R∗
.
It implies that sR∗(X) ≥ρk if event Dρ occurs. On the other hand if ¯E occurs, then the number
of centers chosen by the end of round R∗is less than k and, consequently, the number of jumps of
Pt(X) in the interval [0, T] is less than k:
PT (X) ≡QsR∗(X)(X) < k.
22

--- Page 23 ---
Hence, we can bound Pr( ¯E and Dρ | c1) as follows:
Pr( ¯E and Dρ) ≤Pr
 Dρ and QsR∗(X) < k | c1
 ≤
≤Pr
 Dρ and Qρk(X) < k | c1
 ≤Pr
 Qρk(X) < k | c1
.
Random variable Qρk(X) has the Poisson distribution with parameter ρk and is independent of c1.
By the Chernoﬀbound, the probability that Qρk(X) ≤k −1 is at most (as in Part II of the proof):
Pr
Qρk(X) ≤k −1
	 ≤e−ρk eρk
k −1
k−1
= e−(ρ−1)k−1ρk−1 ·

k
k −1
k−1
|
{z
}
≤e
≤e−(ρ−1)kρk−1.
This completes the proof of Claim 7.2.
Let
Z =
ℓR∗
k
R∗
· cost(X, CT )
cost(X, {c1}).
Then, by Claim 7.2,
Pr
  ¯E and Z ≥ρR∗| c1
 ≤e−(ρ−1)kρk−1.
(6)
Write,
E
1( ¯E) · Z | c1
 =
Z ∞
0
Pr
 1( ¯E) and Z ≥r | c1
dr ≤1 +
Z ∞
1
Pr
 1( ¯E) and Z ≥r | c1
 dr.
We now substitute r = ρR∗and then use (6):
E
Z · 1( ¯E) | c1
 ≤1 + R∗
Z ∞
1
Pr
  ¯E and Z ≥ρR∗| c1
 · ρR∗−1dρ
≤1 + R∗
Z ∞
1
e−(ρ−1)kρk+R∗−2dρ.
We note that R∗< k, since our algorithm chooses at least one center in each round. Thus,
by Lemma 7.3 (which we prove below), the integral on the right hand side is upper bounded by
eR∗/2 · (4/e)R∗. Hence,
E
Z · 1( ¯E) | c1
 ≤1 + R∗·
4
e
R∗−2
.
Multiplying both sides of the inequality by (k/ℓR∗)R∗·cost(X, {c1}) and taking the expectation over
c1, we get the desired inequality:
E
cost(X, CT ) · 1( ¯E)
 ≤

1 + R∗
4
e
R∗ k
ℓR∗
R∗
Ec1
cost(X, {c1}

=

1 + R∗4
e
R∗−2 k
ℓR∗
R∗
· 2 OPT1(X)
< 2R∗
 4k
eℓR∗
R∗
OPT1(X).
This ﬁnishes the proof of Theorem 7.1.
23

--- Page 24 ---
Lemma 7.3. For R∗< k, we have
Z ∞
1
e−(ρ−1)kρk+R∗−2dρ ≤e
2
4
e
R∗
.
Proof. Since e−(ρ−1)ρ ≤1 for all ρ ≥1, we have e−(ρ−1)kρk ≤e−(ρ−1)R∗ρR∗for any R∗< k. Thus,
we have
Z ∞
1
e−(ρ−1)kρk+R∗−2dρ ≤
Z ∞
1
e−(ρ−1)R∗ρ2R∗−3dρ = eR∗Z ∞
1
e−ρR∗ρ2R∗−3dρ
= eR∗Z ∞
1
(e−ρρ2)R∗ρ−3dρ.
Observe that e−ρρ2 ≤4/e2 for any ρ ≥1. Hence,
(e−ρρ2)R∗= (e−ρρ2)R∗−1 · e−ρρ2 ≤(4/e2)R∗−1e−ρρ2.
Thus,
Z ∞
1
e−(ρ−1)kρk+R∗−2dρ ≤4R∗−1 · eR∗
e2(R∗−1)
·
Z ∞
1
e−ρ
ρ
dρ = 4R∗−1
eR∗−2 · 1
4 =
4
e
R∗−2
.
7.2
Lazy implementation of k-means++ER
We now describe how we can eﬃciently implement the k-means++ER algorithm using a lazy reser-
voir sampling. We remind the reader that the time of the ﬁrst jump of a Poisson process with
parameter λ is distributed as the exponential distribution with parameter λ. Imagine for a mo-
ment, that the arrival rates of our Poisson processes were constant. Then, in order to select the
ﬁrst k jumps, we would generate independent exponential random variables with parameters λ(x)
for all x and choose k smallest values among them. This algorithm is known as the reservoir sam-
pling(see Efraimidis and Spirakis (2006)). To adapt this algorithm to our needs, we need to update
the arrival rates of the exponential random variables. Loosely speaking, we do so by generating
exponential random variables with rate 1 for Poisson processes Qs(x) which are described above
and then updating the speeds λt(x) of variables st(x). We now formally describe the algorithm.
In the beginning of every round i, we recompute costs of all points in the data set.
Then,
we draw an independent exponential random variable Sx with rate 1 for every point x, and let
St(x) = Sx . We set
τt(x) = St(x)
λt(x).
Think of St(x) as the distance st(x) needs to travel till process Qs(x) jumps; λt(x) is the speed
of point st(x); and τt(x) is the time left till Qs(x) = Pt(x) jumps if the speed λt does not change.
Among all points x ∈X, we select a tentative set of centers Z for this round. The set Z contains all
points x with ti−1 + τt(x) ≤ti. This is the set of all points for which their Poisson processes would
jump in the current round if their arrival rates remained the same till the end of the round. Since
the arrival rates can only decrease in our algorithm, we know for sure that for points x outside of
Z, the corresponding processes Pt(x) will not jump in this round. Thus, we can safely ignore those
points during the current round.
24

--- Page 25 ---
We also note that in the unlikely event that the initial set Z is empty, we choose x with the
smallest time τt(x) and add it to the set of centers Ct. (This is equivalent to choosing a point
with probability proportional to cost(x, Ct) by the memorylessness property of the exponential
distribution).
The steps we described above – updating costs cost(x, Ct), drawing exponential random vari-
ables Sx, and selecting points in the set Z – can be performed in parallel using one pass over the
data set. In the rest of the current round, our algorithm deals only with the set Z whose size in
expectation is at most ℓ(see below).
While the set Z is not empty we do the following. We choose x ∈Z with the smallest value of
τt(x). This x corresponds to the process that jumps ﬁrst. Then, we perform the following updates:
We add x to the set of centers Ct. We set the “current time” t to t = t′ + τt′(x), where t′ is the
time of the previous update. If x is the ﬁrst center selected in the current round, then we let t′ to
be the time when the round started (i.e., ti−1). We recompute the arrival rates (speeds) λt(x) for
each x in Z. Finally, we update the values of all τt(x) for x ∈Z using the formula
τt(x) = St(x) −λt′(x) · (t −t′)
λt(x)
,
here λt′(x) · (t −t′) is the distance variable st(x) moved from the position where it was at time t′;
St(x) −λt′(x) · (t −t′) is the remaining distance st(x) needs to travel till the process Qt(x) jumps;
and τt(x) is the remaining time till Pt(x) jumps if we do not update its arrival rate. After we
update τt(x), we prune the set Z. Speciﬁcally, we remove from set Z all points x with t+τt(x) > ti.
As before, we know for sure that if x is removed from Z, then the corresponding processes Pt(x)
will not jump in the current round.
This algorithm simulates the process we described in the previous section. The key observation
is that Poisson processes Pt(x) we associate with points x removed from Z cannot jump in this
round and thus can be safely removed from our consideration. We now show that the expected size
of the set Z is at most ℓ. In the next section, we analyze the running time of this algorithm.
Then we show that the expected size of the set Z in the beginning of each round i + 1 is at
most ℓ. Since every point x belongs to Z with probability
Pr{x ∈Z} = Pr

Sx
cost(x, Cti) ≤
ℓ
cost(X, Cti)

= Pr

Sx ≤ℓ· cost(x, Cti)
cost(X, Cti)

.
The right hand side is the probability that the Poisson process Qs(x) with rate 1 jumps in the inter-
val of length ℓ· cost(x, Cti)/cost(X, Cti) which is upper bounded by the expected number of jumps
of Qs(x) in this interval. The expected number of jumps exactly equals ℓ· cost(x, Cti)/cost(X, Cti).
Thus, the expected size of Z is upper bounded as
E|Z| =
X
z∈X
Pr{z ∈Z} ≤
X
z∈X
ℓ· cost(z, Cti)
cost(X, Cti) = ℓ.
7.3
Run time analysis
According to our analysis above, the number of new centers chosen at each round of k-means++ER
is at most the size of set Z, which is O(ℓ) with high probability. In the beginning of every round,
we need to update costs of all data points, which requires O(nℓd) time. In each round, we also
25

--- Page 26 ---
need to maintain the rates of all points in set Z, which needs O(ℓ2d) time. Thus, the total running
time for k-means++ER with R rounds is O(Rnℓd). We note that before running our algorithm, we
can reduce the dimension d of the space to O(log k) using the Johnson–Lindenstrauss transform
(see Johnson and Lindenstrauss (1984)). This will increase the approximation factor by a factor of
(1 + ε) but make the algorithm considerably faster (see Makarychev et al. (2019), Becchetti et al.
(2019), and Boutsidis et al. (2010)).
References
A. Aggarwal, A. Deshpande, and R. Kannan. Adaptive sampling for k-means clustering. In Approx-
imation, Randomization, and Combinatorial Optimization. Algorithms and Techniques, pages
15–28. Springer, 2009.
S. Ahmadian, A. Norouzi-Fard, O. Svensson, and J. Ward. Better guarantees for k-means and
euclidean k-median by primal-dual algorithms. SIAM Journal on Computing, pages FOCS17–97,
2019.
D. Aloise, A. Deshpande, P. Hansen, and P. Popat.
Np-hardness of euclidean sum-of-squares
clustering. Machine learning, 75(2):245–248, 2009.
D. Arthur and S. Vassilvitskii. k-means++: The advantages of careful seeding. In Proceedings of
the eighteenth annual ACM-SIAM symposium on Discrete algorithms, pages 1027–1035. Society
for Industrial and Applied Mathematics, 2007.
P. Awasthi, M. Charikar, R. Krishnaswamy, and A. K. Sinop. The hardness of approximation of
euclidean k-means. In 31st International Symposium on Computational Geometry (SoCG 2015).
Schloss Dagstuhl-Leibniz-Zentrum fuer Informatik, 2015.
O. Bachem, M. Lucic, and A. Krause. Distributed and provably good seedings for k-means in
constant rounds.
In Proceedings of the 34th International Conference on Machine Learning-
Volume 70, pages 292–300. JMLR. org, 2017.
B. Bahmani, B. Moseley, A. Vattani, R. Kumar, and S. Vassilvitskii. Scalable k-means++. Pro-
ceedings of the VLDB Endowment, 5(7):622–633, 2012.
L. Becchetti, M. Bury, V. Cohen-Addad, F. Grandoni, and C. Schwiegelshohn. Oblivious dimension
reduction for k-means: beyond subspaces and the johnson–lindenstrauss lemma. In Proceedings
of the 51st Annual ACM SIGACT Symposium on Theory of Computing, pages 1039–1050, 2019.
B. Boehmke and B. M. Greenwell. Hands-on machine learning with R. CRC Press, 2019.
C. Boutsidis, A. Zouzias, and P. Drineas. Random projections for k-means clustering. In Advances
in Neural Information Processing Systems, pages 298–306, 2010.
T. Brunsch and H. Röglin. A bad instance for k-means++. Theoretical Computer Science, 505:
19–26, 2013.
D. Choo, C. Grunau, J. Portmann, and V. Rozhoň. k-means++: few more steps yield constant
approximation. In Proceedings of the 37th International Conference on Machine Learning, pages
7849–7057. JMLR. org, 2020.
26

--- Page 27 ---
S. Dasgupta. The hardness of k-means clustering. Department of Computer Science and Engineer-
ing, University of California, San Diego, 2008.
S. Dasgupta.
UCSD CSE 291,
Lecture Notes:
Geometric
Algorithms,
2013.
URL:
https://cseweb.ucsd.edu/~dasgupta/291-geom/kmeans.pdf. Last visited on 2020/06/01.
D. Dua and C. Graﬀ. UCI ML repository, 2017. URL http://archive.ics.uci.edu/ml.
P. S. Efraimidis and P. G. Spirakis. Weighted random sampling with a reservoir. Information
Processing Letters, 97(5):181–185, 2006.
R. Elber. Kdd-Cup, 2004. URL http://osmot.cs.cornell.edu/kddcup/.
W. B. Johnson and J. Lindenstrauss. Extensions of lipschitz mappings into a hilbert space. Con-
temporary mathematics, 26(189-206):1, 1984.
T. Kanungo, D. M. Mount, N. S. Netanyahu, C. D. Piatko, R. Silverman, and A. Y. Wu.
A
local search approximation algorithm for k-means clustering. Computational Geometry, 28(2):
89 – 112, 2004.
ISSN 0925-7721.
doi: https://doi.org/10.1016/j.comgeo.2004.03.003.
URL
http://www.sciencedirect.com/science/article/pii/S0925772104000215.
S. Lattanzi and C. Sohler.
A better k-means++ algorithm via local search.
In International
Conference on Machine Learning, pages 3662–3671, 2019.
E. Lee, M. Schmidt, and J. Wright. Improved and simpliﬁed inapproximability for k-means. Infor-
mation Processing Letters, 120:40–43, 2017.
S. Lloyd. Least squares quantization in pcm. IEEE transactions on information theory, 28(2):
129–137, 1982.
K. Makarychev, Y. Makarychev, M. Sviridenko, and J. Ward. A bi-criteria approximation algorithm
for k-means. Approximation, Randomization, and Combinatorial Optimization. Algorithms and
Techniques, 2016.
K. Makarychev, Y. Makarychev, and I. Razenshteyn. Performance of johnson–lindenstrauss trans-
form for k-means and k-medians clustering. In Proceedings of the 51st Annual ACM SIGACT
Symposium on Theory of Computing, pages 1027–1038, 2019.
M. Mitzenmacher and E. Upfal. Probability and computing: Randomization and probabilistic tech-
niques in algorithms and data analysis. Cambridge university press, 2017.
R. Ostrovsky, Y. Rabani, L. Schulman, and C. Swamy. The eﬀectiveness of lloyd-type methods
for the k-means problem. In 2006 47th Annual IEEE Symposium on Foundations of Computer
Science (FOCS’06), 2006.
V. Rozhoň.
Simple and sharp analysis of k-means||.
In Proceedings of the 37th International
Conference on Machine Learning, pages 7828–7837. JMLR. org, 2020.
D. Wei. A constant-factor bi-criteria approximation guarantee for k-means++. In Advances in
Neural Information Processing Systems, pages 604–612, 2016.
27

--- Page 28 ---
Appendix
In this appendix, we present our experiments, give proofs omitted in the main part of the paper,
and provide complimentary lower bounds.
A
Experiments
In this section, we present plots that show that the performance of k-means∥and “k-means++
with oversampling and pruning” algorithms are very similar in practice. Below, we compare the
following algorithms on the datasets BioTest from KDD Cup 2004 (Elber, 2004) and COVTYPE
from the UCI ML repository (Dua and Graﬀ, 2017):
• Regular k-means++. The performance of this algorithm is shown with a solid black line on
the plots below.
• k-means∥without pruning. This algorithm samples k centers using k-means∥with T = 5
rounds and ℓ= k/T.
• k-means∥. This algorithm ﬁrst samples 5k centers using k-means∥and then subsamples k
centers using k-means++. The performance of this algorithm is shown with a dashed blue
line on the plots below.
• k-means++ with oversampling and pruning. This algorithm ﬁrst samples 5k centers using
k-means++ and then subsamples k centers using k-means++.
The performance of this
algorithm is shown with a thin red line on the plots below.
For each k = 5, 10, · · · , 200, we ran these algorithms for 50 iterations and took their average.
We normalized all costs by dividing them by the cost of k-means++ with k = 1000 centers.
0
50
100
150
200
2
4
6
8
10
#centers
cost
BioTest
k-means++
BiCriteria k-means++ w/Pruning
k-means∥
10
20
30
40
50
3
4
5
6
#centers
cost
BioTest
k-means++
BiCriteria k-means++ w/Pruning
k-means∥
28

--- Page 29 ---
0
50
100
150
200
0
20
40
#centers
cost
COVTYPE
k-means++
BiCriteria k-means++ w/Pruning
k-means∥
10
20
30
40
50
10
15
20
25
30
#centers
cost
COVTYPE
k-means++
BiCriteria k-means++ w/Pruning
k-means∥
0
50
100
150
200
2
4
6
8
10
#centers
cost
BioTest
k-means++
k-means∥without Pruning
10
20
30
40
50
4
6
8
10
#centers
cost
BioTest
k-means++
k-means∥without Prunning
0
50
100
150
200
0
20
40
60
#centers
cost
COVTYPE
k-means++
k-means∥without Prunning
10
20
30
40
50
10
20
30
40
50
60
#centers
cost
COVTYPE
k-means++
k-means∥without Pruning
29

--- Page 30 ---
B
Details for Preliminaries
For any set of points Y ⊂Rd, let µ = P
x∈Y x/ |Y| be the centroid of the cluster Y. Then, the
optimal cost of Y with one center,
OPT1(Y) =
X
x∈Y
∥x −µ∥2 =
P
(x,y)∈Y×Y∥x −y∥2
2 |Y|
.
This is a well known formula which is often used for analyzing of k-means algorithms. For com-
pleteness, we give a proof below.
Proof. Consider any point z ∈Rd, then we have:
cost(Y, {z}) =
X
x∈Y
∥x −z∥2 =
X
x∈Y
∥(x −µ) + (µ −z)∥2
=
X
x∈Y

∥x −µ∥2 + ∥µ −z∥2 + 2 ⟨x −µ, µ −z⟩

=
X
x∈Y
∥x −µ∥2 + |Y| · ∥µ −z∥2 + 2
* X
x∈Y
(x −µ), µ −z
+
=
X
x∈Y
∥x −µ∥2 + |Y| · ∥µ −z∥2.
Thus, the optimal choice of z to minimize cost(Y, {z}) is µ and OPT1(Y) = P
x∈Y∥x −µ∥2.
X
x∈Y
∥x −µ∥2 =
X
x∈Y
⟨x −µ, x −µ⟩=
X
x∈Y
⟨x, x −µ⟩
=
X
x∈Y
*
x, x −
X
y∈Y
y
|Y|
+
=
1
|Y|
X
(x,y)∈Y×Y
⟨x, x −y⟩
=
1
2 |Y|


X
(x,y)∈Y×Y
⟨x, x −y⟩+
X
(x,y)∈Y×Y
⟨y, y −x⟩


=
P
(x,y)∈Y×Y∥x −y∥2
2 |Y|
.
C
Lower bounds
C.1
Lower bound on the cost of covered clusters
We show the following lower bound on the expected cost of a covered cluster in k-means++. There-
fore, the 5-approximation in Lemma 4.1 is tight.
Theorem C.1. For any ε > 0, there exists an instance of k-means such that for a set P ∈X
and a set of centers C ∈Rd, if a new center c is sampled from P with probability Pr(c = x) =
cost(x, C)/cost(P, C), then
Ec [cost(P, C ∪{c})] ≥(5 −ε)OPT1(P).
30

--- Page 31 ---
Proof. Consider the following one dimensional example, where P contains t points at 0 and one
point at 1, and the closest center already chosen in C to P is at −1.
−1
0
t
1
1
The new center c will be chosen at 0 with probability
t
t+4, and at 1 with probability
4
t+4. Then,
the expected cost of P is
Ec [cost(P, C ∪{c})] = 1 ·
t
t + 4 + t ·
4
t + 4 =
5t
t + 4;
and the optimal cost of P is OPT1(P) ≤1. Thus, by choosing t ≥4(5 −ε)/ε, we have
Ec [cost(P, C ∪{c})] ≥(5 −ε)OPT1(P).
C.2
Lower bound on the bi-criteria approximation
In this section, we show that the bi-criteria approximation bound of O(ln k
∆) is tight up to constant
factor. Our proof follows the approach by Brunsch and Röglin (2013). We show the following
theorem.
Theorem C.2.
For every k > 1 and ∆≤k, there exists an instance X of k-means such that the
bi-criteria k-means++ algorithm with k + ∆centers returns a solution of cost greater than
1
8 log k
∆· OPTk(X)
with probability at least 1 −e−
√
k/2.
Remark: This implies that the expected cost of bi-criteria k-means with k + ∆centers is at
least
1 −e−
√
k/2
8
· log k
∆· OPTk(X).
Proof. For every k and ∆≥
√
k, we consider the following instance. The ﬁrst cluster is a scaled
version of the standard simplex with N ≫k vertices centered at the origin, which is called the heavy
cluster. The length of the edges in this simplex is 1/
√
N −1. Each of the remaining k −1 clusters
contains a single point on k −1 axes, which are called light clusters. These clusters are located at
distance √α from the center of the heavy cluster and
√
2α from each other, where α = ln(k/∆)
4∆
.
For the sake of analysis, let us run k-means++ till we cover all clusters. At the ﬁrst step, the
k-means++ algorithm almost certainly selects a center from the heavy cluster since N ≫k. Then,
at each step, the algorithm can select a center either from one of uncovered light clusters or from
the heavy cluster. In the former case, we say that the algorithm hits a light cluster, and in the latter
case we say that the algorithm misses a light cluster. Below, we show that with high probability
the algorithm makes at least 2∆misses before it covers all but ∆light clusters.
31

--- Page 32 ---
Lemma C.3. Let ∆≥
√
k. By the time the k-means++ algorithm covers all but ∆light clusters,
it makes greater than 2∆misses with probability at least 1 −e−
√
k/2.
Proof sketch. Let ε = 1/
√
N. Observe that k-means++ almost certainly covers all clusters in εN
steps (since N ≫k). So in the rest of this proof sketch, we assume that the number chosen centers
is at most εN and, consequently, at least (1 −ε)N points in the heavy cluster are not selected as
centers. Hence, the cost of the heavy cluster is at least 1 −ε.
Consider a step of the algorithm when exactly u light clusters remain uncovered. At this step,
the total cost of all light clusters is αu (we assume for simplicity that distance between the light
clusters and the closest chosen center in the heavy cluster is the same as the distance to the origin).
The cost of the heavy cluster is at least 1 −ε. The probability that the algorithm chooses a center
from the heavy cluster and thus misses a light cluster is at least (1 −ε)/(1 + αu).
Deﬁne random variables {Xu} as follows. Let Xu = 1 if the algorithm misses a cluster at least
once when the number of uncovered light clusters is u; and let Xu = 0, otherwise. Then, {Xu} are
independent Bernoulli random variables. For each u, we have P [Xu = 1] ≥(1 −ε)/(1 + αu).
Observe that the total number of misses is lower bounded by Pk−1
u=∆Xu. Then, we have
E
" k−1
X
u=∆
Xu
#
≥(1 −ε)
k−1
X
u=∆
1
1 + αu ≥(1 −ε)
Z k
∆
du
1 + αu
= (1 −ε)α−1 ln 1 + αk
1 + α∆
≥(1 −ε)α−1 ln k
∆= 4(1 −ε)∆.
Let µ = E
hPk−1
u=∆Xu
i
≥4(1 −ε)∆. By the Chernoﬀbound for Bernoulli random variables, we
have
P
"
k
X
u=∆
Xu ≤2∆
#
≤e−µ
 eµ
2∆
2∆
.
Since f(x) = e−x( ex
2∆)2∆is a monotone decreasing function for x ≥2∆, we have
P
"
k
X
u=∆
Xu ≤2∆
#
≤e−(2−4ε)∆· 22∆≤e−∆/2.
Hence, with probability as least 1 −e−
√
k/2, the number of misses is greater than 2∆.
For every k and ∆≥
√
k, consider the instance we constructed. By Lemma C.3, the algorithm
chooses more than k+∆centers to cover all but ∆light clusters with probability at least 1−e−
√
k/2.
Thus, at the time when the algorithm chose k + ∆centers, the number of uncovered light clusters
was greater than ∆. Hence, in the clustering with k + ∆centers sampled by k-means++, the total
cost is at least 1
4 ln (k/∆), while the cost of the optimal solution with k clusters is 1. For every k
and ∆<
√
k, the total cost is at least 1
4 ln(k/∆′) with ∆′ =
√
k extra centers, which concludes the
proof.
32


=== round-0004/verifier.feedback.md ===
High-level assessment. Both reports build on the already sound plateau calculus. I checked the proposed additions against Theorem 3 and standard inequalities. There are two clearly rigorous, valuable per-plateau refinements we can safely curate: (i) the τ≈m/ln L branch (Prover 01), and (ii) an explicit constants instantiation for C_bi=C_fix=5 (Prover 02). I do not see a correctness issue in our current output.md, and the symbolic-constant presentation remains appropriate.

On Prover 01’s τ≈m/ln L branch. The choice τ*:=min{L−1, ⌊m/ln L⌋} is admissible in Theorem 3. In the interesting subcase m ≤ (L−1) ln L (so τ*=⌊m/ln L⌋), for L≥3 and m≥L (the latter holds for plateaus contained in [k0,2k0) by our existing Remark), the elementary bounds used are correct: τ≥m/ln L − 1 implies 2m/τ ≤ 4 ln L, hence ln(2m/τ) ≤ ln(4 ln L); τ/(L−1) ≤ m/((L−1) ln L); and H_{L−2} − H_{τ−1} ≤ ln(L/τ)+1 ≤ 1 + ln ln L. Plugging these into Theorem 3 yields an explicit upper bound whose dominant m,L dependence is (m/L)·(1+ln ln L), strictly improving the earlier (m ln L)/L term. In the complementary subcase m > (L−1) ln L, τ*=L−1 recovers the pure-log branch. This is ready to curate as a corollary and to update the envelope to a “min-of-three.”

On Prover 02’s balanced split and explicit constants. The τ=⌊L/2⌋ instantiation is also a correct, self-contained refinement with safe constants: τ/(L−1)≤2/3, (L−1−τ)/(L−1)≤1/2, ln(2m/τ)≤ln(4m/(L−1)), and H_{L−2}−H_{τ−1}≤ln 3+1 for all L≥3. While useful, we already recorded this template in notes; to keep output.md lean, I propose we defer formalizing it unless we find regimes where it beats both extremes and the new τ≈m/ln L branch. The explicit-constants envelope for C_bi=C_fix=5 is easy to state rigorously; I recommend curating it, and simultaneously extending it to the min-of-three by keeping the same conservative coefficients.

Probabilistic instantiations. The methodology in Corollary 9 is correct. However, the example “η=0.99, c=100 gives ≥0.01−O(1/L)” is slightly optimistic: (1−η)(1−1/c)=0.01×0.99=0.0099. To guarantee ≥1% cleanly, e.g., take η=0.98 and c=2, yielding (1−η)(1−1/c)=0.02×0.5=1% (up to O(1/L)). I will add a corrected numerical remark.

Heavy-coverage. Prover 01’s pathwise inequality H_t(H_cov)≤eH_t(H_cov)≤5∑_H OPT1(P) is plausible, but the justification “Lemma 4.1 at first hit” needs careful citation and definitions (what exactly is eH_t?). Prover 02’s metric-separation lemma sketch is suggestive but not yet rigorous: the lower bound D(y)≥(λ−1)·sqrt(diam2(P)) for y outside P does not hold unconditionally because y may lie close to a previously chosen center outside H. Neither heavy-coverage item is ready for output.md. They can remain in notes with caveats.

Next steps for provers.
- Tighten the τ≈m/ln L branch: (a) isolate small-L cases (L=2) and state the branch for L≥3; (b) give a clean version of the global min-of-three aggregation (plateau-wise, then window-wise) and, optionally, provide explicit constants when C_bi=C_fix=5.
- If we choose to include the balanced split, add it as a corollary and compare numerically against the min-of-three to identify regimes where it is preferred.
- Probabilistic corollary: add a short remark with a parameter choice that truly yields ≥1% without asymptotic wiggle room.
- Heavy coverage: formalize eH_t, state any use of supermartingale properties precisely, and derive a correct denominator lower bound cost_t(X)≥β·U_t(H) under a verifiable hypothesis before any collision summation.
- Lower bound: proceed with a concrete instance and compute OPT_k exactly across the window, then adapt Arthur–Vassilvitskii’s “misses” argument to the k-averaged setting.


=== notes.md ===
Problem and notation
- X ⊂ R^d finite dataset; cost(C) = ∑_{x∈X} min_{c∈C} ||x−c||^2.
- OPT_k := min_{|C|=k} cost(C), nonincreasing in k.
- ALG_k := cost of k-means++ after k centers; E_++[·] denotes expectation over its randomness.
- We study E_{k∼U({k0,…,2k0−1})} E_++[ ALG_k / OPT_k ].

Key external input (Makarychev–Reddy–Shan 2020)
- There is an absolute constant C_bi (quoted as 5 in MRS) such that for any r≥1, Δ≥1,
  E_++[cost after r+Δ centers] ≤ C_bi · min{ 2 + 1/(2e) + ln(2r/Δ), 1 + r/(e(Δ−1)) } · OPT_r.
- For Δ=0 (fixed-k), there is an absolute constant C_fix (also ≲5 in MRS) with E_++[ALG_k] ≤ C_fix · (ln k + O(1)) · OPT_k.

Per-k oversampling bound (robust to s-misalignment)
- For any k, define s(k) := min{ s∈{1,…,k} : OPT_s ≤ 2·OPT_k }. Let Δ(k)=k−s(k).
- If Δ(k)≥1, then
  E_++[ALG_k] ≤ 2·C_bi · min{ 2 + 1/(2e) + ln(2 s(k)/Δ(k)), 1 + s(k)/(e(Δ(k)−1)) } · OPT_k.
- If Δ(k)=0, fall back to E_++[ALG_k] ≤ C_fix · (ln k + O(1)) · OPT_k.
Note: On any factor-2 plateau I=[m,m+L) (i.e., OPT_k∈(OPT_m/2,OPT_m] for k∈I), we have s(k)≤m and Δ(k)≥k−m; since the MRS bound is monotone in (s,Δ), replacing (s(k),Δ(k)) by (m,k−m) yields a valid upper bound.

Per-plateau averaging and Stirling control
- For a factor-2 plateau I=[m,m+L), L≥2, uniform k over I gives
  E_{k,++}[ALG_k/OPT_k] ≤ 2·C_bi · (2 + 1/(2e) + (1/(L−1))∑_{Δ=1}^{L−1} ln(2m/Δ)) + (C_fix(ln m + O(1)))/L.
- Using ln(n!) ≥ n ln n − n + 1 (n≥1),
  (1/(L−1))∑_{Δ=1}^{L−1} ln(2m/Δ) = ln(2m) − ln((L−1)!)/(L−1) ≤ 1 + ln(2m/(L−1)).
- Hence E_{k,++}[ALG_k/OPT_k] ≤ const + const·ln^+(2m/(L−1)) + O((ln m)/L), where ln^+(x)=max{ln x,0} and all const’s are absolute.

Decomposition over [k0,2k0)
- Partition [k0,2k0) into maximal factor-2 plateaus I_j=[m_j, m_j+L_j). For L_j≥2, the per-plateau average is bounded as above. For L_j=1, the single k in I_j contributes ≤ C_fix(ln m_j + O(1)). Averaging with weights L_j/k0 yields a global bound explicitly in terms of {(m_j,L_j)}; in the worst case many L_j=1 plateaus can force Θ(log k0).

Consequences and regimes
- Long plateaus: If some I=[m,m+L) satisfies L ≥ m/polylog(k0), then E_{k∈I,++}[ALG/OPT] = O(log log k0). If a constant fraction of [k0,2k0) is covered by such plateaus, the global average is O(log log k0).
- Worst case without structure: If many plateaus are short (e.g., L_j=1), the global average can be Θ(log k0), aligning with classic worst-case fixed-k behavior.

Open directions
- Structural conditions on OPT(·) implying many long plateaus (e.g., bounded discrete derivative decay, quasiconvexity/regularity of k·OPT_k) to get unconditional O(log log k0).
- Heavy-coverage lemma (conjectural): under strong scale separation OPT_{k1} ≫ OPT_k, show k-means++ hits all k1 heavy clusters w.h.p. before collisions, then apply bi-criteria on the remaining part.
- Lower bounds: Construct instances with Δ(k)=1 for a 1−o(1) fraction of k in [k0,2k0), giving an Ω(log k0) average and clarifying optimality of random-k smoothing.
Refinements and corrections for plateau averaging and monotonicity

- Monotonicity of the MRS bi-criteria bound. Define for integers s≥1 and Δ≥1 (interpreting the second branch only for Δ≥2):
  f(s,Δ) := min{ a + ln(2s/Δ), 1 + s/(e(Δ−1)) }, where a := 2 + 1/(2e).
  Then f is nondecreasing in s and nonincreasing in Δ. Consequently, on any factor-2 plateau I=[m,m+L) and for k=m+Δ with Δ≥1, the per-k bound from Theorem 1 satisfies
  f(s(k),Δ(k)) ≤ f(m,Δ), so we may safely replace (s(k),Δ(k)) by (m,Δ) in upper bounds.

- Correct τ-split averaging (fixing Proposition B). On a factor-2 plateau I=[m,m+L) with L≥2, for any integer τ∈{1,…,L−1}:
  (1/(L−1)) ∑_{Δ=1}^{L−1} min{ a+ln(2m/Δ), 1 + m/(e(Δ−1)) } (with the second term read only for Δ≥2)
  ≤ (τ/(L−1)) [ a + 1 + ln(2m/τ) ]
    + ((L−1−τ)/(L−1))
    + (m/(e(L−1))) · (H_{L−2} − H_{τ−1}).
  Proof sketch: Split the sum at τ and use Stirling ln(τ!) ≥ τ ln τ − τ + 1 to bound ∑_{Δ≤τ} ln(2m/Δ) ≤ τ(1+ln(2m/τ)). For the tail, sum 1/(Δ−1) as H_{L−2}−H_{τ−1}. This corrects two issues in earlier drafts: (i) the constant a must be multiplied by τ/(L−1), and (ii) ln(2m/Δ) cannot be upper bounded by ln(2m/τ) when Δ≤τ.

- Hybrid extremes recovered from the τ-split:
  • Choosing τ=L−1 gives the pure-logarithmic average a+1+ln(2m/(L−1)).
  • Choosing τ=1 gives the “hybrid extreme” 1 + (a+ln(2m))/(L−1) + (m/(e(L−1))) H_{L−2}.
  Hence we may take the minimum of these two regimes for a tighter bound, plateau by plateau.

- Long-plateau corollary. If L≥α m with α∈(0,1], then a+1+ln(2m/(L−1)) ≤ a+1+ln(2/α)+O(1/L), so the plateau-average is O(1) up to the endpoint term of order (ln m)/L.

- Quantile-in-Δ bound (careful threshold). For any η∈(0,1), let T := max{2, ⌈η(L−1)⌉+1}. Then, for all Δ≥T, by the large-Δ branch
  E_++[ALG_{m+Δ}/OPT_{m+Δ}] ≤ 2·C_bi·(1 + m/(e(T−1))).
  The fraction of k∈I with Δ≥T is (L−T+1)/L = 1 − η − O(1/L). This yields a rigorous “with probability ≥ 1−η−O(1/L) over k uniform on the plateau” expected-ratio bound; Markov then converts it into a constant-factor joint (k,++) probability if desired.

- Global decomposition improvement. Using the τ-split per plateau and optimizing τ as a function of (m,L) leads to a refined global bound that, informally, scales like
  K0 + K1 · min{ ln^+(2m/(L−1)), (m ln L)/L } + K2 · (ln m)/L,
  up to absolute constants K0,K1,K2. Formalizing this from Theorem 3 is a next step.

Open directions and next steps
- Optimize τ in the τ-split bound and write a clean, closed-form per-plateau bound. Then aggregate over plateaus to strengthen Corollary 3.
- Probability guarantees: phrase clean conditions under which at least a 1% measure of k (in [k0,2k0) or on a specific plateau) yields a constant expected ratio, and apply Markov to obtain joint (k,++) probability.
- Heavy-coverage: formalize the supermartingale for heavy uncovered mass (precise citation from MRS), bound expected heavy-collisions by O(k1/R), and lift to high probability via Freedman/Azuma with a stopping time.
- Lower bounds: construct OPT(·) with OPT_{k−1}/OPT_k ≈ 1 − Θ(1/ln k0) for most k to force Δ(k)=1 on a 1−o(1) fraction of the window, yielding an Ω(log k0) average.
Additions and clarifications

- Geometric constraint on plateau length within the window. If a factor-2 plateau I=[m,m+L) is contained in [k0,2k0), then L ≤ 2k0−m ≤ m. In particular, for L≥2 we have m/(L−1) ≥ 1. This prevents sign errors when simplifying expressions involving m/(L−1).

- Min-of-extremes and an upper envelope. From Theorem 3 and Corollary 4 we can (and will) use the minimum of the two extreme instantiations τ=L−1 (pure log) and τ=1 (hybrid extreme). Using H_q ≤ ln q + 1, this yields a clean per-plateau upper envelope of the form
  E_{k∈I} E_++[ALG_k/OPT_k] ≤ K0 + K1 · min{ ln^+(2m/(L−1)), (m ln L)/L } + K2 · (ln m)/L,
for absolute constants K0,K1,K2 depending only on C_bi,C_fix. This is the regime split we will use in global aggregation: the log term dominates when L≪m, while the (m ln L)/L term is O(1) when L is a constant fraction of m.

- Safe balanced-split (τ=⌊L/2⌋) template (optional). The following uniform bounds hold for L≥3 and τ=⌊L/2⌋:
  • τ/(L−1) ≤ 2/3 and (L−1−τ)/(L−1) ≤ 1/2.
  • ln(2m/τ) ≤ ln(4m/(L−1)).
  • H_{L−2} − H_{τ−1} ≤ ln 3 + 1 (with small-L cases checked directly).
Plugging these into Theorem 3 yields the rigorous estimate
  E_{k∈I} E_++[ALG_k/OPT_k]
  ≤ 2C_bi [ (2/3)(a + 1 + ln(4m/(L−1))) + 1/2 + (m/(e(L−1)))(ln 3 + 1) ]
    + (C_fix(\ln m + O(1)))/L.
This improves the (m ln L)/L term to a pure (m/L) term at the expense of a larger coefficient on the logarithmic piece. It may be tighter than the min-of-extremes envelope in intermediate regimes. If used, constants must be kept as above (1/2 is not a safe coefficient for τ/(L−1) when L is even).

- Probability ≥ 1% guarantees under a long-plateau condition. From Corollary 6: fix η∈(0,1) and define T := max{2, ⌈η(L−1)⌉+1}. For at least a (1−η−O(1/L)) fraction of k∈I we have
  E_++[ALG_k/OPT_k] ≤ 2C_bi (1 + m/(e(T−1))).
By Markov, for any c≥1,
  P_{++}[ ALG_k/OPT_k ≤ c · 2C_bi (1 + m/(e(T−1))) ] ≥ 1 − 1/c
for each such k. Therefore, jointly over k uniform on I and ++ randomness,
  P[ ALG_k/OPT_k ≤ c · 2C_bi (1 + m/(e(T−1))) ] ≥ (1−η−O(1/L))(1−1/c).
If L ≥ α m, then m/(T−1) ≤ 1/(α η) up to 1+O(1/L) factors, giving an explicit constant bound. If the plateau covers a γ-fraction of [k0,2k0), the global probability is ≥ γ times the RHS. Example: with C_bi=5, α=1, η=0.99, c=100, we get a ≥1% guarantee with a factor ≤ 100·2·5·(1 + 1/(0.99 e)).

- Heavy-coverage program (caveats and plan). Any per-step “collision within heavy” probability bound must be stated conditionally on the current filtration F_t: under D^2 sampling,
  P[collision in H at step t | F_t] = cost_t(covered H) / cost_t(all).
Avoid ratio-of-expectations. A plausible route is to bound cost_t(covered H) in terms of the uncovered heavy mass U_t(H) via the MRS eH_t supermartingale (in expectation and eventually with concentration), while lower bounding cost_t(all) by U_t(H) + U_t(L). To obtain a β-persistence-type condition from scale separation, quantify the expected reduction of U_t(H) when sampling outside H and show it is o(U_t(H)) under strong separation (e.g., OPT_{k1} ≥ k^C · OPT_k). With these in place, the expected number of heavy collisions can be bounded by summing the conditional probabilities until all heavy clusters are hit, and then lifted to high probability by Freedman/Azuma.

- Lower bounds. To show random-k smoothing cannot beat Θ(log k0) in general, aim for an instance where Δ(k)=1 for a 1−o(1) fraction of k in [k0,2k0). Concretely, calibrate radii so OPT_{k−1}/OPT_k ≈ 1 + Θ(1/ln k0) across most of the window, ensuring s(k)=k−1 on most k. Then adapt Arthur–Vassilvitskii’s lower-bound calculation to the averaged setting.
New per-plateau refinement: τ≈m/ln L branch and an updated envelope

- Additional τ-instantiation. On a factor-2 plateau I=[m,m+L) with L≥3, choose τ*:=min{L−1, ⌊m/ln L⌋}. Then Theorem 3 yields
  E_{k∈I} E_++[ALG/OPT]
  ≤ 2C_bi [ (τ*/(L−1))(a+1+ln(2m/τ*)) + ((L−1−τ*)/(L−1)) + (m/(e(L−1)))(H_{L−2} − H_{τ*−1}) ]
    + (C_fix(\ln m + O(1)))/L,
  where a:=2+1/(2e).
  In the subcase m ≤ (L−1) ln L (hence τ*=⌊m/ln L⌋) and assuming m≥L (which holds for plateaus contained in [k0,2k0)), we can bound
  • ln(2m/τ*) ≤ ln(4 ln L)
  • τ*/(L−1) ≤ m/((L−1) ln L)
  • H_{L−2} − H_{τ*−1} ≤ 1 + ln ln L
  to conclude
  E_{k∈I} E_++[ALG/OPT]
  ≤ 2C_bi [ 1 + (m/((L−1) ln L))(a+1+\ln(4\ln L)) + (m/(e(L−1)))(1 + \ln\ln L) ]
    + (C_fix(\ln m + O(1)))/L.
  This improves the (m\ln L)/L tail from the τ=1 branch to ≈ (m/L)(1+\ln\ln L).

- Updated min-of-three envelope (plateau-wise). Together with the τ=L−1 (pure-log) and τ=1 (hybrid) instantiations, we may use
  E_{k∈I} E_++[ALG/OPT] ≤ K0 + K1·min{ ln^+(2m/(L−1)), (m\ln L)/L, (m(1+\ln\ln L))/L } + K2·(\ln m)/L,
  for absolute constants K0,K1,K2 depending only on C_bi,C_fix. This strictly strengthens the previous min-of-two envelope in regimes where ln ln L ≪ ln L.

- Global aggregation (window-wise). For the maximal factor-2 plateau partition [k0,2k0)=\bigsqcup_j I_j with I_j=[m_j,m_j+L_j), one may take, plateau by plateau, the minimum of the three branches above (with the third used for L_j≥3), and sum with weights L_j/k0:
  E_{k\sim U([k0,2k0))} E_++[ALG/OPT]
  ≤ (1/k0) \sum_j L_j [ K0 + K1·min{ ln^+(2m_j/(L_j−1)), (m_j\ln L_j)/L_j, (m_j(1+\ln\ln L_j))/L_j } ]
    + (K2/k0) \sum_j \ln m_j.

- Balanced split reminder. The τ=⌊L/2⌋ instantiation yields
  E_{k∈I} E_++[ALG/OPT]
  ≤ 2C_bi [ (2/3)(a+1+\ln(4m/(L−1))) + 1/2 + (m/(e(L−1)))(\ln 3 + 1) ] + (C_fix(\ln m + O(1)))/L,
  valid for all L≥3. This removes the ln L factor in the tail at the expense of a larger constant on the log piece.

- Probabilistic 1% instantiation (correction). From Corollary 9, for any η∈(0,1) and c≥1,
  P_{k∈I,++}[ ALG/OPT ≤ c·B_I ] ≥ (1−η−O(1/L))(1−1/c), with B_I=2C_bi(1 + m/(e(T−1))), T=\max\{2,\lceil η(L−1)\rceil+1\}.
  To guarantee at least 1% mass uniformly (up to O(1/L)), take η=0.98 and c=2, giving (1−η)(1−1/c)=0.02·0.5=0.01. If, in addition, L≥α m, then B_I ≤ 2C_bi(1 + 1/(α e η)) up to O(1/L).

Caveat on heavy-coverage sketches. The pathwise control and metric-separation heuristics are promising but currently lack fully rigorous denominators: one must explicitly quantify cost_t(X) in terms of U_t(H) under precise conditions, and avoid uncontrolled uses of triangle inequality against existing centers outside H. These ideas remain in-development items in notes, not yet suitable for output.


=== output.md ===
Theorems curated in this file depend on the following standard notation and external input.
- For a finite dataset X⊂R^d, write OPT_k for the optimal k-means cost with k centers, and ALG_k for the cost produced by k-means++ after k centers; E_++[·] is expectation over the algorithm’s randomness.
- External input (Makarychev–Reddy–Shan, 2020): There exists an absolute constant C_bi such that for all integers r≥1 and Δ≥1,
  E_++[cost after r+Δ centers] ≤ C_bi · min{ 2 + 1/(2e) + ln(2r/Δ), 1 + r/(e(Δ−1)) } · OPT_r.
  There is also an absolute constant C_fix with E_++[ALG_k] ≤ C_fix·(ln k + O(1))·OPT_k for all k (the fixed-k bound). We treat C_bi and C_fix as absolute numerical constants.

Theorem 1 (Per-k oversampling bound via a 2-approximate anchor).
Fix k≥1 and define
  s(k) := min{ s∈{1,…,k} : OPT_s ≤ 2·OPT_k },    Δ(k) := k − s(k).
Then:
- If Δ(k) ≥ 1, we have
  E_++[ALG_k] ≤ 2·C_bi · min{ 2 + 1/(2e) + ln(2 s(k)/Δ(k)), 1 + s(k)/(e(Δ(k)−1)) } · OPT_k.
- If Δ(k) = 0, we have E_++[ALG_k] ≤ C_fix·(ln k + O(1))·OPT_k.
Proof.
For Δ(k)≥1, apply the MRS bi-criteria inequality with r = s(k) and Δ = k−s(k):
  E_++[ALG_k] = E_++[cost after s(k)+Δ(k) centers]
  ≤ C_bi · min{ 2 + 1/(2e) + ln(2 s(k)/Δ(k)), 1 + s(k)/(e(Δ(k)−1)) } · OPT_{s(k)}.
By definition of s(k), OPT_{s(k)} ≤ 2·OPT_k, giving the stated bound. For Δ(k)=0 the fixed-k bound yields the second claim. ∎

Theorem 2 (Averaging over a factor-2 plateau).
Let I = {m, m+1, …, m+L−1} with L≥2 and assume OPT_k ∈ (OPT_m/2, OPT_m] for all k∈I. Then for k uniform in I and over the k-means++ randomness,
  E_{k∈I} E_++[ ALG_k / OPT_k ] ≤ A0 + A1 · ln^+( 2m/(L−1) ) + (A2 · ln m)/L,
where ln^+(x) := max{ln x,0} and A0,A1,A2 are absolute constants depending only on C_bi,C_fix.
Proof.
Average the bound from Theorem 1 over k∈I. For Δ:=k−m≥1, we may upper bound s(k) by m and lower bound Δ(k) by Δ (since s(k)≤m and Δ(k)≥Δ on a factor-2 plateau), and use only the logarithmic branch of MRS to get
  E_++[ALG_{m+Δ}/OPT_{m+Δ}] ≤ 2·C_bi · ( 2 + 1/(2e) + ln(2m/Δ) ).
Averaging Δ from 1 to L−1 gives
  (1/(L−1))∑_{Δ=1}^{L−1} ln(2m/Δ) = ln(2m) − ln((L−1)!)/(L−1) ≤ 1 + ln(2m/(L−1)),
where we used ln(n!) ≥ n ln n − n + 1 for n=L−1≥1. Multiplying by (L−1)/L and adding the Δ=0 endpoint contribution C_fix(ln m + O(1))/L yields the claim with suitable absolute constants A0,A1,A2. Replacing ln by ln^+ only weakens the bound. ∎

Corollary 3 (Decomposition across [k0,2k0)).
Partition the integer interval [k0,2k0) into maximal factor-2 plateaus I_j = [m_j, m_j+L_j), j=1,…,J, so OPT_k ∈ (OPT_{m_j}/2, OPT_{m_j}] for k∈I_j, and ∑_j L_j = k0. Then, for k uniform on [k0,2k0),
  E_k E_++[ ALG_k / OPT_k ]
  ≤ (1/k0) ∑_{j : L_j≥2} L_j [ A0 + A1 · ln^+( 2 m_j/(L_j−1) ) ]
    + (1/k0) ∑_{j : L_j≥2} A2 · ln m_j
    + (1/k0) ∑_{j : L_j=1} C_fix · (ln m_j + O(1)).
In particular, if a constant fraction of [k0,2k0) is covered by plateaus satisfying L_j ≥ m_j / polylog(k0), then the contribution of those plateaus is O(log log k0), and the remaining fraction must be controlled separately; in the worst case with many L_j=1 plateaus the global average can be Θ(log k0).
Proof.
Average the bound of Theorem 2 over plateaus with weights L_j/k0, and handle L_j=1 plateaus by the fixed-k bound. ∎
Lemma (Monotonicity of the MRS bi-criteria bound).
Let a := 2 + 1/(2e), and for integers s≥1, Δ≥1 define
  F(s,Δ) := min{ a + ln(2s/Δ), 1 + s/(e(Δ−1)) },
where the second term is interpreted only for Δ≥2 (or taken as +∞ for Δ=1). Then:
- For fixed Δ, F is nondecreasing in s.
- For fixed s, F is nonincreasing in Δ (for all Δ≥1 in the above interpretation).
Proof.
For Δ≥2, both functions s ↦ a + ln(2s/Δ) and s ↦ 1 + s/(e(Δ−1)) are nondecreasing in s, hence their pointwise minimum is nondecreasing. For Δ≥2, both functions Δ ↦ a + ln(2s/Δ) and Δ ↦ 1 + s/(e(Δ−1)) are nonincreasing in Δ, hence the minimum is nonincreasing. For the transition from Δ=1 to Δ=2, note that F(s,1) = a + ln(2s) ≥ a + ln(s) ≥ a + ln(2s/2) ≥ F(s,2), since the logarithmic branch decreases with Δ and adding the second branch at Δ=2 can only decrease the minimum. Thus F is nonincreasing in Δ for all Δ≥1. ∎

Theorem 3 (Plateau averaging with a tunable split of the two MRS branches).
Let I = {m, m+1, …, m+L−1} be a factor-2 plateau (i.e., OPT_k ∈ (OPT_m/2, OPT_m] for k∈I) with L≥2. For any integer τ with 1≤τ≤L−1, for k uniform on I and over k-means++ randomness we have
  E_{k∈I} E_++[ ALG_k / OPT_k ]
  ≤ 2·C_bi · [ (τ/(L−1)) · ( a + 1 + ln(2m/τ) )
                + ((L−1−τ)/(L−1))
                + (m/(e(L−1))) · ( H_{L−2} − H_{τ−1} ) ]
    + (C_fix · (ln m + O(1)))/L,
where H_r denotes the r-th harmonic number (H_0:=0).
Proof.
Fix k=m+Δ with Δ∈{1,…,L−1}. By Theorem 1, and Lemma (monotonicity),
  E_++[ALG_{m+Δ}/OPT_{m+Δ}] ≤ 2·C_bi · min{ a + ln(2m/Δ), 1 + m/(e(Δ−1)) },
with the second branch applicable for Δ≥2. Averaging Δ uniformly over {1,…,L−1}, split the sum at τ:
  (1/(L−1)) ∑_{Δ=1}^{L−1} min{…}
  ≤ (1/(L−1)) ∑_{Δ=1}^{τ} [ a + ln(2m/Δ) ]
    + (1/(L−1)) ∑_{Δ=τ+1}^{L−1} [ 1 + m/(e(Δ−1)) ].
For the first sum, use ∑_{Δ=1}^{τ} ln(2m/Δ) = τ ln(2m) − ln(τ!) ≤ τ[1 + ln(2m/τ)] by ln(τ!) ≥ τ ln τ − τ. For the second, ∑_{Δ=τ+1}^{L−1} 1 = L−1−τ and ∑_{Δ=τ+1}^{L−1} 1/(Δ−1) = H_{L−2} − H_{τ−1}. This yields the displayed bound. Finally, incorporate the Δ=0 endpoint k=m which has probability 1/L and contributes at most C_fix·(ln m + O(1))/L by the fixed-k bound. ∎

Corollary 4 (Hybrid extremes recovered; strengthening Theorem 2).
In the setting of Theorem 3:
- Choosing τ = L−1 gives
  E_{k∈I} E_++[ ALG_k / OPT_k ] ≤ 2·C_bi · ( a + 1 + ln(2m/(L−1)) ) + (C_fix · (ln m + O(1)))/L.
  This recovers Theorem 2 (up to constant reparameterization by a and the explicit +1 from Stirling).
- Choosing τ = 1 gives
  E_{k∈I} E_++[ ALG_k / OPT_k ] ≤ 2·C_bi · ( 1 + (a+ln(2m))/(L−1) + (m/(e(L−1))) · H_{L−2} )
    + (C_fix · (ln m + O(1)))/L.
Using H_q ≤ ln q + 1, this yields a complementary regime where the bound scales like 1 + O((ln m)/L) + O((m ln L)/L).
Proof.
Both bounds are immediate from Theorem 3 with the specified τ and the inequality H_q ≤ ln q + 1 for q≥1. ∎

Corollary 5 (Explicit constants on long plateaus).
If L ≥ α m for some fixed α∈(0,1], then for k uniform on I,
  E_{k∈I} E_++[ ALG_k / OPT_k ]
  ≤ 2·C_bi · ( a + 1 + ln(2/α) ) + (C_fix · (ln m + O(1)))/L.
Proof.
Apply Corollary 4 with τ=L−1 and use ln(2m/(L−1)) ≤ ln(2/α) + O(1/L). The endpoint term is unchanged. ∎

Corollary 6 (Quantile-in-Δ guarantee on a plateau).
Fix η∈(0,1) and let T := max{2, ⌈η(L−1)⌉+1}. Then for all Δ∈{T,…,L−1},
  E_++[ ALG_{m+Δ} / OPT_{m+Δ} ] ≤ 2·C_bi · ( 1 + m/( e (T−1) ) ).
Consequently, for k uniform on I, the fraction of k for which the above inequality holds is at least (L−T+1)/L = 1 − η − O(1/L).
Proof.
For Δ≥T, the large-Δ branch gives 1 + m/(e(Δ−1)) ≤ 1 + m/(e(T−1)). Combine with Theorem 1 and monotonicity, and note that the set {k: Δ(k)≥T} has size L−T+1 among the L indices in I. ∎
Remark (Plateau length within [k0,2k0)).
Let I = [m,m+L) be a factor-2 plateau contained in [k0,2k0). Then L ≤ 2k0 − m ≤ m. In particular, for L≥2 we have m/(L−1) ≥ 1.
Proof.
Since m+L−1 ≤ 2k0−1, we have L ≤ 2k0−m. As m ≥ k0, it follows that 2k0−m ≤ m. The final claim is immediate. ∎

Corollary 7 (Min-of-extremes plateau bound).
In the setting of Theorem 3 with L≥2, for k uniform on I and the k-means++ randomness,
  E_{k∈I} E_++[ ALG_k / OPT_k ]
  ≤ min{ B_log, B_hyb } + (C_fix · (ln m + O(1)))/L,
where
  B_log := 2·C_bi · ( a + 1 + ln^+( 2m/(L−1) ) ),
  B_hyb := 2·C_bi · ( 1 + (a+\ln(2m))/(L−1) + (m/(e(L−1))) · H_{L−2} ),
and a := 2 + 1/(2e), H_q denotes the q-th harmonic number with H_0:=0, and ln^+(x):=max{ln x,0}.
Proof.
Take τ=L−1 and τ=1 in Theorem 3 to obtain the two bounds (cf. Corollary 4). Replacing ln by ln^+ only increases the RHS. Taking the minimum preserves validity. ∎

Corollary 8 (Upper envelope from the min-of-extremes).
In the setting of Corollary 7, there exist absolute constants K0,K1,K2 (depending only on C_bi and C_fix) such that
  E_{k∈I} E_++[ ALG_k / OPT_k ]
  ≤ K0 + K1 · min{ ln^+( 2m/(L−1) ), (m ln L)/L } + K2 · (\ln m)/L.
Proof.
From Corollary 7 with the logarithmic extreme,
  E ≤ 2·C_bi·(a+1) + 2·C_bi·ln^+(2m/(L−1)) + (C_fix·(\ln m + O(1)))/L.
From the hybrid extreme and H_{L−2} ≤ ln(L−2)+1 ≤ ln L + 1,
  E ≤ 2·C_bi·\Big( 1 + (a+\ln(2m))/(L−1) + (m/(e(L−1))) (\ln L + 1) \Big)
    + (C_fix·(\ln m + O(1)))/L.
Absorb constants into K0 and the terms (ln m)/(L−1), (m/(L−1)) into K2·(\ln m)/L and K1·(m ln L)/L, respectively, using L−1≈L up to absolute factors for L≥2. Taking the minimum of the two displays yields the claim. ∎

Corollary 9 (Plateau-level probability guarantee via quantiles and Markov; global mixing).
Let I=[m,m+L) be a factor-2 plateau with L≥2 and fix η∈(0,1). Define T := max{2, ⌈η(L−1)⌉+1}. Then for at least a (1−η−O(1/L)) fraction of k∈I we have
  E_++[ ALG_k / OPT_k ] ≤ B_I := 2·C_bi · ( 1 + m/( e (T−1) ) ).
Consequently, for any c≥1,
  P_{k\sim U(I),\,++}\big[ ALG_k/OPT_k ≤ c · B_I \big] ≥ (1−η−O(1/L)) · (1 − 1/c).
If, moreover, L ≥ α m for some α∈(0,1], then B_I ≤ 2·C_bi · (1 + 1/(α e η)) + O(1/L). If a plateau I covers a γ-fraction of [k0,2k0) (i.e., L/k0 ≥ γ), then for k uniform on [k0,2k0) and the ++ randomness,
  P\big[ ALG_k/OPT_k ≤ c · B_I \big] ≥ γ · (1−η−O(1/L)) · (1 − 1/c).
Proof.
The first assertion is Corollary 6: a (1−η−O(1/L)) fraction of k satisfy the displayed per-k bound with Δ≥T, hence with E_++ bounded by B_I. For any such k, Markov’s inequality gives P_{++}[ALG_k/OPT_k ≤ c·B_I] ≥ 1−1/c. Averaging uniformly over k on I yields the claim. If L ≥ α m, then T−1 ≥ η(L−1) ≥ η(α m−1), which implies m/(T−1) ≤ 1/(α η) + O(1/L). The global bound follows by restricting to k in I, which occurs with probability ≥ γ. ∎
Corollary 10 (A τ≈m/ln L branch on a plateau).
Let I=[m,m+L) be a factor-2 plateau with L≥3. Define τ*:=min{L−1, ⌊m/ln L⌋}. Then for k uniform on I and over the k-means++ randomness,
  E_{k∈I} E_++[ ALG_k / OPT_k ]
  ≤ 2·C_bi · [ (τ*/(L−1)) · ( a + 1 + ln(2m/τ*) )
                + ((L−1−τ*)/(L−1))
                + (m/(e(L−1))) · ( H_{L−2} − H_{τ*−1} ) ]
    + (C_fix · (ln m + O(1)))/L,
where a:=2 + 1/(2e), and H_r is the r-th harmonic number (H_0:=0). In particular, if m ≤ (L−1) ln L and (as is the case for plateaus inside [k0,2k0)) m≥L, then
  E_{k∈I} E_++[ ALG_k / OPT_k ]
  ≤ 2·C_bi · [ 1 + (m/((L−1) ln L)) · ( a + 1 + ln(4 ln L) )
                + (m/(e(L−1))) · ( 1 + ln ln L ) ]
    + (C_fix · (ln m + O(1)))/L.
Proof.
Apply Theorem 3 with τ=τ*. The displayed inequality is just Theorem 3. In the subcase m ≤ (L−1) ln L, τ*=⌊m/ln L⌋ satisfies τ* ≥ m/ln L − 1, hence 2m/τ* ≤ 4 ln L and ln(2m/τ*) ≤ ln(4 ln L). Also τ*/(L−1) ≤ m/((L−1) ln L). Finally, H_{L−2} − H_{τ*−1} ≤ ln(L/τ*) + 1 ≤ ln((L ln L)/m) + 1 ≤ 1 + ln ln L, using m≥L. Substituting these bounds into Theorem 3 yields the “in particular” display. ∎

Corollary 11 (Updated per-plateau upper envelope: min-of-three).
In the setting of Corollary 7, there exist absolute constants K0,K1,K2 (depending only on C_bi and C_fix) such that for k uniform on I=[m,m+L) with L≥2,
  E_{k∈I} E_++[ ALG_k / OPT_k ]
  ≤ K0 + K1 · min{ ln^+( 2m/(L−1) ), (m ln L)/L, (m(1 + ln ln L))/L } + K2 · (\ln m)/L.
Here ln^+(x):=max{ln x,0}. The third term in the minimum is intended for L≥3; when L=2 the minimum of the first two terms suffices. 
Proof.
Combine: (i) the logarithmic extreme τ=L−1 (Corollary 4), (ii) the τ=1 hybrid extreme (Corollary 4 with H_{L−2}≤ln L+1), and (iii) Corollary 10 in the subcase τ*=⌊m/ln L⌋ (which holds exactly when m ≤ (L−1) ln L). In case (iii), the two m-dependent contributions are bounded by a constant multiple of (m/L)(1+ln ln L), since (m/((L−1) ln L))(a+1+ln(4 ln L)) ≤ (const)·(m/L) and (m/(e(L−1)))(1+ln ln L) ≤ (const)·(m/L)(1+ln ln L), with all constants depending only on C_bi. Absorb all additive O(1) and O((\ln m)/L) terms into K0 and K2. Taking the minimum of the three displays yields the claim. ∎

Corollary 12 (Explicit constants for the envelope when C_bi=C_fix=5).
Assume C_bi=C_fix=5 and L≥2. Then for k uniform on I=[m,m+L),
  E_{k∈I} E_++[ ALG_k / OPT_k ]
  ≤ 35 + 10 · min{ ln^+(2m/(L−1)), (m ln L)/L, (m(1+\ln\ln L))/L } + 5 · (\ln m)/L.
Proof.
From Corollary 7 (logarithmic extreme), E ≤ 2·C_bi·(a+1) + 2·C_bi·ln^+(2m/(L−1)) + (C_fix·(\ln m + O(1)))/L. With a=2+1/(2e) and C_bi=C_fix=5, 2·C_bi·(a+1) ≈ 31.84; round up to 35 to absorb the endpoint O(1/L) terms. The coefficient in front of ln^+(·) is 2·C_bi=10. For the τ=1 branch, the (m ln L)/L contribution carries a factor 2·C_bi/e ≤ 10/e < 10; rounding up to 10 is safe after replacing L−1 by L. For the τ≈m/ln L branch (Corollary 10), the m-dependent part is ≤ const·(m/L)(1+\ln\ln L) with const depending only on C_bi; choosing 10 dominates those constants as well. The endpoint term is ≤ 5·(\ln m + 2)/L and we keep 5·(\ln m)/L after absorbing the additive 10/L into 35. Taking the minimum of the three candidate bounds proves the display. ∎

Remark (Numerical 1% probability instantiation on a single plateau).
In Corollary 9, choose η=0.98 and c=2. Then for k uniform on I and ++ randomness,
  P[ ALG_k/OPT_k ≤ 2 · B_I ] ≥ (1−η−O(1/L)) · (1 − 1/2) = (0.02−O(1/L)) · 0.5 ≥ 0.01 − O(1/L),
with B_I = 2·C_bi·(1 + m/(e(T−1))) and T = max{2, ⌈0.98(L−1)⌉+1}. If additionally L ≥ α m, then B_I ≤ 2·C_bi·(1 + 1/(α e · 0.98)) + O(1/L), giving a fully explicit constant-factor guarantee at ≥1% mass (up to O(1/L)).



Current round tag: Round 0005 — 2025-08-27T19:53:06.679628Z
Return ONLY valid JSON with a single field:
{ "progress_md": "<your progress notes for this round>" }

Read output.md. If you spot gaps, errors, or missing justifications in output.md, point them out clearly inside progress_md.
