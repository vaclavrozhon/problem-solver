--- SYSTEM ---
You are a research mathematician. 

You are being run in a loop with a verifier that checks rigor. Your goal is to make **incremental, auditable progress**. You are not expected to solve the problem at once, but to generate new, testable ideas.

Inputs: 
- task statement 
- notes.md (summary of past progress; optional) 
- output.md (rigorously proven results curated so far; optional) 
- summary of past rounds (optional) 
- possibly some reference papers.

What counts as progress:
- Extract small lemmas/heuristics from literature and state them cleanly with one-line "why useful here".
- Explore small examples & try to break your own claims with toy counterexamples.
- Prove special cases or nontrivial bounds.
- If an approach fails, explain crisply why.
- Point out flaws in notes.md or output.md (but do not rewrite output.md yourself).

**Discipline.** 
- Read notes, outputs, summaries carefully before proposing new work. 
- Reference papers if relevant, but focus on *incremental, checkable steps*. 
- Do not output Markdown code fences, only raw JSON. 
- Length: at least ~200 words. 
- Organize your reasoning with short headings (Ideas, Examples, Obstacles, Next steps), make clear what your claims are and how they are supported. 
- Remember: the verifier curates notes and outputs, you only suggest.

**Return strictly JSON**:
{
  "progress_md": "Your progress notes for this round in Markdown (KaTeX allowed). Point out any gaps in output.md clearly. Do not modify output.md directly."
}


--- USER ---
Work on this problem context:

=== task.tex ===
\newcommand{\X}{X}
\newcommand{\R}{\mathbb{R}}
\newcommand{\cost}{\phi}
\newcommand{\OPT}{\mathrm{OPT}}
\newcommand{\ALG}{\mathrm{ALG}}
%\newcommand{\E}{\mathbb{E}}

\section*{Algorithm, model, and task}

\paragraph{Objective.}
Fix a dataset $\X=\{x_1,\dots,x_n\}\subseteq\R^d$ and the standard $k$-means cost
\begin{equation*}
\cost(C) \;=\; \sum_{x\in \X} \min_{c\in C} \|x-c\|_2^2
\qquad\text{for a center set } C\subseteq\R^d,\ |C|=k.
\end{equation*}
Let $\OPT(k)=\min_{|C|=k}\cost(C)$ and let $\ALG(k)$ denote the cost of the clustering returned by the $k$-means++ seeding.

\paragraph{$k$-means++ seeding (Arthur--Vassilvitskii).}
Pick the first center uniformly from $\X$. Then, iteratively, sample the next center $c$ from $\X$ with probability proportional to $D(x)^2$, where $D(x)$ is the distance from $x$ to its closest previously chosen center. Stop after $k$ centers are chosen; assign each $x$ to its nearest chosen center.

\paragraph{Smoothed random-$k$ experiment.}
Given an integer $k_0\ge 1$, draw $k$ uniformly at random from $\{k_0,k_0{+}1,\dots,2k_0{-}1\}$, run $k$-means++ with this $k$ on $\X$, and set the random variable
\begin{equation*}
\alpha \;=\; \frac{\ALG(k)}{\OPT(k)} \;\ge\; 1.
\end{equation*}

\section*{The task}
Prove the strongest possible asymptotic upper bound on
\begin{equation*}
\E_{k\sim U([k_0,2k_0)),\,\text{$k$-means++}}\!\!\Big[\; \tfrac{\ALG(k)}{\OPT(k)} \;\Big],
\end{equation*}
as a function of $k_0$. Alternatively, prove the strongest possible upper bound on the value that $\tfrac{\ALG(k)}{\OPT(k)}$ achieves with probability at least $0.01$. 

\section*{Context and prior results}

\paragraph{Baseline (fixed $k$).}
For standard $k$-means++ with $k$ centers, $\E[\ALG(k)]\le O(\log k)\cdot \OPT(k)$ and there are instances with $\E[\ALG(k)]\ge \Omega(\log k)\cdot \OPT(k)$.

\paragraph{Oversampling phenomenon.}
Let $k'=(1{+}\varepsilon)k$. Results in the spirit of Theorem~5.1 of an attached paper show that sampling $k'$ centers by $D^2$-sampling (and evaluating against $\OPT(k)$) yields an expected $O(\log (1/\varepsilon))$-approximation. Intuitively, a mild amount of \emph{oversampling} dramatically sharpens the guarantee.

\paragraph{Why randomizing $k$ might help.}
As $k$ increases, $\OPT(k)$ decreases. Group the values of $OPT(k)$ by $\lfloor \log_2 (OPT(k)) \rfloor$. If there is a long "plateau" where $OPT$ stays roughly the same, up to $2$-factor, $k$-means++ effectively oversamples relative to the number of ``active'' clusters present on that scale. This could lead to a better guarantee. 


\section*{Possible approaches and technical ideas}

\paragraph{Important note}
Take the following section only as an inspiration of what might work. Take it with a huge grain of salt. 

\paragraph{1.\ Plateau decomposition of $\OPT(k)$}
Let $k_0=k^{(0)}<k^{(1)}<\cdots<k^{(m)}\le 2k_0$ be the indices at which $\OPT(\cdot)$ drops by a constant factor; define plateaus $I_j=[k^{(j)},k^{(j+1)}\!)$ on which $\OPT(k)$ is constant.
\begin{itemize}
  \item \textbf{Long vs.\ short plateaus.} Call $I_j$ \emph{long} if $|I_j|\ge k_0/\mathrm{poly}\log k_0$, else \emph{short}.
  \item \textbf{Averaging benefit on long plateaus.} If $k$ is uniform over a long plateau and $\OPT$ does not change, then running $k$-means++ with parameter $k$ is akin to oversampling relative to the \emph{effective} cluster count on that scale. Using Theorem~5.1 from the reference with $\varepsilon \approx |I_j|/k$ suggests an $O(\log\log k_0)$ \emph{conditional} bound on that plateau.
  \item \textbf{Bounding the mass of short plateaus.} Either there are few short plateaus at each scale (so their contribution in the uniform average is small), or there are many in a row; in the latter case, we can try to invoke a strengthened oversampling statement (see next) to regain a $\log\log$ bound.
\end{itemize}

\paragraph{2.\ A coverage lemma for the ``heavy'' part}
\label{subsec:coverage}
Aim to prove a statement of the form:

\medskip
\noindent\emph{Hypothesis (strengthening of Thm.~5.1).}
Let $k=k_1{+}k_2$. Suppose $\OPT(k_1) \ge k^{10}\,\OPT(k)$ (a strong scale separation).
Run $k$-means++ for $k'=k_1 + (1+\epsilon)k_2 $ steps. Then, with probability $1 - 1/k^{O(1)}$, the first $k_1$ steps place one center in each of the $k_1$ ``heavy'' clusters before any collision, and overall the returned cost is $O(\log(1/\varepsilon))\cdot \OPT(k)$.

\medskip
\noindent\textbf{Why plausible.}
Under strong separation, the contribution to $D^2$ from uncovered heavy clusters dominates. By the standard $k$-means++ potential argument, the chance to hit an already covered heavy cluster in the early steps is small. 

=== progress.md ===
...
 increments of eH_t(H) are quantified.

### Idea 5: Lower-bound direction tailored to random-k smoothing (plan refinement)
Objective: Build an instance where Δ(k)=1 on a 1−o(1) fraction of k∈[k0,2k0), forcing an Ω(log k0) average even under random-k smoothing.
- Construction sketch: Start from a Brunsch–Röglin-type instance with one large “heavy” simplex cluster and many singleton “light” clusters positioned at radii calibrated so that each additional center mostly “unlocks” one light cluster at a time with only a tiny multiplicative drop in OPT (e.g., OPT_{k−1}/OPT_k ≈ 1 + Θ(1/ln k0)).
- Goal property: For most k in the window, s(k)=k−1, hence Δ(k)=1 and Theorem 1 collapses to the fixed-k O(ln k) bound.
- Next steps: Specify the radii schedule r_i to ensure OPT’s discrete derivative remains within a (1+O(1/ln k0)) multiplicative gap for ≳0.9 k0 steps; then compute E_k E_++[ALG/OPT] ≥ c ln k0 via Arthur–Vassilvitskii’s lower-bound methodology, adapted to the window.

### Concrete next steps (verifiable)
1) Add the “optimized-τ corollary” to output.md with explicit constants and a short proof (as above). This gives an easy-to-use min-of-two bound: logarithmic vs. an (m/L)-driven term.
2) Incorporate the global min-of-two mixture bound and document the regimes it improves upon Theorem 2 (particularly when L is a moderate constant fraction of m).
3) Formalize the 0.01-probability guarantee in the style of Idea 3, with explicit constants for C_bi=5; specify the dependence on the plateau mass p to obtain a global guarantee.
4) Prove the expected heavy-collision bound under the β-persistence assumption cleanly (full write-up). Then investigate proving β-persistence from scale separation R≥k^C using the same pairwise-cost and supermartingale techniques as in MRS’s covered-cost analysis.
5) Begin crystallizing the random-k lower bound: settle a concrete radii schedule ensuring microscopic OPT drops across most of the window and work through the Δ(k)=1 frequency.

### Why these are incremental and auditable
- All new claims are direct instantiations of Theorem 3 or straightforward corollaries (τ=L−1, τ=⌊L/2⌋), with explicit constants; they can be checked algebraically.
- The probability guarantee is a direct, quantifiable application of Corollary 6 and Markov’s inequality.
- The micro-lemma isolates one probabilistic piece (expected heavy-collisions), making the remaining gap (β-persistence) explicit and testable in subsequent rounds.

### Quick audit of output.md (this round)
- The min-of-extremes bound (Corollary 7) and the derived upper envelope (Corollary 8) look correct and rigorously supported by Theorem 3 and H_q ≤ ln q + 1. The “plateau length within [k0,2k0)” Remark is now correctly stated with m/(L−1) ≥ 1.
- The probability corollary (Corollary 9) is also correct in its methodology (quantile-in-Δ + Markov). It cleanly separates the plateau-level and global mixing statements.
- Missing but useful: a rigorously stated “balanced-split” instantiation of Theorem 3 with τ=⌊L/2⌋ (with safe constants) that can improve constants in some intermediate regimes, and an explicit instantiation of constants in Corollary 8 when C_bi=C_fix=5 to facilitate downstream aggregation.

### New, checkable refinements (to be curated)

1) Balanced-split corollary with safe constants (intermediate regime)
- Statement (new). In the setting of Theorem 3 with L≥3 and τ:=⌊L/2⌋, for k uniform on I and ++ randomness,
  E[ALG/OPT] ≤ 2 C_bi [ (2/3)(a + 1 + ln(4m/(L−1))) + 1/2 + (m/(e(L−1)))(ln 3 + 1) ] + (C_fix(ln m + O(1)))/L,
  where a=2+1/(2e).
- Proof sketch (fully checkable): Start from Theorem 3, choose τ=⌊L/2⌋. Use the uniform bounds valid for all L≥3: τ/(L−1) ≤ 2/3; (L−1−τ)/(L−1) ≤ 1/2; H_{L−2}−H_{τ−1} ≤ ln 3 + 1; and ln(2m/τ) ≤ ln(4m/(L−1)). Substitute these into Theorem 3 and simplify. This yields a rigorous bound that can outperform both extremes in some midrange regimes (large L but not a fixed fraction of m), at the expense of slightly bigger constants on the logarithmic term.
- Why useful: Strengthens Corollary 7 by adding a third, explicitly computable candidate bound in the min, sometimes improving over the pure-log extreme when L is moderately large, and over the τ=1 extreme when m/(L−1) is not too big.

2) Explicit constants for the upper-envelope (Corollary 8) when C_bi=C_fix=5
- Claim (explicit instantiation). With C_bi=C_fix=5 and L≥2,
  E_{k∈I} E_++[ALG/OPT] ≤ 35 + 10·min{ ln^+(2m/(L−1)), (m ln L)/L } + 5·(ln m)/L.
- Justification: From Corollary 7, the logarithmic extreme gives 2·C_bi·(a+1) ≤ 2·5·(2+1/(2e)+1) ≤ 32 (more carefully: 10·(3+1/(2e)) ≈ 31.83), we round up to 35 to absorb O(1/L) terms and endpoint constants. The coefficient in front of ln^+(2m/(L−1)) is 2·C_bi=10. The hybrid extreme contributes a (m ln L)/(e(L−1)) term with factor 2·C_bi/e ≤ 10/e ≈ 3.68; we upper-envelope it by 10 for simplicity, safe since L−1≈L for L≥2. The endpoint Δ=0 term adds ≤ C_fix(ln m + 2)/L = 5(ln m + 2)/L, which we cover by 5·(ln m)/L after absorbing constants into the 35. This produces a fully explicit, conservative version of Corollary 8 ready for aggregation.
- Use: Facilitates clean global inequalities without carrying symbolic constants; helpful when deriving numeric probability thresholds from Corollary 9.

3) Plateau-level 1% guarantee: explicit instantiation
- From Corollary 9, choosing η=0.99 and c=100, on a single plateau I=[m,m+L) we get
  P_{k∼U(I),++}[ ALG/OPT ≤ 100·B_I ] ≥ (1−0.99−O(1/L))·0.99 ≈ 0.01 − O(1/L),
  with B_I = 2·C_bi·(1 + m/(e(T−1))) and T=⌈0.99(L−1)⌉+1. If L ≥ α m, then B_I ≤ 2·C_bi·(1 + 1/(α e · 0.99)) ≤ 10·(1 + 1.02/αe). For α=1 and C_bi=5, this gives B_I ≲ 10·(1+0.376) ≈ 13.76, hence ALG/OPT ≤ 1376 with probability ≥ 1% for k uniform on that plateau. If the plateau occupies a γ-fraction of [k0,2k0), the global (k,++) probability is ≥ γ times this quantity.
- Why useful: A ready-to-use “0.01-probability” bound with explicit constants under the simple condition L ≥ α m on one plateau covering a nonnegligible fraction γ of the window.

### Special-case heavy-coverage under metric separation (progress toward Subsec. 2)
- Motivation. The conjectural “heavy coverage” lemma under scale separation is difficult to prove in full generality due to possible reductions of U_t(H) by centers sampled outside heavy clusters. As an incremental step, we formalize and prove a special-case lemma under explicit metric separation, making the dominance/persistence condition verifiable.
- Setup. Let H be the set of k1 optimal clusters (heavy), and L the rest. For each heavy cluster P∈H, let diam2(P):=max_{x∈P}||x−µ_P||^2, and impose the separation condition: for some λ≥10,
  dist(µ_P, y) ≥ λ · sqrt(diam2(P)) for all P∈H and all y∉P.
  This enforces that points outside P are at least λ radii away from its centroid. Let k=k1+k2 and run k-means++ for k′=k1+(1+ε)k2 steps.
- Lemma (special-case heavy coverage). Under the above separation with λ≥10,
  (i) For any step t before covering all k1 heavy clusters, conditioned on the filtration, the probability that the next sample falls in an uncovered heavy cluster is at least 1 − O(1/λ^2).
  (ii) Consequently, with probability ≥ 1 − k1·O(1/λ^2), k-means++ selects one center in each heavy cluster within the first k1 selections that hit heavy clusters (i.e., before any heavy collision).
  (iii) Conditioned on (ii), the remaining k′−k1 steps oversample the light part by a factor (1+ε), hence by MRS Theorem 5.1, the final cost is ≤ 5·O(ln(1/ε))·OPT_k.
- Proof sketch (checkable constants): For (i), before covering P∈H, every x∈P has D(x)^2 ≥ ||x−µ_P||^2 (no heavy center chosen), while for any y∉P, D(y) ≥ dist(y,µ_P)−||x−µ_P|| ≥ (λ−1)·sqrt(diam2(P)) by triangle inequality. Summing over all uncovered heavy clusters and using that OPT1(P)=∑_{x∈P}||x−µ_P||^2, we get that the D^2-mass of uncovered heavy points is at least ∑_{P∈H_uncovered} OPT1(P), while the mass outside these heavy clusters is at most a 1/(λ−1)^2 fraction per point relative to diameters; aggregating, the uncovered-heavy mass dominates by a factor 1−O(1/λ^2). Thus the next D^2-sample lands in uncovered heavy with probability ≥ 1 − O(1/λ^2). Summing the collision probabilities over the k1 heavy hits yields (ii). Part (iii) follows from the bi-criteria bound (MRS Theorem 5.1). A short writeup can fix constants (e.g., replacing O(1/λ^2) by ≤ 5/λ^2) by carefully bounding the total outside mass in terms of ∑_{P∈H} OPT1(P).
- Why useful: Proves the desired phenomenon (coverage before collisions and O(ln(1/ε)) tail) in a clean geometric regime; provides a stepping stone toward deriving persistence from purely scale-separation conditions.

### Lower-bound direction: toward robustness under random-k smoothing
- Goal. Construct a dataset X for which Δ(k)=k−s(k)=1 for a 1−o(1) fraction of k∈[k0,2k0), implying that the plateau calculus cannot improve beyond Θ(log k0) unconditionally.
- Candidate blueprint (adapting MRS App. C.2):
  - Use one “heavy” simplex (many points) centered at the origin with extremely small edge length (so OPT1(heavy) ≈ 1 after scaling) and many “light” singleton clusters placed on coordinate axes at radii r_j chosen so that adding a center primarily “unlocks” one light cluster at a time with a small multiplicative drop in OPT.
  - Calibrate radii r_j so that OPT(k−1)/OPT(k)=1+Θ(1/ln k0) for most k in [k0,2k0), making the OPT sequence nearly flat and ensuring s(k)=k−1 typically.
- Next, compute E_{k,++}[ALG/OPT] over this window using Arthur–Vassilvitskii’s lower-bound technique (the “misses” process), to target a matching Ω(log k0) expectation.
- Concrete next steps: (i) write the exact OPT_k for this family; (ii) verify the discrete derivative control across the window; (iii) adapt the MRS miss-counting lower bound to average over k.

### Examples and sanity checks
- Long dyadic plateau: With OPT halving only once on [k0,2k0), L≈m. Using the explicit constants above, E_{k∈I}E_++[ALG/OPT] ≤ 35 + 10 ln 2 + o(1) ≈ 41.9.
- Intermediate L (balanced split advantage): Take L=m/√ln m. The balanced-split bound gives a term (m/(e(L−1)))(ln 3+1) ≈ O(√ln m), whereas the pure-log extreme gives 10 ln(2√ln m) = O(ln ln m). In this regime, the logarithmic extreme remains asymptotically better; the τ=⌊L/2⌋ bound, however, can numerically improve constants when m/(L−1) is small but L not a constant fraction of m.

### Obstacles and gaps
- Unconditional improvement: Without structural constraints on OPT(·), the plateau calculus still allows Θ(log k0) worst-case averages (e.g., when all L_j=1). No universal O(log log k0) upper bound is currently proven.
- Heavy-coverage beyond metric separation: Deriving a persistence bound U_t(H) ≳ Σ_H purely from OPT-scale separation remains open. The special-case lemma addresses geometric separation but not the general case.

### Next steps (actionable)
1) Add the balanced-split corollary (with τ=⌊L/2⌋) and its proof (with safe constants) to output.md; note it as an optional third candidate in the min.
2) Record the explicit-constants version of Corollary 8 for C_bi=C_fix=5 to support numerical corollaries and global aggregation.
3) Write a short, fully rigorous proof of the special-case heavy-coverage lemma under metric separation (choose concrete constants, e.g., λ≥20 to make probabilities crisp), and add it as a separate lemma with an application corollary (O(ln(1/ε)) under oversampling of light part).
4) Begin formalizing the random-k lower bound: fix a radii schedule {r_j}, prove Δ(k)=1 for ≥ 0.9k0 values in [k0,2k0), and compute the averaged Ω(log k0) lower bound via the miss-counting framework.
5) Explore whether mild regularity assumptions on OPT(·) (e.g., lower bounds on plateau lengths at dyadic scales) can be derived from general geometric principles to yield unconditional O(log log k0) bounds via Corollary 8.

## Round 0004 — 2025-08-27T19:48:23.817717Z

Overview and audit of output.md
- The curated file now contains: (i) the per-k anchor bound (Theorem 1), (ii) per-plateau averaging (Theorem 2), (iii) the monotonicity lemma, (iv) a tunable τ-split (Theorem 3), (v) the hybrid extremes (Corollary 4), (vi) long-plateau constants (Corollary 5), (vii) a quantile-in-Δ lemma (Corollary 6), (viii) the min-of-extremes bound (Corollary 7), (ix) an upper envelope (Corollary 8), and (x) a plateau-level and globally mixed probability statement (Corollary 9). The remark “L ≤ m on [k0,2k0)” is present with the correct implication m/(L−1) ≥ 1.
- I see no correctness issues that require retraction. Constants are not yet fully optimized but are kept symbolic, which is fine for now.

New refinement: an improved “third branch” via choosing τ ≈ m/ln L
Why useful here. Corollary 8 gives the envelope K0 + K1·min{ ln^+(2m/(L−1)), (m ln L)/L } + K2·(ln m)/L. The (m ln L)/L term is sometimes loose. By an explicit choice τ := ⌊m/ln L⌋ in Theorem 3 (when admissible), we can replace (m ln L)/L by (m (1 + ln ln L))/L, which is strictly smaller for L≥e^e, tightening intermediate regimes.

Statement (proposed Corollary: τ = ⌊m/ln L⌋ branch).
- Setting τ* := min{ L−1, ⌊m/ln L⌋ } in Theorem 3, for L≥3 and m≥L, there exist absolute constants K0,K1,K2 (depending only on C_bi,C_fix) such that
  E_{k∈I} E_++[ALG_k/OPT_k]
  ≤ K0 + K1·(m/L)·(1 + ln ln L) + K2·(ln m)/L.
- In particular, when τ* = ⌊m/ln L⌋ ≤ L−1 (i.e., m ≤ (L−1) ln L), one can take
  E_{k∈I} E_++[ALG_k/OPT_k]
  ≤ 2 C_bi·[ 1 + (m/((L−1) ln L))·(a+1+ln(4 ln L)) + (m/(e(L−1)))·(1 + ln ln L) ] + (C_fix·(ln m + 2))/L,
  with a := 2 + 1/(2e).

Proof sketch (fully checkable from Theorem 3).
- Start from Theorem 3 with general τ. Put τ := ⌊m/ln L⌋. We use the following bounds for L≥3 and m≥L:
  1) τ ≥ m/ln L − 1 ⇒ 2m/τ ≤ 4 ln L ⇒ ln(2m/τ) ≤ ln(4 ln L).
  2) τ/(L−1) ≤ (m/ln L)/(L−1) ≤ m/(L ln L).
  3) H_{L−2} − H_{τ−1} ≤ ln((L−2)/(τ−1)) + 1 ≤ ln(L/τ) + 1 ≤ ln( (L ln L)/m ) + 1 ≤ 1 + ln ln L, since L ≤ m.
- Plugging into Theorem 3’s RHS:
  (τ/(L−1))(a + 1 + ln(2m/τ)) ≤ (m/(L ln L))(a+1+ln(4 ln L));
  ((L−1−τ)/(L−1)) ≤ 1;
  (m/(e(L−1)))(H_{L−2} − H_{τ−1}) ≤ (m/(e(L−1)))(1 + ln ln L).
- Add the Δ=0 endpoint term C_fix(ln m + O(1))/L. Absorb additive constants into K0 and account for L−1 ≈ L (since L≥3) to get the displayed form.
- If τ* = L−1 (i.e., m > (L−1) ln L), we revert to the pure-log branch in Corollary 4.

Consequences and regime comparison
- Min-of-three envelope. Taken together with Corollary 7, we can use a min of three branches: (i) pure-log: ln^+(2m/(L−1)), (ii) hybrid (τ=1): (m ln L)/L, (iii) the new τ=⌊m/ln L⌋: (m (1 + ln ln L))/L. This uniformly strengthens Corollary 8 by allowing the improved middle regime.
- Where it helps: When L is moderately large relative to m (e.g., L ≥ m^γ with γ∈(0,1)), both hybrid branches yield O(1). The improvement is most visible for intermediate L where the previous envelope gave O((m ln L)/L) but ln ln L ≪ ln L, so the new bound is smaller by a factor ≈ ln L / ln ln L.
- Checkable example. Let L = m/√ln m (so L ≤ m and L≥3 for m large). Then
  • Pure-log: ln(2m/(L−1)) ≈ ln(2√ln m) = O(ln ln m).
  • Old hybrid: (m ln L)/L ≈ (m ln(m/√ln m))/(m/√ln m) ≈ √ln m · (ln m − (1/2) ln ln m) = Θ(√ln m · ln m) [very loose].
  • New branch: (m (1 + ln ln L))/L ≈ √ln m · (1 + ln(ln m − (1/2) ln ln m)) = Θ(√ln m · ln ln m).
  Hence min{·} now picks the O(ln ln m) pure-log branch anyway, but the new branch dramatically tightens the hybrid alternative and will be advantageous in other intermediate regimes (e.g., L = m / polylog m).

Global aggregation (plateau-wise min-of-three)
- Using the decomposition of Corollary 3, we can strengthen Corollary 8 as follows. There exist absolute constants K0,K1,K2 such that for the covering plateaus I_j=[m_j,m_j+L_j),
  E_{k∼U([k0,2k0))} E_++[ALG_k/OPT_k]
  ≤ (1/k0) ∑_j L_j [ K0 + K1·min{ ln^+(2m_j/(L_j−1)), (m_j ln L_j)/L_j, (m_j (1 + ln ln L_j))/L_j } ]
    + (K2/k0) ∑_j ln m_j.
- This follows by taking, for each plateau, the minimum over the three explicit instantiations τ=L−1, τ=1, τ=⌊m_j/ln L_j⌋, and summing with weights L_j/k0.

Probability-≥0.01 guarantees (clarification)
- Corollary 9 provides the plateau-level and globally mixed guarantees via (η,c). A simple instantiation for “≥0.01 probability”: If a single plateau I occupies a γ≥0.02 fraction of [k0,2k0) and satisfies L ≥ α m, then choosing η=0.5 and c=2 gives
  P[ ALG_k/OPT_k ≤ 2·B_I ] ≥ γ·(0.5−O(1/L))·0.5 ≥ 0.01 for all sufficiently large L,
  with B_I ≤ 2·C_bi (1 + 1/(α e η)) + o(1). This is numerically loose but completely explicit. Tuning (η,c) can improve constants as desired.

Small technical lemmas and clarifications (ready-to-curate)
- Stirling floor correction in the τ=⌊m/ln L⌋ branch: Using τ ≥ m/ln L − 1 and m ≥ L ≥ 3, the inequality 2m/τ ≤ 4 ln L holds (since m/ln L ≥ L/ln L ≥ 3/ln 3 > 2, hence m/ln L − 1 ≥ (1/2)·m/ln L), justifying ln(2m/τ) ≤ ln(4 ln L) cleanly.
- Harmonic tail control: H_{L−2} − H_{τ−1} ≤ ln(L/τ) + 1 works uniformly for L≥3, τ≥1; with τ=⌊m/ln L⌋ and L ≤ m, this gives ≤ 1 + ln ln L.

Heavy-coverage program: a precise, checkable sub-lemma (pathwise bound)
- Pathwise covered-heavy control. For any set H of heavy optimal clusters, define eH_t(P) as in MRS. Then pathwise we have H_t(P) ≤ eH_t(P) ≤ 5·OPT1(P) for every heavy P and every t (the first inequality is by definition of eH, the second is Lemma 4.1 applied at first hit). Hence
  H_t(H_cov) ≤ 5·∑_{P∈H} OPT1(P) deterministically for all t.
- Conditional per-step collision probability bound. At step t conditioned on filtration F_t,
  P[“sample from a covered heavy cluster” | F_t] = H_t(H_cov) / cost_t(X)
  ≤ 5·(∑_{P∈H} OPT1(P)) / cost_t(X).
- This converts the heavy-collision question into lower bounding cost_t(X) (denominator) by a function of the current uncovered-heavy mass U_t(H) (plus possibly light mass). Under a quantitative “dominance/persistence” hypothesis of the form cost_t(X) ≥ β·U_t(H) for all t before heavy coverage completes (β≥1), we immediately get
  P[collision in heavy at step t | F_t] ≤ 5·(∑_H OPT1(P)) / (β·U_t(H)).
  Summing only over steps in which a heavy pick occurs and using U_t(H) ≥ ∑_{P∈H_uncovered} OPT1(P), we find that the expected number of heavy collisions before covering all heavy clusters is bounded by a telescoping potential argument akin to a variable-probability coupon collector. This is a concrete, checkable path: it remains to prove the β-persistence from a scale-separation condition (e.g., OPT_{k1} ≥ k^C·OPT_k) via the eH_t supermartingale and to carry out the summation rigorously (I will do this next).

Examples and sanity checks
- Dyadic plateau (one long plateau): L≈m. All three branches give O(1) (pure-log dominates), consistent with Corollary 5.
- Geometric intermediate L: If L = m / polylog m, then the pure-log branch gives O(log log m); the new τ=⌊m/ln L⌋ branch gives O( (m/L)·ln ln L ) = O( polylog(m)·ln ln m ), which is larger; hence the min will pick the pure-log branch, as desired.
- Short plateaus L=O(1): All hybrid branches devolve to O(m/L), while pure-log gives O(ln m). Min therefore selects O(ln m), matching known worst cases.

Gaps and open tasks (clearly identified)
- output.md currently presents the min-of-two envelope (Corollary 8). The improved τ=⌊m/ln L⌋ branch is not yet recorded; adding it would strengthen the per-plateau bound in some regimes.
- The heavy-coverage argument remains conditional. The pathwise bound H_t(H_cov) ≤ 5∑_H OPT1(P) is now explicit and correct. The key missing piece is to prove a denominator persistence bound cost_t(X) ≥ β·U_t(H) (or a comparable lower bound) from a scale-separation assumption; I will attempt this via the eH_t supermartingale and a careful analysis of how sampling outside H affects U_t(H).
- Lower bound under random-k smoothing: still open; designing an instance with many short plateaus across the entire window requires careful OPT(·) control. This remains an item for a separate development.

Concrete next steps (verifiable)
1) Curate the new τ=⌊m/ln L⌋ corollary into output.md (as a third explicit instantiation of Theorem 3) and add the “min-of-three” per-plateau envelope and its global aggregation counterpart. All inequalities are elementary and already itemized above.
2) Provide explicit numerical constants K0,K1,K2 when C_bi=C_fix=5 for the new branch, e.g., K0≈40, K1≈10, K2≈5 (to be checked and tightened in write-up).
3) For the probability-≥0.01 variant, add a short remark instantiating Corollary 9 with (η,c) tuned to hit 1% under a simple plateau mass condition (as outlined above), giving a concrete constant.
4) Heavy-coverage: formalize “β-persistence from scale separation” and sum conditional collision probabilities over heavy draws until coverage; aim for an explicit bound on expected heavy collisions (e.g., polynomially small in k under scale separation), then lift to high probability via Freedman–Azuma.
5) Explore, even empirically, OPT profiles where the new τ=⌊m/ln L⌋ branch strictly improves the previous envelope, to guide any further analytical tightening (e.g., replacing ln(4 ln L) by ln ln(2m) in some ranges).

## Round 0005 — 2025-08-27T19:54:44.231916Z

Round 0005 — Audit, small fixes, and new, checkable corollaries for global aggregation and probability

Summary of audit of output.md
- The new τ≈m/ln L branch (Corollary 10), the min-of-three per-plateau envelope (Corollary 11), and explicit constants for C_bi=C_fix=5 (Corollary 12) are all correct and valuable. The “≥1% probability” remark now uses η=0.98, c=2, which cleanly gives 1% mass up to O(1/L).
- Minor gap to clarify: Corollary 11 explicitly restricts the third branch (m(1+ln ln L))/L to L≥3. Corollary 12 states a min-of-three for L≥2 without this restriction. For L=2, ln ln L<0, so the third term can be negative and should not be part of the minimum. Suggested fix: add “the third term is intended only for L≥3; for L=2 take the minimum of the first two terms,” mirroring Cor. 11.

New, checkable additions recommended for output.md

1) Global min-of-three aggregation across plateaus (explicit statement)
- Statement (Global envelope). Let [k0,2k0) be partitioned into maximal factor-2 plateaus I_j=[m_j,m_j+L_j), j=1,…,J. Then for k uniform in [k0,2k0) and k-means++ randomness,
  E_{k,++}[ALG_k/OPT_k]
  ≤ (1/k0) Σ_{j: L_j≥2} L_j [ K0 + K1·min{ ln^+(2m_j/(L_j−1)), (m_j ln L_j)/L_j, (m_j(1+ln ln L_j))/L_j } ]
    + (K2/k0) Σ_{j: L_j≥2} ln m_j + (1/k0) Σ_{j: L_j=1} C_fix(ln m_j + O(1)).
  For C_bi=C_fix=5, one can set K0=35, K1=10, K2=5 (as in Corollary 12). For the third term in the minimum, restrict to L_j≥3.
- Why useful: This directly operationalizes Corollaries 11–12 into a window-level bound and makes the tradeoffs transparent. Proof: average Cor. 11 over each plateau with weight L_j/k0, add the Δ=0 endpoint contributions (as in Cor. 3), and handle L_j=1 via the fixed‑k bound.

2) A simple bound on the endpoint-sum contribution (unweighted log term)
- Lemma (Endpoint term cap). With notation above, (1/k0) Σ_{j: L_j≥2} ln m_j ≤ ln(2k0).
  Proof: There are at most J≤k0 plateaus and each m_j≤2k0−1, so Σ ln m_j ≤ J·ln(2k0) ≤ k0·ln(2k0). Dividing by k0 yields the claim. Consequence: the unweighted endpoint term is never more than O(ln k0), matching the worst-case Θ(ln k0) behavior already noted.

3) Long-plateau constant regime — global instantiation with explicit constants
- Corollary (Global constant under a single long plateau). Suppose there exists a plateau I=[m,m+L) with L≥α m and L/k0≥γ. Then using Corollary 5 with C_bi=5 and any L≥3,
  E_{k∈I,++}[ALG/OPT] ≤ 10·(3 + 1/(2e) + ln(2/α)) + o(1) (as L→∞).
  Therefore,
  E_{k,++}[ALG/OPT] ≤ γ·O_α(1) + (1−γ)·[the global min-of-three bound on [k0,2k0)\I].
- This is an immediate corollary of Cor. 5 and the global averaging in Item 1.

4) 1%-probability guarantee — explicit global mixing
- Corollary (≥1% mass under a long plateau). Under the same assumptions as above (L≥α m, L/k0≥γ), Corollary 9 with η=0.98 and c=2 and the Remark after Corollary 12 give
  P_{k∼U([k0,2k0)),++}[ ALG/OPT ≤ 2·B_I ] ≥ γ·(1−η−O(1/L))·(1−1/2) ≥ 0.01·γ − O(γ/L),
  with B_I = 2·C_bi·(1 + 1/(α e η)) + o(1). For C_bi=5 and α=1 this yields ALG/OPT ≤ ~20·(1 + 1/(0.98 e)) ≈ 27.7 on at least 1%·γ of the joint (k,++) mass. This directly instantiates the alternative task objective under a simple, checkable condition.

Examples and sanity checks
- Power-law OPT: OPT_k ∝ k^{−p} with p>0. Over [k0,2k0), a single factor‑2 drop occurs, so plateau lengths satisfy L ≈ (2^{1/p}−1)m = Θ(m), i.e., α=Θ_p(1). Global average is O_p(1) by Item 1.
- Worst-case short plateaus: If L_j=1 for J≈k0 plateaus, Item 2 shows the endpoint term alone is Θ(ln k0), so no unconditional improvement over Θ(ln k0) is possible via plateau calculus.

Obstacles and open tasks
- Constants for small L: For L=2, ensure the third branch is excluded in Corollary 12 (as already noted for Corollary 11). This avoids negative values from ln ln 2 in the minimum.
- Heavy-coverage program: The metric-separation special case remains promising, but a fully general scale-separation-to-persistence derivation is open. Next concrete step: quantify a denominator lower bound cost_t(X) ≥ β·U_t(H) under explicit geometric or separation assumptions, then sum conditional collision probabilities rigorously.

Next steps (concrete and verifiable)
1) Add the global min-of-three aggregation statement (Item 1) to output.md with the explicit constants option for C_bi=C_fix=5; include the L≥3 restriction for the third branch in both Corollaries 11 and 12.
2) Add the endpoint-sum cap lemma (Item 2) as a remark to bound the contribution of Δ=0 endpoints globally by ≤ ln(2k0)·K2.
3) Include the long-plateau global corollary and the 1%-probability global instantiation (Items 3–4), which follow immediately from existing corollaries.
4) Heavy-coverage: attempt a formal bound of the form E[U_{t+1}(H) | F_t, sample outside H] ≥ (1−ε/R)·U_t(H) under a scale separation R≥k^C plus a mild geometric regularity, then proceed to an expected-collisions bound via conditional probability summation.

Why these are incremental and auditable
- All additions are direct aggregations or instantiations of existing per-plateau results, with short proofs and explicit constants.
- The minor fix in Corollary 12 (L≥3 restriction for the third term) is precise and prevents a small edge-case pitfall.
- The probability corollary provides a clean ≥1% guarantee meeting the alternative task statement under transparent conditions.

## Quick audit of output.md (this round)
- The new τ≈m/ln L branch (Corollary 10) is stated correctly with L≥3 and uses the previously recorded remark L≤m on [k0,2k0). The inequalities τ≥m/ln L−1 ⇒ ln(2m/τ)≤ln(4 ln L), τ/(L−1)≤m/((L−1) ln L), and H_{L−2}−H_{τ−1}≤1+ln ln L are valid and inserted properly.
- The min-of-three envelope (Corollary 11) logically follows by taking the minimum of the three explicit instantiations and absorbing additive terms. The explicit-constant version for C_bi=C_fix=5 (Corollary 12) is conservative and sound. A minor constant improvement is possible (see below), but not necessary for asymptotics.
- The probability remark now uses η=0.98 and c=2 to ensure an unconditional ≥1% mass (up to O(1/L)), which fixes the earlier 0.0099 issue.

I found no correctness errors; the presentation is coherent and self-contained for the plateau calculus.

## New, checkable contributions

### Lemma 1 (Global worst-case aggregation recovers O(log k0))
Statement. Let [k0,2k0) be partitioned into maximal factor-2 plateaus I_j=[m_j,m_j+L_j) with ∑_j L_j=k0. Using the pure-log branch from Corollary 7 (and the fixed-k bound for L_j=1), we have the unconditional bound
E_{k∼U([k0,2k0)),++}[ALG_k/OPT_k] ≤ K′ + K″·ln(2k0),
for absolute constants K′,K″ depending only on C_bi,C_fix.
Why useful. This shows our framework unconditionally matches the Θ(log k0) baseline, validating that improvements necessarily require structural information on plateaus.
Sketch proof (fully checkable). For L_j≥2,
(1/k0)·L_j·ln^+(2m_j/(L_j−1)) ≤ (1/k0)·L_j(ln 2 + ln m_j) − (1/k0)·L_j·ln(L_j−1).
Summing over j and using L_j−1≥1 so the last term is ≤0, we get
(1/k0)∑_{j:L_j≥2} L_j ln^+(2m_j/(L_j−1)) ≤ ln 2 + (1/k0)∑_{j:L_j≥2} L_j ln m_j.
Now, for every k∈I_j, m_j≤k, hence ln m_j ≤ ln k. Therefore,
(1/k0)∑_{j:L_j≥2} L_j ln m_j ≤ (1/k0)∑_{k=k0}^{2k0−1} ln k ≤ ln(2k0) − 1 + o(1).
Add the endpoint terms (1/k0)∑_{j:L_j≥2} A2 ln m_j ≤ A2 ln(2k0) and the L_j=1 contribution (bounded by C_fix(ln(2k0)+O(1)) per index). Multiplying by the fixed absolute coefficients from Corollaries 2 and 7 and absorbing constants yields the displayed K′+K″ ln(2k0) bound.

### Corollary 1 (Mixture bound with a long-plateau mass)
Statement. Suppose the set J_long of plateaus satisfying L_j ≥ α m_j covers a γ-fraction of [k0,2k0), i.e., ∑_{j∈J_long} L_j ≥ γ k0, where α,γ∈(0,1]. Then
E_{k}[ALG/OPT] ≤ γ·B_long(α) + (1−γ)·(K′ + K″·ln(2k0)) + O((1/k0)∑_j ln m_j),
with B_long(α) = 2·C_bi·(a+1+ln(2/α)) (cf. Corollary 5) and absolute K′,K″ from Lemma 1.
Why useful. This gives a clean, global interpolation: any nontrivial mass γ of long plateaus pushes the average below the Θ(log k0) worst case by an additive γ·O(1), with explicit constants.
Proof sketch. Average Corollary 5 over j∈J_long with weights L_j/k0 (giving ≤γ·B_long(α)+o(1)), and use Lemma 1 on the complement. The endpoint O((ln m_j)/L_j) terms aggregate into O((1/k0)∑_j ln m_j) ≤ O(ln(2k0)).

### Corollary 2 (Global ≥1% probability guarantee under a single long plateau)
Statement. If some plateau I=[m,m+L) satisfies L ≥ α m and covers a γ-fraction of [k0,2k0), then for k uniform on [k0,2k0) and ++ randomness,
P[ ALG_k/OPT_k ≤ 2·B_I ] ≥ γ·(0.5−O(1/L))·0.5 ≥ 0.01−o(1)
for parameters η=0.98, c=2 in Corollary 9, with B_I=2·C_bi·(1 + 1/(α e η)) + O(1/L).
Why useful. This is a global, fully explicit ≥1% probability guarantee under a single structural condition on OPT(·), mirroring the task’s alternative objective.
Proof. Directly apply Corollary 9 on I (with η=0.98, c=2) and mix with γ.

### Minor constant refinements (optional)
- In Corollary 12 with C_bi=C_fix=5, 2·C_bi·(a+1)=10·(3+1/(2e))≈31.84. One can safely state 33 instead of 35 if desired by allocating at most 1.16 to the endpoint O(1/L) term; this is purely cosmetic.

## Examples and sanity checks
- Worst-case pattern: Taking all plateaus of length 1 (i.e., OPT changes at every k) yields E_k,++[ALG/OPT] = Θ(log k0), matched by Lemma 1.
- Single dyadic-long plateau: If OPT halves once across the window (L≈m), then B_long(1) ≤ 2·C_bi·(a+1+ln 2) and the average is constant; Corollary 2 then gives a concrete ≥1% bound with an explicit constant factor.

## Heavy-coverage program: a clean conditional collision bound (clarified)
We crystallize a conditional, pathwise statement that reduces the heavy-collision analysis to two verifiable parameters.

Lemma 2 (Heavy-collision bound under dominance and covered-cost control).
Let H be a set of k1 “heavy” optimal clusters (in the k-clustering P). Assume that up to the stopping time τ when all heavy clusters are first covered, for every t<τ,
- Dominance: U_t(H) ≥ β · ( H_t(all) + U_t(L) ) for some β>0; and
- Covered-heavy control: H_t(H) ≤ α · S_H, where S_H := ∑_{P∈H} OPT1(P), α≥1 is an absolute constant (α=5 suffices in expectation via the MRS Lemma 4.1; pathwise we can enforce H_t(H) ≤ eH_t(H) ≤ 5 S_H). 
Then the expected number of times k-means++ samples a point from a covered heavy cluster before τ is at most k1 · ( α/(β+1) ) · (S_H/inf_{t<τ} U_t(H)). In particular, if inf_{t<τ} U_t(H) ≥ S_H (a strong persistence condition), then E[# heavy-collisions] ≤ k1·α/(β+1).
Why useful. This isolates the role of a quantitative persistence lower bound on U_t(H) and converts it into an explicit expected-collisions bound. It also cleanly uses the pathwise inequality H_t(H) ≤ eH_t(H) and the supermartingale eH_t.
Proof sketch. At step t<τ, conditional collision probability is p_t := H_t(H)/cost_t(X). By dominance, cost_t(X) = U_t(H) + (H_t(all)+U_t(L)) ≤ U_t(H)·(1 + 1/β). Hence p_t ≤ (α S_H)/((1+1/β) U_t(H)). Each sample that lands in heavy either covers a new heavy cluster or collides. Summing per-step collision probabilities over the at most k1 heavy hits and lower-bounding U_t(H) by inf_{t<τ} U_t(H) yields the bound.
Next step. Derive a sufficient condition (in terms of OPT-scale separation and/or geometric separation) implying inf_{t<τ} U_t(H) ≥ c·S_H with c>0 and β=poly(1), then lift expectation to high probability by Freedman’s inequality for supermartingales.

## Obstacles and open items
- Unconditional improvement barrier. Lemma 1 confirms that without structural constraints, the average cannot beat Θ(log k0), aligning with the fixed-k lower bounds.
- Heavy-coverage derivation from OPT-separation. The key missing piece remains a principled lower bound on U_t(H) in terms of S_H under a scale-separation hypothesis (e.g., OPT_{k1} ≫ OPT_k). We need to quantify how much sampling outside H can decrease U_t(H) before heavy clusters are hit; this likely requires a refined use of the eH_t supermartingale and per-cluster contributions.

## Next steps (concrete, verifiable)
1) Curate Lemma 1 and Corollary 1 into output.md to document unconditional aggregation and explicit mixture with long plateau mass.
2) Add Corollary 2 as a global ≥1% probability statement (with η=0.98, c=2) and explicit constants, referencing Corollary 9.
3) Optionally tighten Corollary 12’s leading constant from 35 to 33, or annotate why 35 is a conservative rounding.
4) For heavy-coverage, formalize Lemma 2 in output.md only after we pin down a verifiable persistence condition; in the meantime, attempt to prove inf_{t<τ} U_t(H) ≥ c·S_H and β=Ω(1) under a clean geometric or OPT separation hypothesis.
5) Provide a compact “regime chart” illustrating which of the three branches dominates as a function of L/m, to guide users of the envelope (purely expository, no new math).

## Round 0006 — 2025-08-27T20:03:23.239533Z

Round 0006 — Audit, minor clarifications, and new, checkable additions (balanced split, multi-plateau probability, and a quantified O(log log k0) mixture)

Quick audit of output.md (minor points and clarifications)
- Absorbing small 1/L terms into constants. In Corollary 8 the step “absorb constants into K0 and the terms (ln m)/(L−1), (m/(L−1)) into K2(ln m)/L and K1(m ln L)/L” is sound when used as an upper envelope: for L≥2, (ln(2m))/(L−1) ≤ 2(ln m)/L + O(1), and a/(L−1) ≤ a, so additive O(1/L) pieces can be folded into K0. It may help to add a one-line remark justifying these two inequalities to prevent confusion.
- Third branch domain. The L≥3 restriction is now clearly stated in Corollaries 11–12 and a clarifying remark follows, which fixes the earlier edge case (ln ln 2<0).
- Endpoint term accounting. Lemma 1 bounds the aggregated endpoint term by ≤ ln(2k0), which is sufficient for all global combinations used later.
I see no correctness issues; below I add compact, checkable refinements that can be curated next.

New, checkable corollaries and remarks

1) Balanced-split corollary (mid-split τ=⌊L/2⌋) with safe constants (fills an intermediate regime)
- Statement. In the setting of Theorem 3 with L≥3 and τ:=⌊L/2⌋, for k uniform on I=[m,m+L),
  E_{k∈I} E_++[ALG/OPT]
  ≤ 2 C_bi [ (2/3)(a + 1 + ln(4m/(L−1))) + 1/2 + (m/(e(L−1)))(ln 3 + 1) ]
    + (C_fix(ln m + O(1)))/L,
  where a:=2+1/(2e).
- Proof sketch (checkable). For L≥3, uniformly: τ/(L−1) ≤ 2/3; (L−1−τ)/(L−1) ≤ 1/2; ln(2m/τ) ≤ ln(4m/(L−1)); and H_{L−2}−H_{τ−1} ≤ ln 3 + 1 (verify directly for L=3,4 and use monotonicity for larger L). Substitute these bounds into Theorem 3. 
- Why useful. This bounds the tail by a pure (m/L) term (up to constants), sometimes improving constants over both extremes and complementing the τ≈m/ln L branch. It can be recorded as an optional fourth candidate in the per-plateau minimum (min-of-four), or just cited as a remark for midrange L.

2) Multi-plateau 1% probability guarantee (global mixing over several long plateaus)
- Statement. Let J_long be a family of plateaus I_j=[m_j,m_j+L_j) with L_j ≥ α m_j, and suppose their total coverage is γ:= (1/k0)∑_{j∈J_long} L_j. Then, with η=0.98 and c=2 (as in Corollary 9),
  P_{k∼U([k0,2k0)),++}[ ALG_k/OPT_k ≤ 2·B_j for k∈I_j for some j∈J_long ]
  ≥ γ·(1−η−O(1/ min_j L_j))·(1−1/2) = 0.01·γ − O(γ/ min_j L_j),
  where for each j, B_j := 2 C_bi (1 + m_j/(e(T_j−1))) with T_j:=max{2,⌈η(L_j−1)⌉+1}, and hence B_j ≤ 2 C_bi (1 + 1/(α e η)) + O(1/L_j).
- Proof. Apply Corollary 9 on each I_j with the same (η,c), then average with weights L_j/k0. The minimum L_j controls the O(1/L_j) term in the product measure.
- Why useful. This extends Corollary 16 from “one long plateau” to “a union covering γ of the window,” yielding a clean ≥1%·γ joint probability with the same explicit constants.

3) Explicit O(log log k0) mixture under polylog-long plateaus (quantified and ready to aggregate)
- Statement (global average). Fix σ≥1. Let S be the set of plateaus I_j with L_j ≥ m_j/(ln k0)^σ, and suppose ∑_{j∈S} L_j ≥ γ k0. Then, for k uniform on [k0,2k0) and ++ randomness,
  E[ALG/OPT] ≤ γ·[ 2 C_bi (a+1+ln(2(ln k0)^σ)) ] + (1−γ)·[K′ + K″ ln(2k0)] + o(1)
  = γ·[ 2 C_bi (a+1+ln 2 + σ ln ln k0) ] + (1−γ)·[K′ + K″ ln(2k0)] + o(1).
- Proof. On each I_j∈S, Corollary 5 with α=1/(ln k0)^σ gives a per-plateau constant a+1+ln(2/α) = a+1+ln 2 + σ ln ln k0, up to o(1). Weight these by L_j/k0 and sum to get the γ term; bound the complement by Corollary 14. The endpoint terms aggregate to o(1) on S (since (ln m_j)/L_j ≤ σ ln ln k0/(ln k0)^σ) and to O(ln k0) on the complement (Lemma 1), which is absorbed into the (1−γ) term.
- Why useful. This is a precise, easy-to-use version of the informal “O(log log k0) on long plateaus” statement with explicit dependence on σ and γ and fully tied to existing corollaries.

Heavy-coverage program — a crisp conditional lemma (expectation form, with parameters explicit)
- Lemma (expected heavy-collision bound under persistence and covered-cost control). Let H be k1 optimal clusters designated “heavy,” and let S_H:=∑_{P∈H} OPT1(P). Define U_t(H) (uncovered heavy cost at step t), H_t(all) and U_t(L) as usual. Suppose up to the stopping time τ (when all heavy clusters become covered):
  (i) Persistence/dominance: U_t(H) ≥ β·(H_t(all) + U_t(L)) for all t<τ;
  (ii) Covered-heavy control in expectation: E[H_t(H)] ≤ α S_H for all t (α=5 suffices by the eH_t supermartingale and Lemma 4.1 of MRS).
  Then the expected number of heavy-collisions (selections in already-covered heavy clusters) before τ is at most (α/(β+1))·k1·(S_H/ inf_{t<τ} U_t(H)). In particular, if inf_{t<τ} U_t(H) ≥ S_H (a strong persistence), then E[collisions] ≤ (α/(β+1))·k1.
- Proof sketch (conditional and checkable). At step t, condition on the filtration. The collision probability into covered heavy is ≤ H_t(H)/cost_t(X) ≤ α S_H/(U_t(H)+H_t(all)+U_t(L)) ≤ α S_H/((β+1)U_t(H)) by (i). Sum this bound over at most k1 heavy hits (each heavy step either covers a new heavy cluster or collides) and use U_t(H) ≥ inf_{s<τ} U_s(H). Take expectations and apply (ii).
- Why useful. This isolates the coupon-collector-like collision control under explicit, testable parameters (β and the persistence lower bound). The remaining step is to prove persistence from scale or geometric separation (e.g., OPT_{k1} ≫ OPT_k), at least in expectation.
- Note on rigor. Unlike earlier sketches, this version avoids pathwise claims about eH_t; it uses the expectation control E[H_t(H)] ≤ 5 S_H and yields an expectation bound. High-probability control can then be pursued via Freedman’s inequality once a concentration statement for eH_t is established.

Examples and sanity checks
- Dyadic single plateau (L≈m). Corollary 5 gives E[ALG/OPT] ≤ 2 C_bi (a+1+ln 2) + o(1). With C_bi=5 and a≈2.183, this is ≤ 10·(3.183 + 0.693) + o(1) ≈ 38.8.
- Polylog-long plateaus. With σ=2 and γ=1/2, the global bound in item 3 reads E[ALG/OPT] ≤ 0.5·[const + 2 C_bi·2 ln ln k0] + 0.5·K″ ln(2k0) + o(1); thus the long-plateau half contributes O(ln ln k0) explicitly.
- Mid-split vs other branches. For L=m/20, the balanced split gives a tail ≈ (m/L)·const ≈ 20·const; the log branch gives ≈ ln(40) ≈ 3.69 up to constants; log dominates here (as expected). For L=m/ln m, the τ≈m/ln L branch yields ≈ (m/L)(1+ln ln L) ≈ ln m (1+ ln ln ln m), which is looser than the log branch’s ≈ ln(2 ln m); again, the min picks the right regime.

Next steps (concrete and verifiable)
1) Curate the balanced-split corollary into output.md (as an optional candidate alongside the three existing branches). It is a direct instantiation of Theorem 3 with uniform, easy-to-check inequalities.
2) Add the multi-plateau ≥1% probability corollary (item 2), which follows immediately from Corollary 9 by mixing; it generalizes Corollary 16.
3) Add the quantified O(log log k0) mixture (item 3) as a corollary under the polylog-long-plateau condition with parameters (σ,γ); this formalizes a recurring use-case with explicit constants.
4) Heavy-coverage: Try to prove a persistence bound inf_{t<τ} U_t(H) ≥ c·S_H and a β=Ω(1) dominance under a clean scale separation hypothesis, first in a simplified geometric regime (e.g., metric separation) and then from OPT_{k1} ≥ k^C·OPT_k; combine with the expectation lemma above. Work towards concentration of eH_t to upgrade to high probability.

Why this advances the program
- The balanced-split adds a simple, rigorous midrange option, improving constants in some regimes without increasing complexity.
- The multi-plateau probability and quantified O(log log k0) mixture turn earlier informal remarks into precise, ready-to-use global statements with explicit parameters and constants.
- The heavy-coverage conditional lemma is a clean step that avoids pathwise pitfalls and sets up the final task (proving persistence from scale/geometry separation) needed to harvest O(log(1/ε)) from oversampling of the light part.

## Round 0006 — 2025-08-27T20:04:06.827647Z

Round 0006 — Small audits, two new τ-instantiations, a discrete-smoothness-to-long-plateaus lemma, and a clearer path for probability bounds

Audit of output.md (no blocking issues)
- Corollary 11/12: The “third branch only for L≥3” restriction is now clearly documented (Remark after Cor. 12). Good.
- Corollary 13 (global aggregation): The weights and treatment of L_j=1 via the fixed-k bound are correct. The endpoint sum cap (Lemma 1) is clean and used properly in Corollary 14.
- Corollary 16: The global ≥1% probability lower bound properly carries the plateau mass factor γ. Constants are explicit.
No correctness errors spotted. The additions below are meant to tighten the per-plateau envelope with simple, checkable instantiations and to connect “discrete smoothness” of OPT(·) to long plateaus.

New, checkable per-plateau corollaries (to add after Corollary 10)
1) Balanced-split with hard constants (τ = ⌊L/2⌋)
Statement. In Theorem 3, for L≥3 and τ:=⌊L/2⌋, for k uniform on I=[m,m+L):
E_{k∈I} E_++[ALG/OPT] ≤ 2·C_bi [ (2/3)(a+1+ln(4m/(L−1))) + 1/2 + (m/(e(L−1)))(ln 3 + 1) ] + (C_fix(ln m + O(1)))/L,
with a:=2+1/(2e).
Justification (fully checkable): For L≥3 and τ=⌊L/2⌋ we have τ/(L−1) ≤ 2/3; (L−1−τ)/(L−1) ≤ 1/2; ln(2m/τ) ≤ ln(4m/(L−1)); and H_{L−2}−H_{τ−1} ≤ ln 3 + 1 (verify small-L cases directly). Substitute these bounds into Theorem 3. Why useful. This removes the ln L factor in the harmonic tail, replacing (m ln L)/L by a pure (m/L) term at the expense of a larger constant on the logarithmic piece. It can be tighter than both existing extremes in midrange regimes.

2) Constant-tail split (τ ≈ (L−1)/e)
Statement. In Theorem 3, for L≥3 and τ:=⌊(L−1)/e⌋ ≥ 1,
E_{k∈I} E_++[ALG/OPT] ≤ 2·C_bi [ (1/e)(a+1+ln(2em/(L−1))) + (1−1/e) + (m/(e(L−1)))·2 ] + (C_fix(ln m + O(1)))/L.
Justification. With τ≈(L−1)/e, we have τ/(L−1) ≤ 1/e and (L−1−τ)/(L−1) ≤ 1−1/e. For the harmonic tail, H_{L−2}−H_{τ−1} ≤ ln((L−1)/(τ))+1 ≤ ln e + 1 = 2. Finally ln(2m/τ) ≤ ln(2em/(L−1)). Substitute into Theorem 3. Why useful. This gives a third, fully explicit branch with a constant harmonic tail contribution 2·(m/(e(L−1))), i.e., a clean O(m/L) tail with a small constant, and a logarithmic piece whose coefficient is 1/e of that in the pure-log term. It sometimes improves over τ=1 for moderate L when ln L is not small but m/L is.

Remarks on regime selection (min-of-five)
- The per-plateau min may be extended to five branches: τ∈{L−1, 1, ⌊m/ln L⌋, ⌊L/2⌋, ⌊(L−1)/e⌋}. All are explicit and easy to verify. A practical recommendation is to keep the three already curated (pure-log, τ=1, τ=⌊m/ln L⌋) and optionally add the constant-tail split τ≈(L−1)/e when one wants a clean O(m/L) tail with a smaller constant than in the τ=1 branch; the balanced-split can be listed as an optional midrange tightening.

Discrete smoothness ⇒ long plateaus: a simple structural lemma
Lemma (discrete-smoothness-to-length). Let (v_k)_{k≥1} be a nonincreasing positive sequence. Suppose on an interval [m, m+L) we have for some δ∈(0,1): for all k∈{m+1,…,m+L−1}, v_{k−1} ≤ (1+δ) v_k. Then any factor-2 drop of v_k over that interval requires L ≥ ⌈ln 2 / ln(1+δ)⌉ ≥ ⌈(ln 2)/δ⌉ − 1.
Proof. For t steps, the total multiplicative drop is at most (1+δ)^t. To drop by 2 we need (1+δ)^t ≥ 2 ⇒ t ≥ ln 2 / ln(1+δ). Since ln(1+δ) ≤ δ, t ≥ (ln 2)/δ. Thus, the length L of a factor-2 plateau must satisfy L−1 ≥ t, giving the displayed bound.
Corollary (long plateau from 1/m-type smoothness). If for all k in [m,2m) we have v_{k−1} ≤ (1 + A/m) v_k with some absolute A>0, then any factor-2 drop within [m,2m) requires L ≥ c_A·m for c_A:= (ln 2)/A − o(1). In particular, L ≥ α m for α:= (ln 2)/A − ε once m is large, hence Corollary 5 applies and the plateau contributes O_α(1) to the average.
Why useful here. Instantiating v_k := OPT(k), this gives a simple, checkable surrogate for “long plateaus”: if the discrete per-step multiplicative decay OPT(k−1)/OPT(k) is at most (1+A/m) on [m,2m), then the corresponding dyadic-scale plateau inside that window is long (L ≳ m), yielding a constant per-plateau bound and, if such windows occupy a γ-fraction of [k0,2k0), a global mixture bound of the form Corollary 15. This isolates a data-agnostic regularity condition that implies the beneficial “oversampling by randomizing k.”

Probability ≥ 1% under discrete smoothness (simple consequence)
Under the same 1/m-type smoothness on [m,2m) with A<∞ (hence L ≥ α m for α=(ln 2)/A−ε), Corollary 9 with η=0.98 and c=2 gives, on that plateau, joint (k,++) probability ≥ (0.02−O(1/L))·0.5 for attaining ALG/OPT ≤ 2·B_I with B_I ≤ 2·C_bi(1 + 1/(α e η)) + o(1). Mixing across [k0,2k0) by the plateau mass γ gives a global ≥ 0.01·γ − o(1) guarantee with an explicit constant, exactly as in Corollary 16.

Examples and quick checks
- If OPT(k−1)/OPT(k) ≤ 1 + (1/10m) for k∈[m,2m), then α≥ (ln 2)·10 ≈ 6.93 > 1. Since L cannot exceed m by the window constraint, this means the dyadic drop cannot happen within [m,2m); hence the entire window is a single plateau (L≈m) and the per-plateau bound is O(1).
- τ≈(L−1)/e branch vs τ=1: when L is large and m/L is moderate (say m/L ≤ c), the new branch replaces (m ln L)/L with ≈(2/e)(m/L) while the log term’s coefficient is reduced by 1/e. Numerically, for L=10^6 and m/L=2, τ=1 gives an O(m ln L/L) term ≈ 2·ln 10^6, while τ≈(L−1)/e gives ≈ (2/e)·2 ≈ 1.47 in the tail, often improving the minimum.

Heavy-coverage program (status and incremental step)
- Conditional collision control revisited. Let H be k1 heavy optimal clusters; write S_H := ∑_{P∈H} OPT1(P). For every t before heavy coverage completes, the per-step collision probability into a covered heavy cluster is p_t^{ch} = H_t(H_cov)/cost_t(X). Using the eH_t supermartingale, we have E[H_t(H_cov)] ≤ 5 S_H and hence E[p_t^{ch}] ≤ E[5 S_H / cost_t(X)]. To turn this into a usable bound, we need a pathwise (or at least conditional-expectation) lower bound on cost_t(X) in terms of the uncovered heavy mass U_t(H); e.g., a persistence condition cost_t(X) ≥ β·U_t(H). This remains the key missing link (as already noted in the verifier feedback). 
- Special-case metric separation (toward a fully rigorous lemma). Under the separation: for every heavy P and every y∉P, dist(µ_P,y) ≥ λ·rad(P) with λ≥20 (rad^2(P) denotes average squared radius contributing to OPT1(P)), one can bound, for all t before covering P, ∑_{x∈P} D_t(x)^2 ≥ OPT1(P) (trivial) and show that the total D^2-mass outside all uncovered heavy clusters is at most a 1/Θ(λ^2) fraction of ∑_{P∈H_uncovered} OPT1(P) plus a constant ·∑_{Q∈H_cov} OPT1(Q). This implies p_t^H := Prob[next pick ∈ uncovered heavy | F_t] ≥ 1 − O(1/λ^2) − O(S_H / ∑_{P∈H_uncovered} OPT1(P)). If heavy mass dominates light (scale separation), then the last term is negligible until near the end, yielding expected heavy-collisions O(k1/λ^2). I will formalize this bound with explicit constants next, carefully isolating: (i) the geometric inequality that caps the outside mass per uncovered heavy cluster by ≲ (1/λ^2)·OPT1(P), and (ii) the use of eH_t to control covered-heavy contributions.

Next steps (concrete, verifiable)
- Curate the two new τ-instantiations: (i) τ=⌊L/2⌋ and (ii) τ=⌊(L−1)/e⌋, as explicit corollaries of Theorem 3 (proofs above). They extend the per-plateau min with constant-tail options.
- Add the discrete-smoothness-to-length lemma and its corollary (1/m-type smoothness ⇒ long plateau ⇒ O(1) bound), to provide a clean structural condition guaranteeing improved averages or ≥1% probability guarantees.
- Heavy coverage: Prove the “outside mass” geometric inequality rigorously under λ-separation, completing the expected heavy-collision bound with explicit constants; then state a combined (λ,R)-separation lemma converting into an O(log(1/ε)) tail via bi-criteria oversampling of the light part.
- Optional constants: If desired, tighten Corollary 12’s K0 from 35 to 33 by re-auditing O(1/L) remainders in the pure-log branch; otherwise keep 35 for safety.



=== summary.md ===

## round-0001 — model summary — 2025-08-27T15:48:20.397295Z
Both provers developed a dyadic “plateau” approach to bound E_{k∼U([k0,2k0))}[ALG(k)/OPT(k)] using the Makarychev–Reddy–Shan (2020) bi-criteria oversampling guarantee. They define 2-plateaus (OPT(k) within a factor 2) and show a per-plateau averaged bound: for a plateau starting at m of length L, the k-means++ average ratio is O(1 + ln(2m/L)) (constants from MRS). A per-k formulation via s(k):=min{s:OPT(s)≤2OPT(k)} and Δ(k)=k−s(k) gives E[ALG(k)] ≤ 10·min{2+1/(2e)+ln(2s/Δ), 1 + s/(e(Δ−1))}·OPT(k) for Δ≥1 (Δ=0 falls back to O(ln k)). Consequences: long plateaus (L ≥ m / polylog m) yield O(log log k0) averages; but many short plateaus can force only the trivial O(log k0) bound, so extra structure (or a “heavy-coverage” high‑probability lemma) is needed for an unconditional improvement. Files relied on: prover-01.out.json, prover-02.out.json. Refs: Makarychev–Reddy–Shan (2020); Arthur–Vassilvitskii (2007).

## round-0002 — model summary — 2025-08-27T18:54:16.798946Z
Round recap: Developed a “hybrid” plateau-averaging lemma that combines both MRS branches to give explicit, checkable plateau averages. For a factor-2 plateau I=[m,m+L) the authors show
E_{k∈I} E_++[ALG_k/OPT_k] ≤ 2·C_bi·min{Ā,B̄} + C_fix(ln m + O(1))/L,
with Ā = 2 + 1/(2e) + ln(2m) − ln((L−1)!)/(L−1) (≤ 3 + 1/(2e) + ln(2m/(L−1))) and
B̄ = 1 + (2 + 1/(2e) + ln(2m))/(L−1) + (m/(e(L−1)))·H_{L−2}.
This yields a two‑regime consolidation (logarithmic branch vs. large‑Δ branch) giving O(1) in both natural regimes, an explicit plateau quantile/probability bound (convertible via Markov; e.g. L ≥ 101·m gives ≈98% on that plateau), and a strengthened global decomposition summing per‑plateau min terms. Also advanced a micro‑lemma toward heavy‑coverage: expected heavy collisions ≤ O(k1/R) under scale separation R ≥ k^C. Verifier: partial success. Cited: prover-02.out.json.

## round-0003 — model summary — 2025-08-27T19:44:49.694910Z
Round 0003 consolidated the plateau-averaging machinery and produced several immediately usable corollaries and a clear plan for advancing heavy-coverage and lower bounds. Key technical fixes: the monotonicity lemma is now present (closing a gap in Theorem 2) and Theorem 3's tunable τ-split correctly handles the Δ=1 endpoint and the harmonic tail. Two practical corollaries were proposed: an optimized-τ (min-of-two) plateau bound with explicit constants (log vs hybrid/m/L regimes) and a global min-of-two aggregation across plateaus. A concrete 1%–probability guarantee was derived under a single long-plateau condition with numeric instantiations (e.g., C_bi=5). Finally, a micro-lemma bounds expected heavy-cluster collisions under a β-persistence hypothesis; the next technical task is to derive β-persistence from OPT-scale separation. Verifier: partial success. Cited files: prover-01.out.json, prover-02.out.json.

## round-0004 — model summary — 2025-08-27T19:53:06.658788Z
This round produced several checkable refinements and concrete instantiations. Key technical additions: a new τ := ⌊m/ln L⌋ branch that replaces the (m ln L)/L hybrid term by (m(1+ln ln L))/L (tightening intermediate regimes); a global per-plateau min-of-three envelope (pure-log, τ=1 hybrid, τ=⌊m/ln L⌋); an optional balanced-split corollary (τ=⌊L/2⌋) for midrange L; explicit numeric constants when C_bi=C_fix=5 (e.g., E[...] ≤ 35 + 10·min{ln^+(2m/(L−1)), (m ln L)/L} + 5·(ln m)/L); a concrete ≥1% probability instantiation; and progress on heavy-coverage: a pathwise bound H_t(H_cov) ≤ 5·∑OPT1 and conversion to conditional collision probabilities, plus a special-case metric-separation lemma (λ≥10) that gives high-probability coverage-before-collisions and O(ln(1/ε)) final cost under oversampling. A lower-bound blueprint (heavy simplex + light singletons) aims to produce Ω(log k0) averaged lower bounds under random-k smoothing. Files used: prover-01.out.json, prover-02.out.json.

## round-0005 — model summary — 2025-08-27T20:00:51.449935Z
Round 0005 audit: two independent checks validated the new τ≈m/ln L branch, the min-of-three per-plateau envelope, and the explicit-constant instantiation (C_bi=C_fix=5). Both provers flagged the same minor edge-case: the third branch involving (m(1+ln ln L))/L should be restricted to L≥3 to avoid negative ln ln L for L=2. They proposed several direct, checkable global additions: (i) a global min-of-three aggregation across factor-2 plateaus with explicit constants (K0=35,K1=10,K2=5), (ii) an endpoint-sum cap lemma (endpoint contribution ≤ ln(2k0)), (iii) a long-plateau global corollary giving constant average on large plateaus, and (iv) an explicit ≥1% probability guarantee using η=0.98, c=2. Prover-02 also supplied an unconditional worst-case lemma recovering E[ALG/OPT] = O(1)+O(ln k0) and a heavy-coverage Lemma that reduces collision-control to proving a persistence lower bound inf_{t<τ} U_t(H) ≥ c·S_H under scale/geometric separation. Main remaining work: formalize that persistence bound to complete the heavy-coverage program. Files: prover-01.out.json and prover-02.out.json (Round 0005). Verifier: partial success.

## round-0006 — model summary — 2025-08-27T20:10:45.434930Z
Round 0006 summary: Both provers audited output.md (no blocking correctness issues) and added multiple checkable corollaries and lemmas. Per-plateau: two new τ-instantiations — balanced-split τ=⌊L/2⌋ and constant-tail τ=⌊(L−1)/e⌋ — with explicit safe constants producing midrange improvements and clean O(m/L) tails. Global mixing: a multi-plateau ≥1% probability corollary (P ≥ 0.01·γ − O(γ/min_j L_j)) and a quantified O(log log k0) mixture for plateaus L_j ≥ m_j/(ln k0)^σ giving a γ-weighted term ≈ 2 C_bi (a+1+ln 2 + σ ln ln k0). Heavy-coverage: a rigorous expectation-form lemma bounds expected heavy-collisions by (α/(β+1))·k1·(S_H / inf_t U_t(H)), reducing the remaining task to proving persistence (a lower bound on uncovered heavy mass); provers propose to obtain this via λ-separation, which would yield collisions O(k1/λ^2). A discrete-smoothness ⇒ long-plateau lemma ties per-step multiplicative smoothness to plateau length L ≳ ln2/δ, enabling the 1% mixing guarantee under mild smoothness. Verifier: partial success. Citations: prover-01.out.json, prover-02.out.json.



=== notes.md ===
Problem and notation
- X ⊂ R^d finite dataset; cost(C) = ∑_{x∈X} min_{c∈C} ||x−c||^2.
- OPT_k := min_{|C|=k} cost(C), nonincreasing in k.
- ALG_k := cost of k-means++ after k centers; E_++[·] denotes expectation over its randomness.
- We study E_{k∼U({k0,…,2k0−1})} E_++[ ALG_k / OPT_k ].

Key external input (Makarychev–Reddy–Shan 2020)
- There is an absolute constant C_bi (quoted as 5 in MRS) such that for any r≥1, Δ≥1,
  E_++[cost after r+Δ centers] ≤ C_bi · min{ 2 + 1/(2e) + ln(2r/Δ), 1 + r/(e(Δ−1)) } · OPT_r.
- For Δ=0 (fixed-k), there is an absolute constant C_fix (also ≲5 in MRS) with E_++[ALG_k] ≤ C_fix · (ln k + O(1)) · OPT_k.

Per-k oversampling bound (robust to s-misalignment)
- For any k, define s(k) := min{ s∈{1,…,k} : OPT_s ≤ 2·OPT_k }. Let Δ(k)=k−s(k).
- If Δ(k)≥1, then
  E_++[ALG_k] ≤ 2·C_bi · min{ 2 + 1/(2e) + ln(2 s(k)/Δ(k)), 1 + s(k)/(e(Δ(k)−1)) } · OPT_k.
- If Δ(k)=0, fall back to E_++[ALG_k] ≤ C_fix · (ln k + O(1)) · OPT_k.
Note: On any factor-2 plateau I=[m,m+L) (i.e., OPT_k∈(OPT_m/2,OPT_m] for k∈I), we have s(k)≤m and Δ(k)≥k−m; since the MRS bound is monotone in (s,Δ), replacing (s(k),Δ(k)) by (m,k−m) yields a valid upper bound.

Per-plateau averaging and Stirling control
- For a factor-2 plateau I=[m,m+L), L≥2, uniform k over I gives
  E_{k,++}[ALG_k/OPT_k] ≤ 2·C_bi · (2 + 1/(2e) + (1/(L−1))∑_{Δ=1}^{L−1} ln(2m/Δ)) + (C_fix(ln m + O(1)))/L.
- Using ln(n!) ≥ n ln n − n + 1 (n≥1),
  (1/(L−1))∑_{Δ=1}^{L−1} ln(2m/Δ) = ln(2m) − ln((L−1)!)/(L−1) ≤ 1 + ln(2m/(L−1)).
- Hence E_{k,++}[ALG_k/OPT_k] ≤ const + const·ln^+(2m/(L−1)) + O((ln m)/L), where ln^+(x)=max{ln x,0} and all const’s are absolute.

Decomposition over [k0,2k0)
- Partition [k0,2k0) into maximal factor-2 plateaus I_j=[m_j, m_j+L_j). For L_j≥2, the per-plateau average is bounded as above. For L_j=1, the single k in I_j contributes ≤ C_fix(ln m_j + O(1)). Averaging with weights L_j/k0 yields a global bound explicitly in terms of {(m_j,L_j)}; in the worst case many L_j=1 plateaus can force Θ(log k0).

Consequences and regimes
- Long plateaus: If some I=[m,m+L) satisfies L ≥ m/polylog(k0), then E_{k∈I,++}[ALG/OPT] = O(log log k0). If a constant fraction of [k0,2k0) is covered by such plateaus, the global average is O(log log k0).
- Worst case without structure: If many plateaus are short (e.g., L_j=1), the global average can be Θ(log k0), aligning with classic worst-case fixed-k behavior.

Open directions
- Structural conditions on OPT(·) implying many long plateaus (e.g., bounded discrete derivative decay, quasiconvexity/regularity of k·OPT_k) to get unconditional O(log log k0).
- Heavy-coverage lemma (conjectural): under strong scale separation OPT_{k1} ≫ OPT_k, show k-means++ hits all k1 heavy clusters w.h.p. before collisions, then apply bi-criteria on the remaining part.
- Lower bounds: Construct instances with Δ(k)=1 for a 1−o(1) fraction of k in [k0,2k0), giving an Ω(log k0) average and clarifying optimality of random-k smoothing.
Refinements and corrections for plateau averaging and monotonicity

- Monotonicity of the MRS bi-criteria bound. Define for integers s≥1 and Δ≥1 (interpreting the second branch only for Δ≥2):
  f(s,Δ) := min{ a + ln(2s/Δ), 1 + s/(e(Δ−1)) }, where a := 2 + 1/(2e).
  Then f is nondecreasing in s and nonincreasing in Δ. Consequently, on any factor-2 plateau I=[m,m+L) and for k=m+Δ with Δ≥1, the per-k bound from Theorem 1 satisfies
  f(s(k),Δ(k)) ≤ f(m,Δ), so we may safely replace (s(k),Δ(k)) by (m,Δ) in upper bounds.

- Correct τ-split averaging (fixing Proposition B). On a factor-2 plateau I=[m,m+L) with L≥2, for any integer τ∈{1,…,L−1}:
  (1/(L−1)) ∑_{Δ=1}^{L−1} min{ a+ln(2m/Δ), 1 + m/(e(Δ−1)) } (with the second term read only for Δ≥2)
  ≤ (τ/(L−1)) [ a + 1 + ln(2m/τ) ]
    + ((L−1−τ)/(L−1))
    + (m/(e(L−1))) · (H_{L−2} − H_{τ−1}).
  Proof sketch: Split the sum at τ and use Stirling ln(τ!) ≥ τ ln τ − τ + 1 to bound ∑_{Δ≤τ} ln(2m/Δ) ≤ τ(1+ln(2m/τ)). For the tail, sum 1/(Δ−1) as H_{L−2}−H_{τ−1}. This corrects two issues in earlier drafts: (i) the constant a must be multiplied by τ/(L−1), and (ii) ln(2m/Δ) cannot be upper bounded by ln(2m/τ) when Δ≤τ.

- Hybrid extremes recovered from the τ-split:
  • Choosing τ=L−1 gives the pure-logarithmic average a+1+ln(2m/(L−1)).
  • Choosing τ=1 gives the “hybrid extreme” 1 + (a+ln(2m))/(L−1) + (m/(e(L−1))) H_{L−2}.
  Hence we may take the minimum of these two regimes for a tighter bound, plateau by plateau.

- Long-plateau corollary. If L≥α m with α∈(0,1], then a+1+ln(2m/(L−1)) ≤ a+1+ln(2/α)+O(1/L), so the plateau-average is O(1) up to the endpoint term of order (ln m)/L.

- Quantile-in-Δ bound (careful threshold). For any η∈(0,1), let T := max{2, ⌈η(L−1)⌉+1}. Then, for all Δ≥T, by the large-Δ branch
  E_++[ALG_{m+Δ}/OPT_{m+Δ}] ≤ 2·C_bi·(1 + m/(e(T−1))).
  The fraction of k∈I with Δ≥T is (L−T+1)/L = 1 − η − O(1/L). This yields a rigorous “with probability ≥ 1−η−O(1/L) over k uniform on the plateau” expected-ratio bound; Markov then converts it into a constant-factor joint (k,++) probability if desired.

- Global decomposition improvement. Using the τ-split per plateau and optimizing τ as a function of (m,L) leads to a refined global bound that, informally, scales like
  K0 + K1 · min{ ln^+(2m/(L−1)), (m ln L)/L } + K2 · (ln m)/L,
  up to absolute constants K0,K1,K2. Formalizing this from Theorem 3 is a next step.

Open directions and next steps
- Optimize τ in the τ-split bound and write a clean, closed-form per-plateau bound. Then aggregate over plateaus to strengthen Corollary 3.
- Probability guarantees: phrase clean conditions under which at least a 1% measure of k (in [k0,2k0) or on a specific plateau) yields a constant expected ratio, and apply Markov to obtain joint (k,++) probability.
- Heavy-coverage: formalize the supermartingale for heavy uncovered mass (precise citation from MRS), bound expected heavy-collisions by O(k1/R), and lift to high probability via Freedman/Azuma with a stopping time.
- Lower bounds: construct OPT(·) with OPT_{k−1}/OPT_k ≈ 1 − Θ(1/ln k0) for most k to force Δ(k)=1 on a 1−o(1) fraction of the window, yielding an Ω(log k0) average.
Additions and clarifications

- Geometric constraint on plateau length within the window. If a factor-2 plateau I=[m,m+L) is contained in [k0,2k0), then L ≤ 2k0−m ≤ m. In particular, for L≥2 we have m/(L−1) ≥ 1. This prevents sign errors when simplifying expressions involving m/(L−1).

- Min-of-extremes and an upper envelope. From Theorem 3 and Corollary 4 we can (and will) use the minimum of the two extreme instantiations τ=L−1 (pure log) and τ=1 (hybrid extreme). Using H_q ≤ ln q + 1, this yields a clean per-plateau upper envelope of the form
  E_{k∈I} E_++[ALG_k/OPT_k] ≤ K0 + K1 · min{ ln^+(2m/(L−1)), (m ln L)/L } + K2 · (ln m)/L,
for absolute constants K0,K1,K2 depending only on C_bi,C_fix. This is the regime split we will use in global aggregation: the log term dominates when L≪m, while the (m ln L)/L term is O(1) when L is a constant fraction of m.

- Safe balanced-split (τ=⌊L/2⌋) template (optional). The following uniform bounds hold for L≥3 and τ=⌊L/2⌋:
  • τ/(L−1) ≤ 2/3 and (L−1−τ)/(L−1) ≤ 1/2.
  • ln(2m/τ) ≤ ln(4m/(L−1)).
  • H_{L−2} − H_{τ−1} ≤ ln 3 + 1 (with small-L cases checked directly).
Plugging these into Theorem 3 yields the rigorous estimate
  E_{k∈I} E_++[ALG_k/OPT_k]
  ≤ 2C_bi [ (2/3)(a + 1 + ln(4m/(L−1))) + 1/2 + (m/(e(L−1)))(ln 3 + 1) ]
    + (C_fix(\ln m + O(1)))/L.
This improves the (m ln L)/L term to a pure (m/L) term at the expense of a larger coefficient on the logarithmic piece. It may be tighter than the min-of-extremes envelope in intermediate regimes. If used, constants must be kept as above (1/2 is not a safe coefficient for τ/(L−1) when L is even).

- Probability ≥ 1% guarantees under a long-plateau condition. From Corollary 6: fix η∈(0,1) and define T := max{2, ⌈η(L−1)⌉+1}. For at least a (1−η−O(1/L)) fraction of k∈I we have
  E_++[ALG_k/OPT_k] ≤ 2C_bi (1 + m/(e(T−1))).
By Markov, for any c≥1,
  P_{++}[ ALG_k/OPT_k ≤ c · 2C_bi (1 + m/(e(T−1))) ] ≥ 1 − 1/c
for each such k. Therefore, jointly over k uniform on I and ++ randomness,
  P[ ALG_k/OPT_k ≤ c · 2C_bi (1 + m/(e(T−1))) ] ≥ (1−η−O(1/L))(1−1/c).
If L ≥ α m, then m/(T−1) ≤ 1/(α η) up to 1+O(1/L) factors, giving an explicit constant bound. If the plateau covers a γ-fraction of [k0,2k0), the global probability is ≥ γ times the RHS. Example: with C_bi=5, α=1, η=0.99, c=100, we get a ≥1% guarantee with a factor ≤ 100·2·5·(1 + 1/(0.99 e)).

- Heavy-coverage program (caveats and plan). Any per-step “collision within heavy” probability bound must be stated conditionally on the current filtration F_t: under D^2 sampling,
  P[collision in H at step t | F_t] = cost_t(covered H) / cost_t(all).
Avoid ratio-of-expectations. A plausible route is to bound cost_t(covered H) in terms of the uncovered heavy mass U_t(H) via the MRS eH_t supermartingale (in expectation and eventually with concentration), while lower bounding cost_t(all) by U_t(H) + U_t(L). To obtain a β-persistence-type condition from scale separation, quantify the expected reduction of U_t(H) when sampling outside H and show it is o(U_t(H)) under strong separation (e.g., OPT_{k1} ≥ k^C · OPT_k). With these in place, the expected number of heavy collisions can be bounded by summing the conditional probabilities until all heavy clusters are hit, and then lifted to high probability by Freedman/Azuma.

- Lower bounds. To show random-k smoothing cannot beat Θ(log k0) in general, aim for an instance where Δ(k)=1 for a 1−o(1) fraction of k in [k0,2k0). Concretely, calibrate radii so OPT_{k−1}/OPT_k ≈ 1 + Θ(1/ln k0) across most of the window, ensuring s(k)=k−1 on most k. Then adapt Arthur–Vassilvitskii’s lower-bound calculation to the averaged setting.
New per-plateau refinement: τ≈m/ln L branch and an updated envelope

- Additional τ-instantiation. On a factor-2 plateau I=[m,m+L) with L≥3, choose τ*:=min{L−1, ⌊m/ln L⌋}. Then Theorem 3 yields
  E_{k∈I} E_++[ALG/OPT]
  ≤ 2C_bi [ (τ*/(L−1))(a+1+ln(2m/τ*)) + ((L−1−τ*)/(L−1)) + (m/(e(L−1)))(H_{L−2} − H_{τ*−1}) ]
    + (C_fix(\ln m + O(1)))/L,
  where a:=2+1/(2e).
  In the subcase m ≤ (L−1) ln L (hence τ*=⌊m/ln L⌋) and assuming m≥L (which holds for plateaus contained in [k0,2k0)), we can bound
  • ln(2m/τ*) ≤ ln(4 ln L)
  • τ*/(L−1) ≤ m/((L−1) ln L)
  • H_{L−2} − H_{τ*−1} ≤ 1 + ln ln L
  to conclude
  E_{k∈I} E_++[ALG/OPT]
  ≤ 2C_bi [ 1 + (m/((L−1) ln L))(a+1+\ln(4\ln L)) + (m/(e(L−1)))(1 + \ln\ln L) ]
    + (C_fix(\ln m + O(1)))/L.
  This improves the (m\ln L)/L tail from the τ=1 branch to ≈ (m/L)(1+\ln\ln L).

- Updated min-of-three envelope (plateau-wise). Together with the τ=L−1 (pure-log) and τ=1 (hybrid) instantiations, we may use
  E_{k∈I} E_++[ALG/OPT] ≤ K0 + K1·min{ ln^+(2m/(L−1)), (m\ln L)/L, (m(1+\ln\ln L))/L } + K2·(\ln m)/L,
  for absolute constants K0,K1,K2 depending only on C_bi,C_fix. This strictly strengthens the previous min-of-two envelope in regimes where ln ln L ≪ ln L.

- Global aggregation (window-wise). For the maximal factor-2 plateau partition [k0,2k0)=\bigsqcup_j I_j with I_j=[m_j,m_j+L_j), one may take, plateau by plateau, the minimum of the three branches above (with the third used for L_j≥3), and sum with weights L_j/k0:
  E_{k\sim U([k0,2k0))} E_++[ALG/OPT]
  ≤ (1/k0) \sum_j L_j [ K0 + K1·min{ ln^+(2m_j/(L_j−1)), (m_j\ln L_j)/L_j, (m_j(1+\ln\ln L_j))/L_j } ]
    + (K2/k0) \sum_j \ln m_j.

- Balanced split reminder. The τ=⌊L/2⌋ instantiation yields
  E_{k∈I} E_++[ALG/OPT]
  ≤ 2C_bi [ (2/3)(a+1+\ln(4m/(L−1))) + 1/2 + (m/(e(L−1)))(\ln 3 + 1) ] + (C_fix(\ln m + O(1)))/L,
  valid for all L≥3. This removes the ln L factor in the tail at the expense of a larger constant on the log piece.

- Probabilistic 1% instantiation (correction). From Corollary 9, for any η∈(0,1) and c≥1,
  P_{k∈I,++}[ ALG/OPT ≤ c·B_I ] ≥ (1−η−O(1/L))(1−1/c), with B_I=2C_bi(1 + m/(e(T−1))), T=\max\{2,\lceil η(L−1)\rceil+1\}.
  To guarantee at least 1% mass uniformly (up to O(1/L)), take η=0.98 and c=2, giving (1−η)(1−1/c)=0.02·0.5=0.01. If, in addition, L≥α m, then B_I ≤ 2C_bi(1 + 1/(α e η)) up to O(1/L).

Caveat on heavy-coverage sketches. The pathwise control and metric-separation heuristics are promising but currently lack fully rigorous denominators: one must explicitly quantify cost_t(X) in terms of U_t(H) under precise conditions, and avoid uncontrolled uses of triangle inequality against existing centers outside H. These ideas remain in-development items in notes, not yet suitable for output.
Global aggregation and probability instantiations (added)

- Global min-of-three aggregator. From Corollary 11 (per-plateau min-of-three) and linearity, we can average over the maximal factor-2 plateau partition [k0,2k0)=⊔_j I_j=[m_j,m_j+L_j). For L_j≥2 each plateau contributes its per-plateau bound with weight L_j/k0, and L_j=1 plateaus contribute via the fixed-k bound. This yields a clean, explicit global formula with constants K0,K1,K2 depending only on C_bi,C_fix. When C_bi=C_fix=5, one can set K0=35, K1=10, K2=5. The third branch (m(1+ln ln L))/L is to be read only for L≥3.

- Endpoint-sum control. The unweighted endpoint contribution aggregates as (K2/k0)∑_{j:L_j≥2} ln m_j. This is always ≤ K2·ln(2k0) since there are ≤k0 plateaus and m_j≤2k0−1 for all j. Thus endpoint terms never exceed O(ln k0).

- Unconditional aggregation bound. Using only the pure-log branch and the easy inequality ln m_j ≤ ln k for k∈I_j, we recover E_{k∼U([k0,2k0)),++}[ALG/OPT] ≤ K′ + K″·ln(2k0) for absolute K′,K″. This validates that, without structural information (e.g., long plateaus), the Θ(log k0) behavior is unavoidable.

- Long-plateau mixture. If a γ-fraction of the window is covered by plateaus with L≥α m, then that portion contributes γ·O_α(1) (by Corollary 5), while the rest is ≤ (1−γ)·O(ln k0). This gives a transparent global interpolation.

- ≥1% global probability under a single long plateau. If I=[m,m+L) with L≥α m covers a γ-fraction of the window, then by Corollary 9 with η=0.98 and c=2 we get P_{k,++}[ALG/OPT ≤ 2·B_I] ≥ γ·(1−η−O(1/L))·(1−1/2) = 0.01·γ − O(γ/L), where B_I = 2·C_bi·(1 + m/(e(T−1))) and T = max{2,⌈0.98(L−1)⌉+1}. If L≥α m then B_I ≤ 2·C_bi(1 + 1/(α e·0.98)) + O(1/L).

Caveat and next steps for heavy-coverage

- The pathwise heavy-collision lemma remains a program: define U_t(H) (uncovered heavy mass), H_t(H) (covered heavy mass), and the stopping time τ when all heavy clusters are covered. A key missing ingredient is a quantitative persistence lower bound inf_{t<τ} U_t(H) ≥ c·S_H (S_H is the sum of single-cluster OPT costs over H) under an explicit separation hypothesis (e.g., OPT_{k1} ≫ OPT_k or geometric separation). Once such a bound and a dominance parameter β>0 (U_t(H) ≥ β·(H_t(all)+U_t(L))) are established, summing conditional collision probabilities gives an expected-collisions bound, which can then be upgraded to high probability via Freedman’s inequality. We should avoid any pathwise inequality that is only justified in expectation (e.g., eH_t supermartingale bounds) without a concentration argument.
Round 0006 updates: audits, fixes, and new curated directions

1) Balanced split (τ=⌊L/2⌋) is safe and useful
- For L≥3 and τ=⌊L/2⌋ we have the uniform bounds: τ/(L−1)≤2/3, (L−1−τ)/(L−1)≤1/2, ln(2m/τ)≤ln(4m/(L−1)), and H_{L−2}−H_{τ−1}≤ln 3+1 (check L=3,4 directly; for larger L use H_n−H_m≤ln((n+1)/(m+1))≤ln 3). Plugging these into Theorem 3 produces a clean O(m/L) tail with explicit constants. We will curate this to output.md.

2) “Constant-tail split” (τ≈(L−1)/e): caution on constants
- With τ=⌊(L−1)/e⌋, τ≥1 only when L≥4. The bound H_{L−2}−H_{τ−1}≤2 is false at L=6 (H_4≈2.083). A rigorous statement should either (i) retain the form H_{L−2}−H_{τ−1}≤1+ln((L−1)/τ), or (ii) give a safe constant (e.g., ≤3 or ≤4) uniformly for L≥4. Until this is adjusted, we will not add this branch to output.md.

3) Multi-plateau ≥1% probability (mixing)
- If a disjoint family of long plateaus {I_j=[m_j,m_j+L_j)} with L_j≥α m_j covers a γ-fraction of [k0,2k0), then by Corollary 9 with η=0.98 and c=2,
  P[ALG/OPT ≤ 2·B_j on the event k∈I_j for some j] ≥ γ·(1−η)·(1−1/2) − O(γ/\min_j L_j) = 0.01·γ − O(γ/\min_j L_j),
  where B_j=2C_bi(1+m_j/(e(T_j−1))), T_j=max{2,⌈η(L_j−1)⌉+1}, and hence B_j≤2C_bi(1+1/(α e η))+O(1/L_j). We will add this global mixing corollary to output.md.

4) Quantified O(log log k0) mixture under polylog-long plateaus
- Let S be the set of plateaus with L_j≥m_j/(ln k0)^σ for some σ≥1 and suppose ∑_{j∈S} L_j≥γ k0. Then
  E[ALG/OPT] ≤ γ·[2C_bi(a+1+ln(const)+σ ln ln k0)] + (1−γ)·[K′+K″ ln(2k0)] + o(1),
  where const is an absolute constant (2 or 4; absorbed into O(1)). The o(1) comes from endpoint terms: their contribution over S is (C_fix/k0)∑_{j∈S} ln m_j ≤ C_fix·|S|·ln(2k0)/k0 = O(γ (ln k0)^σ ln k0/k0)→0, since |S|≤(γ k0)/min_j L_j and min_j L_j≥k0/(ln k0)^σ. We will curate a precise corollary to output.md.

5) Discrete smoothness ⇒ long plateaus (corrected rounding)
- Lemma (sequence form). If (v_k) is nonincreasing and for all k in {m+1,…,m+L−1} we have v_{k−1}≤(1+δ) v_k, then v_{m+L−1}≥v_m/(1+δ)^{L−1}. Hence a factor-2 drop within [m,m+L) requires L≥1+⌈ln 2/ln(1+δ)⌉. Using ln(1+δ)≤δ, it suffices that L≥1+⌈(ln 2)/δ⌉.
- Corollary (1/m-type smoothness on [m,2m)). If v_{k−1}≤(1+A/m) v_k for all k∈[m+1,2m−1], then any factor-2 drop within [m,2m) requires L≥1+⌈(ln 2) m/A⌉. Thus: if A≥ln 2, then L≥((ln 2)/A)·m up to lower-order terms; if A<ln 2, then a factor-2 drop cannot occur within the window, so the whole [m,2m) is one plateau (L=m). This cleanly connects “discrete smoothness” to long plateaus.

6) Heavy-coverage program: rigor requirements
- To bound the expected number of heavy-collisions, one must avoid ratio-of-expectations. Work with p_t^{ch}=cost_t(covered heavy)/cost_t(all) conditioned on the filtration. A usable inequality is p_t^{ch}≤H_t(H_cov)/((β+1)U_t(H)) once we have a deterministic persistence bound cost_t(all)≥(β+1)U_t(H). Then ∑_{t<τ}E[p_t^{ch}|F_t]≤(1/(β+1))·∑_{t<τ}E[H_t(H_cov)/U_t(H)], which still needs (a) a deterministic or high-prob lower bound on U_t(H) until τ, and (b) control of H_t(H_cov) (e.g., via a supermartingale with concentration). These should be established under explicit separation assumptions before drawing collision bounds.

Next steps
- Curate to output.md: (i) balanced split corollary; (ii) multi-plateau 1% mixing; (iii) quantified O(log log k0) mixture; (iv) discrete-smoothness lemma and corollary.
- Either repair or defer the τ≈(L−1)/e branch.
- For heavy coverage: prove a persistence inequality cost_t(all)≥(β+1)U_t(H) under a clean separation hypothesis; then bound expected collisions; then lift to high probability via Freedman.


=== output.md ===
Theorems curated in this file depend on the following standard notation and external input.
- For a finite dataset X⊂R^d, write OPT_k for the optimal k-means cost with k centers, and ALG_k for the cost produced by k-means++ after k centers; E_++[·] is expectation over the algorithm’s randomness.
- External input (Makarychev–Reddy–Shan, 2020): There exists an absolute constant C_bi such that for all integers r≥1 and Δ≥1,
  E_++[cost after r+Δ centers] ≤ C_bi · min{ 2 + 1/(2e) + ln(2r/Δ), 1 + r/(e(Δ−1)) } · OPT_r.
  There is also an absolute constant C_fix with E_++[ALG_k] ≤ C_fix·(ln k + O(1))·OPT_k for all k (the fixed-k bound). We treat C_bi and C_fix as absolute numerical constants.

Theorem 1 (Per-k oversampling bound via a 2-approximate anchor).
Fix k≥1 and define
  s(k) := min{ s∈{1,…,k} : OPT_s ≤ 2·OPT_k },    Δ(k) := k − s(k).
Then:
- If Δ(k) ≥ 1, we have
  E_++[ALG_k] ≤ 2·C_bi · min{ 2 + 1/(2e) + ln(2 s(k)/Δ(k)), 1 + s(k)/(e(Δ(k)−1)) } · OPT_k.
- If Δ(k) = 0, we have E_++[ALG_k] ≤ C_fix·(ln k + O(1))·OPT_k.
Proof.
For Δ(k)≥1, apply the MRS bi-criteria inequality with r = s(k) and Δ = k−s(k):
  E_++[ALG_k] = E_++[cost after s(k)+Δ(k) centers]
  ≤ C_bi · min{ 2 + 1/(2e) + ln(2 s(k)/Δ(k)), 1 + s(k)/(e(Δ(k)−1)) } · OPT_{s(k)}.
By definition of s(k), OPT_{s(k)} ≤ 2·OPT_k, giving the stated bound. For Δ(k)=0 the fixed-k bound yields the second claim. ∎

Theorem 2 (Averaging over a factor-2 plateau).
Let I = {m, m+1, …, m+L−1} with L≥2 and assume OPT_k ∈ (OPT_m/2, OPT_m] for all k∈I. Then for k uniform in I and over the k-means++ randomness,
  E_{k∈I} E_++[ ALG_k / OPT_k ] ≤ A0 + A1 · ln^+( 2m/(L−1) ) + (A2 · ln m)/L,
where ln^+(x) := max{ln x,0} and A0,A1,A2 are absolute constants depending only on C_bi,C_fix.
Proof.
Average the bound from Theorem 1 over k∈I. For Δ:=k−m≥1, we may upper bound s(k) by m and lower bound Δ(k) by Δ (since s(k)≤m and Δ(k)≥Δ on a factor-2 plateau), and use only the logarithmic branch of MRS to get
  E_++[ALG_{m+Δ}/OPT_{m+Δ}] ≤ 2·C_bi · ( 2 + 1/(2e) + ln(2m/Δ) ).
Averaging Δ from 1 to L−1 gives
  (1/(L−1))∑_{Δ=1}^{L−1} ln(2m/Δ) = ln(2m) − ln((L−1)!)/(L−1) ≤ 1 + ln(2m/(L−1)),
where we used ln(n!) ≥ n ln n − n + 1 for n=L−1≥1. Multiplying by (L−1)/L and adding the Δ=0 endpoint contribution C_fix(ln m + O(1))/L yields the claim with suitable absolute constants A0,A1,A2. Replacing ln by ln^+ only weakens the bound. ∎

Corollary 3 (Decomposition across [k0,2k0)).
Partition the integer interval [k0,2k0) into maximal factor-2 plateaus I_j = [m_j, m_j+L_j), j=1,…,J, so OPT_k ∈ (OPT_{m_j}/2, OPT_{m_j}] for k∈I_j, and ∑_j L_j = k0. Then, for k uniform on [k0,2k0),
  E_k E_++[ ALG_k / OPT_k ]
  ≤ (1/k0) ∑_{j : L_j≥2} L_j [ A0 + A1 · ln^+( 2 m_j/(L_j−1) ) ]
    + (1/k0) ∑_{j : L_j≥2} A2 · ln m_j
    + (1/k0) ∑_{j : L_j=1} C_fix · (ln m_j + O(1)).
In particular, if a constant fraction of [k0,2k0) is covered by plateaus satisfying L_j ≥ m_j / polylog(k0), then the contribution of those plateaus is O(log log k0), and the remaining fraction must be controlled separately; in the worst case with many L_j=1 plateaus the global average can be Θ(log k0).
Proof.
Average the bound of Theorem 2 over plateaus with weights L_j/k0, and handle L_j=1 plateaus by the fixed-k bound. ∎
Lemma (Monotonicity of the MRS bi-criteria bound).
Let a := 2 + 1/(2e), and for integers s≥1, Δ≥1 define
  F(s,Δ) := min{ a + ln(2s/Δ), 1 + s/(e(Δ−1)) },
where the second term is interpreted only for Δ≥2 (or taken as +∞ for Δ=1). Then:
- For fixed Δ, F is nondecreasing in s.
- For fixed s, F is nonincreasing in Δ (for all Δ≥1 in the above interpretation).
Proof.
For Δ≥2, both functions s ↦ a + ln(2s/Δ) and s ↦ 1 + s/(e(Δ−1)) are nondecreasing in s, hence their pointwise minimum is nondecreasing. For Δ≥2, both functions Δ ↦ a + ln(2s/Δ) and Δ ↦ 1 + s/(e(Δ−1)) are nonincreasing in Δ, hence the minimum is nonincreasing. For the transition from Δ=1 to Δ=2, note that F(s,1) = a + ln(2s) ≥ a + ln(s) ≥ a + ln(2s/2) ≥ F(s,2), since the logarithmic branch decreases with Δ and adding the second branch at Δ=2 can only decrease the minimum. Thus F is nonincreasing in Δ for all Δ≥1. ∎

Theorem 3 (Plateau averaging with a tunable split of the two MRS branches).
Let I = {m, m+1, …, m+L−1} be a factor-2 plateau (i.e., OPT_k ∈ (OPT_m/2, OPT_m] for k∈I) with L≥2. For any integer τ with 1≤τ≤L−1, for k uniform on I and over k-means++ randomness we have
  E_{k∈I} E_++[ ALG_k / OPT_k ]
  ≤ 2·C_bi · [ (τ/(L−1)) · ( a + 1 + ln(2m/τ) )
                + ((L−1−τ)/(L−1))
                + (m/(e(L−1))) · ( H_{L−2} − H_{τ−1} ) ]
    + (C_fix · (ln m + O(1)))/L,
where H_r denotes the r-th harmonic number (H_0:=0).
Proof.
Fix k=m+Δ with Δ∈{1,…,L−1}. By Theorem 1, and Lemma (monotonicity),
  E_++[ALG_{m+Δ}/OPT_{m+Δ}] ≤ 2·C_bi · min{ a + ln(2m/Δ), 1 + m/(e(Δ−1)) },
with the second branch applicable for Δ≥2. Averaging Δ uniformly over {1,…,L−1}, split the sum at τ:
  (1/(L−1)) ∑_{Δ=1}^{L−1} min{…}
  ≤ (1/(L−1)) ∑_{Δ=1}^{τ} [ a + ln(2m/Δ) ]
    + (1/(L−1)) ∑_{Δ=τ+1}^{L−1} [ 1 + m/(e(Δ−1)) ].
For the first sum, use ∑_{Δ=1}^{τ} ln(2m/Δ) = τ ln(2m) − ln(τ!) ≤ τ[1 + ln(2m/τ)] by ln(τ!) ≥ τ ln τ − τ. For the second, ∑_{Δ=τ+1}^{L−1} 1 = L−1−τ and ∑_{Δ=τ+1}^{L−1} 1/(Δ−1) = H_{L−2} − H_{τ−1}. This yields the displayed bound. Finally, incorporate the Δ=0 endpoint k=m which has probability 1/L and contributes at most C_fix·(ln m + O(1))/L by the fixed-k bound. ∎

Corollary 4 (Hybrid extremes recovered; strengthening Theorem 2).
In the setting of Theorem 3:
- Choosing τ = L−1 gives
  E_{k∈I} E_++[ ALG_k / OPT_k ] ≤ 2·C_bi · ( a + 1 + ln(2m/(L−1)) ) + (C_fix · (ln m + O(1)))/L.
  This recovers Theorem 2 (up to constant reparameterization by a and the explicit +1 from Stirling).
- Choosing τ = 1 gives
  E_{k∈I} E_++[ ALG_k / OPT_k ] ≤ 2·C_bi · ( 1 + (a+ln(2m))/(L−1) + (m/(e(L−1))) · H_{L−2} )
    + (C_fix · (ln m + O(1)))/L.
Using H_q ≤ ln q + 1, this yields a complementary regime where the bound scales like 1 + O((ln m)/L) + O((m ln L)/L).
Proof.
Both bounds are immediate from Theorem 3 with the specified τ and the inequality H_q ≤ ln q + 1 for q≥1. ∎

Corollary 5 (Explicit constants on long plateaus).
If L ≥ α m for some fixed α∈(0,1], then for k uniform on I,
  E_{k∈I} E_++[ ALG_k / OPT_k ]
  ≤ 2·C_bi · ( a + 1 + ln(2/α) ) + (C_fix · (ln m + O(1)))/L.
Proof.
Apply Corollary 4 with τ=L−1 and use ln(2m/(L−1)) ≤ ln(2/α) + O(1/L). The endpoint term is unchanged. ∎

Corollary 6 (Quantile-in-Δ guarantee on a plateau).
Fix η∈(0,1) and let T := max{2, ⌈η(L−1)⌉+1}. Then for all Δ∈{T,…,L−1},
  E_++[ ALG_{m+Δ} / OPT_{m+Δ} ] ≤ 2·C_bi · ( 1 + m/( e (T−1) ) ).
Consequently, for k uniform on I, the fraction of k for which the above inequality holds is at least (L−T+1)/L = 1 − η − O(1/L).
Proof.
For Δ≥T, the large-Δ branch gives 1 + m/(e(Δ−1)) ≤ 1 + m/(e(T−1)). Combine with Theorem 1 and monotonicity, and note that the set {k: Δ(k)≥T} has size L−T+1 among the L indices in I. ∎
Remark (Plateau length within [k0,2k0)).
Let I = [m,m+L) be a factor-2 plateau contained in [k0,2k0). Then L ≤ 2k0 − m ≤ m. In particular, for L≥2 we have m/(L−1) ≥ 1.
Proof.
Since m+L−1 ≤ 2k0−1, we have L ≤ 2k0−m. As m ≥ k0, it follows that 2k0−m ≤ m. The final claim is immediate. ∎

Corollary 7 (Min-of-extremes plateau bound).
In the setting of Theorem 3 with L≥2, for k uniform on I and the k-means++ randomness,
  E_{k∈I} E_++[ ALG_k / OPT_k ]
  ≤ min{ B_log, B_hyb } + (C_fix · (ln m + O(1)))/L,
where
  B_log := 2·C_bi · ( a + 1 + ln^+( 2m/(L−1) ) ),
  B_hyb := 2·C_bi · ( 1 + (a+\ln(2m))/(L−1) + (m/(e(L−1))) · H_{L−2} ),
and a := 2 + 1/(2e), H_q denotes the q-th harmonic number with H_0:=0, and ln^+(x):=max{ln x,0}.
Proof.
Take τ=L−1 and τ=1 in Theorem 3 to obtain the two bounds (cf. Corollary 4). Replacing ln by ln^+ only increases the RHS. Taking the minimum preserves validity. ∎

Corollary 8 (Upper envelope from the min-of-extremes).
In the setting of Corollary 7, there exist absolute constants K0,K1,K2 (depending only on C_bi and C_fix) such that
  E_{k∈I} E_++[ ALG_k / OPT_k ]
  ≤ K0 + K1 · min{ ln^+( 2m/(L−1) ), (m ln L)/L } + K2 · (\ln m)/L.
Proof.
From Corollary 7 with the logarithmic extreme,
  E ≤ 2·C_bi·(a+1) + 2·C_bi·ln^+(2m/(L−1)) + (C_fix·(\ln m + O(1)))/L.
From the hybrid extreme and H_{L−2} ≤ ln(L−2)+1 ≤ ln L + 1,
  E ≤ 2·C_bi·\Big( 1 + (a+\ln(2m))/(L−1) + (m/(e(L−1))) (\ln L + 1) \Big)
    + (C_fix·(\ln m + O(1)))/L.
Absorb constants into K0 and the terms (ln m)/(L−1), (m/(L−1)) into K2·(\ln m)/L and K1·(m ln L)/L, respectively, using L−1≈L up to absolute factors for L≥2. Taking the minimum of the two displays yields the claim. ∎

Corollary 9 (Plateau-level probability guarantee via quantiles and Markov; global mixing).
Let I=[m,m+L) be a factor-2 plateau with L≥2 and fix η∈(0,1). Define T := max{2, ⌈η(L−1)⌉+1}. Then for at least a (1−η−O(1/L)) fraction of k∈I we have
  E_++[ ALG_k / OPT_k ] ≤ B_I := 2·C_bi · ( 1 + m/( e (T−1) ) ).
Consequently, for any c≥1,
  P_{k\sim U(I),\,++}\big[ ALG_k/OPT_k ≤ c · B_I \big] ≥ (1−η−O(1/L)) · (1 − 1/c).
If, moreover, L ≥ α m for some α∈(0,1], then B_I ≤ 2·C_bi · (1 + 1/(α e η)) + O(1/L). If a plateau I covers a γ-fraction of [k0,2k0) (i.e., L/k0 ≥ γ), then for k uniform on [k0,2k0) and the ++ randomness,
  P\big[ ALG_k/OPT_k ≤ c · B_I \big] ≥ γ · (1−η−O(1/L)) · (1 − 1/c).
Proof.
The first assertion is Corollary 6: a (1−η−O(1/L)) fraction of k satisfy the displayed per-k bound with Δ≥T, hence with E_++ bounded by B_I. For any such k, Markov’s inequality gives P_{++}[ALG_k/OPT_k ≤ c·B_I] ≥ 1−1/c. Averaging uniformly over k on I yields the claim. If L ≥ α m, then T−1 ≥ η(L−1) ≥ η(α m−1), which implies m/(T−1) ≤ 1/(α η) + O(1/L). The global bound follows by restricting to k in I, which occurs with probability ≥ γ. ∎
Corollary 10 (A τ≈m/ln L branch on a plateau).
Let I=[m,m+L) be a factor-2 plateau with L≥3. Define τ*:=min{L−1, ⌊m/ln L⌋}. Then for k uniform on I and over the k-means++ randomness,
  E_{k∈I} E_++[ ALG_k / OPT_k ]
  ≤ 2·C_bi · [ (τ*/(L−1)) · ( a + 1 + ln(2m/τ*) )
                + ((L−1−τ*)/(L−1))
                + (m/(e(L−1))) · ( H_{L−2} − H_{τ*−1} ) ]
    + (C_fix · (ln m + O(1)))/L,
where a:=2 + 1/(2e), and H_r is the r-th harmonic number (H_0:=0). In particular, if m ≤ (L−1) ln L and (as is the case for plateaus inside [k0,2k0)) m≥L, then
  E_{k∈I} E_++[ ALG_k / OPT_k ]
  ≤ 2·C_bi · [ 1 + (m/((L−1) ln L)) · ( a + 1 + ln(4 ln L) )
                + (m/(e(L−1))) · ( 1 + ln ln L ) ]
    + (C_fix · (ln m + O(1)))/L.
Proof.
Apply Theorem 3 with τ=τ*. The displayed inequality is just Theorem 3. In the subcase m ≤ (L−1) ln L, τ*=⌊m/ln L⌋ satisfies τ* ≥ m/ln L − 1, hence 2m/τ* ≤ 4 ln L and ln(2m/τ*) ≤ ln(4 ln L). Also τ*/(L−1) ≤ m/((L−1) ln L). Finally, H_{L−2} − H_{τ*−1} ≤ ln(L/τ*) + 1 ≤ ln((L ln L)/m) + 1 ≤ 1 + ln ln L, using m≥L. Substituting these bounds into Theorem 3 yields the “in particular” display. ∎

Corollary 11 (Updated per-plateau upper envelope: min-of-three).
In the setting of Corollary 7, there exist absolute constants K0,K1,K2 (depending only on C_bi and C_fix) such that for k uniform on I=[m,m+L) with L≥2,
  E_{k∈I} E_++[ ALG_k / OPT_k ]
  ≤ K0 + K1 · min{ ln^+( 2m/(L−1) ), (m ln L)/L, (m(1 + ln ln L))/L } + K2 · (\ln m)/L.
Here ln^+(x):=max{ln x,0}. The third term in the minimum is intended for L≥3; when L=2 the minimum of the first two terms suffices. 
Proof.
Combine: (i) the logarithmic extreme τ=L−1 (Corollary 4), (ii) the τ=1 hybrid extreme (Corollary 4 with H_{L−2}≤ln L+1), and (iii) Corollary 10 in the subcase τ*=⌊m/ln L⌋ (which holds exactly when m ≤ (L−1) ln L). In case (iii), the two m-dependent contributions are bounded by a constant multiple of (m/L)(1+ln ln L), since (m/((L−1) ln L))(a+1+ln(4 ln L)) ≤ (const)·(m/L) and (m/(e(L−1)))(1+ln ln L) ≤ (const)·(m/L)(1+ln ln L), with all constants depending only on C_bi. Absorb all additive O(1) and O((\ln m)/L) terms into K0 and K2. Taking the minimum of the three displays yields the claim. ∎

Corollary 12 (Explicit constants for the envelope when C_bi=C_fix=5).
Assume C_bi=C_fix=5 and L≥2. Then for k uniform on I=[m,m+L),
  E_{k∈I} E_++[ ALG_k / OPT_k ]
  ≤ 35 + 10 · min{ ln^+(2m/(L−1)), (m ln L)/L, (m(1+\ln\ln L))/L } + 5 · (\ln m)/L.
Proof.
From Corollary 7 (logarithmic extreme), E ≤ 2·C_bi·(a+1) + 2·C_bi·ln^+(2m/(L−1)) + (C_fix·(\ln m + O(1)))/L. With a=2+1/(2e) and C_bi=C_fix=5, 2·C_bi·(a+1) ≈ 31.84; round up to 35 to absorb the endpoint O(1/L) terms. The coefficient in front of ln^+(·) is 2·C_bi=10. For the τ=1 branch, the (m ln L)/L contribution carries a factor 2·C_bi/e ≤ 10/e < 10; rounding up to 10 is safe after replacing L−1 by L. For the τ≈m/ln L branch (Corollary 10), the m-dependent part is ≤ const·(m/L)(1+\ln\ln L) with const depending only on C_bi; choosing 10 dominates those constants as well. The endpoint term is ≤ 5·(\ln m + 2)/L and we keep 5·(\ln m)/L after absorbing the additive 10/L into 35. Taking the minimum of the three candidate bounds proves the display. ∎

Remark (Numerical 1% probability instantiation on a single plateau).
In Corollary 9, choose η=0.98 and c=2. Then for k uniform on I and ++ randomness,
  P[ ALG_k/OPT_k ≤ 2 · B_I ] ≥ (1−η−O(1/L)) · (1 − 1/2) = (0.02−O(1/L)) · 0.5 ≥ 0.01 − O(1/L),
with B_I = 2·C_bi·(1 + m/(e(T−1))) and T = max{2, ⌈0.98(L−1)⌉+1}. If additionally L ≥ α m, then B_I ≤ 2·C_bi·(1 + 1/(α e · 0.98)) + O(1/L), giving a fully explicit constant-factor guarantee at ≥1% mass (up to O(1/L)).
Remark (Clarification on Corollary 12).
In Corollary 12 the third term inside the minimum, (m(1+ln ln L))/L, is intended only for L≥3. For L=2, take the minimum of the first two terms. This mirrors the restriction stated in Corollary 11 and avoids using ln ln 2<0.

Corollary 13 (Global min-of-three aggregation across plateaus; explicit constants available).
Partition [k0,2k0) into maximal factor-2 plateaus I_j=[m_j,m_j+L_j), j=1,…,J, so that ∑_j L_j=k0. Then, for k uniform in [k0,2k0) and the k-means++ randomness,
  E_{k,++}[ ALG_k / OPT_k ]
  ≤ (1/k0) ∑_{j:L_j≥2} L_j · [ K0 + K1 · min{ ln^+(2m_j/(L_j−1)), (m_j ln L_j)/L_j, (m_j(1+ln ln L_j))/L_j } ]
    + (K2/k0) ∑_{j:L_j≥2} ln m_j
    + (1/k0) ∑_{j:L_j=1} C_fix · (ln m_j + O(1)),
where ln^+(x):=max{ln x,0}. The third term in the minimum is to be used only for L_j≥3. Here K0,K1,K2 are absolute constants depending only on C_bi,C_fix. In the numerical case C_bi=C_fix=5 one may take K0=35, K1=10, K2=5.
Proof.
For each j with L_j≥2, apply Corollary 11 to I_j and weight by L_j/k0. For L_j=1, use the fixed-k bound with weight 1/k0. Sum over j. The constants K0,K1,K2 come directly from Corollaries 11–12. ∎

Lemma 1 (Endpoint term cap).
With the notation of Corollary 13,
  (1/k0) ∑_{j:L_j≥2} ln m_j ≤ ln(2k0).
Proof.
There are at most J≤k0 plateaus and for all j we have m_j≤2k0−1. Thus ∑_{j:L_j≥2} ln m_j ≤ ∑_{j=1}^{J} ln(2k0) ≤ k0·ln(2k0). Divide by k0. ∎

Corollary 14 (Unconditional global O(log k0) bound).
There exist absolute constants K′,K″ (depending only on C_bi,C_fix) such that for k uniform on [k0,2k0) and ++ randomness,
  E_{k,++}[ ALG_k / OPT_k ] ≤ K′ + K″ · ln(2k0).
Proof.
Use Theorem 2 (or Corollary 7 with τ=L−1) on each plateau with L_j≥2 and average with weights L_j/k0, and handle L_j=1 via the fixed-k bound. For L_j≥2,
  (1/k0)∑_j L_j · ln^+(2m_j/(L_j−1)) ≤ (1/k0)∑_j L_j · ln(2m_j)
  = ln 2 + (1/k0)∑_j L_j ln m_j.
Since for every k∈I_j we have m_j≤k, ∑_j L_j ln m_j ≤ ∑_{k=k0}^{2k0−1} ln k ≤ k0·ln(2k0), giving a contribution ≤ ln(2k0). The endpoint term is bounded by Lemma 1 up to a constant factor, and the L_j=1 part contributes ≤ C_fix·(ln(2k0)+O(1)). Absorb all absolute factors into K′,K″. ∎

Corollary 15 (Global constant under a single long plateau; explicit constants).
Suppose there exists a plateau I=[m,m+L) contained in [k0,2k0) with L ≥ α m for some α∈(0,1] and covering a γ-fraction of the window (i.e., L/k0 ≥ γ). Then
  E_{k,++}[ ALG_k / OPT_k ]
  ≤ γ · [ 2·C_bi · ( a + 1 + ln(2/α) ) + (C_fix · (\ln m + O(1)))/L ]
    + (1−γ) · [ K′ + K″ · ln(2k0) ],
where a:=2+1/(2e) and K′,K″ are the constants from Corollary 14. In particular, as L→∞ the term (C_fix·(\ln m))/L=o(1), so the γ-contribution is O_α(1).
Proof.
Average Corollary 5 over I (weight L/k0=γ) and use Corollary 14 on the complement fraction 1−γ. ∎

Corollary 16 (Global ≥1% probability under a single long plateau; explicit constants).
Under the assumptions of Corollary 15, let η=0.98, c=2, and define T:=max{2,⌈η(L−1)⌉+1} and
  B_I := 2·C_bi · ( 1 + m / ( e (T−1) ) ).
Then, for k uniform on [k0,2k0) and ++ randomness,
  P[ ALG_k / OPT_k ≤ c · B_I ] ≥ γ · (1−η−O(1/L)) · (1 − 1/c) = 0.01·γ − O(γ/L).
If moreover L ≥ α m, then B_I ≤ 2·C_bi · (1 + 1/(α e η)) + O(1/L). For C_bi=5 and α=1, this yields the explicit threshold c·B_I ≤ 20 · (1 + 1/(0.98 e)) ≈ 27.7 on at least 1%·γ − O(γ/L) of the joint (k,++) mass.
Proof.
Apply Corollary 9 on I (giving a per-k bound with probability ≥(1−η−O(1/L)) and then Markov with c=2, hence factor (1−1/2)), and multiply by the mixing weight L/k0=γ. The long-plateau simplification of B_I follows from m/(T−1) ≤ 1/(α η) + O(1/L). ∎
Corollary 17 (Balanced split τ=⌊L/2⌋: an O(m/L) tail with explicit constants).
Let I=[m,m+L) be a factor-2 plateau with L≥3. Set τ:=⌊L/2⌋. Then, for k uniform on I and over the k-means++ randomness,
  E_{k∈I} E_++[ ALG_k / OPT_k ]
  ≤ 2·C_bi · [ (2/3)·( a + 1 + ln( 4m/(L−1) ) ) + 1/2 + (m/(e(L−1)))·( ln 3 + 1 ) ]
    + (C_fix · (ln m + O(1)))/L,
where a:=2 + 1/(2e).
Proof.
Apply Theorem 3 with τ=⌊L/2⌋ and use the uniform bounds for L≥3: τ/(L−1)≤2/3; (L−1−τ)/(L−1)≤1/2; ln(2m/τ)≤ln(4m/(L−1)); and H_{L−2}−H_{τ−1}≤ln 3 + 1 (verified directly for L=3,4 and by monotonicity of H_n−H_m thereafter). Substitute these into Theorem 3. The endpoint term is unchanged. ∎

Corollary 18 (Multi-plateau ≥1% probability guarantee by mixing).
Let {I_j=[m_j,m_j+L_j)} be disjoint factor-2 plateaus with L_j≥α m_j for all j in some index set J, and let their total coverage be γ:= (1/k0)∑_{j∈J} L_j. Fix η∈(0,1) and c≥1. For each j set T_j:=max{2, ⌈η(L_j−1)⌉+1} and B_j:=2·C_bi·(1 + m_j/(e(T_j−1))). Then, for k uniform on [k0,2k0) and over the ++ randomness,
  P\big[ ALG_k/OPT_k ≤ c · B_j for the unique j with k∈I_j \big]
  ≥ γ · (1−η) · (1 − 1/c) − O( γ / L_{\min} ),
where L_{\min}:=\min_{j∈J} L_j. In particular, with η=0.98 and c=2 this gives
  P[ ALG_k/OPT_k ≤ 2·B_j for the unique j with k∈I_j ] ≥ 0.01·γ − O(γ/L_{\min}).
Moreover, if L_j≥α m_j for all j, then B_j ≤ 2·C_bi·(1 + 1/(α e η)) + O(1/L_j), so a uniform threshold B_*=2·C_bi·(1 + 1/(α e η)) + o(1) can be used.
Proof.
Apply Corollary 9 on each plateau I_j with the same (η,c), yielding for k uniform on I_j the bound E_++[ALG_k/OPT_k]≤B_j on a (1−η−O(1/L_j)) fraction of k. For any such k, Markov gives P_{++}[ALG_k/OPT_k ≤ c·B_j]≥1−1/c. Weighting by L_j/k0 and summing over j∈J produces the stated lower bound. The O(·) term is ∑_j (L_j/k0)·O(1/L_j)=O(|J|/k0)=O(γ/L_{\min}). The long-plateau simplification of B_j follows from m_j/(T_j−1)≤1/(α η)+O(1/L_j). ∎

Lemma 2 (Discrete smoothness implies long plateaus).
Let (v_k)_{k≥1} be a nonincreasing positive sequence. Fix integers m≥1 and L≥2. Suppose that for all k∈{m+1,…,m+L−1} we have
  v_{k−1} ≤ (1+δ)·v_k
for some δ∈(0,1). Then
  v_{m+L−1} ≥ v_m / (1+δ)^{L−1}.
Consequently, if a factor-2 drop occurs within [m,m+L) (i.e., v_{m+L−1} ≤ v_m/2), then necessarily
  L ≥ 1 + ⌈ ln 2 / ln(1+δ) ⌉ ≥ 1 + ⌈ (\ln 2)/δ ⌉.
Proof.
Iterating the inequality gives v_{m+t} ≥ v_m/(1+δ)^t for 0≤t≤L−1. If v_{m+L−1}≤v_m/2, then (1+δ)^{L−1}≥2, which is equivalent to L≥1+⌈ln 2/ln(1+δ)⌉. Since ln(1+δ)≤δ, the second inequality follows. ∎

Corollary 19 (1/m-type discrete smoothness on [m,2m) forces long plateaus).
Let (v_k) be nonincreasing and assume that for some absolute A>0 and all k∈{m+1,…,2m−1},
  v_{k−1} ≤ (1 + A/m)·v_k.
Then any factor-2 drop within [m,2m) requires
  L ≥ 1 + ⌈ (\ln 2)·m / A ⌉.
In particular: if A≥\ln 2 then L ≥ ((\ln 2)/A)·m up to +1 rounding; if A<\ln 2, then a factor-2 drop cannot occur within the window and the entire [m,2m) is one plateau (L=m).
Proof.
Apply Lemma 2 with δ=A/m and note that the window has at most m steps, so if 1+⌈(\ln 2)·m/A⌉>m then the drop cannot happen within [m,2m). ∎

Corollary 20 (Quantified O(log log k0) mixture under polylog-long plateaus).
Fix σ≥1 and let S be the set of plateaus I_j=[m_j,m_j+L_j) with L_j ≥ m_j/(\ln k0)^σ. Suppose these plateaus cover a γ-fraction of [k0,2k0), i.e., ∑_{j∈S} L_j ≥ γ k0. Then, for k uniform on [k0,2k0) and ++ randomness,
  E_{k,++}[ ALG_k / OPT_k ]
  ≤ γ · [ 2·C_bi · ( a + 1 + ln 2 + σ·\ln\ln k0 ) ]
    + (1−γ) · [ K′ + K″·\ln(2k0) ]
    + o(1),
where a:=2+1/(2e) and K′,K″ are the constants from Corollary 14. The o(1) term can be taken as O( (\ln(2k0)) (\ln k0)^σ / k0 ).
Proof.
On each I_j∈S, Corollary 5 with α=1/(\ln k0)^σ yields E_{k∈I_j,++}[ALG/OPT] ≤ 2·C_bi·(a+1+\ln(2/α)) + (C_fix·\ln m_j)/L_j. Since \ln(2/α)=\ln 2 + σ·\ln\ln k0, the first term equals the bracket. Weighting by L_j/k0 and summing over j∈S gives γ times the bracket plus (C_fix/k0)∑_{j∈S}\ln m_j. Now |S|≤(∑_{j∈S} L_j)/\min_{j∈S} L_j ≤ γ k0 / (k0/(\ln k0)^σ) = γ (\ln k0)^σ, so (C_fix/k0)∑_{j∈S}\ln m_j ≤ C_fix·|S|·\ln(2k0)/k0 = O( (\ln(2k0))(\ln k0)^σ / k0 ) = o(1). The complement fraction 1−γ is bounded by Corollary 14. ∎


=== 1.pdf ===

--- Page 1 ---
k-means++: The Advantages of Careful Seeding
David Arthur ∗
Sergei Vassilvitskii†
Abstract
The k-means method is a widely used clustering technique
that seeks to minimize the average squared distance between
points in the same cluster. Although it oﬀers no accuracy
guarantees, its simplicity and speed are very appealing in
practice. By augmenting k-means with a very simple, ran-
domized seeding technique, we obtain an algorithm that is
Θ(log k)-competitive with the optimal clustering.
Prelim-
inary experiments show that our augmentation improves
both the speed and the accuracy of k-means, often quite
dramatically.
1
Introduction
Clustering is one of the classic problems in machine
learning and computational geometry. In the popular
k-means formulation, one is given an integer k and a set
of n data points in Rd. The goal is to choose k centers
so as to minimize φ, the sum of the squared distances
between each point and its closest center.
Solving this problem exactly is NP-hard, even with
just two clusters [10], but twenty-ﬁve years ago, Lloyd
[20] proposed a local search solution that is still very
widely used today (see for example [1, 11, 15]). Indeed,
a recent survey of data mining techniques states that it
“is by far the most popular clustering algorithm used in
scientiﬁc and industrial applications” [5].
Usually referred to simply as k-means, Lloyd’s
algorithm begins with k arbitrary centers, typically
chosen uniformly at random from the data points. Each
point is then assigned to the nearest center, and each
center is recomputed as the center of mass of all points
assigned to it. These two steps (assignment and center
calculation) are repeated until the process stabilizes.
One can check that the total error φ is monotoni-
cally decreasing, which ensures that no clustering is re-
peated during the course of the algorithm. Since there
are at most kn possible clusterings, the process will al-
ways terminate. In practice, very few iterations are usu-
ally required, which makes the algorithm much faster
∗Stanford University, Supported in part by NDSEG Fellow-
ship, NSF Grant ITR-0331640, and grants from Media-X and
SNRC.
†Stanford University, Supported in part by NSF Grant ITR-
0331640, and grants from Media-X and SNRC.
than most of its competitors.
Unfortunately, the empirical speed and simplicity
of the k-means algorithm come at the price of accuracy.
There are many natural examples for which the algo-
rithm generates arbitrarily bad clusterings (i.e.,
φ
φOPT is
unbounded even when n and k are ﬁxed). Furthermore,
these examples do not rely on an adversarial placement
of the starting centers, and the ratio can be unbounded
with high probability even with the standard random-
ized seeding technique.
In this paper, we propose a way of initializing
k-means by choosing random starting centers with
very speciﬁc probabilities.
Speciﬁcally, we choose a
point p as a center with probability proportional to p’s
contribution to the overall potential. Letting φ denote
the potential after choosing centers in this way, we show
the following.
Theorem 1.1. For any set of data points, E[φ] ≤
8(ln k + 2)φOP T .
This sampling is both fast and simple, and it already
achieves approximation guarantees that k-means can-
not.
We propose using it to seed the initial centers
for k-means, leading to a combined algorithm we call
k-means++.
This complements a very recent result of Ostrovsky
et al. [24], who independently proposed much the same
algorithm. Whereas they showed this randomized seed-
ing is O(1)-competitive on data sets following a certain
separation condition, we show it is O(log k)-competitive
on all data sets.
We also show that the analysis for Theorem 1.1 is
tight up to a constant factor, and that it can be eas-
ily extended to various potential functions in arbitrary
metric spaces.
In particular, we can also get a sim-
ple O(log k) approximation algorithm for the k-median
objective. Furthermore, we provide preliminary experi-
mental data showing that in practice, k-means++ really
does outperform k-means in terms of both accuracy and
speed, often by a substantial margin.
1.1
Related work As a fundamental problem in
machine learning, k-means has a rich history. Because
of its simplicity and its observed speed, Lloyd’s method
[20] remains the most popular approach in practice,

--- Page 2 ---
despite its limited accuracy. The convergence time of
Lloyd’s method has been the subject of a recent series
of papers [2, 4, 8, 14]; in this work we focus on improving
its accuracy.
In the theory community, Inaba et al. [16] were
the ﬁrst to give an exact algorithm for the k-means
problem, with the running time of O(nkd). Since then, a
number of polynomial time approximation schemes have
been developed (see [9, 13, 19, 21] and the references
therein). While the authors develop interesting insights
into the structure of the clustering problem, their
algorithms are highly exponential (or worse) in k, and
are unfortunately impractical even for relatively small
n, k and d.
Kanungo et al. [17] proposed an O(n3ϵ−d) algorithm
that is (9 + ϵ)-competitive.
However, n3 compares
unfavorably with the almost linear running time of
Lloyd’s method, and the exponential dependence on d
can also be problematic. For these reasons, Kanungo et
al. also suggested a way of combining their techniques
with Lloyd’s algorithm, but in order to avoid the
exponential dependence on d, their approach sacriﬁces
all approximation guarantees.
Mettu and Plaxton [22] also achieved a constant-
probability O(1) approximation using a technique called
successive sampling. They match our running time of
O(nkd), but only if k is suﬃciently large and the spread
is suﬃciently small. In practice, our approach is simpler,
and our experimental results seem to be better in terms
of both speed and accuracy.
Very recently, Ostrovsky et al. [24] independently
proposed an algorithm that is essentially identical to
ours, although their analysis is quite diﬀerent. Letting
φOPT,k denote the optimal potential for a k-clustering
on a given data set, they prove k-means++ is O(1)-
competitive in the case where
φOPT,k
φOPT,k−1
≤ϵ2.
The
intuition here is that if this condition does not hold,
then the data is not well suited for clustering with the
given value for k.
Combining this result with ours gives a strong
characterization of the algorithm’s performance.
In
particular, k-means++ is never worse than O(log k)-
competitive, and on very well formed data sets, it
improves to being O(1)-competitive.
Overall, the seeding technique we propose is similar
in spirit to that used by Meyerson [23] for online facility
location, and Mishra et al. [12] and Charikar et al. [6]
in the context of k-median clustering.
However, our
analysis is quite diﬀerent from those works.
2
Preliminaries
In this section, we formally deﬁne the k-means problem,
as well as the k-means and k-means++ algorithms.
For the k-means problem, we are given an integer k
and a set of n data points X ⊂Rd. We wish to choose
k centers C so as to minimize the potential function,
φ =
X
x∈X
min
c∈C ∥x −c∥2.
Choosing these centers implicitly deﬁnes a clustering
– for each center, we set one cluster to be the set of
data points that are closer to that center than to any
other. As noted above, ﬁnding an exact solution to the
k-means problem is NP-hard.
Throughout the paper, we will let COPT denote the
optimal clustering for a given instance of the k-means
problem, and we will let φOPT denote the corresponding
potential.
Given a clustering C with potential φ, we
also let φ(A) denote the contribution of A ⊂X to the
potential (i.e., φ(A) = P
x∈A minc∈C∥x −c∥2).
2.1
The k-means algorithm The k-means method
is a simple and fast algorithm that attempts to locally
improve an arbitrary k-means clustering. It works as
follows.
1. Arbitrarily choose k initial centers C = {c1, . . . , ck}.
2. For each i ∈{1, . . . , k}, set the cluster Ci to be the
set of points in X that are closer to ci than they
are to cj for all j ̸= i.
3. For each i ∈{1, . . . , k}, set ci to be the center of
mass of all points in Ci: ci =
1
|Ci|
P
x∈Ci x.
4. Repeat Steps 2 and 3 until C no longer changes.
It is standard practice to choose the initial centers
uniformly at random from X. For Step 2, ties may be
broken arbitrarily, as long as the method is consistent.
Steps 2 and 3 are both guaranteed to decrease φ, so
the algorithm makes local improvements to an arbitrary
clustering until it is no longer possible to do so. To see
that Step 3 does in fact decreases φ, it is helpful to recall
a standard result from linear algebra (see [14]).
Lemma 2.1. Let S be a set of points with center of
mass c(S), and let z be an arbitrary point.
Then,
P
x∈S∥x −z∥2 −P
x∈S∥x −c(S)∥2 = |S| · ∥c(S) −z∥2.
Monotonicity for Step 3 follows from taking S to be a
single cluster and z to be its initial center.
As discussed above, the k-means algorithm is at-
tractive in practice because it is simple and it is gener-
ally fast. Unfortunately, it is guaranteed only to ﬁnd a
local optimum, which can often be quite poor.
2.2
The k-means++ algorithm The k-means algo-
rithm begins with an arbitrary set of cluster centers.
We propose a speciﬁc way of choosing these centers. At

--- Page 3 ---
any given time, let D(x) denote the shortest distance
from a data point x to the closest center we have al-
ready chosen. Then, we deﬁne the following algorithm,
which we call k-means++.
1a. Choose an initial center c1 uniformly at random
from X.
1b. Choose the next center ci, selecting ci = x′ ∈X
with probability
D(x′)2
P
x∈X D(x)2 .
1c. Repeat Step 1b until we have chosen a total of k
centers.
2-4. Proceed as with the standard k-means algorithm.
We call the weighting used in Step 1b simply “D2
weighting”.
3
k-means++ is O(log k)-competitive
In this section, we prove our main result.
Theorem 3.1. If C is constructed with k-means++,
then the corresponding potential function φ satisﬁes
E[φ] ≤8(ln k + 2)φOPT.
In fact, we prove this holds after only Step 1 of the
algorithm above.
Steps 2 through 4 can then only
decrease φ. Not surprisingly, our experiments show this
local optimization is important in practice, although it
is diﬃcult to quantify this theoretically.
Our analysis consists of two parts. First, we show
that k-means++ is competitive in those clusters of COPT
from which it chooses a center. This is easiest in the
case of our ﬁrst center, which is chosen uniformly at
random.
Lemma 3.1. Let A be an arbitrary cluster in COPT, and
let C be the clustering with just one center, which is
chosen uniformly at random from A. Then, E[φ(A)] =
2φOPT(A).
Proof. Let c(A) denote the center of mass of the data
points in A.
By Lemma 2.1, we know that since
COPT is optimal, it must be using c(A) as the center
corresponding to the cluster A. Using the same lemma
again, we see E[φ(A)] is given by,
X
a0∈A
1
|A| ·
 X
a∈A
∥a −a0∥2
!
=
1
|A|
X
a0∈A
 X
a∈A
∥a −c(A)∥2 + |A| · ∥a0 −c(A)∥2
!
=
2
X
a∈A
∥a −c(A)∥2,
and the result follows.
Our next step is to prove an analog of Lemma 3.1
for the remaining centers, which are chosen with D2
weighting.
Lemma 3.2. Let A be an arbitrary cluster in COPT, and
let C be an arbitrary clustering.
If we add a random
center to C from A, chosen with D2 weighting, then
E[φ(A)] ≤8φOPT(A).
Proof. The probability that we choose some ﬁxed a0 as
our center, given that we are choosing our center from
A, is precisely
D(a0)2
P
a∈A D(a)2 . Furthermore, after choos-
ing the center a0, a point a will contribute precisely
min(D(a), ∥a −a0∥)2 to the potential. Therefore,
E[φ(A)] =
X
a0∈A
D(a0)2
P
a∈A D(a)2
X
a∈A
min(D(a), ∥a −a0∥)2.
Note by the triangle inequality that D(a0)
≤
D(a) + ∥a −a0∥for all a, a0.
From this, the power-
mean inequality1 implies that D(a0)2
≤2D(a)2 +
2∥a −a0∥2.
Summing over all a, we then have that
D(a0)2 ≤
2
|A|
P
a∈A D(a)2 +
2
|A|
P
a∈A∥a −a0∥2, and
hence, E[φ(A)] is at most,
2
|A| ·
X
a0∈A
P
a∈A D(a)2
P
a∈A D(a)2 ·
X
a∈A
min(D(a), ∥a −a0∥)2
+ 2
|A| ·
X
a0∈A
P
a∈A∥a −a0∥2
P
a∈A D(a)2
·
X
a∈A
min(D(a), ∥a −a0∥)2.
In the ﬁrst expression, we substitute min(D(a), ∥a −
a0∥)2 ≤∥a −a0∥2, and in the second expression, we
substitute min(D(a), ∥a −a0∥)2 ≤D(a)2. Simplifying,
we then have,
E[φ(A)]
≤
4
|A| ·
X
a0∈A
X
a∈A
∥a −a0∥2
=
8φOPT(A).
The last step here follows from Lemma 3.1.
We have now shown that seeding by D2 weighting
is competitive as long as it chooses centers from each
cluster of COPT, which completes the ﬁrst half of our
argument. We now use induction to show the total error
in general is at most O(log k).
1The power-mean inequality states for any real numbers
a1, · · · , am that Σa2
i ≥
1
m(Σai)2.
It follows from the Cauchy-
Schwarz inequality. We are only using the case m = 2 here, but
we will need the general case for Lemma 3.3.

--- Page 4 ---
Lemma 3.3. Let C be an arbitrary clustering. Choose
u > 0 “uncovered” clusters from COPT, and let Xu
denote the set of points in these clusters.
Also let
Xc = X −Xu. Now suppose we add t ≤u random centers
to C, chosen with D2 weighting. Let C′ denote the the
resulting clustering, and let φ′ denote the corresponding
potential. Then, E[φ′] is at most,

φ(Xc) + 8φOPT(Xu)

· (1 + Ht) + u −t
u
· φ(Xu).
Here, Ht denotes the harmonic sum, 1 + 1
2 + · · · + 1
t .
Proof. We prove this by induction, showing that if the
result holds for (t −1, u) and (t −1, u −1), then it
also holds for (t, u).
Therefore, it suﬃces to check
t = 0, u > 0 and t = u = 1 as our base cases.
If t = 0 and u > 0, the result follows from the fact
that 1 + Ht =
u−t
u
= 1.
Next, suppose t = u = 1.
We choose our one new center from the one uncovered
cluster with probability exactly
φ(Xu)
φ
.
In this case,
Lemma 3.2 guarantees that E[φ′] ≤φ(Xc)+8φOPT(Xu).
Since φ′ ≤φ even if we choose a center from a covered
cluster, we have,
E[φ′]
≤
φ(Xu)
φ
·

φ(Xc) + 8φOPT(Xu)

+ φ(Xc)
φ
· φ
≤
2φ(Xc) + 8φOPT(Xu).
Since 1 + Ht = 2 here, we have shown the result holds
for both base cases.
We now proceed to prove the inductive step. It is
convenient here to consider two cases. First suppose we
choose our ﬁrst center from a covered cluster. As above,
this happens with probability exactly φ(Xc)
φ
. Note that
this new center can only decrease φ.
Bearing this in
mind, apply the inductive hypothesis with the same
choice of covered clusters, but with t decreased by one.
It follows that our contribution to E[φ′] in this case is
at most,
φ(Xc)
φ
·

φ(Xc) + 8φOPT(Xu)

· (1 + Ht−1)
+ u −t + 1
u
· φ(Xu)

.
On the other hand, suppose we choose our ﬁrst
center from some uncovered cluster A. This happens
with probability
φ(A)
φ .
Let pa denote the probability
that we choose a ∈A as our center, given the center is
somewhere in A, and let φa denote φ(A) after we choose
a as our center.
Once again, we apply our inductive
hypothesis, this time adding A to the set of covered
clusters, as well as decreasing both t and u by 1.
It
follows that our contribution to E[φOPT] in this case is
at most,
φ(A)
φ
·
X
a∈A
pa

φ(Xc) + φa + 8φOPT(Xu) −8φOPT(A)

· (1 + Ht−1) + u −t
u −1 ·

φ(Xu) −φ(A)

≤φ(A)
φ
·

φ(Xc) + 8φOPT(Xu)

· (1 + Ht−1)
+ u −t
u −1 ·

φ(Xu) −φ(A)

.
The
last
step
here
follows
from
the
fact
that
P
a∈A paφa ≤8φOPT(A), which is implied by Lemma
3.2.
Now,
the
power-mean
inequality
implies
that
P
A⊂Xu φ(A)2 ≥1
u · φ(Xu)2. Therefore, if we sum over
all uncovered clusters A, we obtain a potential contri-
bution of at most,
φ(Xu)
φ
·

φ(Xc) + 8φOPT(Xu)

· (1 + Ht−1)
+ 1
φ · u −t
u −1 ·

φ(Xu)2 −1
u · φ(Xu)2

=
φ(Xu)
φ
·

φ(Xc) + 8φOPT(Xu)

· (1 + Ht−1)
+ u −t
u
· φ(Xu)

.
Combining the potential contribution to E[φ′] from
both cases, we now obtain the desired bound:
E[φ′] ≤

φ(Xc) + 8φOPT(Xu)

· (1 + Ht−1)
+ u −t
u
· φ(Xu) + φ(Xc)
φ
· φ(Xu)
u
≤

φ(Xc) + 8φOPT(Xu)

·

1 + Ht−1 + 1
u

+ u −t
u
· φ(Xu).
The inductive step now follows from the fact that 1
u ≤1
t .
We specialize Lemma 3.3 to obtain our main result.
Theorem 3.1 If C is constructed with k-means++,
then the corresponding potential function φ satisﬁes
E[φ] ≤8(ln k + 2)φOPT.
Proof. Consider the clustering C after we have com-
pleted Step 1. Let A denote the COPT cluster in which
we chose the ﬁrst center.
Applying Lemma 3.3 with

--- Page 5 ---
t = u = k −1 and with A being the only covered clus-
ter, we have,
E[φOPT] ≤

φ(A) + 8φOPT −8φOPT(A)

· (1 + Hk−1).
The result now follows from Lemma 3.1, and from the
fact that Hk−1 ≤1 + ln k.
4
A matching lower bound
In this section, we show that the D2 seeding used
by k-means++ is no better than Ω(log k)-competitive
in expectation, thereby proving Theorem 3.1 is tight
within a constant factor.
Fix k, and then choose n, ∆, δ with n ≫k and ∆≫
δ. We construct X with n points. First choose k centers
c1, c2, . . . , ck such that ∥ci −cj∥2 = ∆2 −
  n−k
n

· δ2
for all i ̸= j.
Now, for each ci, add data points
xi,1, xi,2, · · · , xi, n
k arranged in a regular simplex with
center ci, side length δ, and radius
q
n−k
2n · δ. If we do
this in orthogonal dimensions for each i, we then have,
∥xi,i′ −xj,j′∥=
 δ
if i=j, or
∆
otherwise.
We prove our seeding technique is in expectation
Ω(log k) worse than the optimal clustering in this case.
Clearly, the optimal clustering has centers {ci},
which leads to an optimal potential of φOPT = n−k
2
· δ2.
Conversely, using an induction similar to that of Lemma
3.3, we show D2 seeding cannot match this bound. As
before, we bound the expected potential in terms of
the number of centers left to choose and the number
of uncovered clusters (those clusters of C0 from which
we have not chosen a center).
Lemma 4.1. Let C be an arbitrary clustering on X with
k −t ≥1 centers, but with u clusters from COPT
uncovered.
Now suppose we add t random centers to
C, chosen with D2 weighting.
Let C′ denote the the
resulting clustering, and let φ′ denote the corresponding
potential.
Furthermore, let α =
n−k2
n
, β =
∆2−2kδ2
∆2
and
H′
u = Pu
i=1
k−i
ki . Then, E[φ′] is at least,
αt+1 ·

nδ2 · (1 + H′
u) · β +
n
k ∆2 −2nδ2
· (u −t)

.
Proof. We prove this by induction on t. If t = 0, note
that,
φ′ = φ =

n −u · n
k −k

· δ2 + u · n
k · ∆2.
Since n−u· n
k ≥n
k , we have n−u· n
k −k
n−u· n
k
≥
n
k −k
n
k
= α. Also,
α, β ≤1. Therefore,
φ′ ≥α ·

n −u · n
k

· δ2 · β + u · n
k · ∆2
.
Finally, since nδ2u ≥u · n
k · δ2 · β and nδ2u ≥nδ2H′
uβ,
we have,
φ′ ≥α ·

nδ2 · (1 + H′
u) · β +
n
k ∆2 −2nδ2
· u

.
This completes the base case.
We now proceed to prove the inductive step. As
with Lemma 3.3, we consider two cases. The probability
that our ﬁrst center is chosen from an uncovered cluster
is,
u · n
k · ∆2
u · n
k · ∆2 + (k −u) · n
k · δ2 −(k −t)δ2
≥
u∆2
u∆2 + (k −u)δ2
≥
α ·
u∆2
u∆2 + (k −u)δ2 .
Applying our inductive hypothesis with t and u both
decreased by 1, we obtain a potential contribution from
this case of at least,
u∆2
u∆2 + (k −u)δ2 · αt+1 ·

nδ2 · (1 + H′
u−1) · β
+
n
k ∆2 −2nδ2
· (u −t)

.
The probability that our ﬁrst center is chosen from
a covered cluster is
(k −u) · n
k · δ2 −(k −t)δ2
u · n
k · ∆2 + (k −u) · n
k · δ2 −(k −t)δ2
≥
(k −u) · n
k · δ2 −(k −t)δ2
(k −u) · n
k · δ2
·
(k −u)δ2
u∆2 + (k −u)δ2
≥
α ·
(k −u)δ2
u∆2 + (k −u)δ2 .
Applying our inductive hypothesis with t decreased by 1
but with u constant, we obtain a potential contribution
from this case of at least,
(k −u)δ2
u∆2 + (k −u)δ2 · αt+1 ·

nδ2 · (1 + H′
u) · β
+
n
k ∆2 −2nδ2
· (u −t + 1)

.
Therefore, E[φ′] is at least,
αt+1 ·

nδ2 · (1 + H′
u) · β +
n
k ∆2 −2nδ2
· (u −t)

+
αt+1
u∆2 + (k −u)δ2 ·

(k −u)δ2 ·
n
k ∆2 −2nδ2

−u∆2 ·

H′(u) −H′(u −1)

· nδ2 · β

.

--- Page 6 ---
However, H′
u −H′
u−1 = k−u
ku and β = ∆2−2kδ2
∆2
, so
u∆2 ·

H′(u) −H′(u −1)

· nδ2 · β
= (k −u)δ2 ·
n
k ∆2 −2nδ2
,
and the result follows.
As in the previous section, we obtain the desired
result by specializing the induction.
Theorem 4.1. D2 seeding is no better than 2(ln k)-
competitive.
Proof. Suppose a clustering with potential φ is con-
structed using k-means++ on X described above. Ap-
ply Lemma 4.1 with u = t = k −1 after the ﬁrst
center has been chosen.
Noting that 1 + H′
k−1 =
1 + Pk−1
i=1
  1
i −1
k

= Hk > ln k, we then have,
E[φ] ≥αkβ · nδ2 · ln k.
Now, ﬁx k and δ but let n and ∆approach inﬁnity.
Then α and β both approach 1, and the result follows
from the fact that φOPT = n−k
2
· δ2.
5
Generalizations
Although the k-means algorithm itself applies only
in vector spaces with the potential function φ
=
P
x∈X minc∈C∥x −c∥2, we note that our seeding tech-
nique does not have the same limitations. In this sec-
tion, we discuss extending our results to arbitrary met-
ric spaces with the more general potential function,
φ[ℓ] = P
x∈X minc∈C∥x −c∥ℓfor ℓ≥1. In particular,
note that the case of ℓ= 1 is the k-medians potential
function.
These generalizations require only one change to
the algorithm itself. Instead of using D2 seeding, we
switch to Dℓseeding – i.e., we choose x0 as a center
with probability
D(x0)ℓ
P
x∈X D(x)ℓ.
For the analysis, the most important change ap-
pears in Lemma 3.1. Our original proof uses an inner
product structure that is not available in the general
case. However, a slightly weaker result can be proven
using only the triangle inequality.
Lemma 5.1. Let A be an arbitrary cluster in COPT, and
let C be the clustering with just one center, which is cho-
sen uniformly at random from A. Then, E[φ[ℓ](A)] ≤
2ℓφ[ℓ]
OPT(A).
Proof. Let c denote the center of A in COPT. Then,
E[φ[ℓ](A)]
=
1
|A|
X
a0∈A
X
a∈A
∥a −a0∥ℓ
≤
2ℓ−1
|A|
X
a0∈A
X
a∈A
 ∥a −c∥ℓ+ ∥a0 −c∥ℓ
=
2ℓφ[ℓ]
OPT(A).
The second step here follows from the triangle inequality
and the power-mean inequality.
The rest of our upper bound analysis carries
through without change, except that in the proof of
Lemma 3.2, we lose a factor of 2ℓ−1 from the power-
mean inequality, instead of just 2. Putting everything
together, we obtain the general theorem.
Theorem 5.1. If C is constructed with Dℓseeding,
then the corresponding potential function φ[ℓ] satisﬁes,
E[φ[ℓ]] ≤22ℓ(ln k + 2)φ[ℓ]
OPT.
6
Empirical results
In order to evaluate k-means++ in practice, we have
implemented and tested it in C++ [3]. In this section,
we discuss the results of these preliminary experiments.
We found that D2 seeding substantially improves both
the running time and the accuracy of k-means.
6.1
Datasets We
evaluated
the
performance
of
k-means and k-means++ on four datasets.
The ﬁrst dataset, Norm25, is synthetic. To generate
it, we chose 25 “true” centers uniformly at random
from a 15-dimensional hypercube of side length 500.
We then added points from Gaussian distributions of
variance 1 around each true center. Thus, we obtained
a number of well separated Gaussians with the the true
centers providing a good approximation to the optimal
clustering.
We chose the remaining datasets from real-world
examples oﬀthe UC-Irvine Machine Learning Reposi-
tory. The Cloud dataset [7] consists of 1024 points in 10
dimensions, and it is Philippe Collard’s ﬁrst cloud cover
database. The Intrusion dataset [18] consists of 494019
points in 35 dimensions, and it represents features avail-
able to an intrusion detection system. Finally, the Spam
dataset [25] consists of 4601 points in 58 dimensions,
and it represents features available to an e-mail spam
detection system.
For each dataset, we tested k = 10, 25, and 50.
6.2
Metrics Since we were testing randomized seed-
ing processes, we ran 20 trials for each case. We report

--- Page 7 ---
Average φ
Minimum φ
Average T
k
k-means
k-means++
k-means
k-means++
k-means
k-means++
10
1.365 · 105
8.47%
1.174 · 105
0.93%
0.12
46.72%
25
4.233 · 104
99.96%
1.914 · 104
99.92%
0.90
87.79%
50
7.750 · 103
99.81%
1.474 · 101
0.53%
2.04
−1.62%
Table 1:
Experimental results on the Norm25 dataset (n = 10000, d = 15).
For k-means, we list the
actual potential and time in seconds.
For k-means++, we list the percentage improvement over k-means:
100% ·
 1 −k-means++ value
k-means value

.
Average φ
Minimum φ
Average T
k
k-means
k-means++
k-means
k-means++
k-means
k-means++
10
7.921 · 103
22.33%
6.284 · 103
10.37%
0.08
51.09%
25
3.637 · 103
42.76%
2.550 · 103
22.60%
0.11
43.21%
50
1.867 · 103
39.01%
1.407 · 103
23.07%
0.16
41.99%
Table 2: Experimental results on the Cloud dataset (n = 1024, d = 10). For k-means, we list the actual potential
and time in seconds. For k-means++, we list the percentage improvement over k-means.
Average φ
Minimum φ
Average T
k
k-means
k-means++
k-means
k-means++
k-means
k-means++
10
3.387 · 108
93.37%
3.206 · 108
94.40%
63.94
44.49%
25
3.149 · 108
99.20%
3.100 · 108
99.32%
257.34
49.19%
50
3.079 · 108
99.84%
3.076 · 108
99.87%
917.00
66.70%
Table 3: Experimental results on the Intrusion dataset (n = 494019, d = 35). For k-means, we list the actual
potential and time in seconds. For k-means++, we list the percentage improvement over k-means.
Average φ
Minimum φ
Average T
k
k-means
k-means++
k-means
k-means++
k-means
k-means++
10
3.698 · 104
49.43%
3.684 · 104
54.59%
2.36
69.00%
25
3.288 · 104
88.76%
3.280 · 104
89.58%
7.36
79.84%
50
3.183 · 104
95.35%
2.384 · 104
94.30%
12.20
75.76%
Table 4: Experimental results on the Spam dataset (n = 4601, d = 58). For k-means, we list the actual potential
and time in seconds. For k-means++, we list the percentage improvement over k-means.

--- Page 8 ---
the minimum and the average potential (actually di-
vided by the number of points), as well as the mean
running time. Our implementations are standard with
no special optimizations.
6.3
Results The results for k-means and k-means++
are displayed in Tables 1 through 4. We list the absolute
results for k-means, and the percentage improvement
achieved by k-means++ (e.g., a 90% improvement in
the running time is equivalent to a factor 10 speedup).
We observe that k-means++ consistently outperformed
k-means, both by achieving a lower potential value, in
some cases by several orders of magnitude, and also by
having a faster running time. The D2 seeding is slightly
slower than uniform seeding, but it still leads to a faster
algorithm since it helps the local search converge after
fewer iterations.
The synthetic example is a case where standard
k-means does very badly.
Even though there is an
“obvious” clustering, the uniform seeding will inevitably
merge some of these clusters, and the local search will
never be able to split them apart (see [12] for further
discussion of this phenomenon).
The careful seeding
method of k-means++ avoided this problem altogether,
and it almost always attained the optimal clustering on
the synthetic dataset.
The diﬀerence between k-means and k-means++
on the real-world datasets was also substantial.
In
every case, k-means++ achieved at least a 10% accuracy
improvement over k-means, and it often performed
much better.
Indeed, on the Spam and Intrusion
datasets, k-means++ achieved potentials 20 to 1000
times smaller than those achieved by standard k-means.
Each trial also completed two to three times faster, and
each individual trial was much more likely to achieve a
good clustering.
7
Conclusion and future work
We have presented a new way to seed the k-means
algorithm that is O(log k)-competitive with the optimal
clustering. Furthermore, our seeding technique is as fast
and as simple as the k-means algorithm itself, which
makes it attractive in practice.
Towards that end,
we ran preliminary experiments on several real-world
datasets, and we observed that k-means++ substantially
outperformed standard k-means in terms of both speed
and accuracy.
Although our analysis of the expected potential
E[φ] achieved by k-means++ is tight to within a con-
stant factor, a few open questions still remain. Most
importantly, it is standard practice to run the k-means
algorithm multiple times, and then keep only the best
clustering found. This raises the question of whether
k-means++ achieves asymptotically better results if it is
allowed several trials. For example, if k-means++ is run
2k times, our arguments can be modiﬁed to show it is
likely to achieve a constant approximation at least once.
We ask whether a similar bound can be achieved for a
smaller number of trials.
Also, experiments showed that k-means++ generally
performed better if it selected several new centers during
each iteration, and then greedily chose the one that
decreased φ as much as possible.
Unfortunately, our
proofs do not carry over to this scenario. It would be
interesting to see a comparable (or better) asymptotic
result proven here.
Finally, we are currently working on a more thor-
ough experimental analysis. In particular, we are mea-
suring the performance of not only k-means++ and stan-
dard k-means, but also other variants that have been
suggested in the theory community.
Acknowledgements
We would like to thank Rajeev Motwani for his helpful
comments.
References
[1] Pankaj K. Agarwal and Nabil H. Mustafa.
k-means
projective clustering. In PODS ’04: Proceedings of the
twenty-third ACM SIGMOD-SIGACT-SIGART sym-
posium on Principles of database systems, pages 155–
165, New York, NY, USA, 2004. ACM Press.
[2] D. Arthur and S. Vassilvitskii.
Worst-case and
smoothed analysis of the ICP algorithm, with an ap-
plication to the k-means method.
In Symposium on
Foundations of Computer Science, 2006.
[3] David Arthur and Sergei Vassilvitskii.
k-means++
test
code.
http://www.stanford.edu/∼darthur/
kMeansppTest.zip.
[4] David Arthur and Sergei Vassilvitskii.
How slow is
the k-means method?
In SCG ’06: Proceedings of
the twenty-second annual symposium on computational
geometry. ACM Press, 2006.
[5] Pavel Berkhin.
Survey of clustering data mining
techniques.
Technical report, Accrue Software, San
Jose, CA, 2002.
[6] Moses Charikar, Liadan O’Callaghan, and Rina Pani-
grahy. Better streaming algorithms for clustering prob-
lems. In STOC ’03: Proceedings of the thirty-ﬁfth an-
nual ACM symposium on Theory of computing, pages
30–39, New York, NY, USA, 2003. ACM Press.
[7] Philippe Collard’s cloud cover database. ftp://ftp.
ics.uci.edu/pub/machine-learning-databases/
undocumented/taylor/cloud.data.
[8] Sanjoy Dasgupta. How fast is k-means? In Bernhard
Sch¨olkopf and Manfred K. Warmuth, editors, COLT,
volume 2777 of Lecture Notes in Computer Science,
page 735. Springer, 2003.

--- Page 9 ---
[9] W. Fernandez de la Vega, Marek Karpinski, Claire
Kenyon, and Yuval Rabani. Approximation schemes
for clustering problems. In STOC ’03: Proceedings of
the thirty-ﬁfth annual ACM symposium on Theory of
computing, pages 50–58, New York, NY, USA, 2003.
ACM Press.
[10] P. Drineas, A. Frieze, R. Kannan, S. Vempala, and
V. Vinay. Clustering large graphs via the singular value
decomposition. Mach. Learn., 56(1-3):9–33, 2004.
[11] Fr´ed´eric Gibou and Ronald Fedkiw.
A fast hybrid
k-means level set algorithm for segmentation. In 4th
Annual Hawaii International Conference on Statistics
and Mathematics, pages 281–291, 2005.
[12] Sudipto Guha, Adam Meyerson, Nina Mishra, Rajeev
Motwani, and Liadan O’Callaghan.
Clustering data
streams: Theory and practice. IEEE Transactions on
Knowledge and Data Engineering, 15(3):515–528, 2003.
[13] Sariel Har-Peled and Soham Mazumdar. On coresets
for k-means and k-median clustering.
In STOC ’04:
Proceedings of the thirty-sixth annual ACM symposium
on Theory of computing, pages 291–300, New York,
NY, USA, 2004. ACM Press.
[14] Sariel Har-Peled and Bardia Sadri.
How fast is the
k-means method?
In SODA ’05: Proceedings of the
sixteenth annual ACM-SIAM symposium on Discrete
algorithms, pages 877–885, Philadelphia, PA, USA,
2005. Society for Industrial and Applied Mathematics.
[15] R.
Herwig,
A.J.
Poustka,
C.
Muller,
C.
Bull,
H. Lehrach, and J O’Brien. Large-scale clustering of
cdna-ﬁngerprinting data.
Genome Research, 9:1093–
1105, 1999.
[16] Mary Inaba, Naoki Katoh, and Hiroshi Imai. Applica-
tions of weighted voronoi diagrams and randomization
to variance-based k-clustering: (extended abstract). In
SCG ’94: Proceedings of the tenth annual symposium
on Computational geometry, pages 332–339, New York,
NY, USA, 1994. ACM Press.
[17] Tapas Kanungo, David M. Mount, Nathan S. Ne-
tanyahu, Christine D. Piatko, Ruth Silverman, and An-
gela Y. Wu. A local search approximation algorithm
for k-means clustering.
Comput. Geom., 28(2-3):89–
112, 2004.
[18] KDD Cup 1999 dataset.
http://kdd.ics.uci.edu/
/databases/kddcup99/kddcup99.html.
[19] Amit Kumar, Yogish Sabharwal, and Sandeep Sen. A
simple linear time (1 + ϵ)-approximation algorithm for
k-means clustering in any dimensions. In FOCS ’04:
Proceedings of the 45th Annual IEEE Symposium on
Foundations of Computer Science (FOCS’04), pages
454–462, Washington, DC, USA, 2004. IEEE Com-
puter Society.
[20] Stuart P. Lloyd. Least squares quantization in pcm.
IEEE Transactions on Information Theory, 28(2):129–
136, 1982.
[21] Jir´ı Matousek. On approximate geometric k-clustering.
Discrete & Computational Geometry,
24(1):61–84,
2000.
[22] Ramgopal R. Mettu and C. Greg Plaxton.
Optimal
time bounds for approximate clustering.
In Adnan
Darwiche and Nir Friedman, editors, UAI, pages 344–
351. Morgan Kaufmann, 2002.
[23] A. Meyerson. Online facility location. In FOCS ’01:
Proceedings of the 42nd IEEE symposium on Founda-
tions of Computer Science, page 426, Washington, DC,
USA, 2001. IEEE Computer Society.
[24] R. Ostrovsky, Y. Rabani, L. Schulman, and C. Swamy.
The eﬀectiveness of Lloyd-type methods for the k-
Means problem.
In Symposium on Foundations of
Computer Science, 2006.
[25] Spam e-mail database.
http://www.ics.uci.edu/
∼mlearn/databases/spambase/.


=== 2010.14487v1.pdf ===

--- Page 1 ---
arXiv:2010.14487v1  [cs.LG]  27 Oct 2020
Improved Guarantees for k-means++ and k-means++ Parallel
Konstantin Makarychev, Aravind Reddy, and Liren Shan
Department of Computer Science
Northwestern University
Evanston, IL, USA
Abstract
In this paper, we study k-means++ and k-means∥, the two most popular algorithms for the
classic k-means clustering problem. We provide novel analyses and show improved approxima-
tion and bi-criteria approximation guarantees for k-means++ and k-means∥. Our results give a
better theoretical justiﬁcation for why these algorithms perform extremely well in practice. We
also propose a new variant of k-means∥algorithm (Exponential Race k-means++) that has the
same approximation guarantees as k-means++.
1
Introduction
k-means clustering is one of the most commonly encountered unsupervised learning problems. Given
a set of n data points in Euclidean space, our goal is to partition them into k clusters (each
characterized by a center), such that the sum of squares of distances of data points to their nearest
centers is minimized. The most popular heuristic for solving this problem is Lloyd’s algorithm
(Lloyd, 1982), often referred to simply as “the k-means algorithm".
Lloyd’s algorithm uses iterative improvements to ﬁnd a locally optimal k-means clustering. The
performance of Lloyd’s algorithm crucially depends on the quality of the initial clustering, which
is deﬁned by the initial set of centers, called a seed. Arthur and Vassilvitskii (2007) and Ostrovsky,
Rabani, Schulman, and Swamy (2006) developed an elegant randomized seeding algorithm, known
as the k-means++ algorithm. It works by choosing the ﬁrst center uniformly at random from the
data set and then choosing the subsequent k−1 centers by randomly sampling a single point in each
round with the sampling probability of every point proportional to its current cost. That is, the
probability of choosing any data point x is proportional to the squared distance to its closest already
chosen center. This squared distance is often denoted by D2(x). Arthur and Vassilvitskii (2007)
proved that the expected cost of the initial clustering obtained by k-means++ is at most 8 (ln k + 2)
times the cost of the optimal clustering i.e., k-means++ gives an 8 (ln k + 2)-approximation for the
k-means problem. They also provided a family of k-means instances for which the approximation
factor of k-means++ is 2 ln k and thus showed that their analysis of k-means++ is almost tight.
Due to its speed, simplicity, and good empirical performance, k-means++ is the most widely
used algorithm for k-means clustering. It is employed by such machine learning libraries as Apache
The conference version of this paper will appear in the proceedings of the 34th Conference on Neural Information
Processing Systems (NeurIPS 2020). Author order is alphabetical.
1

--- Page 2 ---
Spark MLlib, Google BigQuery, IBM SPSS, Intel DAAL, and Microsoft ML.NET. In addition
to k-means++, these libraries implement a scalable variant of k-means++ called k-means∥(read
“k-means parallel”) designed by Bahmani, Moseley, Vattani, Kumar, and Vassilvitskii (2012). Some-
what surprisingly, k-means∥not only works better in parallel than k-means++ but also slightly
outperforms k-means++ in practice in the single machine setting (see Bahmani et al. (2012) and
Figure 1 below). However, theoretical guarantees for k-means∥are substantially weaker than for
k-means++.
The k-means∥algorithm makes T passes over the data set (usually T = 5). In every round,
it independently draws approximately ℓ= Θ(k) random centers according to the D2 distribution.
After each round it recomputes the distances to the closest chosen centers and updates D2(x) for
all x in the data set. Thus, after T rounds, k-means∥chooses approximately Tℓcenters. It then
selects k centers among Tℓcenters using k-means++.
Our contributions.
In this paper, we improve the theoretical guarantees for k-means++,
k-means∥, and Bi-Criteria k-means++ (which we deﬁne below).
First, we show that the expected cost of the solution output by k-means++ is at most 5(ln k+2)
times the optimal solution’s cost. This improves upon the bound of 8(ln k+2) shown by Arthur and
Vassilvitskii (2007) and directly improves the approximation factors for several algorithms which
use k-means++ as a subroutine like Local Search k-means++ (Lattanzi and Sohler, 2019). To
obtain this result, we give a reﬁned analysis of the expected cost of covered clusters (see Lemma
3.2 in Arthur and Vassilvitskii (2007) and Lemma 4.1 in this paper). We also show that our new
bound on the expected cost of covered clusters is tight (see Lemma C.1).
Then, we address the question of why the observed performance of k-means∥is better than the
performance of k-means++. There are two possible explanations for this fact. (1) This may be the
case because k-means∥picks k centers in two stages. At the ﬁrst stage, it samples ℓT ≥k centers.
At the second stage, it prunes centers and chooses k centers among ℓT centers using k-means++.
(2) This may also be the case because k-means∥updates the distribution function D2(x) once
in every round. That is, it recomputes D2(x) once for every ℓchosen centers, while k-means++
recomputes D2(x) every time it chooses a center. In this paper, we empirically demonstrate that
the ﬁrst explanation is correct. First, we noticed that k-means∥for ℓ· T = k is almost identical
with k-means++ (see Appendix A). Second, we compare k-means∥with another algorithm which
we call Bi-Criteria k-means++ with Pruning. This algorithm also works in two stages: At the
Bi-Criteria k-means++ stage, it chooses k + ∆centers in the data set using k-means++. Then, at
the Pruning stage, it picks k centers among the k +∆centers selected at the ﬁrst stage again using
k-means++. Our experiments on the standard data sets BioTest from KDD-Cup 2004 (Elber, 2004)
and COVTYPE from the UCI ML repository (Dua and Graﬀ, 2017) show that the performance
of k-means∥and Bi-Criteria k-means++ with Pruning are essentially identical (see Figures 1 and
Appendix A).
These results lead to another interesting question: How good are k-means++ and k-means∥
algorithms that sample k+∆instead of k centers? The idea of oversampling using k-means++ was
studied earlier in the literature under the name of bi-criteria approximation. Aggarwal, Deshpande,
and Kannan (2009) showed that with constant probability, sampling k + ∆centers by k-means++
provides a constant-factor approximation if ∆≥βk for some constant β > 0. Wei (2016) improved
on this result by showing an expected approximation ratio of 8(1 + 1.618k/∆). Note that for bi-
criteria algorithms we compare the expected cost of the clustering with k +∆centers they produce
and the cost of the optimal clustering with exactly k centers.
2

--- Page 3 ---
10
20
30
40
50
3
4
5
6
#centers
cost
(a) BioTest
k-means++
BiCriteria k-means++ w/Pruning
k-means∥
10
20
30
40
50
10
15
20
25
30
#centers
cost
(b) COVTYPE
k-means++
BiCriteria k-means++ w/Pruning
k-means∥
Figure 1: Performance of k-means++, k-means∥, and Bi-Criteria k-means++ with pruning on the
BioTest and COVTYPE datasets. For k = 10, 15, · · · , 50, we ran these algorithms for 50 iterations
and took their average. We normalized the clustering costs by dividing them by cost1000(X).
In this paper, we show that the expected bi-criteria approximation ratio for k-means++ with
∆additional centers is at most the minimum of two bounds:
(A) 5

2 + 1
2e + ln 2k
∆

for 1 ≤∆≤2k; and (B) 5

1 +
k
e (∆−1)

for ∆≥1
Both bounds are better than the bound by Wei (2016). The improvement is especially noticeable
for small values of ∆. More speciﬁcally, when the number of additional centers is ∆= k/ log k, our
approximation guarantee is O(log log k) while Wei (2016) gives an O(log k) approximation.
We believe that our results for small values of ∆provide an additional explanation for why
k-means++ works so well in practice. Consider a data scientist who wants to cluster a data set
X with k∗true clusters (i.e. k∗latent groups). Since she does not know the actual value of k∗,
she uses the elbow method (Boehmke and Greenwell, 2019) or some other heuristic to ﬁnd k. Our
results indicate that if she chooses slightly more number of clusters (for instance, 1.05k∗), then she
will get a constant bi-criteria approximation to the optimal clustering.
We also note that our bounds on the approximation factor smoothly transition from the regular
(∆= 0) to bi-criteria (∆> 0) regime. We complement our analysis with an almost matching lower
bound of Θ(log(k/∆)) on the approximation factor of k-means for ∆≤k (see Appendix C).
We then analyze Bi-Criteria k-means∥algorithm, the variant of k-means∥that does not prune
centers at the second stage.
In their original paper, Bahmani, Moseley, Vattani, Kumar, and
Vassilvitskii (2012) showed that the expected cost of the solution for k-means∥with T rounds and
oversampling parameter ℓis at most:
16
1 −αOPTk(X) +
1 + α
2
T
OPT1(X),
where α = exp

−

1 −e−ℓ/(2k)
; OPTk(X) is the cost of the optimal k-means clustering of X;
OPT1(X) is the cost of the optimal clustering of X with 1 center (see Section 2 for details). We
note that OPT1(X) ≫OPTk(X).
For ℓ= k, this result gives a bound of ≈49 OPTk(X) +
0.83T OPT1(X).
Bachem, Lucic, and Krause (2017) improved the approximation guarantee for
3

--- Page 4 ---
ℓ≥k to
26OPTk(X) + 2
 k
eℓ
T
OPT1(X).
In this work, we improve this bound for ℓ≥k and also obtain a better bound for ℓ< k. For ℓ≥k,
we show that the cost of k-means∥without pruning is at most
8OPTk(X) + 2
 k
eℓ
T
OPT1(X).
For ℓ< k, we give a bound of
5
1 −e−ℓ
k
OPTk(X) + 2

e−ℓ
k
T
OPT1(X)
Finally, we give a new parallel variant of the k-means++ algorithm, which we call Exponential
Race k-means++ (k-means++ER). This algorithm is similar to k-means∥. In each round, it also
selects ℓcandidate centers in parallel (some of which may be dropped later) making one pass over
the data set. However, after T rounds, it returns exactly k centers. The probability distribution
of these centers is identical to the distribution of centers output by k-means++. The expected
number of rounds is bounded as follows:
O
k
ℓ+ log OPT1(X)
OPTk(X)

.
This algorithm oﬀers a unifying view on k-means++ and k-means∥. We describe it in Section 7.
Other related work. Dasgupta (2008) and Aloise, Deshpande, Hansen, and Popat (2009) showed
that k-means problem is NP-hard. Awasthi, Charikar, Krishnaswamy, and Sinop (2015) proved that
it is also NP-hard to approximate k-means objective within a factor of (1 + ε) for some constant
ε > 0 (see also Lee, Schmidt, and Wright (2017)). We also mention that k-means was studied not
only for Euclidean spaces but also for arbitrary metric spaces.
There are several known constant factor approximation algorithms for the k-means problem.
Kanungo, Mount, Netanyahu, Piatko, Silverman, and Wu (2004) gave a 9 + ε approximation local
search algorithm. Ahmadian, Norouzi-Fard, Svensson, and Ward (2019) proposed a primal-dual
algorithm with an approximation factor of 6.357.
This is the best known approximation for k-
means. Makarychev, Makarychev, Sviridenko, and Ward (2016) gave constant-factor bi-criteria
approximation algorithms based on linear programming and local search. Note that although these
algorithms run in polynomial time, they do not scale well to massive data sets.
Lattanzi and
Sohler (2019) provided a constant factor approximation by combining the local search idea with
the k-means++ algorithm. Choo, Grunau, Portmann, and Rozhoň (2020) further improved upon
this result by reducing the number of local search steps needed from O(k log log k) to O(k).
Independently and concurrently to our work, Rozhoň (2020) gave an interesting analysis for
k-means∥by viewing it as a balls into bins problem and showed that O(log n/ log log n) rounds
suﬃce to give a constant approximation with high probability.
Acknowledgments. We would like to thank all the reviewers for their helpful comments. Kon-
stantin Makarychev, Aravind Reddy, and Liren Shan were supported in part by NSF grants CCF-
1955351 and HDR TRIPODS CCF-1934931. Aravind Reddy was also supported in part by NSF
CCF-1637585.
4

--- Page 5 ---
2
Preliminaries
Given a set of points X = {x1, x2, · · · , xn} ⊆Rd and an integer k ≥1, the k-means clustering
problem is to ﬁnd a set C of k centers in Rd to minimize
cost(X, C) :=
X
x∈X
min
c∈C ∥x −c∥2.
For any integer i ≥1, let us deﬁne OPTi(X) := min|C|=i cost (X, C) . Thus, OPTk(X) refers to the
cost of the optimal solution for the k-means problem. Let C∗denote a set of optimal centers. We
use {Pi}k
i=1 to denote the clusters induced by the center set C∗.
For any Y ⊆X, the cost of Y with center set C, denoted by cost (Y, C) = P
x∈Y minc∈C∥x−c∥2.
The optimal cost for subset Y with i centers is OPTi(Y). Let µ = P
x∈Y x/ |Y| be the centroid of
the cluster Y. Then, we have the following closed form expression for the optimal cost of Y with
one center (see Appendix B for proof),
OPT1(Y) =
X
x∈Y
∥x −µ∥2 =
P
(x,y)∈Y×Y∥x −y∥2
2 |Y|
.
(1)
k-means++ seeding: The k-means++ algorithm samples the ﬁrst center uniformly at random
from the given points and then samples k −1 centers sequentially from the given points with
probability of each point being sampled proportional to its cost i.e. cost(x, C)/cost(X, C).
Algorithm 1 k-means++ seeding
1: Sample a point c uniformly at random from X and set C1 = {c}.
2: for t = 2 to k do
3:
Sample x ∈X w.p. cost(x, Ct)/cost(X, Ct).
4:
Ct = Ct−1 ∪{x}.
5: end for
6: Return Ck
k-means∥and k-means∥Pois seeding: In the k-means∥algorithm, the ﬁrst center is chosen
uniformly at random from X. But after that, at each round, the algorithm samples each point
independently with probability min {ℓ· cost(x, C)/cost(X, C), 1} where ℓis the oversampling pa-
rameter chosen by the user and it usually lies between 0.1k and 10k. The algorithm runs for T
rounds (where T is also a parameter chosen by the user) and samples around ℓT points, which is
usually strictly larger than k. This oversampled set is then weighted using the original data set X
and a weighted version of k-means++ is run on this set to get the ﬁnal k-centers. We only focus
on the stage in which we get the oversampled set because the guarantees for the second stage come
directly from k-means++.
For the sake of analysis, we also consider a diﬀerent implementation of k-means∥, which we
call k-means∥Pois
(Algorithm 3).
This algorithm diﬀers from k-means∥in that each point is
sampled independently with probability 1 −exp(−ℓ· cost(x, C)/cost(X, C)) rather than min{ℓ·
cost(x, C)/cost(X, C), 1}. In practice, there is essentially no diﬀerence between k-means∥and k-
means∥Pois, since ℓ· cost(x, C)/cost(X, C) is a very small number for all x and thus the sampling
probabilities for k-means∥and k-means∥Pois are almost equal.
5

--- Page 6 ---
Algorithm 2 k-means∥seeding
1: Sample a point c uniformly from X and
set C1 = {c}
2: for t = 1 to T do
3:
Sample each point x into C′ indepen-
dently w.p. min{1, λt(x)} where
λt(x) = ℓ· cost(x, Ct)/cost(X, Ct)
4:
Let Ct+1 = Ct ∪C′.
5: end for
Algorithm 3 k-means∥Pois seeding
1: Sample a point c uniformly from X and
set C1 = {c}
2: for t = 1 to T do
3:
Sample each point x into C′ indepen-
dently w.p. 1 −e−λt(x) where
λt(x) = ℓ· cost(x, Ct)/cost(X, Ct)
4:
Let Ct+1 = Ct ∪C′.
5: end for
In the rest of the paper, we focus only on the seeding step of k-means++, k-means∥, and k-
means∥Pois and ignore Lloyd’s iterations as the approximation guarantees for these algorithms come
entirely from the seeding step.
3
General framework
In this section, we describe a general framework we use to analyze k-means++ and k-means∥Pois.
Consider k-means++ or k-means∥Pois algorithm.
Let Ct be the set of centers chosen by this
algorithm after step t. For the sake of analysis, we assume that Ct is an ordered set or list of
centers, and the order of centers in Ct is the same as the order in which our algorithm chooses these
centers. We explain how to order centers in k-means∥Pois algorithm in Section 6. We denote by T
the stopping time of the algorithm. Observe that after step t of the algorithm, the probabilities of
choosing a new center in k-means++ or a batch of new centers in k-means∥Pois are deﬁned by the
current costs of points in X which, in turn, are completely determined by the current set of centers
Ct. Thus, the states of the algorithm form a Markov chain.
In our analysis, we ﬁx the optimal clustering P = {P1, . . . , Pk} (if this clustering is not unique,
we pick an arbitrary optimal clustering). The optimal cost of each cluster Pi is OPT1(Pi) and the
optimal cost of the entire clustering is OPTk(X) = Pk
i=1 OPT1(Pi).
Following the notation in Arthur and Vassilvitskii (2007), we say that a cluster Pi is hit or
covered by a set of centers C if C ∩Pi ̸= ∅; otherwise, we say that Pi is not hit or uncovered. We
split the cost of each cluster Pi into two components which we call the covered and uncovered costs
of Pi. For a given set of centers C,
The covered or hit cost of Pi,
H(Pi, C) :=
(
cost(Pi, C),
if Pi is covered by C
0,
otherwise.
The uncovered cost of Pi,
U(Pi, C) :=
(
0,
if Pi is covered by C
cost(Pi, C),
otherwise.
Let H(X, C) = Pk
i=1 H(Pi, C) and U(X, C) = Pk
i=1 U(Pi, C). Then,
cost(X, C) = H(X, C) + U(X, C).
For the sake of brevity, we deﬁne costt(Y) := cost(Y, Ct) for any Y ⊆X, Ht(Pi) := H(Pi, Ct), and
Ut(Pi) := U(Pi, Ct). In Section 4, we show that for any t, we have E[Ht(X)] ≤5OPTk(X), which is
an improvement over the bound of 8OPTk(X) given by Arthur and Vassilvitskii (2007). Then, in
6

--- Page 7 ---
Sections 5 and 6, we analyze the expected uncovered cost U(X, CT ) for k-means++ and k-means∥
algorithms.
Consider a center c in C. We say that c is a miss if another center c′ covers the same cluster
in P as c, and c′ appears before c in the ordered set C. We denote the number of misses in C by
M(C) and the the number of clusters in P not covered by centers in C by K(C).
Observe that the stochastic processes Ut(Pi) with discrete time t are non-increasing since the
algorithm never removes centers from the set Ct and therefore the distance from any point x to
Ct never increases. Similarly, the processes Ht(Pi) are non-increasing after the step ti when Pi is
covered ﬁrst time. In this paper, we sometimes use a proxy eHt(Pi) for Ht(Pi), which we deﬁne
as follows. If Pi is covered by Ct, then eHt(Pi) = Hti(Pi), where ti ≤t is the ﬁrst time when Pi
is covered by Ct. If Pi is not covered by Ct, then eHt(Pi) = 5OPT1(Pi). It is easy to see that
Ht(Pi) ≤eHt′(Pi) for all t ≤t′. In Section 4, we also show that eHt(Pi) is a supermartingale i.e.,
E[ eHt′(Pi) | Ct] ≤eHt(Pi) for all t ≤t′.
4
Bound on the cost of covered clusters
In this section, we improve the bound by Arthur and Vassilvitskii (2007) on the expected cost
of a covered cluster in k-means++. Our bound also works for k-means∥Pois algorithm. Pick an
arbitrary cluster Pi in the optimal solution P = {P1, . . . , Pk} and consider an arbitrary state
Ct = {c1, . . . , cN} of the k-means++ or k-means∥Pois algorithm. Let Dt+1 be the set of new centers
the algorithm adds to Ct at step t (for k-means++, Dt+1 contains only one center). Suppose now
that centers in Dt+1 cover Pi i.e. Dt+1 ∩Pi ̸= ∅. We show that the expected cost of cluster Pi
after step (t + 1) conditioned on the event {Dt+1 ∩Pi ̸= ∅} and the current state of the algorithm
Ct is upper bounded by 5OPT1(Pi) i.e.
E [cost(Pi, Ct+1) | Ct, {Dt+1 ∩Pi ̸= ∅}] ≤5OPT1(Pi).
(2)
We now prove the main lemma.
Lemma 4.1. Consider an arbitrary set of centers C = {c1, . . . , cN} ⊆Rd and an arbitrary set
P ⊆X. Pick a random point c in P with probability Pr(c = x) = cost(x, C)/cost(P, C). Let
C′ = C ∪{c}. Then, Ec [cost(P, C′)] ≤5OPT1(P).
Remarks: Lemma 3.2 in the paper by Arthur and Vassilvitskii (2007) gives a bound of 8OPT1(P).
We also show in Appendix C that our bound is tight (see Lemma C.1).
Proof. The cost of any point y after picking center c equals the squared distance from y to the
set of centers C′ = C ∪{c}, which in turn equals min{cost(y, C), ∥y −c∥2}.
Thus, if a point
x ∈P is chosen as a center, then the cost of point y equals min{cost(y, C), ∥x −y∥2}.
Since
Pr(c = x) = cost(x, C)/cost(P, C), we have
Ec
cost(P, C′)
 =
X
x∈P
y∈P
cost(x, C)
cost(P, C) · min{cost(y, C), ∥x −y∥2}.
We write the right hand side in a symmetric form with respect to x and y. To this end, we deﬁne
function f as follows:
f(x, y) = cost(x, C) · min
n
∥x −y∥2, cost(y, C)
o
+ cost(y, C) · min
n
∥x −y∥2, cost(x, C)
o
.
7

--- Page 8 ---
Note that f(x, y) = f(y, x). Then,
Ec
cost(P, C′)
 =
1
2cost(P, C)
X
(x,y)∈P ×P
f(x, y).
We now give an upper bound on f(x, y) and then use this bound to ﬁnish the proof of Lemma 4.1.
Lemma 4.2. For any x, y ∈P, we have f(x, y) ≤5 min {cost(x, C), cost(y, C)} ∥x −y∥2.
Proof. Since f(x, y) is a symmetric function with respect to x and y, we may assume without loss of
generality that cost(x, C) ≤cost(y, C). Then, we need to show that f(x, y) ≤5cost(x, C)∥x −y∥2.
Consider three cases.
Case 1: If cost(x, C) ≤cost(y, C) ≤∥x −y∥2, then
f(x, y) = 2cost(x, C)cost(y, C) ≤2cost(x, C)∥x −y∥2.
Case 2: If cost(x, C) ≤∥x −y∥2 ≤cost(y, C), then
f(x, y) = cost(x, C)∥x −y∥2 + cost(y, C)cost(x, C).
By the triangle inequality, we have
cost(y, C) ≤
q
cost(x, C) + ∥x −y∥
2
≤4∥x −y∥2.
Thus, f(x, y) ≤5cost(x, C)∥x −y∥2.
Case 3: If ∥x −y∥2 ≤cost(x, C) ≤cost(y, C), then
f(x, y) = (cost(x, C) + cost(y, C)) ∥x −y∥2.
By the triangle inequality,
cost(y, C) ≤
q
cost(x, C) + ∥x −y∥
2
≤4cost(x, C).
Thus, we have f(x, y) ≤5cost(x, C)∥x −y∥2.
In all cases, the desired inequality holds. This concludes the proof of Lemma 4.2.
We use Lemma 4.2 to bound the expected cost of P. Let φ∗be a vector in RP with φ∗
x =
cost(x, C) for any x ∈P. Then, f(x, y) ≤5 min
n
φ∗
x, φ∗
y
o
∥x −y∥2. Since cost(P, C) = P
z∈P φ∗
z, we
have
Ec
cost(P, C′)
 ≤
5 P
(x,y)∈P ×P min
n
φ∗
x, φ∗
y
o
∥x −y∥2
2 P
z∈P φ∗z
|
{z
}
5F (φ∗)
.
For arbitrary vector φ ∈RP
≥0, deﬁne the following function:
F(φ) =
P
(x,y)∈P ×P min {φx, φy} ∥x −y∥2
2 P
z∈P φz
.
(3)
8

--- Page 9 ---
We have Ec [cost(P, C′)] ≤5F(φ∗). Thus, to ﬁnish the proof of Lemma 4.1, it suﬃces to show that
F(φ) ≤OPT1(P) for every φ ≥0 and particularly for φ = φ∗. By Lemma 4.3 (which we state and
prove below), function F(φ) is maximized when φ ∈{0, 1}P . Let φ∗∗be a maximizer of F(φ) in
{0, 1}P and P ′ = {x ∈P : φ∗∗
x = 1}. Observe that
F(φ∗∗) =
P
(x,y)∈P ′×P ′∥x −y∥2
2|P ′|
= OPT1(P ′).
Here we used the closed form expression (1) for the optimal cost of cluster P ′. Since P ′ ⊂P, we
have OPT1(P ′) ≤OPT1(P). Thus, F(φ∗) ≤F(φ∗∗) ≤OPT1(P).
Lemma 4.3. There exists a maximizer φ∗∗of F(φ) in the region {φ ≥0} such that φ ∈{0, 1}P .
Proof. Let m = |P| be the size of the cluster P and Π be the set of all bisections or permutations
π : {1, . . . , m} →P. Partition the set {φ ≥0} into m! regions (“cones over order polytopes”):
{φ : φ ≥0} = ∪π∈ΠOπ,
where Oπ = {φ : 0 ≤φπ(1) ≤φπ(2) ≤· · · ≤φπ(m)}. We show that for every π ∈Π, there exists a
maximizer φ∗∗of F(φ) in the region Oπ, such that φ∗∗∈{0, 1}P . Therefore, there exists a global
maximizer φ∗∗that belongs {0, 1}P
Fix a π ∈Π. Denote by V the hyperplane {φ : P
x∈P φx = 1}. Observe that F is a scale
invariant function i.e., F(φ) = F(λφ) for every λ > 0. Thus, for every φ ∈Oπ, there exists a
φ′ ∈Oπ ∩V (namely, φ′ = φ/(P
x∈P φx)) such that F(φ′) = F(φ). Hence, max{F(φ) : φ ∈Oπ} =
max{F(φ) : φ ∈Oπ ∩V }. Note that for φ ∈V , the denominator of (3) equals 2, and for φ ∈Oπ,
the numerator of (3) is a linear function of φ. Therefore, F(φ) is a linear function in the convex
set Oπ ∩V . Consequently, one of the maximizers of F must be an extreme point of Oπ ∩V .
The polytope Oπ ∩V is deﬁned by m inequalities and one equality. Thus, for every extreme
point φ of this polytope, all inequalities φπ(i) ≤φπ(i+1) but one must be tight. In other words, for
some j < m, we have
0 = φπ(1) = · · · = φπ(j) < φπ(j+1) = · · · = φπ(m).
(4)
Therefore, there exists a maximizer φ of F(φ) in Oπ ∩V satisfying (4) for some j. After rescaling
φ – multiplying all coordinates of φ by (m −j) – we obtain a vector φ∗∗whose ﬁrst j coordinates
φ∗∗
π(1), . . . , φ∗∗
π(j) are zeroes and the last m −j coordinates φ∗∗
π(j+1), . . . , φ∗∗
π(m) are ones. Thus, φ∗∗∈
{0, 1}P . Since F is rescaling invariant, F(φ∗∗) = F(φ). This concludes the proof.
Replacing the bound in Lemma 3.2 from the analysis of Arthur and Vassilvitskii (2007) by our
bound from Lemma 4.1 gives the following result (see also Lemma 5.6).
Theorem 4.4. The approximation factor of k-means++ is at most 5(ln k + 2).
We now state an important corollary of Lemma 4.1.
Corollary 4.5.
For every P ∈P, the process eHt(P) for k-means++ is a supermartingale i.e.,
E
h
eHt+1(X) | Ct
i
≤eHt(X).
9

--- Page 10 ---
Proof. The value of eHt(X) changes only if at step t, we cover a yet uncovered cluster P. In this
case, the value of eHt+1(P) changes by the new cost of P minus 5OPT(P). By Lemma 4.1 this
quantity is non-positive in expectation.
Since the process eHt(P) is a supermartingale, we have E[ eHt(P)] ≤eH0(P) = 5OPT1(P). Hence,
E[Ht(P)] ≤E[ eHt(P)] = 5OPT1(P). Thus, E[Ht(X)] ≤5OPTk(X). Since costt(X) = Ht(X) +
Ut(X) and we have a bound on the expectation of the covered cost, Ht(X), in the remaining
sections, we shall only analyze the uncovered cost Ut(X).
5
Bi-criteria approximation of k-means++
In this section, we give a bi-criteria approximation guarantee for k-means++.
Theorem 5.1. Let costk+∆(X) be the cost of the clustering with k + ∆centers sampled by the
k-means++ algorithm. Then, for ∆≥1, the expected cost E [costk+∆(X)] is upper bounded by
(below (a)+ denotes max(a, 0)).
min
n
2 + 1
2e +

ln 2k
∆
+
, 1 +
k
e (∆−1)
o
5OPTk(X).
Note that the above approximation guarantee is the minimum of two bounds: (1) 2+ 1
2e + ln 2k
∆
for 1 ≤∆≤2k; and (2) 1 +
k
e(∆−1) for ∆≥1. The second bound is stronger than the ﬁrst bound
when ∆/k ⪆0.085.
5.1
Proof overview of Theorem 5.1
We now present a high level overview of the proof and then give a formal proof. Our proof consists
of three steps.
First, we prove bound (2) on the expected cost of the clustering returned by k-means++ after
k + ∆rounds. We argue that the expected cost of the covered clusters is bounded by 5OPTk(X)
(see Section 3) and thus it is suﬃcient to bound the expected cost of uncovered clusters. Consider
an optimal cluster P ∈P. We need to estimate the probability that it is not covered after k + ∆
rounds. We upper bound this probability by the probability that the algorithm does not cover P
before it makes ∆misses (note: after k + ∆rounds k-means++ must make at least ∆misses).
In this overview, we make the following simplifying assumptions (which turn out to be satisﬁed
in the worst case for bi-criteria k-means++): Suppose that the uncovered cost of cluster P does not
decrease before it is covered and equals U(P) and, moreover, the total cost of all covered clusters
almost does not change and equals H(X) (this may be the case if one large cluster contributes
most of the covered cost, and that cluster is covered at the ﬁrst step of k-means++). Under these
assumptions, the probability that k-means++ chooses ∆centers in the already covered clusters
and does not choose a single center in P equals (H(X)/(U(P) + H(X)))∆. If k-means++ does
not choose a center in P, the uncovered cost of cluster P is U(P); otherwise, the uncovered cost of
cluster P is 0. Thus, the expected uncovered cost of P is (H(X)/(U(P)+H(X)))∆U(P). It is easy
to show that (H(X)/(U(P) + H(X)))∆U(P) ≤H(X)/(e(∆−1)). Thus, the expected uncovered
cost of all clusters is at most
k
(e(∆−1))E[H(X)] ≤
k
(e(∆−1))5OPTk(X).
10

--- Page 11 ---
Then, we use ideas from Arthur and Vassilvitskii (2007), Dasgupta (2013) to prove the following
statement: Let us count the cost of uncovered clusters only when the number of misses after k
rounds of k-means++ is greater than ∆/2. Then the expected cost of uncovered clusters is at most
O(log(k/∆)) · OPTk(X). That is, E[H(Uk(X) · 1{M(Ck) ≥∆/2}] ≤O(log(k/∆)) · OPTk(X).
Finally, we combine the previous two steps to get bound (1). We argue that if the number of
misses after k rounds of k-means++ is less than ∆/2, then almost all clusters are covered. Hence,
we can apply bound (2) to k′ ≤∆/2 uncovered clusters and ∆remaining rounds of k-means++
and get a 5(1+1/(2e)) approximation. If the number of misses is greater than ∆/2, then the result
from the previous step yields an O(log(k/∆)) approximation.
5.2
Analysis of k-means++
In this section, we analyze the bi-criteria k-means++ algorithm and prove Theorem 5.1. To this end,
we establish the ﬁrst and second bounds from Theorem 5.1 on the expected cost of the clustering
after k + ∆rounds of k-means. We will start with the second bound.
5.2.1
Bi-criteria bound for large ∆
Lemma 5.2. The following bi-criteria bound holds
E [costk+∆(X)] ≤5

1 +
k
e (∆−1)

OPTk(X).
Consider the discrete time Markov chain Ct associated with k-means++ algorithm (see Sec-
tion 3). Let P ∈P be an arbitrary cluster in the optimal solution. Partition all states of the
Markov chain into k + ∆disjoint groups M0, M1, · · · , Mk+∆−1 and H. Each set Mi contains all
states C with i misses that do not cover P: Mi = {C : M(C) = i, P ∩C = ∅} . The set H contains
all states C that cover P: H = {C : P ∩C ̸= ∅}.
We now deﬁne a new Markov chain Xt. To this end, we ﬁrst expand the set of states {C}. For
every state C of the process Ct, we create two additional “virtual” states Ca and Cb. Then, we let
X2t = Ct for every even step 2t, and
X2t+1 =
(
Ca
t ,
if Ct+1 ∈Mi
Cb
t ,
if Ct+1 ∈Mi+1 ∪H.
for every odd step 2t + 1. We stop Xt when Ct stops or when Ct hits the set H (i.e., Ct ∈H).
Loosely speaking, Xt follows Markov chain Ct but makes additional intermediate stops. When Ct
moves from one state in Mi to another state in Mi, X2t+1 stops in Ca
t ; and when Ct moves from
a state in Mi to a state in Mi+1 or H, X2t+1 stops in Cb
t .
Write transition probabilities for Xt:
P [X2t+1 = Ca | X2t = C] = U(X, C) −U(P, C)
cost(X, C)
,
P
h
X2t+1 = Cb | X2t = C
i
= U(P, C) + H(X, C)
cost(X, C)
,
and for all C ∈Mi and C′ = C ∪{x} ∈Mi,
P
X2t+2 = C′ | X2t+1 = Ca =
cost(x, C)
U(X, C) −U(P, C),
11

--- Page 12 ---
for all C ∈Mi and C′ = C ∪{x} ∈Mi+1 ∪H,
P
h
X2t+2 = C′ | X2t+1 = Cbi
=
cost(x, C)
U(P, C) + H(X, C).
Above, U(X, C) −U(P, C) is the cost of points in all uncovered clusters except for P. If we pick a
center from these clusters, we will necessarily cover a new cluster, and therefore X2t+2 will stay in
Mi. Similarly, U(P, C) + H(X, C) is the cost of all covered clusters plus the cost of P. If we pick
a center from these clusters, then X2t+2 will move to Mi+1 or H.
Deﬁne another Markov chain {Yt}. The transition probabilities of {Yt} are the same as the
transition probabilities of Xt except Y never visits states in H and therefore for C ∈Mi and
C′ = C ∪{x} ∈Mi+1, we have
P
h
Y2t+2 = C′ | Y2t+1 = Cbi
= cost(x, C)
H(X, C) .
We now prove a lemma that relates probabilities of visiting states by Xt and Yt.
Lemma 5.3.
For every t ≤k + ∆and states C′ ∈Mi, C′′ ∈M∆, we have
P [C′′ ∈{Xj} | X2t = C′]
P [C′′ ∈{Yj} | Y2t = C′] ≤
 
eH(X, C′′)
eH(X, C′′) + U(P, C′′)
!∆−i
where {C′′ ∈{Xj}} and {C′′ ∈{Yj}} denote the events X visits C′′ and Y visits C′′, respectively.
Proof. Consider the unique path p from C′ to C′′ in the state space of X (note that the transition
graphs for X and Y are directed trees). The probability of transitioning from C′ to C′′ for X and
Y equals the product of respective transition probabilities for every edge on the path. Recall that
transitions probabilities for X and Y are the same for all states but Cb, where C ∈∪jMj. The
number of such states on the path p is equal to the number transitions from Mj to Mj+1, since X
and Y can get from Mj to Mj+1 only through a state Cb on the boundary of Mj and Mj+1. The
number of transitions from Mj to Mj+1 equals ∆−i. For each state Cb on the path, the ratio of
transition probabilities from Cb to the next state C ∪{x} for Markov chains X and Y equals
H(X, C)
U(P, C) + H(X, C) ≤
eH(X, C′′)
U(P, C′′) + eH(X, C′′)
,
here we used that (a) U(P, C) ≥U(P, C′′) since Ut(P) is a non-increasing process; and (b)
H(P, C) ≤eH(P, C′′) since Ht(P) ≤eHt′(P) if t ≤t′ (see Section 3).
We now prove an analog of Corollary 4.5 for eH(X, Yj).
Lemma 5.4.
eH(X, Yt) is a supermartingale.
Proof. If Yj = C, then Yj+1 can only be in
n
Ca, Cbo
. Since eH(X, Ca) = eH(X, Cb) = eH(X, C), we
have E
h
eH(X, Yj+1) | Yj = C
i
= eH(X, Yj).
If Yj = Ca, then Yj+1 = C′ where the new center c should be in uncovered clusters with respect
to Ct.
E
H(P ′, Yj+1) | Yj = Ca, c ∈P ′ ≤5OPT1(P ′),
12

--- Page 13 ---
which implies
E
h
eH(P ′, Yj+1) | Yj = Ca, c ∈P ′i
≤eH(P ′, Yj).
Therefore, we have
E
h
eH(X, Yj+1) | Yj = Cai
≤eH(X, Yj).
If Yj = Cb, then for any possible state C′ of Yj+1, the new center should be in covered clusters with
respect to C. By deﬁnition, we must have eH(X, C′) = eH(X, C) = eH(X, Cb). Thus, it holds that
E
h
eH(X, Yj+1) | Yj = Cbi
= eH(X, Yj).
Combining all these cases, we get
n
eH(X, Yj)
o
is a supermartingale.
We now use Lemma 5.3 and Lemma 5.4 to bound the expected uncovered cost of P after k + ∆
rounds of k-means++.
Lemma 5.5.
For any cluster P ∈P and t ≤k + ∆, we have
E [Uk+∆(P) | Ct] ≤
eHt(X)
e(∆−M(Ct) −1).
Proof. Since k-means++ samples k + ∆centers and the total number of clusters in the optimal
solution P is k, k-means++ must make ∆misses. Hence, the process {Xt} which follows k-means++
must either visit a state in M≥∆or stop in H (recall that we stop process Xt if it reaches H).
If {Xt} stops in group H, then the cluster P is covered which means that Uk+∆(P) = 0. Let
∂M∆be the frontier of M∆i.e., the states that Xt visits ﬁrst when it reaches M∆(recall that
the transition graph of Xt is a tree). The expected cost E [Uk+∆(P) | Ct] is upper bounded by the
expected uncovered cost of P at time when Ct reaches ∂M∆. Thus,
E [Uk+∆(P) | Ct] ≤
X
C∈∂M∆
P [C ∈{Xj} | Ct] U(P, C).
Observe that by Lemma 5.3, for any C ∈∂M∆, we have
P [C ∈{Xj} | Ct] U(P, C) ≤P [C ∈{Yj} | Ct]
 
eH(X, C)
eH(X, C) + U(P, C)
!∆′
U(P, C).
Let f(x) = x(1/(1+ x))∆′. Then, f(x) is maximized at x = 1/(∆′ −1) and the maximum value
f(1/(∆′ −1)) = 1/(e(∆′ −1)). Therefore, for every C ∈∂M∆, we have
P[C ∈{Xj} | Ct]U(P, C) ≤P [C ∈{Yj} | Ct] f
 
U(P, C)
eH(X, C)
!
eH(X, C)
≤P [C ∈{Yj} | Ct]
eH(X, C)
e(∆′ −1).
Let τ = min {j : Yj ∈∂M∆} be the stopping time when Yj ﬁrst visits ∂M∆. We get
X
C∈∂M∆
P [C ∈{Yj} | Ct] eH(X, C) = E
h
eH(X, Yτ) | Ct
i
.
13

--- Page 14 ---
By Lemma 5.4, eH(X, Yj) is a supermartingale. Thus, by the optional stopping theorem,
E
h
eH(X, Yτ) | Ct
i
≤eH(X, Ct).
Therefore, we have
E [Uk+∆(P) | Ct] ≤
eHt(X)
e(∆−M(Ct) −1),
This concludes the proof.
We now add up bounds from Lemma 5.5 with t = 0 for all clusters P ∈P and obtain Lemma 5.2.
5.3
Bi-criteria bound for small ∆
In this section, we give another bi-criteria approximation guarantee for k-means++.
Lemma 5.6. Let costk+∆(X) be the cost of the the clustering resulting from sampling k+∆centers
according to the k-means++ algorithm (for ∆∈{1, . . . , 2k}). Then,
E [costk+∆(X)] ≤5

2 + 1
2e + ln 2k
∆

OPTk(X).
Proof. Consider k-means++ clustering algorithm and the corresponding random process Ct. Fix
a κ ∈{1, . . . , k}. Let τ be the ﬁrst iteration1 (stopping time) when K(Cτ) ≤κ if K(Ck) ≤κ; and
τ = k, otherwise. We refer the reader to Section 3 for deﬁnitions of M(Ct), Ut(X) = U(X, Ct), and
K(Ct).
We separately analyze the cost of uncovered clusters after the ﬁrst τ steps and the last k′ −τ
steps, where k′ = k + ∆is the total number of centers chosen by k-means++.
The ﬁrst step of our proof follows the analysis of k-means++ by Dasgupta (2013), and by
Arthur and Vassilvitskii (2007). Deﬁne a potential function Ψ (see Dasgupta 2013):
Ψt := M(Ct)U(X, Ct)
K(Ct)
.
If K(Ct) = 0, then M(Ct) and U(X, Ct) must be 0 and we let Ψt = 0
We use the following result by Dasgupta (2013) to estimate E[Ψτ(X)] in Lemma 5.8.
Lemma 5.7 (Dasgupta (2013)). For any 0 ≤t ≤k, we have
E [Ψt+1 −Ψt | Ct] ≤H(X, Ct)
K(Ct) .
Lemma 5.8. Then, the following bound holds:
E[Ψτ(X)] ≤5

1 + ln

k
κ + 1

OPTk(X).
1Recall, that K(Ct) is a non-increasing stochastic process with K(C0) = k.
14

--- Page 15 ---
Proof. Note that Ψ1 = 0 as M(C1) = 0. Thus,
E[Ψτ] ≤
τ−1
X
t=1
E
Ψt+1 −Ψt
 ≤E
h τ−1
X
t=1
H(X, Ct)
K(Ct)
i
.
Using the inequality H(X, Ct) ≤eHk(X) (see Section 3), we get:
E[Ψτ] ≤E
h τ−1
X
t=1
eHk(X)
K(Ct)
i
≤E
h
eHk(X) ·
τ−1
X
t=1
1
K(Ct)
i
.
Observe that K(C1), . . . , K(Cτ−1) is a non-increasing sequence in which two consecutive terms are
either equal or K(Ci+1) = K(Ci) −1. Moreover, K(C1) = k and K(Cτ−1) > κ. Therefore, by
Lemma 5.9 (see below), for every realization C0, C1, . . . , Cτ, we have:
τ−1
X
t=1
1
K(Ct) ≤1 + log k/(κ+1).
Thus,
E[Ψτ] ≤(1 + log k/(κ+1))E[ eHk(X)] ≤5(1 + log k/(κ+1)) OPTk(X).
This concludes the proof.
Let κ = ⌊(∆−1)/2⌋. By Lemma 5.8, we have
E
hM(Cτ)Uτ(X)
K(Cτ)
i
≤5

1 + ln 2k
∆

OPTk(X).
Since Ut(X) is a non-increasing stochastic process, we have E[Uk+∆(X)] ≤E[Uτ(X)]. Thus,
E
hM(Cτ)
K(Cτ) · Uk+∆(X)
i
≤5

1 + ln 2k
∆

OPTk(X).
Our goal is to bound E[Uk′(X)]. Write,
E[Uk′(X)] = E
hM(Cτ)
K(Cτ) · Uk′(X)
i
+ E
hK(Cτ) −M(Cτ)
K(Cτ)
· Uk′(X)
i
.
The ﬁrst term on the right hand side is upper bounded by 5
 1+ ln 2k
∆
OPTk(X). We now estimate
the second term, which we denote by (∗).
Note that K(Ct) −M(Ct) = k −t, since the number of uncovered clusters after t steps of
k-means++ equals the number of misses plus the number of steps remaining. Particularly, if τ = k,
we have K(Cτ) −M(Cτ) = K(Ck) −M(Ck) = 0. Consequently, if τ = k, then the second term (∗)
equals 0. Thus, we only need to consider the case, when τ < k. Note that in this case K(Cτ) = κ.
By Lemma 5.2 (applied to all uncovered clusters), we have
E[Uk′(X) | Cτ, τ] ≤
K(Cτ)
e(∆′ −1)
eHτ(X),
where ∆′ = ∆−M(Cτ).
15

--- Page 16 ---
Thus,
E
hK(Cτ) −M(Cτ)
K(Cτ)
· Uk′(X) | Cτ, τ
i
≤K(Cτ) −M(Cτ)
K(Cτ)
·
K(Cτ)
e(∆′ −1) · eHτ(X) = (∗∗).
Plugging in K(Cτ) = κ and the expression for ∆′ (see above), and using that κ ≤(∆−1)/2, we
get
(∗∗) =
κ −M(Cτ)
e(∆−M(Cτ) −1). · eHτ(X) ≤1
2e
eHτ(X).
Finally, taking the expectation over all Cτ, we obtain the bound
E
hK(Cτ) −M(Cτ)
K(Cτ)
· Uk′(X)
i
≤5OPT1(X)
2e
.
Thus, E[Uk′(X)] ≤5(1 + 1/2e + ln 2k/∆)OPTk(X). Therefore,
E[costk′(X)] = E[Hk′(X)] + Uk′(X) ≤5
 2 + 1
2e + ln 2k
∆
 OPTk(X).
We now prove Lemma 5.9.
Lemma 5.9.
For any t ≤k integers a1 ≥a2 ≥· · · ≥at such that a1 = k, at > κ and ai −ai+1 ∈
{0, 1} for all 1 ≤i < t, the following inequality holds
t
X
i=1
1
ai
≤1 + log

k
κ + 1

.
Proof. It is easy to see that the sum is maximized when t = k, and the sequence a1, . . . , ak is as
follows:
1
k,
1
k −1, . . . ,
1
κ + 2
|
{z
}
(k−(κ+1)) terms
,
1
κ + 1, . . . ,
1
κ + 1
|
{z
}
(κ+1) terms
.
The sum of the ﬁrst (k −(κ + 1)) terms is upper bounded by
Z 1/k
1/(κ+1)
1
x dx = ln
k
κ + 1.
The sum of the last (κ + 1) terms is 1.
6
Analysis of k-means∥
In this section, we give a sketch of analysis for the k-means∥algorithm. Speciﬁcally, we show upper
bounds on the expected cost of the solution after T rounds.
16

--- Page 17 ---
Theorem 6.1. The expected cost of the clustering returned by k-means∥algorithm after T rounds
are upper bounded as follows:
for ℓ< k,
E [costT+1(X)] ≤

e−ℓ
k
T
E [cost1(X)] + 5OPTk(X)
1 −e−ℓ
k
;
for ℓ≥k,
E [costT+1(X)] ≤
 k
eℓ
T
E [cost1(X)] + 5OPTk(X)
1 −k/eℓ
.
Remark: For the second bound (ℓ≥k), the additive term 5OPTk(X)/(1 −k/(eℓ)) ≤8OPTk(X).
The probability that a point is sampled by k-means∥is strictly greater than the probability
that it is sampled by k-means∥Pois since 1 −e−λ < λ for all λ > 0. Thus, for every round, we can
couple k-means∥Pois and k-means∥so that each point sampled by k-means∥Pois is also sampled by
k-means∥. Thus, the expected cost returned by k-means∥is at most the expected cost returned
by k-means∥Pois. In the following analysis, we show an upper bound for the expected cost of the
solution returned by k-means∥Pois.
As a thought experiment, consider a modiﬁed k-means∥Pois algorithm. This algorithm is given
the set X, parameter k, and additionally the optimal solution P = {P1, . . . , Pk}. Although this
modiﬁed algorithm is useless in practice as we do not know the optimal solution in advance, it will
be helpful for our analysis.
In every round t, the modiﬁed algorithm ﬁrst draws independent Poisson random variables
Zt(Pi) ∼Pois(λt(Pi)) for every cluster i ∈{1, . . . , k} with rate λt(Pi) = P
x∈Pi λt(x). Then, for
each i ∈{1, . . . , k}, it samples Zt(Pi) points x ∈Pi with repetitions from Pi, picking every point
x with probability λt(x)/λt(Pi) and adds them to the set of centers Ct. We assume that points in
every set Ct are ordered in the same way as they were chosen by this algorithm.
We claim that the distribution of the output sets CT of this algorithm is exactly the same as
in the original k-means∥Pois algorithm. Therefore, we can analyze the modiﬁed algorithm instead
of k-means∥Pois, using the framework described in Sections 3.
Lemma 6.2.
The sets Ct in the original and modiﬁed k-means∥Pois algorithms are identically
distributed.
Proof. Consider |Pi| independent Poisson point processes Nx(a) with rates λt(x), where x ∈Pi
(here, we use variable a for time). Suppose we add a center x at step t of the algorithm if Nx(t) ≥1.
On the one hand, the probability that we choose x is equal to 1 −e−λt(x) which is exactly the
probability that k-means∥Pois picks x as a center at step t. On the other hand, the sum NPi =
P
x∈Pi Nx is a Poisson point process with rate λt(Pi). Thus, the total number of jumps in the
interval [0, 1] of processes Nx with x ∈Pi is distributed as Zt(Pi). Moreover, the probability that
Nx jumps at time a conditioned on the event that NPi jumps at time a is λt(x)/λt(Pi). Thus, for
every jump of NPi, we choose one random center x with probability λt(x)/λt(Pi).
Lemma 6.3.
For k-means∥algorithm with parameter ℓ, the following bounds hold:
for ℓ< k,
E [costt+1(X)] ≤e−ℓ
k · E [costt(X)] + 5OPTk(X);
for ℓ≥k,
E [costt+1(X)] ≤
 k
eℓ

· E [costt(X)] + 5OPTk(X).
Proof. Since the expected cost returned by k-means∥is at most the expected cost returned by
k-means∥Pois, we analyze the expected cost of the clustering after one step of k-means∥Pois.
17

--- Page 18 ---
If the algorithm covers cluster Pi at round t, then at the next round, its uncovered cost equals
0. The number of centers chosen in Pi is determined by the Poisson random variable Zt+1(Pi).
Hence, Pi is uncovered at round t + 1 only if Zt+1(Pi) = 0. Since Ut(Pi) is non-increasing in t and
Ut(Pi) ≤costt(Pi), we have
E [Ut+1(Pi) | Ct] ≤P [Zt+1(Pi) = 0] Ut(Pi) ≤exp

−ℓcostt(Pi)
costt(X)

costt(Pi).
Deﬁne two function: f(x) = e−x · x; and g(x) = f(x) for x ∈[0, 1] and g(x) = e−1 for x ∈[1, ∞).
Then,
E [Ut+1(X) | Ct] ≤
 
1
k
k
X
i=1
f
ℓcostt(Pi)
costt(X)
!
kcostt(X)
ℓ
.
Since g(x) ≤f(x), and g(x) is concave for x ≥0, we have
E [Ut+1(X) | Ct] ≤
 
1
k
k
X
i=1
g
ℓcostt(Pi)
costt(X)
!
kcostt(X)
ℓ
≤g
 ℓ
k
 kcostt(X)
ℓ
.
Here, we use that P
i costt(Pi) = costt(X).
Therefore, for ℓ≤k, we have
E [Ut+1(X) | Ct] ≤

e−ℓ
k

costt(X);
and for ℓ≥k, we have
E [Ut+1(X) | Ct] ≤
 k
eℓ

costt(X).
Similar to Corollary 4.5, the process eHt(P) for k-means∥Pois is also a supermartingale, which
implies E [Ht+1(X)] ≤5OPTk(X). This concludes the proof.
Proof of Theorem 6.1. Applying the bound from Lemma 6.3 for t times, we get the following results.
For ℓ≤k,
E [costt+1(X)] ≤

e−ℓ
k
t
E [cost1(X)] + 5OPTk(X)ηt,
where ηt = Pt
j=1

e−ℓ
k
j−1 <
1
1−e−ℓ
k . For ℓ≥k,
E [costt+1(X)] ≤
 k
eℓ
t
E [cost1(X)] + 5OPTk(X)ηt,
where ηt = Pt
j=1

k
eℓ
j−1 ≤
1
1−k
eℓ.
Corollary 6.4.
Consider a data set X with more than k distinct points. Let
T = ln E
 cost1(X)
OPTk(X))

and ℓ> k. Then, after T rounds of k-means∥, the expected cost of clustering E [costT (X)] is at
most 9OPTk(X).
18

--- Page 19 ---
7
Exponential Race k-means++ and Reservoir Sampling
In this section, we show how to implement k-means++ algorithm in parallel using R passes over the
data set. This implementation, which we refer to as k-means++ER (exponential race k-means++),
is very similar to k-means∥, but has stronger theoretical guarantees. Like k-means∥, in every round,
k-means++ER tentatively selects ℓcenters, in expectation. However, in the same round, it removes
some of the just selected centers (without making another pass over the data set). Consequently,
by the end of each iteration, the algorithm keeps at most k centers.
We can run k-means++ER till it samples exactly k centers; in which case, the distribution of k
sampled centers is identical to the distribution of the regular k-means++, and the expected number
of rounds or passes over the data set R is upper bounded by
O
k
ℓ+ log OPT1(X)
OPTk(X)

.
We note that R is never greater than k. We can also run this algorithm for at most R∗rounds.
Then, the expected cost of the clustering is at most
5(ln k + 2) OPTk(X) + 5R∗
 4k
eℓR∗
R∗
· OPT1(X).
7.1
Algorithm
In this section, we give a high level description of our k-means++ER algorithm. In Section 7.2, we
show how to eﬃciently implement k-means++ER using lazy updates and explain why our algorithm
makes R passes over the data set.
The algorithm simulates n continuous-time stochastic processes.
Each stochastic process is
associated with one of the points in the data set. We denote the process corresponding to x ∈X
by Pt(x). Stochastic process Pt(x) is a Poisson process with variable arrival rate λt(x).
The algorithm chooses the ﬁrst center c1 uniformly at random in X and sets the arrival rate
of each process Pt(x) to be λt(x) = cost(x, {c1}). Then, it waits till one of the Poisson processes
Pt(x) jumps. When process Pt(x) jumps, the algorithm adds the point x ∈X (corresponding to
that process) to the set of centers Ct and updates the arrival rates of all processes to be
λt(y) = cost(y, Ct)
for all y ∈X. Note that if y is a center, then the arrival rate λt(y) is 0.
The algorithm also maintains a round counter R. In the lazy version of this algorithm (which
we describe in the next section), the algorithm makes a pass over the data set and samples a new
batch of centers every time this counter is incremented. Additionally, at the end of each round, the
algorithm checks if it chose at least one center in that round, and in the unlikely event that it did
not, it selects one center with probability proportional to the costs of the points.
Initially, the algorithm sets R = 0, t0 = 0, and t1 = ℓ/cost(X, {c1}). Then, at each time point
ti (i ≥1), we increment R and compute
ti+1 = ti + ℓ/cost(X, Cti),
where Cti is the set of all centers selected before time ti. We refer to the time frame [ti−1, ti] for
i ≥1 as the i-th round. The algorithm stops when one of the following conditions holds true (1) the
19

--- Page 20 ---
number of sampled centers is k; or (2) the round counter R equals the prespeciﬁed threshold R∗,
which may be ﬁnite or inﬁnite.
Before analyzing this algorithm, we mention that every Poisson process Pt with a variable
arrival rate λt can be coupled with a Poisson process Qs with rate 1. To this end, we substitute
the variable
s(t) =
Z t
0
λτdτ,
and let
Pt ≡Qs(t).
Observe that the expected number of arrivals for process Qs in the inﬁnitesimal interval [s, s + ds]
is ds = λtdt which is exactly the same as for process Pt.
It is convenient to think about the variables s as “current position”, t as “current time”, and λt
as “current speed” of s. To generate process Pt(x), we can ﬁrst generate Poisson process Qs(x) with
arrival rate 1 and then move the position st(x) with speed λt(x). The process Pt(x) = Qst(x)(x) is
a Poisson process with variable arrival rate λt(x).
Theorem 7.1. I. If the number of rounds is not bounded (i.e., R∗= ∞), then the distribution of
centers returned by k-means++ER is identical to the distribution of centers returned by k-means++.
II. Moreover, the expected number of rounds R is upper bounded by
(1 + ok(1)) ·

⌈k
ℓ⌉+ log 2 OPT1(X)
OPTk(X)

,
and never exceeds k.
III. If the threshold R∗is given (R∗< ∞), then the cost of the solution after R∗rounds is upper
bounded by
5(ln k + 2) OPTk(X) + 2R∗
 4k
eℓR∗
R∗
· OPT1(X).
Proof of Part I. For the sake of analysis, we assume that after the algorithm outputs solution C, it
does not terminate, but instead continues to simulate Poisson processes Pt(x). It also continues to
update the set Ct (but, of course, not the solution) and the arrival rates λt(x) till the set Ct contains
k centers. Once |Ct| = k, the algorithm stops updating the set of centers Ct and arrival rates but
still simulates continuous-time processes Pt(x). Clearly, this additional phase of the algorithm does
not aﬀect the solution since it starts after the solution is already returned to the user.
We prove by induction on i that the ﬁrst i centers c1, . . . , ci have exactly the same joint distri-
bution as in k-means++. Indeed, the ﬁrst center c1 is drawn uniformly at random from the data
set X as in k-means++. Suppose centers c1, . . . , ci are already selected. Then, we choose the next
center ci+1 at the time of the next jump of one of the Poisson processes Pt(x). Observe that the
conditional probability that a particular process Pt(x) jumps given that one of the processes Pt(y)
(y ∈X) jumps is proportional to λt(x), which in turn equals the current cost(x, Ct) of point x.
Hence, the distribution of center ci+1 is the same as in k-means++. This completes the proof of
item I.
Proof of Part II. We now show items II and III. Deﬁne process
Pt(X) =
X
x∈X
Pt(x).
20

--- Page 21 ---
Its rate λt(X) equals P
x∈X λt(x). We couple this process with a Poisson Qs(X) with arrival rate
1 as discussed above. We want to estimate the number of centers chosen by the algorithm in the
ﬁrst R′ rounds. To this end, we count the number of jumps of the Poisson process Pt(X) (recall
that we add a new center to Ct whenever Pt(X) jumps unless |Ct| already contains k centers). The
number of jumps equals PtR′ which, in turn, equals QsR′ where sR′(X) is the position of s(X) at
time tR′:
sR′(X) =
Z tR′
0
λτ(X) dτ =
R′−1
X
i=0
Z ti+1
ti
λτ(X) dτ ≥
R′−1
X
i=0
(ti+1 −ti) · λti+1(X).
Here, we used that λt(X) is non-increasing, and thus, λti+1(X) ≤λτ(X) for all τ ∈[ti, ti+1]. We
now recall that (ti+1 −ti) = ℓ/cost(X, Cti) and λti+1(X) = cost(X, Cti+1). Hence,
sR′(X) ≥ℓ
R′−1
X
i=0
cost(X, Cti+1)
cost(X, Cti) .
By the inequality of arithmetic and geometric means, we have
sR′(X) ≥ℓ· R′
 R′−1
Y
i=0
cost(X, Cti+1)
cost(X, Cti)
!1/R′
= ℓ· R′
 
cost(X, CtR′ )
cost(X, Ct0)
!1/R′
(5)
= ℓ· R′
 
cost(X, CtR′ )
cost(X, {c1})
!1/R′
.
We now use this equation to prove items II and III. For item II, we let random variable R′ to
be
R′ = 2e⌈k/ℓ⌉+ log cost(X, {c1})
OPTk(X)
.
Note that R′ depends on the ﬁrst center c1 (which is chosen in the very beginning of the algorithm)
but not on the Poisson processes Pt(x).
Since, Ct always contains at most k centers, we have
cost(x, CtR′ ) ≥OPTk(X), and consequently
sR′(X) ≥ℓ· R′
 
OPTk(X)
cost(X, {c1})
!1/R′
> ℓ· 2e⌈k/ℓ⌉· 1/e ≥2k.
The expected number of jumps of the Poisson process Qs(X) in the interval [0, sR′(X)] equals
QsR(X)(X). Observe that
QsR(X)(X) ≥Q2k(X)
and Q2k(X) is a Poisson random variable with parameter 2k. By the Chernoﬀbound2, it makes
fewer than k jumps with exponentially small probability in k; namely, with probability at most
(e/2)−k. Thus, with probability at least 1 −(e/2)−k, the algorithm selects k centers in the ﬁrst R′
rounds. Moreover, if it does not happen in the ﬁrst R∗rounds, then it selects k centers by the end
of the second R′ rounds again with probability at least 1 −(e/2)−k and so on. Hence, the expected
2We use the bound Pr{P ≤k} ≤e−λ eλ/kk, where P is a Poisson random variable with parameter λ and k < λ.
See e.g., Theorem 5.4.2 in Mitzenmacher and Upfal (2017).
21

--- Page 22 ---
number of rounds till it selects k centers is (1 + ok(1))R′. Finally, observe that the expectation of
cost(X, {c1}) over the choice of the ﬁrst center equals 2 OPTk(X). Since log(·) is a convex function,
we have
E[R′] ≤2e⌈k/ℓ⌉+ log 2 OPT1(X)
OPTk(X) .
Therefore, we showed that the expected number of rounds is upper bounded by the right hand side
of the expression above times a multiplicative factor of (1+ok(1)). A slightly more careful analysis
gives a bound of
(1 + ok(1))
 
e⌈k/ℓ⌉+ log 2 OPT1(X)
OPTk(X)
!
.
This concludes the proof of item II.
Proof of Part III. We now prove item III. Denote T = tR∗. Consider the event
E =
algorithm samples k centers in the ﬁrst R∗rounds
	.
Let ¯E be the complimentary events to E. Then,
E
cost(X, CT )
 = E
cost(X, CT ) · 1(E)
 + E
cost(X, CT ) · 1( ¯E)
.
We now separately upper bound each of the terms on the right hand side. It is easy to upper bound
the ﬁrst term:
E[cost(X, CT ) · 1(E)] ≤5(ln k + 2) · OPTk(X),
because the distribution of centers returned by k-means++ER is identical to the distribution of
centers returned by k-means++. We now bound the second term. Denote by Dρ the event
Dρ =

cost(X, CT ) ≥
 ρk
ℓR∗
R∗
cost(X, {c1})

.
We prove the following claim.
Claim 7.2. The following inequality holds for every real number ρ ∈[1, ℓR∗/k] and any choice of
the ﬁrst center c1:
Pr
  ¯E and Dρ | c1
 ≤e−(ρ−1)kρk−1.
Proof. We use inequality (5) with R′ = R∗:
sR∗(X) ≥ℓ· R∗
 
cost(X, CT )
cost(X, {c1})
!1/R∗
.
It implies that sR∗(X) ≥ρk if event Dρ occurs. On the other hand if ¯E occurs, then the number
of centers chosen by the end of round R∗is less than k and, consequently, the number of jumps of
Pt(X) in the interval [0, T] is less than k:
PT (X) ≡QsR∗(X)(X) < k.
22

--- Page 23 ---
Hence, we can bound Pr( ¯E and Dρ | c1) as follows:
Pr( ¯E and Dρ) ≤Pr
 Dρ and QsR∗(X) < k | c1
 ≤
≤Pr
 Dρ and Qρk(X) < k | c1
 ≤Pr
 Qρk(X) < k | c1
.
Random variable Qρk(X) has the Poisson distribution with parameter ρk and is independent of c1.
By the Chernoﬀbound, the probability that Qρk(X) ≤k −1 is at most (as in Part II of the proof):
Pr
Qρk(X) ≤k −1
	 ≤e−ρk eρk
k −1
k−1
= e−(ρ−1)k−1ρk−1 ·

k
k −1
k−1
|
{z
}
≤e
≤e−(ρ−1)kρk−1.
This completes the proof of Claim 7.2.
Let
Z =
ℓR∗
k
R∗
· cost(X, CT )
cost(X, {c1}).
Then, by Claim 7.2,
Pr
  ¯E and Z ≥ρR∗| c1
 ≤e−(ρ−1)kρk−1.
(6)
Write,
E
1( ¯E) · Z | c1
 =
Z ∞
0
Pr
 1( ¯E) and Z ≥r | c1
dr ≤1 +
Z ∞
1
Pr
 1( ¯E) and Z ≥r | c1
 dr.
We now substitute r = ρR∗and then use (6):
E
Z · 1( ¯E) | c1
 ≤1 + R∗
Z ∞
1
Pr
  ¯E and Z ≥ρR∗| c1
 · ρR∗−1dρ
≤1 + R∗
Z ∞
1
e−(ρ−1)kρk+R∗−2dρ.
We note that R∗< k, since our algorithm chooses at least one center in each round. Thus,
by Lemma 7.3 (which we prove below), the integral on the right hand side is upper bounded by
eR∗/2 · (4/e)R∗. Hence,
E
Z · 1( ¯E) | c1
 ≤1 + R∗·
4
e
R∗−2
.
Multiplying both sides of the inequality by (k/ℓR∗)R∗·cost(X, {c1}) and taking the expectation over
c1, we get the desired inequality:
E
cost(X, CT ) · 1( ¯E)
 ≤

1 + R∗
4
e
R∗ k
ℓR∗
R∗
Ec1
cost(X, {c1}

=

1 + R∗4
e
R∗−2 k
ℓR∗
R∗
· 2 OPT1(X)
< 2R∗
 4k
eℓR∗
R∗
OPT1(X).
This ﬁnishes the proof of Theorem 7.1.
23

--- Page 24 ---
Lemma 7.3. For R∗< k, we have
Z ∞
1
e−(ρ−1)kρk+R∗−2dρ ≤e
2
4
e
R∗
.
Proof. Since e−(ρ−1)ρ ≤1 for all ρ ≥1, we have e−(ρ−1)kρk ≤e−(ρ−1)R∗ρR∗for any R∗< k. Thus,
we have
Z ∞
1
e−(ρ−1)kρk+R∗−2dρ ≤
Z ∞
1
e−(ρ−1)R∗ρ2R∗−3dρ = eR∗Z ∞
1
e−ρR∗ρ2R∗−3dρ
= eR∗Z ∞
1
(e−ρρ2)R∗ρ−3dρ.
Observe that e−ρρ2 ≤4/e2 for any ρ ≥1. Hence,
(e−ρρ2)R∗= (e−ρρ2)R∗−1 · e−ρρ2 ≤(4/e2)R∗−1e−ρρ2.
Thus,
Z ∞
1
e−(ρ−1)kρk+R∗−2dρ ≤4R∗−1 · eR∗
e2(R∗−1)
·
Z ∞
1
e−ρ
ρ
dρ = 4R∗−1
eR∗−2 · 1
4 =
4
e
R∗−2
.
7.2
Lazy implementation of k-means++ER
We now describe how we can eﬃciently implement the k-means++ER algorithm using a lazy reser-
voir sampling. We remind the reader that the time of the ﬁrst jump of a Poisson process with
parameter λ is distributed as the exponential distribution with parameter λ. Imagine for a mo-
ment, that the arrival rates of our Poisson processes were constant. Then, in order to select the
ﬁrst k jumps, we would generate independent exponential random variables with parameters λ(x)
for all x and choose k smallest values among them. This algorithm is known as the reservoir sam-
pling(see Efraimidis and Spirakis (2006)). To adapt this algorithm to our needs, we need to update
the arrival rates of the exponential random variables. Loosely speaking, we do so by generating
exponential random variables with rate 1 for Poisson processes Qs(x) which are described above
and then updating the speeds λt(x) of variables st(x). We now formally describe the algorithm.
In the beginning of every round i, we recompute costs of all points in the data set.
Then,
we draw an independent exponential random variable Sx with rate 1 for every point x, and let
St(x) = Sx . We set
τt(x) = St(x)
λt(x).
Think of St(x) as the distance st(x) needs to travel till process Qs(x) jumps; λt(x) is the speed
of point st(x); and τt(x) is the time left till Qs(x) = Pt(x) jumps if the speed λt does not change.
Among all points x ∈X, we select a tentative set of centers Z for this round. The set Z contains all
points x with ti−1 + τt(x) ≤ti. This is the set of all points for which their Poisson processes would
jump in the current round if their arrival rates remained the same till the end of the round. Since
the arrival rates can only decrease in our algorithm, we know for sure that for points x outside of
Z, the corresponding processes Pt(x) will not jump in this round. Thus, we can safely ignore those
points during the current round.
24

--- Page 25 ---
We also note that in the unlikely event that the initial set Z is empty, we choose x with the
smallest time τt(x) and add it to the set of centers Ct. (This is equivalent to choosing a point
with probability proportional to cost(x, Ct) by the memorylessness property of the exponential
distribution).
The steps we described above – updating costs cost(x, Ct), drawing exponential random vari-
ables Sx, and selecting points in the set Z – can be performed in parallel using one pass over the
data set. In the rest of the current round, our algorithm deals only with the set Z whose size in
expectation is at most ℓ(see below).
While the set Z is not empty we do the following. We choose x ∈Z with the smallest value of
τt(x). This x corresponds to the process that jumps ﬁrst. Then, we perform the following updates:
We add x to the set of centers Ct. We set the “current time” t to t = t′ + τt′(x), where t′ is the
time of the previous update. If x is the ﬁrst center selected in the current round, then we let t′ to
be the time when the round started (i.e., ti−1). We recompute the arrival rates (speeds) λt(x) for
each x in Z. Finally, we update the values of all τt(x) for x ∈Z using the formula
τt(x) = St(x) −λt′(x) · (t −t′)
λt(x)
,
here λt′(x) · (t −t′) is the distance variable st(x) moved from the position where it was at time t′;
St(x) −λt′(x) · (t −t′) is the remaining distance st(x) needs to travel till the process Qt(x) jumps;
and τt(x) is the remaining time till Pt(x) jumps if we do not update its arrival rate. After we
update τt(x), we prune the set Z. Speciﬁcally, we remove from set Z all points x with t+τt(x) > ti.
As before, we know for sure that if x is removed from Z, then the corresponding processes Pt(x)
will not jump in the current round.
This algorithm simulates the process we described in the previous section. The key observation
is that Poisson processes Pt(x) we associate with points x removed from Z cannot jump in this
round and thus can be safely removed from our consideration. We now show that the expected size
of the set Z is at most ℓ. In the next section, we analyze the running time of this algorithm.
Then we show that the expected size of the set Z in the beginning of each round i + 1 is at
most ℓ. Since every point x belongs to Z with probability
Pr{x ∈Z} = Pr

Sx
cost(x, Cti) ≤
ℓ
cost(X, Cti)

= Pr

Sx ≤ℓ· cost(x, Cti)
cost(X, Cti)

.
The right hand side is the probability that the Poisson process Qs(x) with rate 1 jumps in the inter-
val of length ℓ· cost(x, Cti)/cost(X, Cti) which is upper bounded by the expected number of jumps
of Qs(x) in this interval. The expected number of jumps exactly equals ℓ· cost(x, Cti)/cost(X, Cti).
Thus, the expected size of Z is upper bounded as
E|Z| =
X
z∈X
Pr{z ∈Z} ≤
X
z∈X
ℓ· cost(z, Cti)
cost(X, Cti) = ℓ.
7.3
Run time analysis
According to our analysis above, the number of new centers chosen at each round of k-means++ER
is at most the size of set Z, which is O(ℓ) with high probability. In the beginning of every round,
we need to update costs of all data points, which requires O(nℓd) time. In each round, we also
25

--- Page 26 ---
need to maintain the rates of all points in set Z, which needs O(ℓ2d) time. Thus, the total running
time for k-means++ER with R rounds is O(Rnℓd). We note that before running our algorithm, we
can reduce the dimension d of the space to O(log k) using the Johnson–Lindenstrauss transform
(see Johnson and Lindenstrauss (1984)). This will increase the approximation factor by a factor of
(1 + ε) but make the algorithm considerably faster (see Makarychev et al. (2019), Becchetti et al.
(2019), and Boutsidis et al. (2010)).
References
A. Aggarwal, A. Deshpande, and R. Kannan. Adaptive sampling for k-means clustering. In Approx-
imation, Randomization, and Combinatorial Optimization. Algorithms and Techniques, pages
15–28. Springer, 2009.
S. Ahmadian, A. Norouzi-Fard, O. Svensson, and J. Ward. Better guarantees for k-means and
euclidean k-median by primal-dual algorithms. SIAM Journal on Computing, pages FOCS17–97,
2019.
D. Aloise, A. Deshpande, P. Hansen, and P. Popat.
Np-hardness of euclidean sum-of-squares
clustering. Machine learning, 75(2):245–248, 2009.
D. Arthur and S. Vassilvitskii. k-means++: The advantages of careful seeding. In Proceedings of
the eighteenth annual ACM-SIAM symposium on Discrete algorithms, pages 1027–1035. Society
for Industrial and Applied Mathematics, 2007.
P. Awasthi, M. Charikar, R. Krishnaswamy, and A. K. Sinop. The hardness of approximation of
euclidean k-means. In 31st International Symposium on Computational Geometry (SoCG 2015).
Schloss Dagstuhl-Leibniz-Zentrum fuer Informatik, 2015.
O. Bachem, M. Lucic, and A. Krause. Distributed and provably good seedings for k-means in
constant rounds.
In Proceedings of the 34th International Conference on Machine Learning-
Volume 70, pages 292–300. JMLR. org, 2017.
B. Bahmani, B. Moseley, A. Vattani, R. Kumar, and S. Vassilvitskii. Scalable k-means++. Pro-
ceedings of the VLDB Endowment, 5(7):622–633, 2012.
L. Becchetti, M. Bury, V. Cohen-Addad, F. Grandoni, and C. Schwiegelshohn. Oblivious dimension
reduction for k-means: beyond subspaces and the johnson–lindenstrauss lemma. In Proceedings
of the 51st Annual ACM SIGACT Symposium on Theory of Computing, pages 1039–1050, 2019.
B. Boehmke and B. M. Greenwell. Hands-on machine learning with R. CRC Press, 2019.
C. Boutsidis, A. Zouzias, and P. Drineas. Random projections for k-means clustering. In Advances
in Neural Information Processing Systems, pages 298–306, 2010.
T. Brunsch and H. Röglin. A bad instance for k-means++. Theoretical Computer Science, 505:
19–26, 2013.
D. Choo, C. Grunau, J. Portmann, and V. Rozhoň. k-means++: few more steps yield constant
approximation. In Proceedings of the 37th International Conference on Machine Learning, pages
7849–7057. JMLR. org, 2020.
26

--- Page 27 ---
S. Dasgupta. The hardness of k-means clustering. Department of Computer Science and Engineer-
ing, University of California, San Diego, 2008.
S. Dasgupta.
UCSD CSE 291,
Lecture Notes:
Geometric
Algorithms,
2013.
URL:
https://cseweb.ucsd.edu/~dasgupta/291-geom/kmeans.pdf. Last visited on 2020/06/01.
D. Dua and C. Graﬀ. UCI ML repository, 2017. URL http://archive.ics.uci.edu/ml.
P. S. Efraimidis and P. G. Spirakis. Weighted random sampling with a reservoir. Information
Processing Letters, 97(5):181–185, 2006.
R. Elber. Kdd-Cup, 2004. URL http://osmot.cs.cornell.edu/kddcup/.
W. B. Johnson and J. Lindenstrauss. Extensions of lipschitz mappings into a hilbert space. Con-
temporary mathematics, 26(189-206):1, 1984.
T. Kanungo, D. M. Mount, N. S. Netanyahu, C. D. Piatko, R. Silverman, and A. Y. Wu.
A
local search approximation algorithm for k-means clustering. Computational Geometry, 28(2):
89 – 112, 2004.
ISSN 0925-7721.
doi: https://doi.org/10.1016/j.comgeo.2004.03.003.
URL
http://www.sciencedirect.com/science/article/pii/S0925772104000215.
S. Lattanzi and C. Sohler.
A better k-means++ algorithm via local search.
In International
Conference on Machine Learning, pages 3662–3671, 2019.
E. Lee, M. Schmidt, and J. Wright. Improved and simpliﬁed inapproximability for k-means. Infor-
mation Processing Letters, 120:40–43, 2017.
S. Lloyd. Least squares quantization in pcm. IEEE transactions on information theory, 28(2):
129–137, 1982.
K. Makarychev, Y. Makarychev, M. Sviridenko, and J. Ward. A bi-criteria approximation algorithm
for k-means. Approximation, Randomization, and Combinatorial Optimization. Algorithms and
Techniques, 2016.
K. Makarychev, Y. Makarychev, and I. Razenshteyn. Performance of johnson–lindenstrauss trans-
form for k-means and k-medians clustering. In Proceedings of the 51st Annual ACM SIGACT
Symposium on Theory of Computing, pages 1027–1038, 2019.
M. Mitzenmacher and E. Upfal. Probability and computing: Randomization and probabilistic tech-
niques in algorithms and data analysis. Cambridge university press, 2017.
R. Ostrovsky, Y. Rabani, L. Schulman, and C. Swamy. The eﬀectiveness of lloyd-type methods
for the k-means problem. In 2006 47th Annual IEEE Symposium on Foundations of Computer
Science (FOCS’06), 2006.
V. Rozhoň.
Simple and sharp analysis of k-means||.
In Proceedings of the 37th International
Conference on Machine Learning, pages 7828–7837. JMLR. org, 2020.
D. Wei. A constant-factor bi-criteria approximation guarantee for k-means++. In Advances in
Neural Information Processing Systems, pages 604–612, 2016.
27

--- Page 28 ---
Appendix
In this appendix, we present our experiments, give proofs omitted in the main part of the paper,
and provide complimentary lower bounds.
A
Experiments
In this section, we present plots that show that the performance of k-means∥and “k-means++
with oversampling and pruning” algorithms are very similar in practice. Below, we compare the
following algorithms on the datasets BioTest from KDD Cup 2004 (Elber, 2004) and COVTYPE
from the UCI ML repository (Dua and Graﬀ, 2017):
• Regular k-means++. The performance of this algorithm is shown with a solid black line on
the plots below.
• k-means∥without pruning. This algorithm samples k centers using k-means∥with T = 5
rounds and ℓ= k/T.
• k-means∥. This algorithm ﬁrst samples 5k centers using k-means∥and then subsamples k
centers using k-means++. The performance of this algorithm is shown with a dashed blue
line on the plots below.
• k-means++ with oversampling and pruning. This algorithm ﬁrst samples 5k centers using
k-means++ and then subsamples k centers using k-means++.
The performance of this
algorithm is shown with a thin red line on the plots below.
For each k = 5, 10, · · · , 200, we ran these algorithms for 50 iterations and took their average.
We normalized all costs by dividing them by the cost of k-means++ with k = 1000 centers.
0
50
100
150
200
2
4
6
8
10
#centers
cost
BioTest
k-means++
BiCriteria k-means++ w/Pruning
k-means∥
10
20
30
40
50
3
4
5
6
#centers
cost
BioTest
k-means++
BiCriteria k-means++ w/Pruning
k-means∥
28

--- Page 29 ---
0
50
100
150
200
0
20
40
#centers
cost
COVTYPE
k-means++
BiCriteria k-means++ w/Pruning
k-means∥
10
20
30
40
50
10
15
20
25
30
#centers
cost
COVTYPE
k-means++
BiCriteria k-means++ w/Pruning
k-means∥
0
50
100
150
200
2
4
6
8
10
#centers
cost
BioTest
k-means++
k-means∥without Pruning
10
20
30
40
50
4
6
8
10
#centers
cost
BioTest
k-means++
k-means∥without Prunning
0
50
100
150
200
0
20
40
60
#centers
cost
COVTYPE
k-means++
k-means∥without Prunning
10
20
30
40
50
10
20
30
40
50
60
#centers
cost
COVTYPE
k-means++
k-means∥without Pruning
29

--- Page 30 ---
B
Details for Preliminaries
For any set of points Y ⊂Rd, let µ = P
x∈Y x/ |Y| be the centroid of the cluster Y. Then, the
optimal cost of Y with one center,
OPT1(Y) =
X
x∈Y
∥x −µ∥2 =
P
(x,y)∈Y×Y∥x −y∥2
2 |Y|
.
This is a well known formula which is often used for analyzing of k-means algorithms. For com-
pleteness, we give a proof below.
Proof. Consider any point z ∈Rd, then we have:
cost(Y, {z}) =
X
x∈Y
∥x −z∥2 =
X
x∈Y
∥(x −µ) + (µ −z)∥2
=
X
x∈Y

∥x −µ∥2 + ∥µ −z∥2 + 2 ⟨x −µ, µ −z⟩

=
X
x∈Y
∥x −µ∥2 + |Y| · ∥µ −z∥2 + 2
* X
x∈Y
(x −µ), µ −z
+
=
X
x∈Y
∥x −µ∥2 + |Y| · ∥µ −z∥2.
Thus, the optimal choice of z to minimize cost(Y, {z}) is µ and OPT1(Y) = P
x∈Y∥x −µ∥2.
X
x∈Y
∥x −µ∥2 =
X
x∈Y
⟨x −µ, x −µ⟩=
X
x∈Y
⟨x, x −µ⟩
=
X
x∈Y
*
x, x −
X
y∈Y
y
|Y|
+
=
1
|Y|
X
(x,y)∈Y×Y
⟨x, x −y⟩
=
1
2 |Y|


X
(x,y)∈Y×Y
⟨x, x −y⟩+
X
(x,y)∈Y×Y
⟨y, y −x⟩


=
P
(x,y)∈Y×Y∥x −y∥2
2 |Y|
.
C
Lower bounds
C.1
Lower bound on the cost of covered clusters
We show the following lower bound on the expected cost of a covered cluster in k-means++. There-
fore, the 5-approximation in Lemma 4.1 is tight.
Theorem C.1. For any ε > 0, there exists an instance of k-means such that for a set P ∈X
and a set of centers C ∈Rd, if a new center c is sampled from P with probability Pr(c = x) =
cost(x, C)/cost(P, C), then
Ec [cost(P, C ∪{c})] ≥(5 −ε)OPT1(P).
30

--- Page 31 ---
Proof. Consider the following one dimensional example, where P contains t points at 0 and one
point at 1, and the closest center already chosen in C to P is at −1.
−1
0
t
1
1
The new center c will be chosen at 0 with probability
t
t+4, and at 1 with probability
4
t+4. Then,
the expected cost of P is
Ec [cost(P, C ∪{c})] = 1 ·
t
t + 4 + t ·
4
t + 4 =
5t
t + 4;
and the optimal cost of P is OPT1(P) ≤1. Thus, by choosing t ≥4(5 −ε)/ε, we have
Ec [cost(P, C ∪{c})] ≥(5 −ε)OPT1(P).
C.2
Lower bound on the bi-criteria approximation
In this section, we show that the bi-criteria approximation bound of O(ln k
∆) is tight up to constant
factor. Our proof follows the approach by Brunsch and Röglin (2013). We show the following
theorem.
Theorem C.2.
For every k > 1 and ∆≤k, there exists an instance X of k-means such that the
bi-criteria k-means++ algorithm with k + ∆centers returns a solution of cost greater than
1
8 log k
∆· OPTk(X)
with probability at least 1 −e−
√
k/2.
Remark: This implies that the expected cost of bi-criteria k-means with k + ∆centers is at
least
1 −e−
√
k/2
8
· log k
∆· OPTk(X).
Proof. For every k and ∆≥
√
k, we consider the following instance. The ﬁrst cluster is a scaled
version of the standard simplex with N ≫k vertices centered at the origin, which is called the heavy
cluster. The length of the edges in this simplex is 1/
√
N −1. Each of the remaining k −1 clusters
contains a single point on k −1 axes, which are called light clusters. These clusters are located at
distance √α from the center of the heavy cluster and
√
2α from each other, where α = ln(k/∆)
4∆
.
For the sake of analysis, let us run k-means++ till we cover all clusters. At the ﬁrst step, the
k-means++ algorithm almost certainly selects a center from the heavy cluster since N ≫k. Then,
at each step, the algorithm can select a center either from one of uncovered light clusters or from
the heavy cluster. In the former case, we say that the algorithm hits a light cluster, and in the latter
case we say that the algorithm misses a light cluster. Below, we show that with high probability
the algorithm makes at least 2∆misses before it covers all but ∆light clusters.
31

--- Page 32 ---
Lemma C.3. Let ∆≥
√
k. By the time the k-means++ algorithm covers all but ∆light clusters,
it makes greater than 2∆misses with probability at least 1 −e−
√
k/2.
Proof sketch. Let ε = 1/
√
N. Observe that k-means++ almost certainly covers all clusters in εN
steps (since N ≫k). So in the rest of this proof sketch, we assume that the number chosen centers
is at most εN and, consequently, at least (1 −ε)N points in the heavy cluster are not selected as
centers. Hence, the cost of the heavy cluster is at least 1 −ε.
Consider a step of the algorithm when exactly u light clusters remain uncovered. At this step,
the total cost of all light clusters is αu (we assume for simplicity that distance between the light
clusters and the closest chosen center in the heavy cluster is the same as the distance to the origin).
The cost of the heavy cluster is at least 1 −ε. The probability that the algorithm chooses a center
from the heavy cluster and thus misses a light cluster is at least (1 −ε)/(1 + αu).
Deﬁne random variables {Xu} as follows. Let Xu = 1 if the algorithm misses a cluster at least
once when the number of uncovered light clusters is u; and let Xu = 0, otherwise. Then, {Xu} are
independent Bernoulli random variables. For each u, we have P [Xu = 1] ≥(1 −ε)/(1 + αu).
Observe that the total number of misses is lower bounded by Pk−1
u=∆Xu. Then, we have
E
" k−1
X
u=∆
Xu
#
≥(1 −ε)
k−1
X
u=∆
1
1 + αu ≥(1 −ε)
Z k
∆
du
1 + αu
= (1 −ε)α−1 ln 1 + αk
1 + α∆
≥(1 −ε)α−1 ln k
∆= 4(1 −ε)∆.
Let µ = E
hPk−1
u=∆Xu
i
≥4(1 −ε)∆. By the Chernoﬀbound for Bernoulli random variables, we
have
P
"
k
X
u=∆
Xu ≤2∆
#
≤e−µ
 eµ
2∆
2∆
.
Since f(x) = e−x( ex
2∆)2∆is a monotone decreasing function for x ≥2∆, we have
P
"
k
X
u=∆
Xu ≤2∆
#
≤e−(2−4ε)∆· 22∆≤e−∆/2.
Hence, with probability as least 1 −e−
√
k/2, the number of misses is greater than 2∆.
For every k and ∆≥
√
k, consider the instance we constructed. By Lemma C.3, the algorithm
chooses more than k+∆centers to cover all but ∆light clusters with probability at least 1−e−
√
k/2.
Thus, at the time when the algorithm chose k + ∆centers, the number of uncovered light clusters
was greater than ∆. Hence, in the clustering with k + ∆centers sampled by k-means++, the total
cost is at least 1
4 ln (k/∆), while the cost of the optimal solution with k clusters is 1. For every k
and ∆<
√
k, the total cost is at least 1
4 ln(k/∆′) with ∆′ =
√
k extra centers, which concludes the
proof.
32


=== round-0006/verifier.feedback.md ===
High-level audit and triage
- Balanced split (τ=⌊L/2⌋). Both provers’ instantiations are correct and align with the safe template we had in notes. The key inequalities are all valid for L≥3: τ/(L−1)≤2/3, (L−1−τ)/(L−1)≤1/2, ln(2m/τ)≤ln(4m/(L−1)), and H_{L−2}−H_{τ−1}≤ln 3+1 (small-L cases verifiable directly). This is a clean, checkable branch that converts the tail to O(m/L) with explicit constants. This is worth curating into output.md as a separate corollary and can be considered as an optional fourth candidate in the per-plateau minimum.

- “Constant-tail” split (τ≈(L−1)/e). The idea is attractive, but the concrete constants in Prover 02’s writeup are not uniformly valid. In particular, with τ=⌊(L−1)/e⌋, the claim H_{L−2}−H_{τ−1}≤2 fails at L=6 (indeed H_4≈2.083>2). A rigorous statement needs either (i) a domain restriction (e.g., give bounds as functions of L with an explicit 1+ln((L−1)/τ) tail, or state a safe absolute bound such as ≤3 or ≤4 for L≥4), or (ii) keep the bound as 1+ln((L−1)/τ) without collapsing to a universal constant. Also, τ=⌊(L−1)/e⌋≥1 only for L≥4 (for L=3 it is 0). If we keep this branch, we should state L≥4 and adjust constants accordingly. Given the balanced-split already yields a tidy O(m/L) tail with a small constant, I recommend not adding the τ≈(L−1)/e branch to output.md until constants are fully repaired.

- Multi-plateau 1% probability (mixing). Prover 01’s extension from a single long plateau to a disjoint union covering γ of the window is correct. The remainder term aggregates as O(γ/min_j L_j) (since ∑_j(L_j/k0)·O(1/L_j)=O(|J|/k0) and |J|≤γ k0/min L_j). This is a straightforward corollary of our Corollary 9 and should be added to output.md with an explicit statement.

- Quantified O(log log k0) mixture (polylog-long plateaus). The core idea is sound, and the constants track cleanly from Corollary 5 with α=1/(ln k0)^σ: on each such plateau, the per-plateau bound is 2C_bi(a+1+ln 2+σ ln ln k0)+o(1). One technical correction: the endpoint term justification in the prover text is off; the correct aggregation yields (C_fix/k0)∑_{j∈S} ln m_j≤C_fix·|S|·ln(2k0)/k0=O(γ (ln k0)^σ ln k0/k0)=o(1) since |S|≤γ k0/min L_j and min L_j≥k0/(ln k0)^σ. This can be stated cleanly in output.md as a global corollary.

- Discrete smoothness ⇒ long plateaus. The lemma is right in spirit but needs precise rounding: from v_{k−1}≤(1+δ)v_k one gets that a factor-2 drop over t steps requires t≥ln 2/ln(1+δ). Since a window [m,m+L) has L−1 steps, the correct condition is L≥1+⌈ln 2/ln(1+δ)⌉. For δ=A/m on [m,2m), this implies L≥1+⌈(ln 2)·m/A⌉; if A<ln 2, then the dyadic drop cannot occur within the window and the entire window is a (single) plateau of length m. This is a useful structural lemma to include in output.md (as Lemma 2) with the corrected constants, together with the 1/m-type smoothness corollary.

- Heavy-coverage program. The conditional “expected collision” lemma proposed by Prover 01 is not yet rigorous. The step that substitutes H_t(H) by α·S_H in the numerator inside a conditional probability is a ratio-of-expectations pitfall; one must keep p_t^{ch}=H_t(H_cov)/cost_t(all) conditioned on the filtration and either (a) obtain a deterministic lower bound on cost_t(all) in terms of U_t(H) (or a conditional one with high probability), and (b) control H_t(H_cov) pathwise or via concentration (e.g., using a supermartingale and Freedman). Summing a per-step bound “over at most k1 heavy hits” also needs justification: the sum of collision probabilities is over all steps before τ, not only those when a heavy point is sampled. The right approach is to sum E[p_t^{ch}|F_t] across t<τ, then bound the sum by integrating conditional expectations with proper denominators. I recommend: first prove a persistence inequality cost_t(all)≥(β+1)U_t(H) deterministically under a geometric separation hypothesis, derive a clean bound for ∑_{t<τ} E[p_t^{ch}|F_t], and only then bring in (super)martingale tools for concentration.

Actionable next steps
1) Add the balanced-split corollary to output.md (as an optional branch alongside existing ones).
2) Add the multi-plateau 1% probability mixing corollary (with explicit γ, α, η=0.98, c=2) to output.md.
3) Add the quantified O(log log k0) mixture corollary for plateaus with L≥m/(ln k0)^σ to output.md, with a fully explicit o(1) remainder as above.
4) Include the discrete-smoothness ⇒ long plateaus lemma (with the corrected off-by-one and a two-case corollary for A≥ln 2 and A<ln 2).
5) For the τ≈(L−1)/e branch, either fix constants rigorously (domain L≥4 and a safe tail constant), or defer inclusion.
6) Heavy coverage: target a rigorous denominator lower bound (persistence) under an explicit separation; only then attempt expected-collisions and Freedman for high probability.


=== notes.md ===
Problem and notation
- X ⊂ R^d finite dataset; cost(C) = ∑_{x∈X} min_{c∈C} ||x−c||^2.
- OPT_k := min_{|C|=k} cost(C), nonincreasing in k.
- ALG_k := cost of k-means++ after k centers; E_++[·] denotes expectation over its randomness.
- We study E_{k∼U({k0,…,2k0−1})} E_++[ ALG_k / OPT_k ].

Key external input (Makarychev–Reddy–Shan 2020)
- There is an absolute constant C_bi (quoted as 5 in MRS) such that for any r≥1, Δ≥1,
  E_++[cost after r+Δ centers] ≤ C_bi · min{ 2 + 1/(2e) + ln(2r/Δ), 1 + r/(e(Δ−1)) } · OPT_r.
- For Δ=0 (fixed-k), there is an absolute constant C_fix (also ≲5 in MRS) with E_++[ALG_k] ≤ C_fix · (ln k + O(1)) · OPT_k.

Per-k oversampling bound (robust to s-misalignment)
- For any k, define s(k) := min{ s∈{1,…,k} : OPT_s ≤ 2·OPT_k }. Let Δ(k)=k−s(k).
- If Δ(k)≥1, then
  E_++[ALG_k] ≤ 2·C_bi · min{ 2 + 1/(2e) + ln(2 s(k)/Δ(k)), 1 + s(k)/(e(Δ(k)−1)) } · OPT_k.
- If Δ(k)=0, fall back to E_++[ALG_k] ≤ C_fix · (ln k + O(1)) · OPT_k.
Note: On any factor-2 plateau I=[m,m+L) (i.e., OPT_k∈(OPT_m/2,OPT_m] for k∈I), we have s(k)≤m and Δ(k)≥k−m; since the MRS bound is monotone in (s,Δ), replacing (s(k),Δ(k)) by (m,k−m) yields a valid upper bound.

Per-plateau averaging and Stirling control
- For a factor-2 plateau I=[m,m+L), L≥2, uniform k over I gives
  E_{k,++}[ALG_k/OPT_k] ≤ 2·C_bi · (2 + 1/(2e) + (1/(L−1))∑_{Δ=1}^{L−1} ln(2m/Δ)) + (C_fix(ln m + O(1)))/L.
- Using ln(n!) ≥ n ln n − n + 1 (n≥1),
  (1/(L−1))∑_{Δ=1}^{L−1} ln(2m/Δ) = ln(2m) − ln((L−1)!)/(L−1) ≤ 1 + ln(2m/(L−1)).
- Hence E_{k,++}[ALG_k/OPT_k] ≤ const + const·ln^+(2m/(L−1)) + O((ln m)/L), where ln^+(x)=max{ln x,0} and all const’s are absolute.

Decomposition over [k0,2k0)
- Partition [k0,2k0) into maximal factor-2 plateaus I_j=[m_j, m_j+L_j). For L_j≥2, the per-plateau average is bounded as above. For L_j=1, the single k in I_j contributes ≤ C_fix(ln m_j + O(1)). Averaging with weights L_j/k0 yields a global bound explicitly in terms of {(m_j,L_j)}; in the worst case many L_j=1 plateaus can force Θ(log k0).

Consequences and regimes
- Long plateaus: If some I=[m,m+L) satisfies L ≥ m/polylog(k0), then E_{k∈I,++}[ALG/OPT] = O(log log k0). If a constant fraction of [k0,2k0) is covered by such plateaus, the global average is O(log log k0).
- Worst case without structure: If many plateaus are short (e.g., L_j=1), the global average can be Θ(log k0), aligning with classic worst-case fixed-k behavior.

Open directions
- Structural conditions on OPT(·) implying many long plateaus (e.g., bounded discrete derivative decay, quasiconvexity/regularity of k·OPT_k) to get unconditional O(log log k0).
- Heavy-coverage lemma (conjectural): under strong scale separation OPT_{k1} ≫ OPT_k, show k-means++ hits all k1 heavy clusters w.h.p. before collisions, then apply bi-criteria on the remaining part.
- Lower bounds: Construct instances with Δ(k)=1 for a 1−o(1) fraction of k in [k0,2k0), giving an Ω(log k0) average and clarifying optimality of random-k smoothing.
Refinements and corrections for plateau averaging and monotonicity

- Monotonicity of the MRS bi-criteria bound. Define for integers s≥1 and Δ≥1 (interpreting the second branch only for Δ≥2):
  f(s,Δ) := min{ a + ln(2s/Δ), 1 + s/(e(Δ−1)) }, where a := 2 + 1/(2e).
  Then f is nondecreasing in s and nonincreasing in Δ. Consequently, on any factor-2 plateau I=[m,m+L) and for k=m+Δ with Δ≥1, the per-k bound from Theorem 1 satisfies
  f(s(k),Δ(k)) ≤ f(m,Δ), so we may safely replace (s(k),Δ(k)) by (m,Δ) in upper bounds.

- Correct τ-split averaging (fixing Proposition B). On a factor-2 plateau I=[m,m+L) with L≥2, for any integer τ∈{1,…,L−1}:
  (1/(L−1)) ∑_{Δ=1}^{L−1} min{ a+ln(2m/Δ), 1 + m/(e(Δ−1)) } (with the second term read only for Δ≥2)
  ≤ (τ/(L−1)) [ a + 1 + ln(2m/τ) ]
    + ((L−1−τ)/(L−1))
    + (m/(e(L−1))) · (H_{L−2} − H_{τ−1}).
  Proof sketch: Split the sum at τ and use Stirling ln(τ!) ≥ τ ln τ − τ + 1 to bound ∑_{Δ≤τ} ln(2m/Δ) ≤ τ(1+ln(2m/τ)). For the tail, sum 1/(Δ−1) as H_{L−2}−H_{τ−1}. This corrects two issues in earlier drafts: (i) the constant a must be multiplied by τ/(L−1), and (ii) ln(2m/Δ) cannot be upper bounded by ln(2m/τ) when Δ≤τ.

- Hybrid extremes recovered from the τ-split:
  • Choosing τ=L−1 gives the pure-logarithmic average a+1+ln(2m/(L−1)).
  • Choosing τ=1 gives the “hybrid extreme” 1 + (a+ln(2m))/(L−1) + (m/(e(L−1))) H_{L−2}.
  Hence we may take the minimum of these two regimes for a tighter bound, plateau by plateau.

- Long-plateau corollary. If L≥α m with α∈(0,1], then a+1+ln(2m/(L−1)) ≤ a+1+ln(2/α)+O(1/L), so the plateau-average is O(1) up to the endpoint term of order (ln m)/L.

- Quantile-in-Δ bound (careful threshold). For any η∈(0,1), let T := max{2, ⌈η(L−1)⌉+1}. Then, for all Δ≥T, by the large-Δ branch
  E_++[ALG_{m+Δ}/OPT_{m+Δ}] ≤ 2·C_bi·(1 + m/(e(T−1))).
  The fraction of k∈I with Δ≥T is (L−T+1)/L = 1 − η − O(1/L). This yields a rigorous “with probability ≥ 1−η−O(1/L) over k uniform on the plateau” expected-ratio bound; Markov then converts it into a constant-factor joint (k,++) probability if desired.

- Global decomposition improvement. Using the τ-split per plateau and optimizing τ as a function of (m,L) leads to a refined global bound that, informally, scales like
  K0 + K1 · min{ ln^+(2m/(L−1)), (m ln L)/L } + K2 · (ln m)/L,
  up to absolute constants K0,K1,K2. Formalizing this from Theorem 3 is a next step.

Open directions and next steps
- Optimize τ in the τ-split bound and write a clean, closed-form per-plateau bound. Then aggregate over plateaus to strengthen Corollary 3.
- Probability guarantees: phrase clean conditions under which at least a 1% measure of k (in [k0,2k0) or on a specific plateau) yields a constant expected ratio, and apply Markov to obtain joint (k,++) probability.
- Heavy-coverage: formalize the supermartingale for heavy uncovered mass (precise citation from MRS), bound expected heavy-collisions by O(k1/R), and lift to high probability via Freedman/Azuma with a stopping time.
- Lower bounds: construct OPT(·) with OPT_{k−1}/OPT_k ≈ 1 − Θ(1/ln k0) for most k to force Δ(k)=1 on a 1−o(1) fraction of the window, yielding an Ω(log k0) average.
Additions and clarifications

- Geometric constraint on plateau length within the window. If a factor-2 plateau I=[m,m+L) is contained in [k0,2k0), then L ≤ 2k0−m ≤ m. In particular, for L≥2 we have m/(L−1) ≥ 1. This prevents sign errors when simplifying expressions involving m/(L−1).

- Min-of-extremes and an upper envelope. From Theorem 3 and Corollary 4 we can (and will) use the minimum of the two extreme instantiations τ=L−1 (pure log) and τ=1 (hybrid extreme). Using H_q ≤ ln q + 1, this yields a clean per-plateau upper envelope of the form
  E_{k∈I} E_++[ALG_k/OPT_k] ≤ K0 + K1 · min{ ln^+(2m/(L−1)), (m ln L)/L } + K2 · (ln m)/L,
for absolute constants K0,K1,K2 depending only on C_bi,C_fix. This is the regime split we will use in global aggregation: the log term dominates when L≪m, while the (m ln L)/L term is O(1) when L is a constant fraction of m.

- Safe balanced-split (τ=⌊L/2⌋) template (optional). The following uniform bounds hold for L≥3 and τ=⌊L/2⌋:
  • τ/(L−1) ≤ 2/3 and (L−1−τ)/(L−1) ≤ 1/2.
  • ln(2m/τ) ≤ ln(4m/(L−1)).
  • H_{L−2} − H_{τ−1} ≤ ln 3 + 1 (with small-L cases checked directly).
Plugging these into Theorem 3 yields the rigorous estimate
  E_{k∈I} E_++[ALG_k/OPT_k]
  ≤ 2C_bi [ (2/3)(a + 1 + ln(4m/(L−1))) + 1/2 + (m/(e(L−1)))(ln 3 + 1) ]
    + (C_fix(\ln m + O(1)))/L.
This improves the (m ln L)/L term to a pure (m/L) term at the expense of a larger coefficient on the logarithmic piece. It may be tighter than the min-of-extremes envelope in intermediate regimes. If used, constants must be kept as above (1/2 is not a safe coefficient for τ/(L−1) when L is even).

- Probability ≥ 1% guarantees under a long-plateau condition. From Corollary 6: fix η∈(0,1) and define T := max{2, ⌈η(L−1)⌉+1}. For at least a (1−η−O(1/L)) fraction of k∈I we have
  E_++[ALG_k/OPT_k] ≤ 2C_bi (1 + m/(e(T−1))).
By Markov, for any c≥1,
  P_{++}[ ALG_k/OPT_k ≤ c · 2C_bi (1 + m/(e(T−1))) ] ≥ 1 − 1/c
for each such k. Therefore, jointly over k uniform on I and ++ randomness,
  P[ ALG_k/OPT_k ≤ c · 2C_bi (1 + m/(e(T−1))) ] ≥ (1−η−O(1/L))(1−1/c).
If L ≥ α m, then m/(T−1) ≤ 1/(α η) up to 1+O(1/L) factors, giving an explicit constant bound. If the plateau covers a γ-fraction of [k0,2k0), the global probability is ≥ γ times the RHS. Example: with C_bi=5, α=1, η=0.99, c=100, we get a ≥1% guarantee with a factor ≤ 100·2·5·(1 + 1/(0.99 e)).

- Heavy-coverage program (caveats and plan). Any per-step “collision within heavy” probability bound must be stated conditionally on the current filtration F_t: under D^2 sampling,
  P[collision in H at step t | F_t] = cost_t(covered H) / cost_t(all).
Avoid ratio-of-expectations. A plausible route is to bound cost_t(covered H) in terms of the uncovered heavy mass U_t(H) via the MRS eH_t supermartingale (in expectation and eventually with concentration), while lower bounding cost_t(all) by U_t(H) + U_t(L). To obtain a β-persistence-type condition from scale separation, quantify the expected reduction of U_t(H) when sampling outside H and show it is o(U_t(H)) under strong separation (e.g., OPT_{k1} ≥ k^C · OPT_k). With these in place, the expected number of heavy collisions can be bounded by summing the conditional probabilities until all heavy clusters are hit, and then lifted to high probability by Freedman/Azuma.

- Lower bounds. To show random-k smoothing cannot beat Θ(log k0) in general, aim for an instance where Δ(k)=1 for a 1−o(1) fraction of k in [k0,2k0). Concretely, calibrate radii so OPT_{k−1}/OPT_k ≈ 1 + Θ(1/ln k0) across most of the window, ensuring s(k)=k−1 on most k. Then adapt Arthur–Vassilvitskii’s lower-bound calculation to the averaged setting.
New per-plateau refinement: τ≈m/ln L branch and an updated envelope

- Additional τ-instantiation. On a factor-2 plateau I=[m,m+L) with L≥3, choose τ*:=min{L−1, ⌊m/ln L⌋}. Then Theorem 3 yields
  E_{k∈I} E_++[ALG/OPT]
  ≤ 2C_bi [ (τ*/(L−1))(a+1+ln(2m/τ*)) + ((L−1−τ*)/(L−1)) + (m/(e(L−1)))(H_{L−2} − H_{τ*−1}) ]
    + (C_fix(\ln m + O(1)))/L,
  where a:=2+1/(2e).
  In the subcase m ≤ (L−1) ln L (hence τ*=⌊m/ln L⌋) and assuming m≥L (which holds for plateaus contained in [k0,2k0)), we can bound
  • ln(2m/τ*) ≤ ln(4 ln L)
  • τ*/(L−1) ≤ m/((L−1) ln L)
  • H_{L−2} − H_{τ*−1} ≤ 1 + ln ln L
  to conclude
  E_{k∈I} E_++[ALG/OPT]
  ≤ 2C_bi [ 1 + (m/((L−1) ln L))(a+1+\ln(4\ln L)) + (m/(e(L−1)))(1 + \ln\ln L) ]
    + (C_fix(\ln m + O(1)))/L.
  This improves the (m\ln L)/L tail from the τ=1 branch to ≈ (m/L)(1+\ln\ln L).

- Updated min-of-three envelope (plateau-wise). Together with the τ=L−1 (pure-log) and τ=1 (hybrid) instantiations, we may use
  E_{k∈I} E_++[ALG/OPT] ≤ K0 + K1·min{ ln^+(2m/(L−1)), (m\ln L)/L, (m(1+\ln\ln L))/L } + K2·(\ln m)/L,
  for absolute constants K0,K1,K2 depending only on C_bi,C_fix. This strictly strengthens the previous min-of-two envelope in regimes where ln ln L ≪ ln L.

- Global aggregation (window-wise). For the maximal factor-2 plateau partition [k0,2k0)=\bigsqcup_j I_j with I_j=[m_j,m_j+L_j), one may take, plateau by plateau, the minimum of the three branches above (with the third used for L_j≥3), and sum with weights L_j/k0:
  E_{k\sim U([k0,2k0))} E_++[ALG/OPT]
  ≤ (1/k0) \sum_j L_j [ K0 + K1·min{ ln^+(2m_j/(L_j−1)), (m_j\ln L_j)/L_j, (m_j(1+\ln\ln L_j))/L_j } ]
    + (K2/k0) \sum_j \ln m_j.

- Balanced split reminder. The τ=⌊L/2⌋ instantiation yields
  E_{k∈I} E_++[ALG/OPT]
  ≤ 2C_bi [ (2/3)(a+1+\ln(4m/(L−1))) + 1/2 + (m/(e(L−1)))(\ln 3 + 1) ] + (C_fix(\ln m + O(1)))/L,
  valid for all L≥3. This removes the ln L factor in the tail at the expense of a larger constant on the log piece.

- Probabilistic 1% instantiation (correction). From Corollary 9, for any η∈(0,1) and c≥1,
  P_{k∈I,++}[ ALG/OPT ≤ c·B_I ] ≥ (1−η−O(1/L))(1−1/c), with B_I=2C_bi(1 + m/(e(T−1))), T=\max\{2,\lceil η(L−1)\rceil+1\}.
  To guarantee at least 1% mass uniformly (up to O(1/L)), take η=0.98 and c=2, giving (1−η)(1−1/c)=0.02·0.5=0.01. If, in addition, L≥α m, then B_I ≤ 2C_bi(1 + 1/(α e η)) up to O(1/L).

Caveat on heavy-coverage sketches. The pathwise control and metric-separation heuristics are promising but currently lack fully rigorous denominators: one must explicitly quantify cost_t(X) in terms of U_t(H) under precise conditions, and avoid uncontrolled uses of triangle inequality against existing centers outside H. These ideas remain in-development items in notes, not yet suitable for output.
Global aggregation and probability instantiations (added)

- Global min-of-three aggregator. From Corollary 11 (per-plateau min-of-three) and linearity, we can average over the maximal factor-2 plateau partition [k0,2k0)=⊔_j I_j=[m_j,m_j+L_j). For L_j≥2 each plateau contributes its per-plateau bound with weight L_j/k0, and L_j=1 plateaus contribute via the fixed-k bound. This yields a clean, explicit global formula with constants K0,K1,K2 depending only on C_bi,C_fix. When C_bi=C_fix=5, one can set K0=35, K1=10, K2=5. The third branch (m(1+ln ln L))/L is to be read only for L≥3.

- Endpoint-sum control. The unweighted endpoint contribution aggregates as (K2/k0)∑_{j:L_j≥2} ln m_j. This is always ≤ K2·ln(2k0) since there are ≤k0 plateaus and m_j≤2k0−1 for all j. Thus endpoint terms never exceed O(ln k0).

- Unconditional aggregation bound. Using only the pure-log branch and the easy inequality ln m_j ≤ ln k for k∈I_j, we recover E_{k∼U([k0,2k0)),++}[ALG/OPT] ≤ K′ + K″·ln(2k0) for absolute K′,K″. This validates that, without structural information (e.g., long plateaus), the Θ(log k0) behavior is unavoidable.

- Long-plateau mixture. If a γ-fraction of the window is covered by plateaus with L≥α m, then that portion contributes γ·O_α(1) (by Corollary 5), while the rest is ≤ (1−γ)·O(ln k0). This gives a transparent global interpolation.

- ≥1% global probability under a single long plateau. If I=[m,m+L) with L≥α m covers a γ-fraction of the window, then by Corollary 9 with η=0.98 and c=2 we get P_{k,++}[ALG/OPT ≤ 2·B_I] ≥ γ·(1−η−O(1/L))·(1−1/2) = 0.01·γ − O(γ/L), where B_I = 2·C_bi·(1 + m/(e(T−1))) and T = max{2,⌈0.98(L−1)⌉+1}. If L≥α m then B_I ≤ 2·C_bi(1 + 1/(α e·0.98)) + O(1/L).

Caveat and next steps for heavy-coverage

- The pathwise heavy-collision lemma remains a program: define U_t(H) (uncovered heavy mass), H_t(H) (covered heavy mass), and the stopping time τ when all heavy clusters are covered. A key missing ingredient is a quantitative persistence lower bound inf_{t<τ} U_t(H) ≥ c·S_H (S_H is the sum of single-cluster OPT costs over H) under an explicit separation hypothesis (e.g., OPT_{k1} ≫ OPT_k or geometric separation). Once such a bound and a dominance parameter β>0 (U_t(H) ≥ β·(H_t(all)+U_t(L))) are established, summing conditional collision probabilities gives an expected-collisions bound, which can then be upgraded to high probability via Freedman’s inequality. We should avoid any pathwise inequality that is only justified in expectation (e.g., eH_t supermartingale bounds) without a concentration argument.
Round 0006 updates: audits, fixes, and new curated directions

1) Balanced split (τ=⌊L/2⌋) is safe and useful
- For L≥3 and τ=⌊L/2⌋ we have the uniform bounds: τ/(L−1)≤2/3, (L−1−τ)/(L−1)≤1/2, ln(2m/τ)≤ln(4m/(L−1)), and H_{L−2}−H_{τ−1}≤ln 3+1 (check L=3,4 directly; for larger L use H_n−H_m≤ln((n+1)/(m+1))≤ln 3). Plugging these into Theorem 3 produces a clean O(m/L) tail with explicit constants. We will curate this to output.md.

2) “Constant-tail split” (τ≈(L−1)/e): caution on constants
- With τ=⌊(L−1)/e⌋, τ≥1 only when L≥4. The bound H_{L−2}−H_{τ−1}≤2 is false at L=6 (H_4≈2.083). A rigorous statement should either (i) retain the form H_{L−2}−H_{τ−1}≤1+ln((L−1)/τ), or (ii) give a safe constant (e.g., ≤3 or ≤4) uniformly for L≥4. Until this is adjusted, we will not add this branch to output.md.

3) Multi-plateau ≥1% probability (mixing)
- If a disjoint family of long plateaus {I_j=[m_j,m_j+L_j)} with L_j≥α m_j covers a γ-fraction of [k0,2k0), then by Corollary 9 with η=0.98 and c=2,
  P[ALG/OPT ≤ 2·B_j on the event k∈I_j for some j] ≥ γ·(1−η)·(1−1/2) − O(γ/\min_j L_j) = 0.01·γ − O(γ/\min_j L_j),
  where B_j=2C_bi(1+m_j/(e(T_j−1))), T_j=max{2,⌈η(L_j−1)⌉+1}, and hence B_j≤2C_bi(1+1/(α e η))+O(1/L_j). We will add this global mixing corollary to output.md.

4) Quantified O(log log k0) mixture under polylog-long plateaus
- Let S be the set of plateaus with L_j≥m_j/(ln k0)^σ for some σ≥1 and suppose ∑_{j∈S} L_j≥γ k0. Then
  E[ALG/OPT] ≤ γ·[2C_bi(a+1+ln(const)+σ ln ln k0)] + (1−γ)·[K′+K″ ln(2k0)] + o(1),
  where const is an absolute constant (2 or 4; absorbed into O(1)). The o(1) comes from endpoint terms: their contribution over S is (C_fix/k0)∑_{j∈S} ln m_j ≤ C_fix·|S|·ln(2k0)/k0 = O(γ (ln k0)^σ ln k0/k0)→0, since |S|≤(γ k0)/min_j L_j and min_j L_j≥k0/(ln k0)^σ. We will curate a precise corollary to output.md.

5) Discrete smoothness ⇒ long plateaus (corrected rounding)
- Lemma (sequence form). If (v_k) is nonincreasing and for all k in {m+1,…,m+L−1} we have v_{k−1}≤(1+δ) v_k, then v_{m+L−1}≥v_m/(1+δ)^{L−1}. Hence a factor-2 drop within [m,m+L) requires L≥1+⌈ln 2/ln(1+δ)⌉. Using ln(1+δ)≤δ, it suffices that L≥1+⌈(ln 2)/δ⌉.
- Corollary (1/m-type smoothness on [m,2m)). If v_{k−1}≤(1+A/m) v_k for all k∈[m+1,2m−1], then any factor-2 drop within [m,2m) requires L≥1+⌈(ln 2) m/A⌉. Thus: if A≥ln 2, then L≥((ln 2)/A)·m up to lower-order terms; if A<ln 2, then a factor-2 drop cannot occur within the window, so the whole [m,2m) is one plateau (L=m). This cleanly connects “discrete smoothness” to long plateaus.

6) Heavy-coverage program: rigor requirements
- To bound the expected number of heavy-collisions, one must avoid ratio-of-expectations. Work with p_t^{ch}=cost_t(covered heavy)/cost_t(all) conditioned on the filtration. A usable inequality is p_t^{ch}≤H_t(H_cov)/((β+1)U_t(H)) once we have a deterministic persistence bound cost_t(all)≥(β+1)U_t(H). Then ∑_{t<τ}E[p_t^{ch}|F_t]≤(1/(β+1))·∑_{t<τ}E[H_t(H_cov)/U_t(H)], which still needs (a) a deterministic or high-prob lower bound on U_t(H) until τ, and (b) control of H_t(H_cov) (e.g., via a supermartingale with concentration). These should be established under explicit separation assumptions before drawing collision bounds.

Next steps
- Curate to output.md: (i) balanced split corollary; (ii) multi-plateau 1% mixing; (iii) quantified O(log log k0) mixture; (iv) discrete-smoothness lemma and corollary.
- Either repair or defer the τ≈(L−1)/e branch.
- For heavy coverage: prove a persistence inequality cost_t(all)≥(β+1)U_t(H) under a clean separation hypothesis; then bound expected collisions; then lift to high probability via Freedman.


=== output.md ===
Theorems curated in this file depend on the following standard notation and external input.
- For a finite dataset X⊂R^d, write OPT_k for the optimal k-means cost with k centers, and ALG_k for the cost produced by k-means++ after k centers; E_++[·] is expectation over the algorithm’s randomness.
- External input (Makarychev–Reddy–Shan, 2020): There exists an absolute constant C_bi such that for all integers r≥1 and Δ≥1,
  E_++[cost after r+Δ centers] ≤ C_bi · min{ 2 + 1/(2e) + ln(2r/Δ), 1 + r/(e(Δ−1)) } · OPT_r.
  There is also an absolute constant C_fix with E_++[ALG_k] ≤ C_fix·(ln k + O(1))·OPT_k for all k (the fixed-k bound). We treat C_bi and C_fix as absolute numerical constants.

Theorem 1 (Per-k oversampling bound via a 2-approximate anchor).
Fix k≥1 and define
  s(k) := min{ s∈{1,…,k} : OPT_s ≤ 2·OPT_k },    Δ(k) := k − s(k).
Then:
- If Δ(k) ≥ 1, we have
  E_++[ALG_k] ≤ 2·C_bi · min{ 2 + 1/(2e) + ln(2 s(k)/Δ(k)), 1 + s(k)/(e(Δ(k)−1)) } · OPT_k.
- If Δ(k) = 0, we have E_++[ALG_k] ≤ C_fix·(ln k + O(1))·OPT_k.
Proof.
For Δ(k)≥1, apply the MRS bi-criteria inequality with r = s(k) and Δ = k−s(k):
  E_++[ALG_k] = E_++[cost after s(k)+Δ(k) centers]
  ≤ C_bi · min{ 2 + 1/(2e) + ln(2 s(k)/Δ(k)), 1 + s(k)/(e(Δ(k)−1)) } · OPT_{s(k)}.
By definition of s(k), OPT_{s(k)} ≤ 2·OPT_k, giving the stated bound. For Δ(k)=0 the fixed-k bound yields the second claim. ∎

Theorem 2 (Averaging over a factor-2 plateau).
Let I = {m, m+1, …, m+L−1} with L≥2 and assume OPT_k ∈ (OPT_m/2, OPT_m] for all k∈I. Then for k uniform in I and over the k-means++ randomness,
  E_{k∈I} E_++[ ALG_k / OPT_k ] ≤ A0 + A1 · ln^+( 2m/(L−1) ) + (A2 · ln m)/L,
where ln^+(x) := max{ln x,0} and A0,A1,A2 are absolute constants depending only on C_bi,C_fix.
Proof.
Average the bound from Theorem 1 over k∈I. For Δ:=k−m≥1, we may upper bound s(k) by m and lower bound Δ(k) by Δ (since s(k)≤m and Δ(k)≥Δ on a factor-2 plateau), and use only the logarithmic branch of MRS to get
  E_++[ALG_{m+Δ}/OPT_{m+Δ}] ≤ 2·C_bi · ( 2 + 1/(2e) + ln(2m/Δ) ).
Averaging Δ from 1 to L−1 gives
  (1/(L−1))∑_{Δ=1}^{L−1} ln(2m/Δ) = ln(2m) − ln((L−1)!)/(L−1) ≤ 1 + ln(2m/(L−1)),
where we used ln(n!) ≥ n ln n − n + 1 for n=L−1≥1. Multiplying by (L−1)/L and adding the Δ=0 endpoint contribution C_fix(ln m + O(1))/L yields the claim with suitable absolute constants A0,A1,A2. Replacing ln by ln^+ only weakens the bound. ∎

Corollary 3 (Decomposition across [k0,2k0)).
Partition the integer interval [k0,2k0) into maximal factor-2 plateaus I_j = [m_j, m_j+L_j), j=1,…,J, so OPT_k ∈ (OPT_{m_j}/2, OPT_{m_j}] for k∈I_j, and ∑_j L_j = k0. Then, for k uniform on [k0,2k0),
  E_k E_++[ ALG_k / OPT_k ]
  ≤ (1/k0) ∑_{j : L_j≥2} L_j [ A0 + A1 · ln^+( 2 m_j/(L_j−1) ) ]
    + (1/k0) ∑_{j : L_j≥2} A2 · ln m_j
    + (1/k0) ∑_{j : L_j=1} C_fix · (ln m_j + O(1)).
In particular, if a constant fraction of [k0,2k0) is covered by plateaus satisfying L_j ≥ m_j / polylog(k0), then the contribution of those plateaus is O(log log k0), and the remaining fraction must be controlled separately; in the worst case with many L_j=1 plateaus the global average can be Θ(log k0).
Proof.
Average the bound of Theorem 2 over plateaus with weights L_j/k0, and handle L_j=1 plateaus by the fixed-k bound. ∎
Lemma (Monotonicity of the MRS bi-criteria bound).
Let a := 2 + 1/(2e), and for integers s≥1, Δ≥1 define
  F(s,Δ) := min{ a + ln(2s/Δ), 1 + s/(e(Δ−1)) },
where the second term is interpreted only for Δ≥2 (or taken as +∞ for Δ=1). Then:
- For fixed Δ, F is nondecreasing in s.
- For fixed s, F is nonincreasing in Δ (for all Δ≥1 in the above interpretation).
Proof.
For Δ≥2, both functions s ↦ a + ln(2s/Δ) and s ↦ 1 + s/(e(Δ−1)) are nondecreasing in s, hence their pointwise minimum is nondecreasing. For Δ≥2, both functions Δ ↦ a + ln(2s/Δ) and Δ ↦ 1 + s/(e(Δ−1)) are nonincreasing in Δ, hence the minimum is nonincreasing. For the transition from Δ=1 to Δ=2, note that F(s,1) = a + ln(2s) ≥ a + ln(s) ≥ a + ln(2s/2) ≥ F(s,2), since the logarithmic branch decreases with Δ and adding the second branch at Δ=2 can only decrease the minimum. Thus F is nonincreasing in Δ for all Δ≥1. ∎

Theorem 3 (Plateau averaging with a tunable split of the two MRS branches).
Let I = {m, m+1, …, m+L−1} be a factor-2 plateau (i.e., OPT_k ∈ (OPT_m/2, OPT_m] for k∈I) with L≥2. For any integer τ with 1≤τ≤L−1, for k uniform on I and over k-means++ randomness we have
  E_{k∈I} E_++[ ALG_k / OPT_k ]
  ≤ 2·C_bi · [ (τ/(L−1)) · ( a + 1 + ln(2m/τ) )
                + ((L−1−τ)/(L−1))
                + (m/(e(L−1))) · ( H_{L−2} − H_{τ−1} ) ]
    + (C_fix · (ln m + O(1)))/L,
where H_r denotes the r-th harmonic number (H_0:=0).
Proof.
Fix k=m+Δ with Δ∈{1,…,L−1}. By Theorem 1, and Lemma (monotonicity),
  E_++[ALG_{m+Δ}/OPT_{m+Δ}] ≤ 2·C_bi · min{ a + ln(2m/Δ), 1 + m/(e(Δ−1)) },
with the second branch applicable for Δ≥2. Averaging Δ uniformly over {1,…,L−1}, split the sum at τ:
  (1/(L−1)) ∑_{Δ=1}^{L−1} min{…}
  ≤ (1/(L−1)) ∑_{Δ=1}^{τ} [ a + ln(2m/Δ) ]
    + (1/(L−1)) ∑_{Δ=τ+1}^{L−1} [ 1 + m/(e(Δ−1)) ].
For the first sum, use ∑_{Δ=1}^{τ} ln(2m/Δ) = τ ln(2m) − ln(τ!) ≤ τ[1 + ln(2m/τ)] by ln(τ!) ≥ τ ln τ − τ. For the second, ∑_{Δ=τ+1}^{L−1} 1 = L−1−τ and ∑_{Δ=τ+1}^{L−1} 1/(Δ−1) = H_{L−2} − H_{τ−1}. This yields the displayed bound. Finally, incorporate the Δ=0 endpoint k=m which has probability 1/L and contributes at most C_fix·(ln m + O(1))/L by the fixed-k bound. ∎

Corollary 4 (Hybrid extremes recovered; strengthening Theorem 2).
In the setting of Theorem 3:
- Choosing τ = L−1 gives
  E_{k∈I} E_++[ ALG_k / OPT_k ] ≤ 2·C_bi · ( a + 1 + ln(2m/(L−1)) ) + (C_fix · (ln m + O(1)))/L.
  This recovers Theorem 2 (up to constant reparameterization by a and the explicit +1 from Stirling).
- Choosing τ = 1 gives
  E_{k∈I} E_++[ ALG_k / OPT_k ] ≤ 2·C_bi · ( 1 + (a+ln(2m))/(L−1) + (m/(e(L−1))) · H_{L−2} )
    + (C_fix · (ln m + O(1)))/L.
Using H_q ≤ ln q + 1, this yields a complementary regime where the bound scales like 1 + O((ln m)/L) + O((m ln L)/L).
Proof.
Both bounds are immediate from Theorem 3 with the specified τ and the inequality H_q ≤ ln q + 1 for q≥1. ∎

Corollary 5 (Explicit constants on long plateaus).
If L ≥ α m for some fixed α∈(0,1], then for k uniform on I,
  E_{k∈I} E_++[ ALG_k / OPT_k ]
  ≤ 2·C_bi · ( a + 1 + ln(2/α) ) + (C_fix · (ln m + O(1)))/L.
Proof.
Apply Corollary 4 with τ=L−1 and use ln(2m/(L−1)) ≤ ln(2/α) + O(1/L). The endpoint term is unchanged. ∎

Corollary 6 (Quantile-in-Δ guarantee on a plateau).
Fix η∈(0,1) and let T := max{2, ⌈η(L−1)⌉+1}. Then for all Δ∈{T,…,L−1},
  E_++[ ALG_{m+Δ} / OPT_{m+Δ} ] ≤ 2·C_bi · ( 1 + m/( e (T−1) ) ).
Consequently, for k uniform on I, the fraction of k for which the above inequality holds is at least (L−T+1)/L = 1 − η − O(1/L).
Proof.
For Δ≥T, the large-Δ branch gives 1 + m/(e(Δ−1)) ≤ 1 + m/(e(T−1)). Combine with Theorem 1 and monotonicity, and note that the set {k: Δ(k)≥T} has size L−T+1 among the L indices in I. ∎
Remark (Plateau length within [k0,2k0)).
Let I = [m,m+L) be a factor-2 plateau contained in [k0,2k0). Then L ≤ 2k0 − m ≤ m. In particular, for L≥2 we have m/(L−1) ≥ 1.
Proof.
Since m+L−1 ≤ 2k0−1, we have L ≤ 2k0−m. As m ≥ k0, it follows that 2k0−m ≤ m. The final claim is immediate. ∎

Corollary 7 (Min-of-extremes plateau bound).
In the setting of Theorem 3 with L≥2, for k uniform on I and the k-means++ randomness,
  E_{k∈I} E_++[ ALG_k / OPT_k ]
  ≤ min{ B_log, B_hyb } + (C_fix · (ln m + O(1)))/L,
where
  B_log := 2·C_bi · ( a + 1 + ln^+( 2m/(L−1) ) ),
  B_hyb := 2·C_bi · ( 1 + (a+\ln(2m))/(L−1) + (m/(e(L−1))) · H_{L−2} ),
and a := 2 + 1/(2e), H_q denotes the q-th harmonic number with H_0:=0, and ln^+(x):=max{ln x,0}.
Proof.
Take τ=L−1 and τ=1 in Theorem 3 to obtain the two bounds (cf. Corollary 4). Replacing ln by ln^+ only increases the RHS. Taking the minimum preserves validity. ∎

Corollary 8 (Upper envelope from the min-of-extremes).
In the setting of Corollary 7, there exist absolute constants K0,K1,K2 (depending only on C_bi and C_fix) such that
  E_{k∈I} E_++[ ALG_k / OPT_k ]
  ≤ K0 + K1 · min{ ln^+( 2m/(L−1) ), (m ln L)/L } + K2 · (\ln m)/L.
Proof.
From Corollary 7 with the logarithmic extreme,
  E ≤ 2·C_bi·(a+1) + 2·C_bi·ln^+(2m/(L−1)) + (C_fix·(\ln m + O(1)))/L.
From the hybrid extreme and H_{L−2} ≤ ln(L−2)+1 ≤ ln L + 1,
  E ≤ 2·C_bi·\Big( 1 + (a+\ln(2m))/(L−1) + (m/(e(L−1))) (\ln L + 1) \Big)
    + (C_fix·(\ln m + O(1)))/L.
Absorb constants into K0 and the terms (ln m)/(L−1), (m/(L−1)) into K2·(\ln m)/L and K1·(m ln L)/L, respectively, using L−1≈L up to absolute factors for L≥2. Taking the minimum of the two displays yields the claim. ∎

Corollary 9 (Plateau-level probability guarantee via quantiles and Markov; global mixing).
Let I=[m,m+L) be a factor-2 plateau with L≥2 and fix η∈(0,1). Define T := max{2, ⌈η(L−1)⌉+1}. Then for at least a (1−η−O(1/L)) fraction of k∈I we have
  E_++[ ALG_k / OPT_k ] ≤ B_I := 2·C_bi · ( 1 + m/( e (T−1) ) ).
Consequently, for any c≥1,
  P_{k\sim U(I),\,++}\big[ ALG_k/OPT_k ≤ c · B_I \big] ≥ (1−η−O(1/L)) · (1 − 1/c).
If, moreover, L ≥ α m for some α∈(0,1], then B_I ≤ 2·C_bi · (1 + 1/(α e η)) + O(1/L). If a plateau I covers a γ-fraction of [k0,2k0) (i.e., L/k0 ≥ γ), then for k uniform on [k0,2k0) and the ++ randomness,
  P\big[ ALG_k/OPT_k ≤ c · B_I \big] ≥ γ · (1−η−O(1/L)) · (1 − 1/c).
Proof.
The first assertion is Corollary 6: a (1−η−O(1/L)) fraction of k satisfy the displayed per-k bound with Δ≥T, hence with E_++ bounded by B_I. For any such k, Markov’s inequality gives P_{++}[ALG_k/OPT_k ≤ c·B_I] ≥ 1−1/c. Averaging uniformly over k on I yields the claim. If L ≥ α m, then T−1 ≥ η(L−1) ≥ η(α m−1), which implies m/(T−1) ≤ 1/(α η) + O(1/L). The global bound follows by restricting to k in I, which occurs with probability ≥ γ. ∎
Corollary 10 (A τ≈m/ln L branch on a plateau).
Let I=[m,m+L) be a factor-2 plateau with L≥3. Define τ*:=min{L−1, ⌊m/ln L⌋}. Then for k uniform on I and over the k-means++ randomness,
  E_{k∈I} E_++[ ALG_k / OPT_k ]
  ≤ 2·C_bi · [ (τ*/(L−1)) · ( a + 1 + ln(2m/τ*) )
                + ((L−1−τ*)/(L−1))
                + (m/(e(L−1))) · ( H_{L−2} − H_{τ*−1} ) ]
    + (C_fix · (ln m + O(1)))/L,
where a:=2 + 1/(2e), and H_r is the r-th harmonic number (H_0:=0). In particular, if m ≤ (L−1) ln L and (as is the case for plateaus inside [k0,2k0)) m≥L, then
  E_{k∈I} E_++[ ALG_k / OPT_k ]
  ≤ 2·C_bi · [ 1 + (m/((L−1) ln L)) · ( a + 1 + ln(4 ln L) )
                + (m/(e(L−1))) · ( 1 + ln ln L ) ]
    + (C_fix · (ln m + O(1)))/L.
Proof.
Apply Theorem 3 with τ=τ*. The displayed inequality is just Theorem 3. In the subcase m ≤ (L−1) ln L, τ*=⌊m/ln L⌋ satisfies τ* ≥ m/ln L − 1, hence 2m/τ* ≤ 4 ln L and ln(2m/τ*) ≤ ln(4 ln L). Also τ*/(L−1) ≤ m/((L−1) ln L). Finally, H_{L−2} − H_{τ*−1} ≤ ln(L/τ*) + 1 ≤ ln((L ln L)/m) + 1 ≤ 1 + ln ln L, using m≥L. Substituting these bounds into Theorem 3 yields the “in particular” display. ∎

Corollary 11 (Updated per-plateau upper envelope: min-of-three).
In the setting of Corollary 7, there exist absolute constants K0,K1,K2 (depending only on C_bi and C_fix) such that for k uniform on I=[m,m+L) with L≥2,
  E_{k∈I} E_++[ ALG_k / OPT_k ]
  ≤ K0 + K1 · min{ ln^+( 2m/(L−1) ), (m ln L)/L, (m(1 + ln ln L))/L } + K2 · (\ln m)/L.
Here ln^+(x):=max{ln x,0}. The third term in the minimum is intended for L≥3; when L=2 the minimum of the first two terms suffices. 
Proof.
Combine: (i) the logarithmic extreme τ=L−1 (Corollary 4), (ii) the τ=1 hybrid extreme (Corollary 4 with H_{L−2}≤ln L+1), and (iii) Corollary 10 in the subcase τ*=⌊m/ln L⌋ (which holds exactly when m ≤ (L−1) ln L). In case (iii), the two m-dependent contributions are bounded by a constant multiple of (m/L)(1+ln ln L), since (m/((L−1) ln L))(a+1+ln(4 ln L)) ≤ (const)·(m/L) and (m/(e(L−1)))(1+ln ln L) ≤ (const)·(m/L)(1+ln ln L), with all constants depending only on C_bi. Absorb all additive O(1) and O((\ln m)/L) terms into K0 and K2. Taking the minimum of the three displays yields the claim. ∎

Corollary 12 (Explicit constants for the envelope when C_bi=C_fix=5).
Assume C_bi=C_fix=5 and L≥2. Then for k uniform on I=[m,m+L),
  E_{k∈I} E_++[ ALG_k / OPT_k ]
  ≤ 35 + 10 · min{ ln^+(2m/(L−1)), (m ln L)/L, (m(1+\ln\ln L))/L } + 5 · (\ln m)/L.
Proof.
From Corollary 7 (logarithmic extreme), E ≤ 2·C_bi·(a+1) + 2·C_bi·ln^+(2m/(L−1)) + (C_fix·(\ln m + O(1)))/L. With a=2+1/(2e) and C_bi=C_fix=5, 2·C_bi·(a+1) ≈ 31.84; round up to 35 to absorb the endpoint O(1/L) terms. The coefficient in front of ln^+(·) is 2·C_bi=10. For the τ=1 branch, the (m ln L)/L contribution carries a factor 2·C_bi/e ≤ 10/e < 10; rounding up to 10 is safe after replacing L−1 by L. For the τ≈m/ln L branch (Corollary 10), the m-dependent part is ≤ const·(m/L)(1+\ln\ln L) with const depending only on C_bi; choosing 10 dominates those constants as well. The endpoint term is ≤ 5·(\ln m + 2)/L and we keep 5·(\ln m)/L after absorbing the additive 10/L into 35. Taking the minimum of the three candidate bounds proves the display. ∎

Remark (Numerical 1% probability instantiation on a single plateau).
In Corollary 9, choose η=0.98 and c=2. Then for k uniform on I and ++ randomness,
  P[ ALG_k/OPT_k ≤ 2 · B_I ] ≥ (1−η−O(1/L)) · (1 − 1/2) = (0.02−O(1/L)) · 0.5 ≥ 0.01 − O(1/L),
with B_I = 2·C_bi·(1 + m/(e(T−1))) and T = max{2, ⌈0.98(L−1)⌉+1}. If additionally L ≥ α m, then B_I ≤ 2·C_bi·(1 + 1/(α e · 0.98)) + O(1/L), giving a fully explicit constant-factor guarantee at ≥1% mass (up to O(1/L)).
Remark (Clarification on Corollary 12).
In Corollary 12 the third term inside the minimum, (m(1+ln ln L))/L, is intended only for L≥3. For L=2, take the minimum of the first two terms. This mirrors the restriction stated in Corollary 11 and avoids using ln ln 2<0.

Corollary 13 (Global min-of-three aggregation across plateaus; explicit constants available).
Partition [k0,2k0) into maximal factor-2 plateaus I_j=[m_j,m_j+L_j), j=1,…,J, so that ∑_j L_j=k0. Then, for k uniform in [k0,2k0) and the k-means++ randomness,
  E_{k,++}[ ALG_k / OPT_k ]
  ≤ (1/k0) ∑_{j:L_j≥2} L_j · [ K0 + K1 · min{ ln^+(2m_j/(L_j−1)), (m_j ln L_j)/L_j, (m_j(1+ln ln L_j))/L_j } ]
    + (K2/k0) ∑_{j:L_j≥2} ln m_j
    + (1/k0) ∑_{j:L_j=1} C_fix · (ln m_j + O(1)),
where ln^+(x):=max{ln x,0}. The third term in the minimum is to be used only for L_j≥3. Here K0,K1,K2 are absolute constants depending only on C_bi,C_fix. In the numerical case C_bi=C_fix=5 one may take K0=35, K1=10, K2=5.
Proof.
For each j with L_j≥2, apply Corollary 11 to I_j and weight by L_j/k0. For L_j=1, use the fixed-k bound with weight 1/k0. Sum over j. The constants K0,K1,K2 come directly from Corollaries 11–12. ∎

Lemma 1 (Endpoint term cap).
With the notation of Corollary 13,
  (1/k0) ∑_{j:L_j≥2} ln m_j ≤ ln(2k0).
Proof.
There are at most J≤k0 plateaus and for all j we have m_j≤2k0−1. Thus ∑_{j:L_j≥2} ln m_j ≤ ∑_{j=1}^{J} ln(2k0) ≤ k0·ln(2k0). Divide by k0. ∎

Corollary 14 (Unconditional global O(log k0) bound).
There exist absolute constants K′,K″ (depending only on C_bi,C_fix) such that for k uniform on [k0,2k0) and ++ randomness,
  E_{k,++}[ ALG_k / OPT_k ] ≤ K′ + K″ · ln(2k0).
Proof.
Use Theorem 2 (or Corollary 7 with τ=L−1) on each plateau with L_j≥2 and average with weights L_j/k0, and handle L_j=1 via the fixed-k bound. For L_j≥2,
  (1/k0)∑_j L_j · ln^+(2m_j/(L_j−1)) ≤ (1/k0)∑_j L_j · ln(2m_j)
  = ln 2 + (1/k0)∑_j L_j ln m_j.
Since for every k∈I_j we have m_j≤k, ∑_j L_j ln m_j ≤ ∑_{k=k0}^{2k0−1} ln k ≤ k0·ln(2k0), giving a contribution ≤ ln(2k0). The endpoint term is bounded by Lemma 1 up to a constant factor, and the L_j=1 part contributes ≤ C_fix·(ln(2k0)+O(1)). Absorb all absolute factors into K′,K″. ∎

Corollary 15 (Global constant under a single long plateau; explicit constants).
Suppose there exists a plateau I=[m,m+L) contained in [k0,2k0) with L ≥ α m for some α∈(0,1] and covering a γ-fraction of the window (i.e., L/k0 ≥ γ). Then
  E_{k,++}[ ALG_k / OPT_k ]
  ≤ γ · [ 2·C_bi · ( a + 1 + ln(2/α) ) + (C_fix · (\ln m + O(1)))/L ]
    + (1−γ) · [ K′ + K″ · ln(2k0) ],
where a:=2+1/(2e) and K′,K″ are the constants from Corollary 14. In particular, as L→∞ the term (C_fix·(\ln m))/L=o(1), so the γ-contribution is O_α(1).
Proof.
Average Corollary 5 over I (weight L/k0=γ) and use Corollary 14 on the complement fraction 1−γ. ∎

Corollary 16 (Global ≥1% probability under a single long plateau; explicit constants).
Under the assumptions of Corollary 15, let η=0.98, c=2, and define T:=max{2,⌈η(L−1)⌉+1} and
  B_I := 2·C_bi · ( 1 + m / ( e (T−1) ) ).
Then, for k uniform on [k0,2k0) and ++ randomness,
  P[ ALG_k / OPT_k ≤ c · B_I ] ≥ γ · (1−η−O(1/L)) · (1 − 1/c) = 0.01·γ − O(γ/L).
If moreover L ≥ α m, then B_I ≤ 2·C_bi · (1 + 1/(α e η)) + O(1/L). For C_bi=5 and α=1, this yields the explicit threshold c·B_I ≤ 20 · (1 + 1/(0.98 e)) ≈ 27.7 on at least 1%·γ − O(γ/L) of the joint (k,++) mass.
Proof.
Apply Corollary 9 on I (giving a per-k bound with probability ≥(1−η−O(1/L)) and then Markov with c=2, hence factor (1−1/2)), and multiply by the mixing weight L/k0=γ. The long-plateau simplification of B_I follows from m/(T−1) ≤ 1/(α η) + O(1/L). ∎
Corollary 17 (Balanced split τ=⌊L/2⌋: an O(m/L) tail with explicit constants).
Let I=[m,m+L) be a factor-2 plateau with L≥3. Set τ:=⌊L/2⌋. Then, for k uniform on I and over the k-means++ randomness,
  E_{k∈I} E_++[ ALG_k / OPT_k ]
  ≤ 2·C_bi · [ (2/3)·( a + 1 + ln( 4m/(L−1) ) ) + 1/2 + (m/(e(L−1)))·( ln 3 + 1 ) ]
    + (C_fix · (ln m + O(1)))/L,
where a:=2 + 1/(2e).
Proof.
Apply Theorem 3 with τ=⌊L/2⌋ and use the uniform bounds for L≥3: τ/(L−1)≤2/3; (L−1−τ)/(L−1)≤1/2; ln(2m/τ)≤ln(4m/(L−1)); and H_{L−2}−H_{τ−1}≤ln 3 + 1 (verified directly for L=3,4 and by monotonicity of H_n−H_m thereafter). Substitute these into Theorem 3. The endpoint term is unchanged. ∎

Corollary 18 (Multi-plateau ≥1% probability guarantee by mixing).
Let {I_j=[m_j,m_j+L_j)} be disjoint factor-2 plateaus with L_j≥α m_j for all j in some index set J, and let their total coverage be γ:= (1/k0)∑_{j∈J} L_j. Fix η∈(0,1) and c≥1. For each j set T_j:=max{2, ⌈η(L_j−1)⌉+1} and B_j:=2·C_bi·(1 + m_j/(e(T_j−1))). Then, for k uniform on [k0,2k0) and over the ++ randomness,
  P\big[ ALG_k/OPT_k ≤ c · B_j for the unique j with k∈I_j \big]
  ≥ γ · (1−η) · (1 − 1/c) − O( γ / L_{\min} ),
where L_{\min}:=\min_{j∈J} L_j. In particular, with η=0.98 and c=2 this gives
  P[ ALG_k/OPT_k ≤ 2·B_j for the unique j with k∈I_j ] ≥ 0.01·γ − O(γ/L_{\min}).
Moreover, if L_j≥α m_j for all j, then B_j ≤ 2·C_bi·(1 + 1/(α e η)) + O(1/L_j), so a uniform threshold B_*=2·C_bi·(1 + 1/(α e η)) + o(1) can be used.
Proof.
Apply Corollary 9 on each plateau I_j with the same (η,c), yielding for k uniform on I_j the bound E_++[ALG_k/OPT_k]≤B_j on a (1−η−O(1/L_j)) fraction of k. For any such k, Markov gives P_{++}[ALG_k/OPT_k ≤ c·B_j]≥1−1/c. Weighting by L_j/k0 and summing over j∈J produces the stated lower bound. The O(·) term is ∑_j (L_j/k0)·O(1/L_j)=O(|J|/k0)=O(γ/L_{\min}). The long-plateau simplification of B_j follows from m_j/(T_j−1)≤1/(α η)+O(1/L_j). ∎

Lemma 2 (Discrete smoothness implies long plateaus).
Let (v_k)_{k≥1} be a nonincreasing positive sequence. Fix integers m≥1 and L≥2. Suppose that for all k∈{m+1,…,m+L−1} we have
  v_{k−1} ≤ (1+δ)·v_k
for some δ∈(0,1). Then
  v_{m+L−1} ≥ v_m / (1+δ)^{L−1}.
Consequently, if a factor-2 drop occurs within [m,m+L) (i.e., v_{m+L−1} ≤ v_m/2), then necessarily
  L ≥ 1 + ⌈ ln 2 / ln(1+δ) ⌉ ≥ 1 + ⌈ (\ln 2)/δ ⌉.
Proof.
Iterating the inequality gives v_{m+t} ≥ v_m/(1+δ)^t for 0≤t≤L−1. If v_{m+L−1}≤v_m/2, then (1+δ)^{L−1}≥2, which is equivalent to L≥1+⌈ln 2/ln(1+δ)⌉. Since ln(1+δ)≤δ, the second inequality follows. ∎

Corollary 19 (1/m-type discrete smoothness on [m,2m) forces long plateaus).
Let (v_k) be nonincreasing and assume that for some absolute A>0 and all k∈{m+1,…,2m−1},
  v_{k−1} ≤ (1 + A/m)·v_k.
Then any factor-2 drop within [m,2m) requires
  L ≥ 1 + ⌈ (\ln 2)·m / A ⌉.
In particular: if A≥\ln 2 then L ≥ ((\ln 2)/A)·m up to +1 rounding; if A<\ln 2, then a factor-2 drop cannot occur within the window and the entire [m,2m) is one plateau (L=m).
Proof.
Apply Lemma 2 with δ=A/m and note that the window has at most m steps, so if 1+⌈(\ln 2)·m/A⌉>m then the drop cannot happen within [m,2m). ∎

Corollary 20 (Quantified O(log log k0) mixture under polylog-long plateaus).
Fix σ≥1 and let S be the set of plateaus I_j=[m_j,m_j+L_j) with L_j ≥ m_j/(\ln k0)^σ. Suppose these plateaus cover a γ-fraction of [k0,2k0), i.e., ∑_{j∈S} L_j ≥ γ k0. Then, for k uniform on [k0,2k0) and ++ randomness,
  E_{k,++}[ ALG_k / OPT_k ]
  ≤ γ · [ 2·C_bi · ( a + 1 + ln 2 + σ·\ln\ln k0 ) ]
    + (1−γ) · [ K′ + K″·\ln(2k0) ]
    + o(1),
where a:=2+1/(2e) and K′,K″ are the constants from Corollary 14. The o(1) term can be taken as O( (\ln(2k0)) (\ln k0)^σ / k0 ).
Proof.
On each I_j∈S, Corollary 5 with α=1/(\ln k0)^σ yields E_{k∈I_j,++}[ALG/OPT] ≤ 2·C_bi·(a+1+\ln(2/α)) + (C_fix·\ln m_j)/L_j. Since \ln(2/α)=\ln 2 + σ·\ln\ln k0, the first term equals the bracket. Weighting by L_j/k0 and summing over j∈S gives γ times the bracket plus (C_fix/k0)∑_{j∈S}\ln m_j. Now |S|≤(∑_{j∈S} L_j)/\min_{j∈S} L_j ≤ γ k0 / (k0/(\ln k0)^σ) = γ (\ln k0)^σ, so (C_fix/k0)∑_{j∈S}\ln m_j ≤ C_fix·|S|·\ln(2k0)/k0 = O( (\ln(2k0))(\ln k0)^σ / k0 ) = o(1). The complement fraction 1−γ is bounded by Corollary 14. ∎



Current round tag: Round 0007 — 2025-08-27T20:10:45.484948Z
Return ONLY valid JSON with a single field:
{ "progress_md": "<your progress notes for this round>" }

Read output.md. If you spot gaps, errors, or missing justifications in output.md, point them out clearly inside progress_md.
