Smoothed k-means++

$k$-means++ is a classic $k$-means algorithm. See [1] for the paper that came up with that algorithm. It is also important to understand their analysis of it. They prove that the algorithm is $O(\log k)$-approximate (in expectation). They also show $\Omega(\log k)$ lower bound. 

I want you to explore whether this guarantee can be improved in the *smoothed* setting. Here is the setup. We are given a dataset $X$ (points in Euclidean space or in general metric space, you choose what suits you) of size $n$. We are given a positive integer $k_0$. Then, the following randomized experiment happens:
1) We sample a uniformly random $k$ from the interval $[k_0, 2k_0)$. 
2) We run $k$-means++ algorithm for that $k$ on $X$. 
3) We compute the ratio $\alpha = ALG(k) / OPT(k)$ where $ALG(k)$ is the $k$-means cost of the solution returned by $k$-means++ and $OPT(k) \le ALG(k)$ is the optimum cost. 

The question is, what is the expected value of $\alpha$, where the expectation is over both step (1) and (2)? We know that $\alpha = O(\log k)$, but I feel that it could be much better, as a function of $k$. 

The task is to get the best possible lower / upper bound on $\alpha$. I am not interested in constant factors, I want to know the answer as a function of $\alpha$. 



